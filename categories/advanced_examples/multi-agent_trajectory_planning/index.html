<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multi-Agent Trajectory Planning ¬∑ RxInfer.jl Examples</title><meta name="title" content="Multi-Agent Trajectory Planning ¬∑ RxInfer.jl Examples"/><meta property="og:title" content="Multi-Agent Trajectory Planning ¬∑ RxInfer.jl Examples"/><meta property="twitter:title" content="Multi-Agent Trajectory Planning ¬∑ RxInfer.jl Examples"/><meta name="description" content="A repository of examples and tutorials for RxInfer.jl, a Julia package for reactive message passing inference in probabilistic models."/><meta property="og:description" content="A repository of examples and tutorials for RxInfer.jl, a Julia package for reactive message passing inference in probabilistic models."/><meta property="twitter:description" content="A repository of examples and tutorials for RxInfer.jl, a Julia package for reactive message passing inference in probabilistic models."/><meta property="og:url" content="https://examples.rxinfer.ml/categories/advanced_examples/multi-agent_trajectory_planning/"/><meta property="twitter:url" content="https://examples.rxinfer.ml/categories/advanced_examples/multi-agent_trajectory_planning/"/><link rel="canonical" href="https://examples.rxinfer.ml/categories/advanced_examples/multi-agent_trajectory_planning/"/><script async src="https://www.googletagmanager.com/gtag/js?id=G-GMFX620VEP"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-GMFX620VEP', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/theme.css" rel="stylesheet" type="text/css"/><link href="../../../assets/header.css" rel="stylesheet" type="text/css"/><script src="../../../assets/header.js"></script><script src="../../../assets/chat.js"></script><link href="../../../assets/favicon.ico" rel="icon" type="image/x-icon"/>
    <meta property="og:title" content="Multi-agent Trajectory Planning - RxInfer Examples">
    <meta name="description" content="This example shows how to plan multi-agents' trajectories while avoiding obstacles and collisions between agents.
">
    <meta property="og:description" content="This example shows how to plan multi-agents' trajectories while avoiding obstacles and collisions between agents.
">
    <meta name="keywords" content="rxinfer, julia, bayesian inference, examples, probabilistic programming, message passing, probabilistic numerics, variational inference, belief propagation, advanced examples">
    <link rel="sitemap" type="application/xml" title="Sitemap" href="https://examples.rxinfer.ml/sitemap.xml">
    </head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.svg" alt="RxInfer.jl Examples logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.svg" alt="RxInfer.jl Examples logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">RxInfer.jl Examples</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><a class="tocitem" href="../../../how_to_contribute/">How to contribute</a></li><li><a class="tocitem" href="../../../autogenerated/list_of_examples/">List of Examples</a></li><li><span class="tocitem">Basic Examples</span><ul><li><a class="tocitem" href="../../basic_examples/bayesian_binomial_regression/">Bayesian Binomial Regression</a></li><li><a class="tocitem" href="../../basic_examples/bayesian_linear_regression/">Bayesian Linear Regression</a></li><li><a class="tocitem" href="../../basic_examples/bayesian_multinomial_regression/">Bayesian Multinomial Regression</a></li><li><a class="tocitem" href="../../basic_examples/coin_toss_model/">Coin Toss Model</a></li><li><a class="tocitem" href="../../basic_examples/feature_functions_in_bayesian_regression/">Feature Functions In Bayesian Regression</a></li><li><a class="tocitem" href="../../basic_examples/hidden_markov_model/">Hidden Markov Model</a></li><li><a class="tocitem" href="../../basic_examples/kalman_filtering_and_smoothing/">Kalman Filtering And Smoothing</a></li><li><a class="tocitem" href="../../basic_examples/pomdp_control/">Pomdp Control</a></li><li><a class="tocitem" href="../../basic_examples/predicting_bike_rental_demand/">Predicting Bike Rental Demand</a></li></ul></li><li><span class="tocitem">Advanced Examples</span><ul><li><a class="tocitem" href="../active_inference_mountain_car/">Active Inference Mountain Car</a></li><li><a class="tocitem" href="../advanced_tutorial/">Advanced Tutorial</a></li><li><a class="tocitem" href="../assessing_people_skills/">Assessing People Skills</a></li><li><a class="tocitem" href="../chance_constraints/">Chance Constraints</a></li><li><a class="tocitem" href="../conjugate-computational_variational_message_passing/">Conjugate-Computational Variational Message Passing</a></li><li><a class="tocitem" href="../drone_dynamics/">Drone Dynamics</a></li><li><a class="tocitem" href="../global_parameter_optimisation/">Global Parameter Optimisation</a></li><li><a class="tocitem" href="../gp_regression_by_ssm/">Gp Regression By Ssm</a></li><li><a class="tocitem" href="../infinite_data_stream/">Infinite Data Stream</a></li><li class="is-active"><a class="tocitem" href>Multi-Agent Trajectory Planning</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Environment-setup"><span>Environment setup</span></a></li><li><a class="tocitem" href="#Door-environment"><span>Door environment</span></a></li><li class="toplevel"><a class="tocitem" href="#Wall-environment"><span>Wall environment</span></a></li><li class="toplevel"><a class="tocitem" href="#Combined-environment"><span>Combined environment</span></a></li><li class="toplevel"><a class="tocitem" href="#Agent-state"><span>Agent state</span></a></li><li><a class="tocitem" href="#Next-Steps"><span>Next Steps</span></a></li><li><a class="tocitem" href="#Model-specification"><span>Model specification</span></a></li><li><a class="tocitem" href="#Constraint-specification"><span>Constraint specification</span></a></li><li><a class="tocitem" href="#Experiments-and-visualizations"><span>Experiments and visualizations</span></a></li><li><a class="tocitem" href="#Practical-Applications-and-Potential-Improvements"><span>Practical Applications and Potential Improvements</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li><li><a class="tocitem" href="../nonlinear_sensor_fusion/">Nonlinear Sensor Fusion</a></li><li><a class="tocitem" href="../robotic_arm/">Robotic Arm</a></li></ul></li><li><span class="tocitem">Problem Specific</span><ul><li><a class="tocitem" href="../../problem_specific/autoregressive_models/">Autoregressive Models</a></li><li><a class="tocitem" href="../../problem_specific/gamma_mixture/">Gamma Mixture</a></li><li><a class="tocitem" href="../../problem_specific/gaussian_mixture/">Gaussian Mixture</a></li><li><a class="tocitem" href="../../problem_specific/hierarchical_gaussian_filter/">Hierarchical Gaussian Filter</a></li><li><a class="tocitem" href="../../problem_specific/invertible_neural_network_tutorial/">Invertible Neural Network Tutorial</a></li><li><a class="tocitem" href="../../problem_specific/litter_model/">Litter Model</a></li><li><a class="tocitem" href="../../problem_specific/ode_parameter_estimation/">Ode Parameter Estimation</a></li><li><a class="tocitem" href="../../problem_specific/probit_model/">Probit Model</a></li><li><a class="tocitem" href="../../problem_specific/rts_vs_bifm_smoothing/">Rts Vs Bifm Smoothing</a></li><li><a class="tocitem" href="../../problem_specific/simple_nonlinear_node/">Simple Nonlinear Node</a></li><li><a class="tocitem" href="../../problem_specific/structural_dynamics_with_augmented_kalman_filter/">Structural Dynamics With Augmented Kalman Filter</a></li><li><a class="tocitem" href="../../problem_specific/universal_mixtures/">Universal Mixtures</a></li></ul></li><li><a class="tocitem" href="../../../how_build_works/">How we build the examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Advanced Examples</a></li><li class="is-active"><a href>Multi-Agent Trajectory Planning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multi-Agent Trajectory Planning</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInferExamples.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInferExamples.jl/blob/main/docs/src/categories/advanced_examples/multi-agent_trajectory_planning/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><div class="admonition is-info"><header class="admonition-header">Contributing</header><div class="admonition-body"><p>This example was automatically generated from a Jupyter notebook in the <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">RxInferExamples.jl</a> repository.</p><p>We welcome and encourage contributions! You can help by:</p><ul><li>Improving this example</li><li>Creating new examples </li><li>Reporting issues or bugs</li><li>Suggesting enhancements</li></ul><p>Visit our <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">GitHub repository</a> to get started. Together we can make <a href="https://github.com/ReactiveBayes/RxInfer.jl">RxInfer.jl</a> even better! üí™</p></div></div><hr/><h1 id="Multi-agent-Trajectory-Planning"><a class="docs-heading-anchor" href="#Multi-agent-Trajectory-Planning">Multi-agent Trajectory Planning</a><a id="Multi-agent-Trajectory-Planning-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-agent-Trajectory-Planning" title="Permalink"></a></h1><p>These examples demonstrate the use of RxInfer for trajectory planning in multi-agent situations. The animations show the inferred trajectories from probabilistic inference. The examples shown in this notebook are based on https://github.com/biaslab/MultiAgentTrajectoryPlanning/blob/main/door.jl, prepared by <a href="https://github.com/Michi-Tsubaki">Michi-Tsubaki</a>, extended by <a href="https://github.com/bvdmitri">bvdmitri</a>. The original code is a part of the paper <a href="https://ieeexplore.ieee.org/document/10645034">Multi-Agent Trajectory Planning with NUV Priors</a> by Bart van Erp.</p><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>This notebook demonstrates multi-agent trajectory planning using probabilistic inference with RxInfer.jl. In this example, we model multiple agents navigating through an environment with obstacles while trying to reach their respective goals. The planning problem is formulated as Bayesian inference, where:</p><ul><li>Agent states evolve according to linear dynamics</li><li>Collision avoidance between agents and obstacles is encoded as probabilistic constraints</li><li>Goal-seeking behavior is represented as prior distributions</li></ul><p>By performing inference on this probabilistic model, we can compute optimal trajectories that balance goal-reaching with collision avoidance. The visualization shows how agents coordinate their movements to navigate efficiently through the environment.</p><pre><code class="language-julia hljs">using LinearAlgebra, RxInfer, Plots, LogExpFunctions, StableRNGs</code></pre><h2 id="Environment-setup"><a class="docs-heading-anchor" href="#Environment-setup">Environment setup</a><a id="Environment-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Environment-setup" title="Permalink"></a></h2><p>To test our ideas, we need an environment to work with. We are going to create a simple environment consisting of a plane with boxes as obstacles. These boxes can be placed anywhere we want on the plane, allowing us to experiment with different configurations and scenarios. This flexible setup will help us evaluate how our multi-agent trajectory planning algorithms perform under various conditions and obstacle arrangements.</p><pre><code class="language-julia hljs"># A simple struct to represent a rectangle, which is defined by its center (x, y) and size (width, height)
Base.@kwdef struct Rectangle
    center::Tuple{Float64, Float64}
    size::Tuple{Float64, Float64}
end

function plot_rectangle!(p, rect::Rectangle)
    # Calculate the x-coordinates of the four corners
    x_coords = rect.center[1] .+ rect.size[1]/2 * [-1, 1, 1, -1, -1]
    # Calculate the y-coordinates of the four corners
    y_coords = rect.center[2] .+ rect.size[2]/2 * [-1, -1, 1, 1, -1]
    
    # Plot the rectangle with a black fill
    plot!(p, Shape(x_coords, y_coords), 
          label = &quot;&quot;, 
          color = :black, 
          alpha = 0.5,
          linewidth = 1.5,
          fillalpha = 0.3)
end

# A simple struct to represent an environment, which is defined by a list of obctales,
# and in this demo the obstacles are just rectangles
Base.@kwdef struct Environment
    obstacles::Vector{Rectangle}
end

function plot_environment!(p, env::Environment)
    for obstacle in env.obstacles
        plot_rectangle!(p, obstacle)
    end
    return p
end

function plot_environment(env::Environment)
    p = plot(size = (800, 400), xlims = (-20, 20), ylims = (-20, 20), aspect_ratio = :equal)
    plot_environment!(p, env)
    return p
end</code></pre><pre><code class="nohighlight hljs">plot_environment (generic function with 1 method)</code></pre><p>In the code above, we&#39;ve defined two key structures for our environment:</p><ol><li><code>Rectangle</code>: A simple structure representing rectangular obstacles, defined by:<ul><li><code>center</code>: The (x,y) coordinates of the rectangle&#39;s center</li><li><code>size</code>: The (width, height) of the rectangle</li></ul></li><li><code>Environment</code>: A structure that contains a collection of obstacles (rectangles)</li></ol><p>We&#39;ve also defined several plotting functions:</p><ul><li><code>plot_rectangle!</code>: Adds a rectangle to an existing plot</li><li><code>plot_environment!</code>: Adds all obstacles in an environment to an existing plot</li><li><code>plot_environment</code>: Creates a new plot and displays the environment</li></ul><p>These structures and functions provide the foundation for visualizing our 2D environment where multi-agent trajectory planning will take place.</p><p>Let&#39;s create a couple of different environments to demonstrate multi-agent trajectory planning. You can experiment with different obstacle configurations by modifying the rectangle positions, sizes, and quantities. This will allow you to test how the agents navigate around various obstacle arrangements and interact with each other in different scenarios.</p><h2 id="Door-environment"><a class="docs-heading-anchor" href="#Door-environment">Door environment</a><a id="Door-environment-1"></a><a class="docs-heading-anchor-permalink" href="#Door-environment" title="Permalink"></a></h2><p>In this environment, we&#39;ll create a scenario resembling a doorway that agents must navigate through. The environment will consist of two wall-like obstacles with a narrow passage between them, simulating a door or gateway. This setup will test the agents&#39; ability to coordinate when passing through a constrained space, which is a common challenge in multi-agent path planning. The narrow passage will force agents to negotiate the right-of-way and potentially wait for each other to pass through, demonstrating emergent cooperative behaviors.</p><pre><code class="language-julia hljs">door_environment = Environment(obstacles = [
    Rectangle(center = (-40, 0), size = (70, 5)),
    Rectangle(center = (40, 0), size = (70, 5))
])

plot_environment(door_environment)</code></pre><p><img src="Multi-agent Trajectory Planning_3_1.png" alt/></p><h1 id="Wall-environment"><a class="docs-heading-anchor" href="#Wall-environment">Wall environment</a><a id="Wall-environment-1"></a><a class="docs-heading-anchor-permalink" href="#Wall-environment" title="Permalink"></a></h1><p>In this environment, we&#39;ll create a scenario with a wall in the center that agents must navigate around. The environment will consist of a single elongated obstacle positioned in the middle of the space, forcing agents to choose whether to go above or below the wall. This setup will test the agents&#39; ability to find efficient paths around obstacles and coordinate with each other to avoid congestion on either side of the wall. It represents a common scenario in multi-agent navigation where agents must make decisions about which route to take when faced with a barrier.</p><pre><code class="language-julia hljs">wall_environment = Environment(obstacles = [
    Rectangle(center = (0, 0), size = (10, 5))
])

plot_environment(wall_environment)</code></pre><p><img src="Multi-agent Trajectory Planning_4_1.png" alt/></p><h1 id="Combined-environment"><a class="docs-heading-anchor" href="#Combined-environment">Combined environment</a><a id="Combined-environment-1"></a><a class="docs-heading-anchor-permalink" href="#Combined-environment" title="Permalink"></a></h1><p>In this environment, we&#39;ll combine the door and wall scenarios to create a more complex navigation challenge. This environment will feature both a narrow doorway that agents must pass through and a wall obstacle they need to navigate around. This combined setup will test the agents&#39; ability to handle multiple types of obstacles in sequence, requiring more sophisticated path planning and coordination. Agents will need to negotiate the doorway and then decide which path to take around the wall, or vice versa depending on their starting and goal positions. This represents a more realistic scenario where environments often contain various types of obstacles that require different navigation strategies.</p><pre><code class="language-julia hljs">combined_environment = Environment(obstacles = [
    Rectangle(center = (-50, 0), size = (70, 2)),
    Rectangle(center = (50, -0), size = (70, 2)),
    Rectangle(center = (5, -1), size = (3, 10))
])

plot_environment(combined_environment)</code></pre><p><img src="Multi-agent Trajectory Planning_5_1.png" alt/></p><h1 id="Agent-state"><a class="docs-heading-anchor" href="#Agent-state">Agent state</a><a id="Agent-state-1"></a><a class="docs-heading-anchor-permalink" href="#Agent-state" title="Permalink"></a></h1><p>In this section, we define states and goals for our agents. Each agent has a initial position and target end position. These states will be used to drive agent movement through the environment. The trajectory planning algorithm will use this information to generate paths from start to destination while avoiding obstacles. We start by first defining the necessary structures and functions for the goals.</p><pre><code class="language-julia hljs">
# Agent plan, encodes start and goal states
Base.@kwdef struct Agent
    radius::Float64
    initial_position::Tuple{Float64, Float64}
    target_position::Tuple{Float64, Float64}
end

function plot_marker_at_position!(p, radius, position; color=&quot;red&quot;, markersize=10.0, alpha=1.0, label=&quot;&quot;)
    # Draw the agent as a circle with the given radius
    Œ∏ = range(0, 2œÄ, 100)
    
    x_coords = position[1] .+ radius .* cos.(Œ∏)
    y_coords = position[2] .+ radius .* sin.(Œ∏)
    
    plot!(p, Shape(x_coords, y_coords); color=color, label=label, alpha=alpha)
    return p
end</code></pre><pre><code class="nohighlight hljs">plot_marker_at_position! (generic function with 1 method)</code></pre><p>Let see how does one of configurations for a single agent might look like in the first door environment. For this we will use two agents with different radius, as well as different initial and taget positions.</p><pre><code class="language-julia hljs">function plot_agent_naive_plan!(p, agent; color = &quot;blue&quot;)
    plot_marker_at_position!(p, agent.radius, agent.initial_position, color = color)
    plot_marker_at_position!(p, agent.radius, agent.target_position, color = color, alpha = 0.1)
    quiver!(p, [ agent.initial_position[1] ], [ agent.initial_position[2] ], quiver = ([ agent.target_position[1] - agent.initial_position[1] ], [ agent.target_position[2] -  agent.initial_position[2] ]))
end

let pe = plot_environment(door_environment)
    agents = [ 
        Agent(radius = 2.5, initial_position = (-4, 10), target_position = (-10, -10)),
        Agent(radius = 1.5, initial_position = (-10, 5), target_position = (10, -15)),
        Agent(radius = 1.0, initial_position = (-15, -10), target_position = (10, 10)),
        Agent(radius = 2.5, initial_position = (0, -10), target_position = (-10, 15))
    ]
    
    colors = Plots.palette(:tab10)
    
    for (k, agent) in enumerate(agents)
        plot_agent_naive_plan!(pe, agent, color = colors[k])
    end
    
    pe
end</code></pre><p><img src="Multi-agent Trajectory Planning_7_1.png" alt/></p><p>The plot above illustrates that naive trajectory from initial to target position will obviously not work and the agents will hit either the wall or each other while trying to execute their plan. Thus we need to come up with a better plan and simultaneously take into account multiple agents in the same environment.</p><h2 id="Next-Steps"><a class="docs-heading-anchor" href="#Next-Steps">Next Steps</a><a id="Next-Steps-1"></a><a class="docs-heading-anchor-permalink" href="#Next-Steps" title="Permalink"></a></h2><p>Now that we have set up our environment, defined our agents, and created utility functions, we are ready to build an RxInfer model to solve this multi-agent trajectory planning problem. In the following sections, we will:</p><ol><li>Define a probabilistic model that captures the dynamics of our agents</li><li>Incorporate collision avoidance constraints between agents and obstacles using NUV priors</li><li>Use message passing to infer optimal trajectories</li><li>Visualize the resulting paths</li></ol><p>This will demonstrate how probabilistic programming with RxInfer can elegantly solve complex planning problems while handling uncertainty and constraints in a principled way.</p><h3 id="Half-space-prior-implementation"><a class="docs-heading-anchor" href="#Half-space-prior-implementation">Half space prior implementation</a><a id="Half-space-prior-implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Half-space-prior-implementation" title="Permalink"></a></h3><p>For our multi-agent trajectory planning model, we need to implement half-space priors to handle collision avoidance constraints. These priors allow us to model the requirement that agents must stay outside of obstacles and maintain safe distances from each other. The mathematical details and theoretical foundation of these half-space priors can be found in the paper referenced at the beginning of this notebook. The implementation below defines the necessary node and message-passing rules for incorporating these constraints into our probabilistic model.</p><pre><code class="language-julia hljs"># Define the probabilistic model for obstacles using halfspace constraints
struct Halfspace end

@node Halfspace Stochastic [out, a, œÉ2, Œ≥]

# rule specification
@rule Halfspace(:out, Marginalisation) (q_a::Any, q_œÉ2::Any, q_Œ≥::Any) = begin
    return NormalMeanVariance(mean(q_a) + mean(q_Œ≥) * mean(q_œÉ2), mean(q_œÉ2))
end

@rule Halfspace(:œÉ2, Marginalisation) (q_out::Any, q_a::Any, q_Œ≥::Any, ) = begin
    # `BayesBase.TerminalProdArgument` is used to ensure that the result of the posterior computation is equal to this value
    return BayesBase.TerminalProdArgument(PointMass( 1 / mean(q_Œ≥) * sqrt(abs2(mean(q_out) - mean(q_a)) + var(q_out))))
end</code></pre><h3 id="Distance-functions-for-collision-avoidance"><a class="docs-heading-anchor" href="#Distance-functions-for-collision-avoidance">Distance functions for collision avoidance</a><a id="Distance-functions-for-collision-avoidance-1"></a><a class="docs-heading-anchor-permalink" href="#Distance-functions-for-collision-avoidance" title="Permalink"></a></h3><p>In addition to the halfspace priors, we need to implement distance functions to properly handle collision avoidance between agents and obstacles. These functions will calculate the distance between agents and obstacles, which is essential for determining when collision avoidance constraints should be activated. The distance functions will be used to ensure that agents maintain safe distances from each other and from obstacles in the environment. In the next section, we&#39;ll define utility functions that include these distance calculations for different geometric shapes like rectangles and circles.</p><pre><code class="language-julia hljs">softmin(x; l=10) = -logsumexp(-l .* x) / l

# state here is a 4-dimensional vector [x, y, vx, vy]
function distance(r::Rectangle, state)
    if abs(state[1] - r.center[1]) &gt; r.size[1] / 2 || abs(state[2] - r.center[2]) &gt; r.size[2] / 2
        # outside of rectangle
        dx = max(abs(state[1] - r.center[1]) - r.size[1] / 2, 0)
        dy = max(abs(state[2] - r.center[2]) - r.size[2] / 2, 0)
        return sqrt(dx^2 + dy^2)
    else
        # inside rectangle
        return max(abs(state[1] - r.center[1]) - r.size[1] / 2, abs(state[2] - r.center[2]) - r.size[2] / 2)
    end
end

function distance(env::Environment, state)
    return softmin([distance(obstacle, state) for obstacle in env.obstacles])
end</code></pre><pre><code class="nohighlight hljs">distance (generic function with 2 methods)</code></pre><p>We use the <code>softmin</code> function to create a smooth approximation of the minimum distance between an agent and multiple obstacles. Unlike the regular <code>min</code> function which returns the exact minimum value, <code>softmin</code> produces a differentiable approximation that considers all distances with a weighted average, heavily biased toward the smallest values.</p><p>The parameter <code>l</code> controls the &quot;sharpness&quot; of the approximation - with larger values making the function behave more like the true minimum. This smoothness is particularly valuable in optimization contexts as it:</p><ol><li>Avoids discontinuities that could cause numerical issues during inference</li><li>Provides gradient information from all obstacles, not just the closest one</li><li>Creates a more stable optimization landscape for trajectory planning</li></ol><p>When calculating the distance between an agent and the environment, <code>softmin</code> helps create a continuous repulsive field around all obstacles, allowing for more natural avoidance behaviors.</p><h2 id="Model-specification"><a class="docs-heading-anchor" href="#Model-specification">Model specification</a><a id="Model-specification-1"></a><a class="docs-heading-anchor-permalink" href="#Model-specification" title="Permalink"></a></h2><p>We use RxInfer&#39;s <code>@model</code> macro to specify the model. In the current example we fix our model to exactly 4 agents to simplify the model creation and construction. We also define auxiliary functions <code>g</code> and <code>h</code> which computes distance with agent&#39;s radius offset and minimum distance between all agents pairwise.</p><pre><code class="language-julia hljs"># Helper function, distance with radius offset
function g(environment, radius, state)
    return distance(environment, state) - radius
end

# Helper function, finds minimum distances between agents pairwise
function h(environment, radiuses, states...)
    # Calculate pairwise distances between all agents
    distances = Real[]
    n = length(states)

    for i in 1:n
        for j in (i+1):n
            push!(distances, norm(states[i] - states[j]) - radiuses[i] - radiuses[j])
        end
    end

    return softmin(distances)
end</code></pre><pre><code class="nohighlight hljs">h (generic function with 1 method)</code></pre><pre><code class="language-julia hljs"># For more details about the model, please refer to the original paper
@model function path_planning_model(environment, agents, goals, nr_steps)

    # Model&#39;s parameters are fixed, refer to the original 
    # paper&#39;s implementation for more details about these parameters
    local dt = 1
    local A  = [1 dt 0 0; 0 1 0 0; 0 0 1 dt; 0 0 0 1]
    local B  = [0 0; dt 0; 0 0; 0 dt]
    local C  = [1 0 0 0; 0 0 1 0]
    local Œ≥  = 1

    local control
    local state
    local path   
    
    # Extract radiuses of each agent in a separate collection
    local rs = map((a) -&gt; a.radius, agents)

    # Model is fixed for 4 agents
    for k in 1:4

        # Prior on state, the state structure is 4 dimensional, where
        # [ x_position, x_velocity, y_position, y_velocity ]
        state[k, 1] ~ MvNormal(mean = zeros(4), covariance = 1e2I)

        for t in 1:nr_steps

            # Prior on controls
            control[k, t] ~ MvNormal(mean = zeros(2), covariance = 1e-1I)

            # State transition
            state[k, t+1] ~ A * state[k, t] + B * control[k, t]

            # Path model, the path structure is 2 dimensional, where 
            # [ x_position, y_position ]
            path[k, t] ~ C * state[k, t+1]

            # Environmental distance
            zœÉ2[k, t] ~ GammaShapeRate(3 / 2, Œ≥^2 / 2)
            z[k, t]   ~ g(environment, rs[k], path[k, t])
            
            # Halfspase priors were defined previousle in this experiment
            z[k, t] ~ Halfspace(0, zœÉ2[k, t], Œ≥)

        end

        # goal priors (indexing reverse due to definition)
        goals[1, k] ~ MvNormal(mean = state[k, 1], covariance = 1e-5I)
        goals[2, k] ~ MvNormal(mean = state[k, nr_steps+1], covariance = 1e-5I)

    end

    for t = 1:nr_steps

        # observation constraint
        dœÉ2[t] ~ GammaShapeRate(3 / 2, Œ≥^2 / 2)
        d[t] ~ h(environment, rs, path[1, t], path[2, t], path[3, t], path[4, t])
        d[t] ~ Halfspace(0, dœÉ2[t], Œ≥)

    end

end

@constraints function path_planning_constraints()
    # Mean-field variational constraints on the parameters
    q(d, dœÉ2) = q(d)q(dœÉ2)
    q(z, zœÉ2) = q(z)q(zœÉ2)
end</code></pre><pre><code class="nohighlight hljs">path_planning_constraints (generic function with 1 method)</code></pre><h2 id="Constraint-specification"><a class="docs-heading-anchor" href="#Constraint-specification">Constraint specification</a><a id="Constraint-specification-1"></a><a class="docs-heading-anchor-permalink" href="#Constraint-specification" title="Permalink"></a></h2><pre><code class="language-julia hljs">function path_planning(; environment, agents, nr_iterations = 350, nr_steps = 40, seed = 42)
    # Fixed number of agents
    nr_agents = 4

    # Form goals compatible with the model
    goals = hcat(
        map(agents) do agent
            return [
                [ agent.initial_position[1], 0, agent.initial_position[2], 0 ],
                [ agent.target_position[1], 0, agent.target_position[2], 0 ]
            ]
        end...
    )
    
    rng = StableRNG(seed)
    
    # Initialize variables, more details about initialization 
    # can be found in the original paper
    init = @initialization begin

        q(dœÉ2) = repeat([PointMass(1)], nr_steps)
        q(zœÉ2) = repeat([PointMass(1)], nr_agents, nr_steps)
        q(control) = repeat([PointMass(0)], nr_steps)

        Œº(state) = MvNormalMeanCovariance(randn(rng, 4), 100I)
        Œº(path) = MvNormalMeanCovariance(randn(rng, 2), 100I)

    end

    # Define approximation methods for the non-linear functions used in the model
    # `Linearization` is a simple and fast approximation method, but it is not
    # the most accurate one. For more details about the approximation methods,
    # please refer to the RxInfer documentation
    door_meta = @meta begin 
        h() -&gt; Linearization()
        g() -&gt; Linearization()
    end

    results = infer(
        model 			= path_planning_model(environment = environment, agents = agents, nr_steps = nr_steps),
        data  			= (goals = goals, ),
        initialization  = init,
        constraints 	= path_planning_constraints(),
        meta 			= door_meta,
        iterations 		= nr_iterations,
        returnvars 		= KeepLast(), 
        options         = (limit_stack_depth = 300, )
    )

    return results
end</code></pre><pre><code class="nohighlight hljs">path_planning (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">function execute_and_save_animation(environment, agents; gifname = &quot;result.gif&quot;, kwargs...)
    result = path_planning(environment = environment, agents = agents; kwargs...)
    paths  = mean.(result.posteriors[:path])
    
    nr_agents, nr_steps = size(paths)
    colors = Plots.palette(:tab10)

    animation = @animate for t in 1:nr_steps
        frame = plot_environment(environment)
    
        for k in 1:nr_agents
            position = paths[k, t]          
            path = paths[k, 1:t]
            
            plot_marker_at_position!(frame, agents[k].radius, position, color = colors[k])
            plot_marker_at_position!(frame, agents[k].radius, agents[k].target_position, color = colors[k], alpha = 0.2)
            plot!(frame, getindex.(path, 1), getindex.(path, 2); linestyle=:dash, label=&quot;&quot;, color=colors[k])
        end

        frame
    end

    # assign the path to save the image
    gif(animation, gifname, fps=15, show_msg = false)
    
    return nothing
end</code></pre><pre><code class="nohighlight hljs">execute_and_save_animation (generic function with 1 method)</code></pre><pre><code class="language-julia hljs"># These are the same agents as in the beginning of the notebook, but copy-pasted here 
# for easier experimentation, closer to the actual experiments
agents = [
    Agent(radius = 2.5, initial_position = (-4, 10), target_position = (-10, -10)),
    Agent(radius = 1.5, initial_position = (-10, 5), target_position = (10, -15)),
    Agent(radius = 1.0, initial_position = (-15, -10), target_position = (10, 10)),
    Agent(radius = 2.5, initial_position = (0, -10), target_position = (-10, 15))
]</code></pre><pre><code class="nohighlight hljs">4-element Vector{Main.anonymous.Agent}:
 Main.anonymous.Agent(2.5, (-4.0, 10.0), (-10.0, -10.0))
 Main.anonymous.Agent(1.5, (-10.0, 5.0), (10.0, -15.0))
 Main.anonymous.Agent(1.0, (-15.0, -10.0), (10.0, 10.0))
 Main.anonymous.Agent(2.5, (0.0, -10.0), (-10.0, 15.0))</code></pre><h2 id="Experiments-and-visualizations"><a class="docs-heading-anchor" href="#Experiments-and-visualizations">Experiments and visualizations</a><a id="Experiments-and-visualizations-1"></a><a class="docs-heading-anchor-permalink" href="#Experiments-and-visualizations" title="Permalink"></a></h2><p>The experiments and animations below demonstrate the power of probabilistic inference for multi-agent trajectory planning in different environments. Let&#39;s analyze what we can observe in each scenario:</p><h3 id="Door-environment-2"><a class="docs-heading-anchor" href="#Door-environment-2">Door environment</a><a class="docs-heading-anchor-permalink" href="#Door-environment-2" title="Permalink"></a></h3><p>In the door environment, we see four agents with different sizes navigating through a narrow passage. The agents demonstrate several interesting behaviors:</p><ul><li>When multiple agents approach the doorway simultaneously, they naturally form a queue, with some agents waiting for others to pass through first</li><li>Agents slow down or speed up based on the presence of other agents near the doorway</li><li>Larger agents (with bigger radius) effectively have precedence in tight spaces, as smaller agents can more easily find alternative paths</li></ul><p>The two different seeds (42 and 123) show how small changes in initialization can lead to different coordination patterns, highlighting the inherent variability in multi-agent systems.</p><pre><code class="language-julia hljs">execute_and_save_animation(door_environment, agents; seed = 42, gifname = &quot;door_42.gif&quot;)</code></pre><p><img src="door_42.gif" alt/></p><pre><code class="language-julia hljs">execute_and_save_animation(door_environment, agents; seed = 123, gifname = &quot;door_123.gif&quot;)</code></pre><p><img src="door_123.gif" alt/></p><h3 id="Wall-environment-2"><a class="docs-heading-anchor" href="#Wall-environment-2">Wall environment</a><a class="docs-heading-anchor-permalink" href="#Wall-environment-2" title="Permalink"></a></h3><p>The wall environment forces agents to choose whether to go above or below the obstacle and distribute themselves between the two possible paths to avoid congestion The choice of path (above or below) appears to be influenced by the agent&#39;s initial position.</p><pre><code class="language-julia hljs">execute_and_save_animation(wall_environment, agents; seed = 42, gifname = &quot;wall_42.gif&quot;)</code></pre><p><img src="wall_42.gif" alt/></p><pre><code class="language-julia hljs">execute_and_save_animation(wall_environment, agents; seed = 123, gifname = &quot;wall_123.gif&quot;)</code></pre><p><img src="wall_123.gif" alt/></p><h3 id="Combined-environment-2"><a class="docs-heading-anchor" href="#Combined-environment-2">Combined environment</a><a class="docs-heading-anchor-permalink" href="#Combined-environment-2" title="Permalink"></a></h3><p>The combined environment presents the most complex challenge, requiring agents to navigate both a doorway and a wall. This environment best showcases the power of the approach, as traditional reactive navigation methods would struggle with the compounding complexity of multiple obstacle types.</p><pre><code class="language-julia hljs">execute_and_save_animation(combined_environment, agents; seed = 42, gifname = &quot;combined_42.gif&quot;)</code></pre><p><img src="combined_42.gif" alt/></p><pre><code class="language-julia hljs">execute_and_save_animation(combined_environment, agents; seed = 123, gifname = &quot;combined_123.gif&quot;)</code></pre><p><img src="combined_123.gif" alt/></p><h2 id="Practical-Applications-and-Potential-Improvements"><a class="docs-heading-anchor" href="#Practical-Applications-and-Potential-Improvements">Practical Applications and Potential Improvements</a><a id="Practical-Applications-and-Potential-Improvements-1"></a><a class="docs-heading-anchor-permalink" href="#Practical-Applications-and-Potential-Improvements" title="Permalink"></a></h2><p>The multi-agent trajectory planning approach demonstrated in this notebook has numerous real-world applications:</p><ul><li>Warehouse robotics: Coordinating multiple robots in fulfillment centers to avoid collisions while efficiently picking and delivering items</li><li>Traffic management: Planning trajectories for autonomous vehicles at intersections or in congested areas</li><li>Crowd simulation: Modeling realistic human movement patterns in architectural design or emergency evacuation planning</li><li>Drone swarms: Coordinating groups of UAVs for tasks like search and rescue, surveillance, or package delivery</li></ul><p>Future extensions to this work could include:</p><ul><li>Handling dynamic obstacles that move or change over time</li><li>Incorporating uncertainty in agent dynamics and sensing</li><li>Scaling to much larger numbers of agents through more efficient inference algorithms</li><li>Adding communication constraints between agents for more realistic modeling</li><li>Incorporating heterogeneous agent types with different dynamics and capabilities</li></ul><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>This notebook has demonstrated how probabilistic inference can be used to solve the complex problem of multi-agent trajectory planning. By formulating the planning problem as Bayesian inference, we&#39;ve shown how agents can coordinate their movements to navigate through various challenging environments while avoiding collisions with obstacles and each other.</p><hr/><div class="admonition is-info"><header class="admonition-header">Contributing</header><div class="admonition-body"><p>This example was automatically generated from a Jupyter notebook in the <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">RxInferExamples.jl</a> repository.</p><p>We welcome and encourage contributions! You can help by:</p><ul><li>Improving this example</li><li>Creating new examples </li><li>Reporting issues or bugs</li><li>Suggesting enhancements</li></ul><p>Visit our <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">GitHub repository</a> to get started. Together we can make <a href="https://github.com/ReactiveBayes/RxInfer.jl">RxInfer.jl</a> even better! üí™</p></div></div><hr/><div class="admonition is-compat"><header class="admonition-header">Environment</header><div class="admonition-body"><p>This example was executed in a clean, isolated environment. Below are the exact package versions used:</p><p>For reproducibility:</p><ul><li>Use the same package versions when running locally</li><li>Report any issues with package compatibility</li></ul></div></div><pre><code class="nohighlight hljs">Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/multi-agent_trajectory_planning/Project.toml`
  [2ab3a3ac] LogExpFunctions v0.3.29
  [91a5bcdd] Plots v1.40.9
  [86711068] RxInfer v4.2.0
  [860ef19b] StableRNGs v1.0.2
  [37e2e46d] LinearAlgebra v1.11.0
</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../infinite_data_stream/">¬´ Infinite Data Stream</a><a class="docs-footer-nextpage" href="../nonlinear_sensor_fusion/">Nonlinear Sensor Fusion ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Created in <a href="https://biaslab.github.io/">BIASlab</a>, maintained by <a href="https://github.com/ReactiveBayes">ReactiveBayes</a>, powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Friday 7 March 2025 13:50">Friday 7 March 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
