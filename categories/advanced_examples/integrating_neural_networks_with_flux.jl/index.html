<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Integrating Neural Networks With Flux.Jl ¬∑ RxInfer.jl Examples</title><meta name="title" content="Integrating Neural Networks With Flux.Jl ¬∑ RxInfer.jl Examples"/><meta property="og:title" content="Integrating Neural Networks With Flux.Jl ¬∑ RxInfer.jl Examples"/><meta property="twitter:title" content="Integrating Neural Networks With Flux.Jl ¬∑ RxInfer.jl Examples"/><meta name="description" content="Integrating Neural Networks with Flux.jl with RxInfer.jl\nThis example shows how to use RxInfer.jl together with Flux.jl to incorporate neural networks into probabilistic models.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="og:description" content="Integrating Neural Networks with Flux.jl with RxInfer.jl\nThis example shows how to use RxInfer.jl together with Flux.jl to incorporate neural networks into probabilistic models.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="twitter:description" content="Integrating Neural Networks with Flux.jl with RxInfer.jl\nThis example shows how to use RxInfer.jl together with Flux.jl to incorporate neural networks into probabilistic models.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="og:url" content="https://examples.rxinfer.com/categories/advanced_examples/integrating_neural_networks_with_flux.jl/"/><meta property="twitter:url" content="https://examples.rxinfer.com/categories/advanced_examples/integrating_neural_networks_with_flux.jl/"/><link rel="canonical" href="https://examples.rxinfer.com/categories/advanced_examples/integrating_neural_networks_with_flux.jl/"/><script async src="https://www.googletagmanager.com/gtag/js?id=G-GMFX620VEP"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-GMFX620VEP', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/theme.css" rel="stylesheet" type="text/css"/><link href="../../../assets/header.css" rel="stylesheet" type="text/css"/><script src="../../../assets/header.js"></script><script src="../../../assets/chat.js"></script><link href="../../../assets/favicon.ico" rel="icon" type="image/x-icon"/>
    <meta property="og:title" content="Integrating Neural Networks with Flux.jl - RxInfer Examples">
    <meta name="description" content="This example shows how to use RxInfer.jl together with Flux.jl to incorporate neural networks into probabilistic models.
">
    <meta property="og:description" content="This example shows how to use RxInfer.jl together with Flux.jl to incorporate neural networks into probabilistic models.
">
    <meta name="keywords" content="rxinfer, julia, bayesian inference, examples, probabilistic programming, message passing, probabilistic numerics, variational inference, belief propagation, advanced examples, neural networks, deep learning, integration, Flux.jl">
    <link rel="sitemap" type="application/xml" title="Sitemap" href="https://examples.rxinfer.com/sitemap.xml">
    </head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.svg" alt="RxInfer.jl Examples logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.svg" alt="RxInfer.jl Examples logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">RxInfer.jl Examples</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><a class="tocitem" href="../../../how_to_contribute/">How to contribute</a></li><li><a class="tocitem" href="../../../autogenerated/list_of_examples/">List of Examples</a></li><li><span class="tocitem">Basic Examples</span><ul><li><a class="tocitem" href="../../basic_examples/bayesian_binomial_regression/">Bayesian Binomial Regression</a></li><li><a class="tocitem" href="../../basic_examples/bayesian_linear_regression/">Bayesian Linear Regression</a></li><li><a class="tocitem" href="../../basic_examples/bayesian_multinomial_regression/">Bayesian Multinomial Regression</a></li><li><a class="tocitem" href="../../basic_examples/bayesian_networks/">Bayesian Networks</a></li><li><a class="tocitem" href="../../basic_examples/coin_toss_model/">Coin Toss Model</a></li><li><a class="tocitem" href="../../basic_examples/contextual_bandits/">Contextual Bandits</a></li><li><a class="tocitem" href="../../basic_examples/feature_functions_in_bayesian_regression/">Feature Functions In Bayesian Regression</a></li><li><a class="tocitem" href="../../basic_examples/forgetting_factors_for_online_inference/">Forgetting Factors For Online Inference</a></li><li><a class="tocitem" href="../../basic_examples/hidden_markov_model/">Hidden Markov Model</a></li><li><a class="tocitem" href="../../basic_examples/incomplete_data/">Incomplete Data</a></li><li><a class="tocitem" href="../../basic_examples/kalman_filtering_and_smoothing/">Kalman Filtering And Smoothing</a></li><li><a class="tocitem" href="../../basic_examples/pomdp_control/">Pomdp Control</a></li><li><a class="tocitem" href="../../basic_examples/predicting_bike_rental_demand/">Predicting Bike Rental Demand</a></li></ul></li><li><span class="tocitem">Advanced Examples</span><ul><li><a class="tocitem" href="../active_inference_mountain_car/">Active Inference Mountain Car</a></li><li><a class="tocitem" href="../advanced_tutorial/">Advanced Tutorial</a></li><li><a class="tocitem" href="../assessing_people_skills/">Assessing People Skills</a></li><li><a class="tocitem" href="../bayesian_structured_time_series/">Bayesian Structured Time Series</a></li><li><a class="tocitem" href="../chance_constraints/">Chance Constraints</a></li><li><a class="tocitem" href="../conjugate-computational_variational_message_passing/">Conjugate-Computational Variational Message Passing</a></li><li><a class="tocitem" href="../drone_dynamics/">Drone Dynamics</a></li><li><a class="tocitem" href="../gp_regression_by_ssm/">Gp Regression By Ssm</a></li><li><a class="tocitem" href="../infinite_data_stream/">Infinite Data Stream</a></li><li class="is-active"><a class="tocitem" href>Integrating Neural Networks With Flux.Jl</a><ul class="internal"><li><a class="tocitem" href="#Bayesian-Inference-meets-Neural-Networks"><span>Bayesian Inference meets Neural Networks</span></a></li><li><a class="tocitem" href="#Define-the-Neural-Network"><span>Define the Neural Network</span></a></li><li><a class="tocitem" href="#Probabilistic-Model-Specification"><span>Probabilistic Model Specification</span></a></li></ul></li><li><a class="tocitem" href="../learning_dynamics_with_vaes/">Learning Dynamics With Vaes</a></li><li><a class="tocitem" href="../multi-agent_trajectory_planning/">Multi-Agent Trajectory Planning</a></li><li><a class="tocitem" href="../nonlinear_sensor_fusion/">Nonlinear Sensor Fusion</a></li><li><a class="tocitem" href="../parameter_optimisation_with_optim.jl/">Parameter Optimisation With Optim.Jl</a></li><li><a class="tocitem" href="../robotic_arm/">Robotic Arm</a></li></ul></li><li><span class="tocitem">Problem Specific</span><ul><li><a class="tocitem" href="../../problem_specific/autoregressive_models/">Autoregressive Models</a></li><li><a class="tocitem" href="../../problem_specific/gamma_mixture/">Gamma Mixture</a></li><li><a class="tocitem" href="../../problem_specific/gaussian_mixture/">Gaussian Mixture</a></li><li><a class="tocitem" href="../../problem_specific/hierarchical_gaussian_filter/">Hierarchical Gaussian Filter</a></li><li><a class="tocitem" href="../../problem_specific/invertible_neural_network_tutorial/">Invertible Neural Network Tutorial</a></li><li><a class="tocitem" href="../../problem_specific/ising_model/">Ising Model</a></li><li><a class="tocitem" href="../../problem_specific/litter_model/">Litter Model</a></li><li><a class="tocitem" href="../../problem_specific/ode_parameter_estimation/">Ode Parameter Estimation</a></li><li><a class="tocitem" href="../../problem_specific/probit_model/">Probit Model</a></li><li><a class="tocitem" href="../../problem_specific/rts_vs_bifm_smoothing/">Rts Vs Bifm Smoothing</a></li><li><a class="tocitem" href="../../problem_specific/simple_nonlinear_node/">Simple Nonlinear Node</a></li><li><a class="tocitem" href="../../problem_specific/structural_dynamics_with_augmented_kalman_filter/">Structural Dynamics With Augmented Kalman Filter</a></li><li><a class="tocitem" href="../../problem_specific/universal_mixtures/">Universal Mixtures</a></li></ul></li><li><span class="tocitem">Experimental Examples</span><ul><li><a class="tocitem" href="../../experimental_examples/bayesian_trust_learning/">Bayesian Trust Learning</a></li><li><a class="tocitem" href="../../experimental_examples/large_language_models/">Large Language Models</a></li><li><a class="tocitem" href="../../experimental_examples/latent_vector_autoregressive_model/">Latent Vector Autoregressive Model</a></li><li><a class="tocitem" href="../../experimental_examples/recurrent_switching_linear_dynamical_system/">Recurrent Switching Linear Dynamical System</a></li></ul></li><li><a class="tocitem" href="../../../how_build_works/">How we build the examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Advanced Examples</a></li><li class="is-active"><a href>Integrating Neural Networks With Flux.Jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Integrating Neural Networks With Flux.Jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInferExamples.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInferExamples.jl" title="View source on GitHub"><span class="docs-icon fa-solid">ÔÖú</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><div class="admonition is-info" id="Contributing-baba9dc142ba7ccb"><header class="admonition-header">Contributing<a class="admonition-anchor" href="#Contributing-baba9dc142ba7ccb" title="Permalink"></a></header><div class="admonition-body"><p>This example was automatically generated from a Jupyter notebook in the <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">RxInferExamples.jl</a> repository.</p><p>We welcome and encourage contributions! You can help by:</p><ul><li>Improving this example</li><li>Creating new examples </li><li>Reporting issues or bugs</li><li>Suggesting enhancements</li></ul><p>Visit our <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">GitHub repository</a> to get started. Together we can make <a href="https://github.com/ReactiveBayes/RxInfer.jl">RxInfer.jl</a> even better! üí™</p></div></div><hr/><h1 id="Integrating-Neural-Networks-with-Flux.jl"><a class="docs-heading-anchor" href="#Integrating-Neural-Networks-with-Flux.jl">Integrating Neural Networks with Flux.jl</a><a id="Integrating-Neural-Networks-with-Flux.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Integrating-Neural-Networks-with-Flux.jl" title="Permalink"></a></h1><p>This advanced tutorial demonstrates the powerful combination of probabilistic programming with deep learning in Julia, specifically showing how to integrate neural networks built with Flux.jl into probabilistic models using RxInfer.jl.</p><pre><code class="language-julia hljs">using RxInfer, Flux, Random, Plots, LinearAlgebra, StableRNGs, ForwardDiff</code></pre><p>In this example, our focus is on Bayesian state estimation in a Nonlinear State-Space Model with unknown dynamics. The main challenge in this scenario is that the dynamics of the system are often unknown or too complex to model analytically. Traditional approaches might struggle with capturing the nonlinear relationships in such systems. Neural networks offer a powerful solution by learning these complex dynamics directly from data, but incorporating them into a Bayesian framework requires careful integration to maintain probabilistic interpretations and uncertainty quantification. This tutorial demonstrates how to overcome these challenges by combining the flexibility of neural networks with the principled uncertainty handling of probabilistic programming. Specifically, we will utilize the time series generated by the <a href="https://en.wikipedia.org/wiki/Lorenz_system">Lorenz system</a> as an example. </p><pre><code class="language-julia hljs"># Lorenz system equations to be used to generate dataset
Base.@kwdef mutable struct Lorenz
    dt::Float64
    œÉ::Float64
    œÅ::Float64
    Œ≤::Float64
    x::Float64
    y::Float64
    z::Float64
end

# Define the Lorenz dynamics
function step!(l::Lorenz)
    dx = l.œÉ * (l.y - l.x);         l.x += l.dt * dx
    dy = l.x * (l.œÅ - l.z) - l.y;   l.y += l.dt * dy
    dz = l.x * l.y - l.Œ≤ * l.z;     l.z += l.dt * dz
end

function create_dataset(rng, œÉ, œÅ, Œ≤_nom; variance = 1f0, n_steps = 100, p_train = 0.8, p_test = 0.2)
    attractor = Lorenz(0.02, œÉ, œÅ, Œ≤_nom/3.0, 1, 1, 1)
    signal       = [Float32[1.0, 1.0, 1.0]]
    noisy_signal = [last(signal) + randn(rng, Float32, 3) * variance]
    for i in 1:(n_steps - 1)
        step!(attractor)
        push!(signal, Float32[attractor.x, attractor.y, attractor.z])
        push!(noisy_signal, last(signal) + randn(rng, Float32, 3) * variance) 
    end

    return (
        parameters = (œÉ, œÅ, Œ≤_nom),
        signal = signal, 
        noisy_signal = noisy_signal
    )
end</code></pre><pre><code class="nohighlight hljs">create_dataset (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">rng      = StableRNG(999) # dummy rng
variance = 2f0
dataset  = create_dataset(rng, 11, 23, 6; variance = variance, n_steps = 200);</code></pre><p>The dataset generated above represents the Lorenz system, a well-known chaotic dynamical system. We&#39;ve created both clean trajectories following the exact Lorenz equations and noisy observations by adding Gaussian noise with variance <code>2.0</code>. The dataset contains <code>200</code> time steps, providing sufficient data to train our neural network model. The parameters used for this Lorenz system are <code>œÉ=11</code>, <code>œÅ=23</code>, and <code>Œ≤=6</code>. This noisy dataset will allow us to test our neural network&#39;s ability to filter out noise and recover the underlying dynamics.</p><pre><code class="language-julia hljs"># Extract first samples from datasets
sample_clean = dataset.signal
sample_noisy = dataset.noisy_signal

# Pre-allocate arrays for better performance
n_points = length(sample_clean)
gx, gy, gz = zeros(n_points), zeros(n_points), zeros(n_points)
rx, ry, rz = zeros(n_points), zeros(n_points), zeros(n_points)

# Extract coordinates
for i in 1:n_points
    # Noisy observations
    rx[i], ry[i], rz[i] = sample_noisy[i][1], sample_noisy[i][2], sample_noisy[i][3]
    # True state
    gx[i], gy[i], gz[i] = sample_clean[i][1], sample_clean[i][2], sample_clean[i][3]
end

# Create three projection plots
p1 = scatter(rx, ry, label=&quot;Noisy observations&quot;, alpha=0.7, markersize=2, title = &quot;X-Y Projection&quot;)
plot!(p1, gx, gy, label=&quot;True state&quot;, linewidth=2)

p2 = scatter(rx, rz, label=&quot;Noisy observations&quot;, alpha=0.7, markersize=2, title = &quot;X-Z Projection&quot;)
plot!(p2, gx, gz, label=&quot;True state&quot;, linewidth=2)

p3 = scatter(ry, rz, label=&quot;Noisy observations&quot;, alpha=0.7, markersize=2, title = &quot;Y-Z Projection&quot;)
plot!(p3, gy, gz, label=&quot;True state&quot;, linewidth=2)

# Combine plots with improved layout
plot(p1, p2, p3, size=(900, 250), layout=(1,3), margin=5Plots.mm)</code></pre><p><img src="Integrating Neural Networks with Flux.jl_4_1.png" alt/></p><p>The plots above visualize our noisy Lorenz system dataset from three different perspectives. We can clearly see how the noise (represented by the scattered points) obscures the true underlying dynamics (shown by the solid lines). The Lorenz system&#39;s characteristic butterfly-shaped attractor is visible in these projections, though the noisy observations make it challenging to discern the exact trajectory. This visualization highlights the challenge our neural network will face: it must learn to filter out the Gaussian noise (with variance 2.0) and recover the true state of the system at each time step. The X-Y, X-Z, and Y-Z projections each provide a different view of the same 3D dynamical system, helping us understand the full complexity of the dataset.</p><h2 id="Bayesian-Inference-meets-Neural-Networks"><a class="docs-heading-anchor" href="#Bayesian-Inference-meets-Neural-Networks">Bayesian Inference meets Neural Networks</a><a id="Bayesian-Inference-meets-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Inference-meets-Neural-Networks" title="Permalink"></a></h2><p>Our objective is to compute the marginal posterior distribution of the latent (hidden) state <span>$x_k$</span> at each time step <span>$k$</span>, considering the history of measurements up to that time step:</p><p class="math-container">\[p(x_k | y_{1:k}).\]</p><p>The above expression represents the probability distribution of the latent state <span>$x_k$</span> given the measurements <span>$y_{1:k}$</span> up to time step <span>$k$</span>. The hidden dynamics of the Lorenz system exhibit nonlinearities and hence cannot be solved in the closed form. One manner of solving this problem is by introducing a neural network to approximate the transition matrix of the Lorenz system. </p><p class="math-container">\[\begin{aligned}
A_{k-1}=NN(y_{k-1}) \\
p(x_k | x_{k-1})=\mathcal{N}(x_k | A_{k-1}x_{k-1}, Q) \\
p(y_k | x_k)=\mathcal{N}(y_k | Bx_k, R)
\end{aligned}\]</p><p>where <span>$NN$</span> is the neural network. The input is the observation <span>$y_{k-1}$</span>, and output is the trasition matrix <span>$A_{k-1}$</span>. <span>$B$</span> denote distortion or measurment matrix. <span>$Q$</span> and <span>$R$</span> are covariance matrices. </p><h2 id="Define-the-Neural-Network"><a class="docs-heading-anchor" href="#Define-the-Neural-Network">Define the Neural Network</a><a id="Define-the-Neural-Network-1"></a><a class="docs-heading-anchor-permalink" href="#Define-the-Neural-Network" title="Permalink"></a></h2><p>We&#39;ll define a neural network using Flux.jl to approximate the transition matrix of the Lorenz system. The neural network will take the observation vector as input and output a transformation matrix that predicts the next state. This approach allows us to capture the nonlinear dynamics of the system while maintaining a tractable inference framework.</p><p>For demonstration purposes, we&#39;ll use a relatively simple neural network architecture - a single Dense layer.  However, this approach can be extended to more complex architectures such as deeper networks, convolutional  networks, or recurrent networks, depending on the complexity of the system dynamics you&#39;re trying to model. The key idea is that the neural network learns to predict the transition dynamics of the system based on  observations, which can then be integrated into our probabilistic state-space model.</p><pre><code class="language-julia hljs">function make_neural_network(rng = StableRNG(1234))
    model = Dense(3 =&gt; 3)

    # Initialize the weights and biases of the neural network
    flat, rebuild = Flux.destructure(model)

    # We use a fixed random seed for reproducibility
    rand!(rng, flat)

    # Return the neural network with fixed weights and biases
    return rebuild(flat)
end</code></pre><pre><code class="nohighlight hljs">make_neural_network (generic function with 2 methods)</code></pre><h2 id="Probabilistic-Model-Specification"><a class="docs-heading-anchor" href="#Probabilistic-Model-Specification">Probabilistic Model Specification</a><a id="Probabilistic-Model-Specification-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilistic-Model-Specification" title="Permalink"></a></h2><p>Now we&#39;ll define our probabilistic state-space model using RxInfer.jl. This model will incorporate the neural network&#39;s predictions of the transition matrices. The model consists of two main components: the state transition equation, which uses our neural network to predict how the state evolves, and the observation equation, which relates the hidden state to the measurements. By combining these components, we create a framework that can handle the nonlinear dynamics of the Lorenz system while maintaining computational tractability.</p><pre><code class="language-julia hljs">@model function ssm(y, As, Q, B, R)
    
    x_prior_mean = ones(Float32, 3)
    x_prior_cov  = Matrix(Diagonal(ones(Float32, 3)))
    
    x[1] ~ MvNormal(mean = x_prior_mean, cov = x_prior_cov)
    y[1] ~ MvNormal(mean = B * x[1], cov = R)
    
    for i in 2:length(y)
        x[i] ~ MvNormal(mean = As[i - 1] * x[i - 1], cov = Q) 
        y[i] ~ MvNormal(mean = B * x[i], cov = R)
    end
end</code></pre><p>We set distortion matrix <span>$B$</span> and the covariance matrices <span>$Q$</span> and <span>$R$</span> as identity matrix. We assume that the observation noise is Gaussian with variance 2.0.</p><pre><code class="language-julia hljs">Q = diageye(Float32, 3)
B = diageye(Float32, 3)
R = variance * diageye(Float32, 3)
;</code></pre><p>Before proceeding with inference, we need to build a function that extracts the transition matrix <span>$A$</span> from the neural network&#39;s output. These matrices will be fixed during the inference process.</p><pre><code class="language-julia hljs">function get_matrices_from_neural_network(data, neural_network)
    dd = hcat(data...)
    As = neural_network(dd)
    return map(c -&gt; Matrix(Diagonal(c)), eachcol(As))
end</code></pre><pre><code class="nohighlight hljs">get_matrices_from_neural_network (generic function with 1 method)</code></pre><h3 id="Un-trained-network"><a class="docs-heading-anchor" href="#Un-trained-network">Un-trained network</a><a id="Un-trained-network-1"></a><a class="docs-heading-anchor-permalink" href="#Un-trained-network" title="Permalink"></a></h3><p>Before network training, we show the inference results for the hidden states:</p><p>In this section, we&#39;ll demonstrate how our model performs with an untrained neural network. This will serve as a baseline to compare against after training. We expect the inference results to be poor since the untrained network generates random transition matrices that don&#39;t capture the true dynamics of the system. The plots below will visualize the true states, noisy observations, and the inferred states for each of the three coordinates in our state space model.</p><pre><code class="language-julia hljs"># Performance on an instance from the testset before training
untrained_neural_network = make_neural_network()
untrained_transition_matrices = get_matrices_from_neural_network(dataset.noisy_signal, untrained_neural_network)

untrained_result = infer(
    model = ssm(As = untrained_transition_matrices, Q = Q, B = B, R = R), 
    data  = (y = dataset.noisy_signal, ), 
    returnvars = (x = KeepLast(), )
)</code></pre><pre><code class="nohighlight hljs">Inference results:
  Posteriors       | available for (x)</code></pre><pre><code class="language-julia hljs"># A helper function for plotting
function plot_coordinate(result, i; title = &quot;&quot;)
    p = scatter(getindex.(dataset.noisy_signal, i), label=&quot;Observations&quot;, alpha=0.7, markersize=2, title = title)
    plot!(getindex.(dataset.signal, i), label=&quot;True states&quot;, linewidth=2)
    plot!(getindex.(mean.(result.posteriors[:x]), i), ribbon=sqrt.(getindex.(var.(result.posteriors[:x]), i)), label=&quot;Inferred states&quot;, linewidth=2)
    return p
end

function plot_coordinates(result)
    p1 = plot_coordinate(result, 1, title = &quot;First coordinate&quot;)
    p2 = plot_coordinate(result, 2, title = &quot;Second coordinate&quot;)
    p3 = plot_coordinate(result, 3, title = &quot;Third coordinate&quot;)
    return plot(p1, p2, p3, size = (1000, 600), layout = (3, 1), legend=:bottomleft)
end</code></pre><pre><code class="nohighlight hljs">plot_coordinates (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">plot_coordinates(untrained_result)</code></pre><p><img src="Integrating Neural Networks with Flux.jl_11_1.png" alt/></p><p>As we can see from the plots above, the inference results with an untrained neural network are essentially nonsense. The inferred states (green lines) fail to track the true states (orange lines) and instead produce arbitrary values with large uncertainty bands. This is expected since the untrained neural network generates random transition matrices that don&#39;t capture the actual dynamics of the system. The large discrepancy between the inferred and true states demonstrates why proper training of the neural network is necessary to achieve meaningful results.</p><h3 id="Training-the-network"><a class="docs-heading-anchor" href="#Training-the-network">Training the network</a><a id="Training-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-network" title="Permalink"></a></h3><p>In this part, we use the Free Energy as the objective function to optimize the weights of our neural network.  Free Energy is a variational inference objective that balances model fit with complexity. By minimizing Free Energy, we encourage the neural network to learn transition matrices that:</p><ol><li>Accurately predict the next state given the current state (reducing prediction error)</li><li>Maintain appropriate uncertainty in the predictions</li><li>Capture the underlying dynamics of the system without overfitting to noise</li></ol><p>The optimization process will iteratively update the neural network weights using gradient descent, with the goal of finding transition matrices that best explain our observed data.</p><pre><code class="language-julia hljs"># free energy objective to be optimized during training
function make_fe_tot_est(rebuild, data; Q = Q, B = B, R = R)
    function fe_tot_est(v)
        nn = rebuild(v)
        result = infer(
            model = ssm(As = get_matrices_from_neural_network(data, nn), Q = Q, B = B, R = R), 
            data  = (y = data, ), 
            returnvars = (x = KeepLast(), ),
            free_energy = true,
            session = nothing
        )
        return result.free_energy[end]
    end
end</code></pre><pre><code class="nohighlight hljs">make_fe_tot_est (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">function train!(neural_network, data; num_epochs = 500)
    rule = Flux.Optimise.Adam()
    state = Flux.setup(rule, neural_network)

    x, rebuild = Flux.destructure(neural_network)
    fe_tot_est_ = make_fe_tot_est(rebuild, data)

    run_epochs!(rebuild, fe_tot_est_, state, neural_network; num_epochs = num_epochs)
end

function run_epochs!(rebuild::F, fe_tot_est::I, state::S, neural_network::N; num_epochs::Int = 100) where {F, I, S, N}
    print_each = num_epochs √∑ 10
    start_time = time()
    for epoch in 1:num_epochs
        flat, _ = Flux.destructure(neural_network)
        if epoch % print_each == 0
            current_value = fe_tot_est(flat)
            elapsed = time() - start_time
            remaining = elapsed / epoch * (num_epochs - epoch)
            println(&quot;Epoch $epoch/$num_epochs: Free Energy = $current_value, ETA: $(round(remaining; digits=1)) seconds&quot;)
        end
        grads = ForwardDiff.gradient(fe_tot_est, flat);
        Flux.update!(state, neural_network, rebuild(grads))
    end
    # Calculate and print total training time
    total_time = time() - start_time
    println(&quot;Finished in $(round(total_time; digits=1)) seconds&quot;)
end</code></pre><pre><code class="nohighlight hljs">run_epochs! (generic function with 1 method)</code></pre><p>Now that we have defined our neural network architecture, dataset, and training functions, we can proceed with the training process. We&#39;ll train the neural network to learn the underlying dynamics of our state-space model from noisy observations. The training will optimize the free energy objective function using the Adam optimizer over multiple epochs. This process will allow the neural network to capture the non-linear relationships in the data, enabling more accurate state inference compared to traditional linear models. The following cell executes the training with 2000 epochs, which should provide sufficient iterations for convergence.</p><pre><code class="language-julia hljs">trained_neural_network = make_neural_network()

train!(trained_neural_network, dataset.noisy_signal; num_epochs = 1000)</code></pre><pre><code class="nohighlight hljs">Epoch 100/1000: Free Energy = 24170.93445802461, ETA: 363.9 seconds
Epoch 200/1000: Free Energy = 22554.71504016765, ETA: 207.9 seconds
Epoch 300/1000: Free Energy = 20249.690550213167, ETA: 147.5 seconds
Epoch 400/1000: Free Energy = 15243.43215265174, ETA: 112.1 seconds
Epoch 500/1000: Free Energy = 5305.612489972754, ETA: 85.8 seconds
Epoch 600/1000: Free Energy = 1975.3465539892043, ETA: 64.7 seconds
Epoch 700/1000: Free Energy = 1565.5426626657077, ETA: 46.5 seconds
Epoch 800/1000: Free Energy = 1528.6284177456932, ETA: 29.9 seconds
Epoch 900/1000: Free Energy = 1515.1423138831697, ETA: 14.5 seconds
Epoch 1000/1000: Free Energy = 1510.1678087715945, ETA: 0.0 seconds
Finished in 141.9 seconds</code></pre><p>Now let&#39;s analyze the results of our neural network training. We&#39;ll visualize how well our trained model can infer the true states from noisy observations. The plots below will show the original noisy observations, the true underlying states, and our model&#39;s inferred states with confidence intervals. This comparison will help us evaluate the effectiveness of our neural network-based approach in capturing the non-linear dynamics of the system and filtering out noise.</p><pre><code class="language-julia hljs">trained_transition_matrices = get_matrices_from_neural_network(dataset.noisy_signal, trained_neural_network)

trained_result = infer(
    model = ssm(As = trained_transition_matrices, Q = Q, B = B, R = R), 
    data  = (y = dataset.noisy_signal, ), 
    returnvars = (x = KeepLast(), )
)

plot_coordinates(trained_result)</code></pre><p><img src="Integrating Neural Networks with Flux.jl_15_1.png" alt/></p><p>The results demonstrate the effectiveness of our neural network-based state-space model approach. Despite the significant noise present in the observations (shown as scattered points), our model successfully identifies the underlying hidden signal (shown by the inferred states line). The close alignment between the inferred states and the true states across all three coordinates indicates that the trained neural network has effectively learned the non-linear dynamics of the system. The narrow confidence bands (shown as ribbons) around the inferred states further suggest high confidence in the predictions. This example illustrates how combining neural networks with probabilistic state-space models can provide robust inference in scenarios with complex dynamics and noisy observations.</p><pre><code class="language-julia hljs">ix, iy, iz = zeros(n_points), zeros(n_points), zeros(n_points)

inferred_mean = mean.(trained_result.posteriors[:x])

# Extract coordinates
for i in 1:n_points
    # Inferred mean
    ix[i], iy[i], iz[i] = inferred_mean[i][1], inferred_mean[i][2], inferred_mean[i][3]
end

# Create three projection plots
p1 = scatter(rx, ry, label=&quot;Noisy observations&quot;, alpha=0.7, markersize=2, title = &quot;X-Y Projection&quot;)
plot!(p1, gx, gy, label=&quot;True state&quot;, linewidth=2)
plot!(p1, ix, iy, label=&quot;Inferred Mean&quot;, linewidth=2)

p2 = scatter(rx, rz, label=&quot;Noisy observations&quot;, alpha=0.7, markersize=2, title = &quot;X-Z Projection&quot;)
plot!(p2, gx, gz, label=&quot;True state&quot;, linewidth=2)
plot!(p2, ix, iz, label=&quot;Inferred Mean&quot;, linewidth=2)

p3 = scatter(ry, rz, label=&quot;Noisy observations&quot;, alpha=0.7, markersize=2, title = &quot;Y-Z Projection&quot;)
plot!(p3, gy, gz, label=&quot;True state&quot;, linewidth=2)
plot!(p3, iy, iz, label=&quot;Inferred Mean&quot;, linewidth=2)

# Combine plots with improved layout
plot(p1, p2, p3, size=(900, 250), layout=(1,3), margin=5Plots.mm)</code></pre><p><img src="Integrating Neural Networks with Flux.jl_16_1.png" alt/></p><hr/><div class="admonition is-info" id="Contributing-baba9dc142ba7ccb"><header class="admonition-header">Contributing<a class="admonition-anchor" href="#Contributing-baba9dc142ba7ccb" title="Permalink"></a></header><div class="admonition-body"><p>This example was automatically generated from a Jupyter notebook in the <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">RxInferExamples.jl</a> repository.</p><p>We welcome and encourage contributions! You can help by:</p><ul><li>Improving this example</li><li>Creating new examples </li><li>Reporting issues or bugs</li><li>Suggesting enhancements</li></ul><p>Visit our <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">GitHub repository</a> to get started. Together we can make <a href="https://github.com/ReactiveBayes/RxInfer.jl">RxInfer.jl</a> even better! üí™</p></div></div><hr/><div class="admonition is-compat" id="Environment-ead41e814a894220"><header class="admonition-header">Environment<a class="admonition-anchor" href="#Environment-ead41e814a894220" title="Permalink"></a></header><div class="admonition-body"><p>This example was executed in a clean, isolated environment. Below are the exact package versions used:</p><p>For reproducibility:</p><ul><li>Use the same package versions when running locally</li><li>Report any issues with package compatibility</li></ul></div></div><pre><code class="nohighlight hljs">Status `/tmp/jl_A77yVv/Project.toml`
‚åÖ [587475ba] Flux v0.14.25
‚åÖ [f6369f11] ForwardDiff v0.10.39
  [91a5bcdd] Plots v1.41.6
  [86711068] RxInfer v4.7.0
  [860ef19b] StableRNGs v1.0.4
  [37e2e46d] LinearAlgebra v1.12.0
Info Packages marked with ‚åÖ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`
</code></pre><script type="module">import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: true,
    theme: "neutral"
});
</script></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../infinite_data_stream/">¬´ Infinite Data Stream</a><a class="docs-footer-nextpage" href="../learning_dynamics_with_vaes/">Learning Dynamics With Vaes ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Created in <a href="https://biaslab.github.io/">BIASlab</a>, maintained by <a href="https://github.com/ReactiveBayes">ReactiveBayes</a>, powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.17.0 on <span class="colophon-date" title="Wednesday 25 February 2026 15:00">Wednesday 25 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
