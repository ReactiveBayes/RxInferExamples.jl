<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Contextual Bandits · RxInfer.jl Examples</title><meta name="title" content="Contextual Bandits · RxInfer.jl Examples"/><meta property="og:title" content="Contextual Bandits · RxInfer.jl Examples"/><meta property="twitter:title" content="Contextual Bandits · RxInfer.jl Examples"/><meta name="description" content="Contextual Bandits with RxInfer.jl\nThis notebooks covers RxInfer usage for the Contextual Bandits problem.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="og:description" content="Contextual Bandits with RxInfer.jl\nThis notebooks covers RxInfer usage for the Contextual Bandits problem.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="twitter:description" content="Contextual Bandits with RxInfer.jl\nThis notebooks covers RxInfer usage for the Contextual Bandits problem.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="og:url" content="https://examples.rxinfer.com/categories/basic_examples/contextual_bandits/"/><meta property="twitter:url" content="https://examples.rxinfer.com/categories/basic_examples/contextual_bandits/"/><link rel="canonical" href="https://examples.rxinfer.com/categories/basic_examples/contextual_bandits/"/><script async src="https://www.googletagmanager.com/gtag/js?id=G-GMFX620VEP"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-GMFX620VEP', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/theme.css" rel="stylesheet" type="text/css"/><link href="../../../assets/header.css" rel="stylesheet" type="text/css"/><script src="../../../assets/header.js"></script><script src="../../../assets/chat.js"></script><link href="../../../assets/favicon.ico" rel="icon" type="image/x-icon"/>
    <meta property="og:title" content="Contextual Bandits - RxInfer Examples">
    <meta name="description" content="This notebooks covers RxInfer usage for the Contextual Bandits problem.
">
    <meta property="og:description" content="This notebooks covers RxInfer usage for the Contextual Bandits problem.
">
    <meta name="keywords" content="rxinfer, julia, bayesian inference, examples, probabilistic programming, message passing, probabilistic numerics, variational inference, belief propagation, basic examples, contextual bandits">
    <link rel="sitemap" type="application/xml" title="Sitemap" href="https://examples.rxinfer.com/sitemap.xml">
    </head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.svg" alt="RxInfer.jl Examples logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.svg" alt="RxInfer.jl Examples logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">RxInfer.jl Examples</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><a class="tocitem" href="../../../how_to_contribute/">How to contribute</a></li><li><a class="tocitem" href="../../../autogenerated/list_of_examples/">List of Examples</a></li><li><span class="tocitem">Basic Examples</span><ul><li><a class="tocitem" href="../bayesian_binomial_regression/">Bayesian Binomial Regression</a></li><li><a class="tocitem" href="../bayesian_linear_regression/">Bayesian Linear Regression</a></li><li><a class="tocitem" href="../bayesian_multinomial_regression/">Bayesian Multinomial Regression</a></li><li><a class="tocitem" href="../bayesian_networks/">Bayesian Networks</a></li><li><a class="tocitem" href="../coin_toss_model/">Coin Toss Model</a></li><li class="is-active"><a class="tocitem" href>Contextual Bandits</a><ul class="internal"><li><a class="tocitem" href="#Turning-Ad-Scheduling-Into-a-Learning-Problem"><span>Turning Ad Scheduling Into a Learning Problem</span></a></li><li><a class="tocitem" href="#Contextual-Multi-Armed-Bandits-to-the-Rescue"><span>Contextual Multi-Armed Bandits to the Rescue</span></a></li><li><a class="tocitem" href="#Implementing-CMABs-in-**RxInfer.jl**"><span>Implementing CMABs in <strong>RxInfer.jl</strong></span></a></li><li><a class="tocitem" href="#Generative-model-(consistent-notation)"><span>Generative model (consistent notation)</span></a></li><li><a class="tocitem" href="#Inference-and-decision‑making"><span>Inference &amp; decision‑making</span></a></li></ul></li><li><a class="tocitem" href="../feature_functions_in_bayesian_regression/">Feature Functions In Bayesian Regression</a></li><li><a class="tocitem" href="../forgetting_factors_for_online_inference/">Forgetting Factors For Online Inference</a></li><li><a class="tocitem" href="../hidden_markov_model/">Hidden Markov Model</a></li><li><a class="tocitem" href="../incomplete_data/">Incomplete Data</a></li><li><a class="tocitem" href="../kalman_filtering_and_smoothing/">Kalman Filtering And Smoothing</a></li><li><a class="tocitem" href="../pomdp_control/">Pomdp Control</a></li><li><a class="tocitem" href="../predicting_bike_rental_demand/">Predicting Bike Rental Demand</a></li></ul></li><li><span class="tocitem">Advanced Examples</span><ul><li><a class="tocitem" href="../../advanced_examples/active_inference_mountain_car/">Active Inference Mountain Car</a></li><li><a class="tocitem" href="../../advanced_examples/advanced_tutorial/">Advanced Tutorial</a></li><li><a class="tocitem" href="../../advanced_examples/assessing_people_skills/">Assessing People Skills</a></li><li><a class="tocitem" href="../../advanced_examples/chance_constraints/">Chance Constraints</a></li><li><a class="tocitem" href="../../advanced_examples/conjugate-computational_variational_message_passing/">Conjugate-Computational Variational Message Passing</a></li><li><a class="tocitem" href="../../advanced_examples/drone_dynamics/">Drone Dynamics</a></li><li><a class="tocitem" href="../../advanced_examples/gp_regression_by_ssm/">Gp Regression By Ssm</a></li><li><a class="tocitem" href="../../advanced_examples/infinite_data_stream/">Infinite Data Stream</a></li><li><a class="tocitem" href="../../advanced_examples/integrating_neural_networks_with_flux.jl/">Integrating Neural Networks With Flux.Jl</a></li><li><a class="tocitem" href="../../advanced_examples/learning_dynamics_with_vaes/">Learning Dynamics With Vaes</a></li><li><a class="tocitem" href="../../advanced_examples/multi-agent_trajectory_planning/">Multi-Agent Trajectory Planning</a></li><li><a class="tocitem" href="../../advanced_examples/nonlinear_sensor_fusion/">Nonlinear Sensor Fusion</a></li><li><a class="tocitem" href="../../advanced_examples/parameter_optimisation_with_optim.jl/">Parameter Optimisation With Optim.Jl</a></li><li><a class="tocitem" href="../../advanced_examples/robotic_arm/">Robotic Arm</a></li></ul></li><li><span class="tocitem">Problem Specific</span><ul><li><a class="tocitem" href="../../problem_specific/autoregressive_models/">Autoregressive Models</a></li><li><a class="tocitem" href="../../problem_specific/gamma_mixture/">Gamma Mixture</a></li><li><a class="tocitem" href="../../problem_specific/gaussian_mixture/">Gaussian Mixture</a></li><li><a class="tocitem" href="../../problem_specific/hierarchical_gaussian_filter/">Hierarchical Gaussian Filter</a></li><li><a class="tocitem" href="../../problem_specific/invertible_neural_network_tutorial/">Invertible Neural Network Tutorial</a></li><li><a class="tocitem" href="../../problem_specific/litter_model/">Litter Model</a></li><li><a class="tocitem" href="../../problem_specific/ode_parameter_estimation/">Ode Parameter Estimation</a></li><li><a class="tocitem" href="../../problem_specific/probit_model/">Probit Model</a></li><li><a class="tocitem" href="../../problem_specific/rts_vs_bifm_smoothing/">Rts Vs Bifm Smoothing</a></li><li><a class="tocitem" href="../../problem_specific/simple_nonlinear_node/">Simple Nonlinear Node</a></li><li><a class="tocitem" href="../../problem_specific/structural_dynamics_with_augmented_kalman_filter/">Structural Dynamics With Augmented Kalman Filter</a></li><li><a class="tocitem" href="../../problem_specific/universal_mixtures/">Universal Mixtures</a></li></ul></li><li><span class="tocitem">Experimental Examples</span><ul><li><a class="tocitem" href="../../experimental_examples/bayesian_trust_learning/">Bayesian Trust Learning</a></li><li><a class="tocitem" href="../../experimental_examples/large_language_models/">Large Language Models</a></li><li><a class="tocitem" href="../../experimental_examples/latent_vector_autoregressive_model/">Latent Vector Autoregressive Model</a></li><li><a class="tocitem" href="../../experimental_examples/recurrent_switching_linear_dynamical_system/">Recurrent Switching Linear Dynamical System</a></li></ul></li><li><a class="tocitem" href="../../../how_build_works/">How we build the examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Basic Examples</a></li><li class="is-active"><a href>Contextual Bandits</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Contextual Bandits</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInferExamples.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInferExamples.jl" title="View source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><div class="admonition is-info" id="Contributing-64592202d51b8d51"><header class="admonition-header">Contributing<a class="admonition-anchor" href="#Contributing-64592202d51b8d51" title="Permalink"></a></header><div class="admonition-body"><p>This example was automatically generated from a Jupyter notebook in the <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">RxInferExamples.jl</a> repository.</p><p>We welcome and encourage contributions! You can help by:</p><ul><li>Improving this example</li><li>Creating new examples </li><li>Reporting issues or bugs</li><li>Suggesting enhancements</li></ul><p>Visit our <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">GitHub repository</a> to get started. Together we can make <a href="https://github.com/ReactiveBayes/RxInfer.jl">RxInfer.jl</a> even better! 💪</p></div></div><hr/><h1 id="Contextual-Bandits"><a class="docs-heading-anchor" href="#Contextual-Bandits">Contextual Bandits</a><a id="Contextual-Bandits-1"></a><a class="docs-heading-anchor-permalink" href="#Contextual-Bandits" title="Permalink"></a></h1><p>We will start this notebook with a motivating example.</p><p>Let’s face it—ads are annoying. But very often they’re also one of the few ways to keep the business running. Imagine a free-to-play game for example. The real question isn’t whether to show ads, but when and which kind to show, so that players don’t feel bombarded, leave the game frustrated, or stop spending altogether.</p><p>It’s a balancing act between monetization and player retention.</p><p>If you show ads too aggressively, players quit. If you show ads too cautiously, you leave money on the table. So, how do you decide what to do in each moment of a player’s session?</p><h2 id="Turning-Ad-Scheduling-Into-a-Learning-Problem"><a class="docs-heading-anchor" href="#Turning-Ad-Scheduling-Into-a-Learning-Problem">Turning Ad Scheduling Into a Learning Problem</a><a id="Turning-Ad-Scheduling-Into-a-Learning-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Turning-Ad-Scheduling-Into-a-Learning-Problem" title="Permalink"></a></h2><p>Every player session is different. Some players are engaged, some are rushing through, some just made a purchase, some are one level away from quitting.</p><p>At every possible ad moment, you have choices like:</p><ul><li>Show a <strong>video ad</strong> (high revenue, high interruption)</li><li>Show a <strong>banner ad</strong> (low revenue, low interruption)</li><li>Show <strong>no ad</strong> (defer to later)</li></ul><p>And you have <em>context</em>—information about the session so far:</p><ul><li>Player’s level and progress</li><li>Time since the last ad</li><li>Time spent in the current session</li><li>Whether the player just succeeded or failed at something</li><li>Recent purchases or reward claims</li></ul><p>What you want is a system that <strong>learns from player behavior over time</strong>, figuring out which actions perform best in which contexts—not just maximizing clicks or ad revenue right now, but balancing that with keeping players happy and playing longer.</p><h2 id="Contextual-Multi-Armed-Bandits-to-the-Rescue"><a class="docs-heading-anchor" href="#Contextual-Multi-Armed-Bandits-to-the-Rescue">Contextual Multi-Armed Bandits to the Rescue</a><a id="Contextual-Multi-Armed-Bandits-to-the-Rescue-1"></a><a class="docs-heading-anchor-permalink" href="#Contextual-Multi-Armed-Bandits-to-the-Rescue" title="Permalink"></a></h2><p>This is exactly where <strong>Contextual Multi-Armed Bandits (CMABs)</strong> come in.</p><p>Contextual Multi‑Armed Bandits (CMABs) extend the classic bandit setting by allowing the learner to <strong>observe a context</strong>—a feature vector that describes the current situation—before selecting an action (or <em>arm</em>). The learner’s aim is to <strong>maximise cumulative reward</strong> over time by repeatedly balancing:</p><ul><li><strong>Exploration</strong> – trying arms whose pay‑offs are still uncertain in some contexts.</li><li><strong>Exploitation</strong> – choosing the arm with the highest estimated reward in the current context.</li></ul><p>CMABs sit between simple A/B tests and full reinforcement‑learning problems and are the work‑horse behind many personalisation engines.</p><p>Acting <em>identically</em> for every context wastes opportunity; CMABs formalise how to adapt decisions on‑the‑fly.</p><h2 id="Implementing-CMABs-in-**RxInfer.jl**"><a class="docs-heading-anchor" href="#Implementing-CMABs-in-**RxInfer.jl**">Implementing CMABs in <strong>RxInfer.jl</strong></a><a id="Implementing-CMABs-in-**RxInfer.jl**-1"></a><a class="docs-heading-anchor-permalink" href="#Implementing-CMABs-in-**RxInfer.jl**" title="Permalink"></a></h2><p>In this notebook we will implement a CMAB in <code>RxInfer.jl</code> way. A way to tackle CMAB in RxInfer requires expressing the generative model as a <strong>hierarchical Bayesian linear‑regression</strong> model and then message passing inference will do the rest.</p><h2 id="Generative-model-(consistent-notation)"><a class="docs-heading-anchor" href="#Generative-model-(consistent-notation)">Generative model (consistent notation)</a><a id="Generative-model-(consistent-notation)-1"></a><a class="docs-heading-anchor-permalink" href="#Generative-model-(consistent-notation)" title="Permalink"></a></h2><p>Let</p><ul><li><p class="math-container">\[K\]</p>– number of arms.</li><li><p class="math-container">\[d\]</p>– dimension of the context vector.</li><li><p class="math-container">\[c_t\in\mathbb{R}^d\]</p>– context observed at round <span>$</span>t<span>$</span>.</li><li><p class="math-container">\[a_t\in{1,\dots,K}\]</p>– arm selected at round <span>$</span>t<span>$</span>.</li><li><p class="math-container">\[r_t\in\mathbb{R}\]</p>– realised reward.</li></ul><p>The environment produces rewards according to:</p><ol><li><strong>Global noise precision</strong> <span>$\tau\;\sim\;\mathcal{G}(\alpha_\tau, \beta_\tau)$</span></li><li><strong>Arm‑specific regression parameters</strong>  <span>$k = 1,\dots,K$</span> <span>$\theta_k \;\sim\; \mathcal{N}(m_{0k}, V_{0k}), \qquad \\ \Lambda_k \;\sim\; \operatorname{Wishart}(\nu_{0k}, W_{0k})$</span></li><li><strong>Per interaction latent coefficients</strong>  <span>$t = 1,\dots,T$</span> <span>$\beta_t \;\sim\; \mathcal{N}(\theta_{a_t}, \Lambda_{a_t}^{-1})$</span></li><li><strong>Reward generation</strong> <span>$\mu_t = c_t^\top \beta_t, \qquad \\ r_t \;\sim\; \mathcal{N}(\mu_t, \tau^{-1})$</span></li></ol><p><strong>Interpretation.</strong> Each arm owns a <em>distribution of weight vectors</em> (<span>$\theta_k$</span>), capturing how the context maps to reward for that arm. On every play we draw a concrete weight vector <span>$\beta_t$</span>, compute the expected reward <span>$\mu_t$</span>, and then observe a noisy realisation <span>$r_t$</span>.</p><h2 id="Inference-and-decision‑making"><a class="docs-heading-anchor" href="#Inference-and-decision‑making">Inference &amp; decision‑making</a><a id="Inference-and-decision‑making-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-and-decision‑making" title="Permalink"></a></h2><p>With RxInfer we:</p><ol><li><strong>Declare</strong> the model above with the <code>@model</code> macro. (To make the example simpler, we&#39;ll have two models: one for parameters and one for predictions. A more complex scenario would be to have a single model for both.)</li><li><strong>Stream</strong> incoming tuples <span>$(c_t, a_t, r_t)$</span>.</li><li><strong>Call</strong> <code>infer</code> to update the posterior over <span>$(\theta_k, \Lambda_k, \tau)$</span>.</li><li><strong>Compute predictive distributions</strong> of rewards in a <em>new</em> context <span>$c_{t+1}$</span> via <code>infer</code> on the predictive model:</li><li><strong>Choose</strong> the next arm based on sampled from the predictive distribution.</li></ol><p>Because both learning and prediction are expressed as probabilistic inference, we keep <strong>all uncertainty</strong> in closed form—ideal for principled exploration. The same pipeline generalises easily to non‑linear contexts (via feature maps) or non‑Gaussian rewards by swapping likelihood terms.</p><pre><code class="language-julia hljs">using RxInfer, Distributions, LinearAlgebra, Plots, StatsPlots, ProgressMeter, StableRNGs, Random</code></pre><p>At first let&#39;s generate synthetic data to simulate the CMAB problem.</p><pre><code class="language-julia hljs">using StableRNGs

# Random number generator 
rng = StableRNG(42)

# Data generation parameters
n_train_samples = 300
n_test_samples = 100
n_total_samples = n_train_samples + n_test_samples
n_arms = 10
n_contexts = 50
context_dim = 20
noise_sd = 0.1

# Generate true arm parameters (θ_k in the model description)
arms = [randn(rng, context_dim) for _ in 1:n_arms]

# Generate context feature vectors with missing values
contexts = []
for i in 1:n_contexts
    # Create a vector that can hold both Float64 and Missing values
    context = Vector{Union{Float64,Missing}}(undef, context_dim)

    # Fill with random values initially
    for j in 1:context_dim
        context[j] = randn(rng)
    end

    # Randomly introduce missing values in some contexts
    if rand(rng) &lt; 0.4  # 40% of contexts will have some missing values
        n_missing = rand(rng, 1:2)  # 1-2 missing values per context

        # Simple approach: randomly select indices for missing values
        missing_indices = []
        while length(missing_indices) &lt; n_missing
            idx = rand(rng, 1:context_dim)
            if !(idx in missing_indices)
                push!(missing_indices, idx)
            end
        end

        for idx in missing_indices
            context[idx] = missing
        end
    end

    push!(contexts, context)
end

# Synthetic reward function (reusable)
function compute_reward(arm_params, context_vec, noise_sd=0.0; rng=nothing)
    &quot;&quot;&quot;
    Compute reward for given arm parameters and context.

    Args:
        arm_params: Vector of arm parameters
        context_vec: Context vector (may contain missing values)
        noise_sd: Standard deviation of Gaussian noise to add
        rng: Random number generator (optional, for reproducible noise)

    Returns:
        Scalar reward value
    &quot;&quot;&quot;
    # Calculate the deterministic part of the reward, handling missing values
    mean_reward = 0.0
    valid_dims = 0

    for j in 1:length(context_vec)
        if !ismissing(context_vec[j])
            mean_reward += arm_params[j] * context_vec[j]
            valid_dims += 1
        end
    end

    # Normalize by the number of valid dimensions to maintain similar scale
    if valid_dims &gt; 0
        mean_reward = mean_reward * (length(context_vec) / valid_dims)
    end

    # Add Gaussian noise if requested
    if noise_sd &gt; 0
        if rng !== nothing
            mean_reward += randn(rng) * noise_sd
        else
            mean_reward += randn() * noise_sd
        end
    end

    return mean_reward
end

# Generate training and test data
function generate_bandit_data(n_samples, arms, contexts, noise_sd; rng, start_idx=1)
    &quot;&quot;&quot;
    Generate bandit data for given number of samples.

    Returns:
        (arm_choices, context_choices, rewards)
    &quot;&quot;&quot;
    arm_choices = []
    context_choices = []
    rewards = []

    for i in 1:n_samples
        # Randomly select a context and an arm
        push!(context_choices, rand(rng, 1:length(contexts)))
        push!(arm_choices, rand(rng, 1:length(arms)))

        # Get the selected context and arm
        selected_context = contexts[context_choices[end]]
        selected_arm = arms[arm_choices[end]]

        # Compute reward using the synthetic reward function
        reward = compute_reward(selected_arm, selected_context, noise_sd; rng=rng)
        push!(rewards, reward)
    end

    return arm_choices, context_choices, rewards
end

# Generate training data
println(&quot;Generating training data...&quot;)
train_arm_choices, train_context_choices, train_rewards = generate_bandit_data(
    n_train_samples, arms, contexts, noise_sd; rng=rng
)

# Generate test data
println(&quot;Generating test data...&quot;)
test_arm_choices, test_context_choices, test_rewards = generate_bandit_data(
    n_test_samples, arms, contexts, noise_sd; rng=rng
)

# Display information about the generated data
println(&quot;\nDataset Summary:&quot;)
println(&quot;Training samples: $n_train_samples&quot;)
println(&quot;Test samples: $n_test_samples&quot;)
println(&quot;Total contexts: $(length(contexts))&quot;)
println(&quot;Number of contexts with missing values: &quot;, sum(any(ismissing, ctx) for ctx in contexts))
println(&quot;Arms: $n_arms&quot;)
println(&quot;Context dimension: $context_dim&quot;)
println(&quot;Noise standard deviation: $noise_sd&quot;)

# Show examples of contexts with missing values
println(&quot;\nExamples of contexts with missing values:&quot;)
count = 0
for (i, ctx) in enumerate(contexts)
    if any(ismissing, ctx) &amp;&amp; count &lt; 3  # Show first 3 examples
        println(&quot;Context $i: $ctx&quot;)
        global count += 1
    end
end

# Show sample data
println(&quot;\nTraining data samples (first 5):&quot;)
for i in 1:min(5, length(train_rewards))
    println(&quot;Sample $i: Arm=$(train_arm_choices[i]), Context=$(train_context_choices[i]), Reward=$(round(train_rewards[i], digits=4))&quot;)
end

println(&quot;\nTest data samples (first 5):&quot;)
for i in 1:min(5, length(test_rewards))
    println(&quot;Sample $i: Arm=$(test_arm_choices[i]), Context=$(test_context_choices[i]), Reward=$(round(test_rewards[i], digits=4))&quot;)
end

# Verify the reward function works correctly
println(&quot;\nTesting reward function:&quot;)
test_context = contexts[1]
test_arm = arms[1]
deterministic_reward = compute_reward(test_arm, test_context, 0.0)  # No noise
noisy_reward = compute_reward(test_arm, test_context, noise_sd; rng=rng)  # With noise
println(&quot;Deterministic reward: $(round(deterministic_reward, digits=4))&quot;)
println(&quot;Noisy reward: $(round(noisy_reward, digits=4))&quot;)</code></pre><pre><code class="nohighlight hljs">Generating training data...
Generating test data...

Dataset Summary:
Training samples: 300
Test samples: 100
Total contexts: 50
Number of contexts with missing values: 22
Arms: 10
Context dimension: 20
Noise standard deviation: 0.1

Examples of contexts with missing values:
Context 3: Union{Missing, Float64}[missing, -0.4741385118651381, -1.0989041
9081401, -1.079288892379018, 0.8184199040107111, -0.30409464242950546, -0.6
709508997562322, -0.7469592378369052, 0.21407501633089995, -0.6139813001136
504, 2.8170273653049507, -1.4362435690909499, -0.30112508107598307, -0.3868
83090487843, 0.6563571763621648, 1.401591444397142, 0.6193863742347839, 0.1
2760715013378465, -0.2758495479700435, 1.8822768045661076]
Context 8: Union{Missing, Float64}[-2.3650237776747054, -1.1739461025984783
, 1.128284045692684, -0.8690689832066373, 0.4497591893001418, -0.2617237965
612964, 0.07868265261314639, missing, 1.9119126287271901, 0.719828217704402
8, -2.708690227134825, -2.645555311022844, -0.3202946667428825, 1.398258591
17344, 0.06974735851443013, 1.1639494445584129, -0.36687387238833036, 0.506
2972107773495, -1.3675557327045547, missing]
Context 9: Union{Missing, Float64}[-0.7928407885849085, 0.5230893424666117,
 0.21944291871653826, -0.2951043978830045, missing, 0.6739591611416778, 0.6
091981160535558, 0.37661376790321904, -0.08201072963796563, 0.6762611326141
408, -1.7998621684347569, 0.7079334064897562, -1.5082653360123872, 0.423324
15672698104, 1.380447484940245, -3.325041477189219, 1.1893655835458625, 0.9
25128468276957, -1.62673585658528, -0.667629748025382]

Training data samples (first 5):
Sample 1: Arm=6, Context=11, Reward=2.7556
Sample 2: Arm=2, Context=14, Reward=-3.911
Sample 3: Arm=8, Context=14, Reward=-2.0472
Sample 4: Arm=6, Context=36, Reward=9.6666
Sample 5: Arm=3, Context=21, Reward=-4.1496

Test data samples (first 5):
Sample 1: Arm=1, Context=18, Reward=0.3271
Sample 2: Arm=2, Context=27, Reward=-3.2327
Sample 3: Arm=4, Context=4, Reward=-2.1027
Sample 4: Arm=1, Context=7, Reward=4.7518
Sample 5: Arm=8, Context=24, Reward=3.4953

Testing reward function:
Deterministic reward: -2.2755
Noisy reward: -2.2613</code></pre><pre><code class="language-julia hljs">function create_bandit_plots(arm_choices, context_choices, rewards, title_prefix, color_scheme)
    p1 = scatter(1:length(context_choices), context_choices,
        label=&quot;Context Choices&quot;,
        title=&quot;$title_prefix: Context Selection&quot;,
        xlabel=&quot;Sample&quot;, ylabel=&quot;Context ID&quot;,
        marker=:circle, markersize=6,
        color=color_scheme[:context], alpha=0.7)

    p2 = scatter(1:length(arm_choices), arm_choices,
        label=&quot;Arm Choices&quot;,
        title=&quot;$title_prefix: Arm Selection&quot;,
        xlabel=&quot;Sample&quot;, ylabel=&quot;Arm ID&quot;,
        marker=:diamond, markersize=6,
        color=color_scheme[:arm], alpha=0.7)

    p3 = plot(1:length(rewards), rewards,
        label=&quot;Rewards&quot;,
        title=&quot;$title_prefix: Rewards&quot;,
        xlabel=&quot;Sample&quot;, ylabel=&quot;Reward Value&quot;,
        linewidth=2, marker=:star, markersize=4,
        color=color_scheme[:reward], alpha=0.8)

    hline!(p3, [mean(rewards)], label=&quot;Mean Reward&quot;,
        linestyle=:dash, linewidth=2, color=:black)

    return plot(p1, p2, p3, layout=(3, 1), size=(800, 600))
end

# Create training plots
train_colors = Dict(:context =&gt; :blue, :arm =&gt; :red, :reward =&gt; :green)
train_plot = create_bandit_plots(train_arm_choices, train_context_choices, train_rewards,
    &quot;Training Data&quot;, train_colors)

# Create test plots  
test_colors = Dict(:context =&gt; :lightblue, :arm =&gt; :pink, :reward =&gt; :lightgreen)
test_plot = create_bandit_plots(test_arm_choices, test_context_choices, test_rewards,
    &quot;Test Data&quot;, test_colors)

# Display both
plot(train_plot, test_plot, layout=(1, 2), size=(1600, 600),
    plot_title=&quot;Contextual Bandit Experiment: Training and Test Data&quot;)</code></pre><p><img src="Contextual Bandits_3_1.png" alt/></p><pre><code class="language-julia hljs">@model function conditional_regression(n_arms, priors, past_rewards, past_choices, past_contexts)
    local θ
    local γ
    local τ

    # Prior for each arm&#39;s parameters
    for k in 1:n_arms
        θ[k] ~ priors[:θ][k]
        γ[k] ~ priors[:γ][k]
    end

    # Prior for the noise precision
    τ ~ priors[:τ]

    # Model for past observations
    for n in eachindex(past_rewards)
        arm_vals[n] ~ NormalMixture(switch=past_choices[n], m=θ, p=γ)
        latent_context[n] ~ past_contexts[n]
        past_rewards[n] ~ softdot(arm_vals[n], latent_context[n], τ)
    end
end</code></pre><p>Let&#39;s define the priors.</p><pre><code class="language-julia hljs">priors_rng = StableRNG(42)
priors = Dict(
    :θ =&gt; [MvNormalMeanPrecision(randn(priors_rng, context_dim), diagm(ones(context_dim))) for _ in 1:n_arms],
    :γ =&gt; [Wishart(context_dim + 1, diagm(ones(context_dim))) for _ in 1:n_arms],
    :τ =&gt; GammaShapeRate(1.0, 1.0)
)</code></pre><pre><code class="nohighlight hljs">Dict{Symbol, Any} with 3 entries:
  :γ =&gt; Wishart{Float64, PDMat{Float64, Matrix{Float64}}, Int64}[Distributi
ons.…
  :τ =&gt; ExponentialFamily.GammaShapeRate{Float64}(a=1.0, b=1.0)
  :θ =&gt; MvNormalMeanPrecision{Float64, Vector{Float64}, Matrix{Float64}}[Mv
Norm…</code></pre><p>And finally run the inference.</p><pre><code class="language-julia hljs">function run_inference(; n_arms, priors, past_rewards, past_choices, past_contexts, iterations=50, free_energy=true)
    init = @initialization begin
        q(θ) = priors[:θ]
        q(γ) = priors[:γ]
        q(τ) = priors[:τ]
        q(latent_context) = MvNormalMeanPrecision(zeros(context_dim), Diagonal(ones(context_dim)))
    end

    return infer(
        model=conditional_regression(
            n_arms=n_arms,
            priors=priors,
            past_contexts=past_contexts,
        ),
        data=(
            past_rewards=past_rewards,
            past_choices=past_choices,
        ),
        constraints=MeanField(),
        initialization=init,
        showprogress=true,
        iterations=iterations,
        free_energy=free_energy
    )

end</code></pre><pre><code class="nohighlight hljs">run_inference (generic function with 1 method)</code></pre><pre><code class="language-julia hljs"># Utility function to convert context with missing values to MvNormal distribution
function context_to_mvnormal(context_vec; tiny_precision=1e-6, huge_precision=1e6)
    &quot;&quot;&quot;
    Convert a context vector (potentially with missing values) to MvNormal distribution.

    Args:
        context_vec: Vector that may contain missing values
        tiny_v: Small variance for known values (high precision)
        huge_var: Large variance for missing values (low precision)

    Returns:
        MvNormal distribution
    &quot;&quot;&quot;
    context_mean = Vector{Float64}(undef, length(context_vec))
    context_precision = Vector{Float64}(undef, length(context_vec))

    for j in 1:length(context_vec)
        if ismissing(context_vec[j])
            context_mean[j] = 0.0
            context_precision[j] = tiny_precision
        else
            context_mean[j] = context_vec[j]
            context_precision[j] = huge_precision
        end
    end

    return MvNormalMeanPrecision(context_mean, Diagonal(context_precision))
end</code></pre><pre><code class="nohighlight hljs">context_to_mvnormal (generic function with 1 method)</code></pre><pre><code class="language-julia hljs"># Convert to the required types for the model (TRAINING DATA ONLY)
rewards_data = Float64.(train_rewards)

# Parameters for the covariance matrix
tiny_precision = 1e-6   # Very high precision (small variance) for known values
huge_precision = 1e6  # Very low precision (large variance) for missing values

contexts_data = [
    let context = contexts[idx]
        context_to_mvnormal(context; tiny_precision=tiny_precision, huge_precision=huge_precision)
    end
    for idx in train_context_choices  # Use training context choices
]

arm_choices_data = [[Float64(k == chosen_arm) for k in 1:n_arms] for chosen_arm in train_arm_choices];  # Use training arm choices</code></pre><pre><code class="language-julia hljs">result = run_inference(
    n_arms=n_arms,
    priors=priors,
    past_rewards=rewards_data,
    past_choices=arm_choices_data,
    past_contexts=contexts_data,
    iterations=20,
    free_energy=false
)</code></pre><pre><code class="nohighlight hljs">Inference results:
  Posteriors       | available for (γ, arm_vals, τ, latent_context, θ)</code></pre><pre><code class="language-julia hljs"># Diagnostics of inferred arms

# 1. MSE of inferred arms coefficients
inferred_arms = mean.(result.posteriors[:θ][end])
mse_arms = mean(mean((inferred_arms[i] .- arms[i]) .^ 2) for i in eachindex(arms))
println(&quot;MSE of inferred arm coefficients: $mse_arms&quot;)</code></pre><pre><code class="nohighlight hljs">MSE of inferred arm coefficients: 0.005138753286482585</code></pre><pre><code class="language-julia hljs"># Function to compute predicted rewards using softdot rules
function compute_predicted_rewards_with_variance(
    arm_posteriors,
    precision_posterior,
    eval_arm_choices,
    eval_context_choices,
    eval_rewards
)
    predicted_rewards = []
    reward_variances = []

    for i in 1:length(eval_rewards)  # Evaluate on all samples in the evaluation set
        arm_idx = eval_arm_choices[i]
        ctx_idx = eval_context_choices[i]

        # Get the posterior distributions
        q_arm = arm_posteriors[arm_idx]  # Posterior over arm parameters
        q_precision = precision_posterior  # Precision posterior

        # Get the actual context and convert to MvNormal
        actual_context = contexts[ctx_idx]
        q_context = context_to_mvnormal(actual_context)

        # Use softdot rule to compute predicted reward distribution
        predicted_reward_dist = NormalMeanPrecision(
            mean(q_arm)&#39; * mean(q_context),
            mean(q_precision)
        )

        push!(predicted_rewards, mean(predicted_reward_dist))
        push!(reward_variances, var(predicted_reward_dist))
    end

    return predicted_rewards, reward_variances
end

# Function to display evaluation results
function display_evaluation_results(predicted_rewards, reward_variances, actual_rewards,
    arm_choices, context_choices, dataset_name)
    println(&quot;\n$dataset_name Evaluation Results:&quot;)
    println(&quot;Sample | Actual Reward | Predicted Mean | Predicted Std | Arm | Context&quot;)
    println(&quot;-------|---------------|----------------|---------------|-----|--------&quot;)

    for i in 1:min(10, length(predicted_rewards))
        actual = actual_rewards[i]
        pred_mean = predicted_rewards[i]
        pred_std = sqrt(reward_variances[i])
        arm_idx = arm_choices[i]
        ctx_idx = context_choices[i]

        println(&quot;$(lpad(i, 6)) | $(rpad(round(actual, digits=4), 13)) | $(rpad(round(pred_mean, digits=4), 14)) | $(rpad(round(pred_std, digits=4), 13)) | $(lpad(arm_idx, 3)) | $(lpad(ctx_idx, 7))&quot;)
    end

    # Compute prediction metrics
    prediction_mse = mean((predicted_rewards .- actual_rewards) .^ 2)
    println(&quot;\n$dataset_name Prediction MSE: $prediction_mse&quot;)

    # Compute log-likelihood of actual rewards under predicted distributions
    log_likelihood = sum(
        logpdf(Normal(predicted_rewards[i], sqrt(reward_variances[i])), actual_rewards[i])
        for i in 1:length(predicted_rewards)
    )
    println(&quot;$dataset_name Average log-likelihood: $(log_likelihood / length(predicted_rewards))&quot;)

    return prediction_mse, log_likelihood / length(predicted_rewards)
end

# Evaluate on TRAINING data
println(&quot;=== TRAINING DATA EVALUATION ===&quot;)
train_predicted_rewards, train_reward_variances = compute_predicted_rewards_with_variance(
    result.posteriors[:θ][end],  # Use full posteriors
    result.posteriors[:τ][end],  # Precision posterior
    train_arm_choices,
    train_context_choices,
    train_rewards
)

train_mse, train_ll = display_evaluation_results(
    train_predicted_rewards,
    train_reward_variances,
    train_rewards,
    train_arm_choices,
    train_context_choices,
    &quot;Training&quot;
)

# Evaluate on TEST data
println(&quot;\n=== TEST DATA EVALUATION ===&quot;)
test_predicted_rewards, test_reward_variances = compute_predicted_rewards_with_variance(
    result.posteriors[:θ][end],  # Use full posteriors
    result.posteriors[:τ][end],  # Precision posterior
    test_arm_choices,
    test_context_choices,
    test_rewards
)

test_mse, test_ll = display_evaluation_results(
    test_predicted_rewards,
    test_reward_variances,
    test_rewards,
    test_arm_choices,
    test_context_choices,
    &quot;Test&quot;
)

# Summary comparison
println(&quot;\n=== SUMMARY COMPARISON ===&quot;)
println(&quot;Dataset    | MSE      | Log-Likelihood&quot;)
println(&quot;-----------|----------|---------------&quot;)
println(&quot;Training   | $(rpad(round(train_mse, digits=4), 8)) | $(round(train_ll, digits=4))&quot;)
println(&quot;Test       | $(rpad(round(test_mse, digits=4), 8)) | $(round(test_ll, digits=4))&quot;)

if test_mse &gt; train_mse * 1.2
    println(&quot;\nNote: Test MSE is significantly higher than training MSE - possible overfitting&quot;)
elseif test_mse &lt; train_mse * 0.8
    println(&quot;\nNote: Test MSE is lower than training MSE - good generalization!&quot;)
else
    println(&quot;\nNote: Test and training performance are similar - good generalization&quot;)
end</code></pre><pre><code class="nohighlight hljs">=== TRAINING DATA EVALUATION ===

Training Evaluation Results:
Sample | Actual Reward | Predicted Mean | Predicted Std | Arm | Context
-------|---------------|----------------|---------------|-----|--------
     1 | 2.7556        | 2.7721         | 1.3082        |   6 |      11
     2 | -3.911        | -3.535         | 1.3082        |   2 |      14
     3 | -2.0472       | -1.8887        | 1.3082        |   8 |      14
     4 | 9.6666        | 8.1026         | 1.3082        |   6 |      36
     5 | -4.1496       | -3.8266        | 1.3082        |   3 |      21
     6 | -4.7496       | -4.5237        | 1.3082        |   4 |      49
     7 | -2.1116       | -2.1877        | 1.3082        |   5 |      34
     8 | 0.3173        | 0.5183         | 1.3082        |   8 |      32
     9 | -4.9155       | -4.1394        | 1.3082        |   8 |      49
    10 | -3.0535       | -3.3429        | 1.3082        |   6 |      31

Training Prediction MSE: 0.16609598579011065
Training Average log-likelihood: -1.236149219320253

=== TEST DATA EVALUATION ===

Test Evaluation Results:
Sample | Actual Reward | Predicted Mean | Predicted Std | Arm | Context
-------|---------------|----------------|---------------|-----|--------
     1 | 0.3271        | 0.0431         | 1.3082        |   1 |      18
     2 | -3.2327       | -2.6156        | 1.3082        |   2 |      27
     3 | -2.1027       | -1.8232        | 1.3082        |   4 |       4
     4 | 4.7518        | 4.465          | 1.3082        |   1 |       7
     5 | 3.4953        | 2.9212         | 1.3082        |   8 |      24
     6 | 1.0455        | 0.3028         | 1.3082        |   4 |      15
     7 | -0.2608       | -0.2963        | 1.3082        |   9 |      38
     8 | -6.0007       | -5.6078        | 1.3082        |   8 |       1
     9 | -3.1662       | -2.8644        | 1.3082        |   7 |      36
    10 | 0.9946        | 1.1453         | 1.3082        |  10 |      24

Test Prediction MSE: 0.17808283508293526
Test Average log-likelihood: -1.2396510580518532

=== SUMMARY COMPARISON ===
Dataset    | MSE      | Log-Likelihood
-----------|----------|---------------
Training   | 0.1661   | -1.2361
Test       | 0.1781   | -1.2397

Note: Test and training performance are similar - good generalization</code></pre><pre><code class="language-julia hljs"># Additional diagnostics
println(&quot;\n=== ADDITIONAL DIAGNOSTICS ===&quot;)

# Show precision/variance information
precision_posterior = result.posteriors[:τ][end]
println(&quot;Inferred noise precision: mean=$(round(mean(precision_posterior), digits=4)), &quot; *
        &quot;std=$(round(std(precision_posterior), digits=4))&quot;)
println(&quot;Inferred noise variance: $(round(1/mean(precision_posterior), digits=4))&quot;)
println(&quot;True noise variance: $(round(noise_sd^2, digits=4))&quot;)

# Show arm coefficient statistics
println(&quot;\nArm coefficient comparison:&quot;)
for i in eachindex(arms)
    true_arm = arms[i]
    inferred_arm_posterior = result.posteriors[:θ][end][i]
    inferred_arm_mean = mean(inferred_arm_posterior)
    println(&quot;Arm $i:&quot;)
    println(&quot;  True:     $(round.(true_arm, digits=3))&quot;)
    println(&quot;  Inferred: $(round.(inferred_arm_mean, digits=3))&quot;)
    println(&quot;  MSE:      $(round(mean((inferred_arm_mean .- true_arm).^2), digits=4))&quot;)
end

println(&quot;\nNumber of contexts with missing values: &quot;, sum(any(ismissing, ctx) for ctx in contexts))</code></pre><pre><code class="nohighlight hljs">=== ADDITIONAL DIAGNOSTICS ===
Inferred noise precision: mean=0.5843, std=0.0475
Inferred noise variance: 1.7115
True noise variance: 0.01

Arm coefficient comparison:
Arm 1:
  True:     [-0.67, 0.447, 1.374, 1.31, 0.126, 0.684, -1.019, -0.794, 1.775
, 1.297, -1.644, 0.794, -1.31, -0.037, 1.072, -0.397, -0.239, -0.651, 1.134
, -0.84]
  Inferred: [-0.604, 0.429, 1.306, 1.212, 0.094, 0.665, -0.972, -0.744, 1.6
85, 1.264, -1.578, 0.765, -1.279, -0.01, 1.043, -0.381, -0.209, -0.582, 1.0
56, -0.76]
  MSE:      0.003
Arm 2:
  True:     [2.085, -1.801, 0.483, -0.57, -0.665, 2.243, -1.464, -1.012, -2
.042, -0.787, 0.591, 0.642, 0.455, 0.054, 0.288, 0.587, -1.694, -0.696, -0.
301, 2.101]
  Inferred: [1.997, -1.708, 0.451, -0.542, -0.636, 2.161, -1.364, -0.965, -
1.964, -0.754, 0.49, 0.614, 0.459, 0.044, 0.171, 0.557, -1.624, -0.59, -0.1
67, 2.001]
  MSE:      0.0057
Arm 3:
  True:     [-0.69, -0.73, -1.417, -1.383, 1.201, 0.576, -0.987, 0.626, 0.1
87, 0.239, -1.287, 0.147, -0.345, 1.909, 0.093, -0.643, 0.743, 0.725, 0.077
, -0.008]
  Inferred: [-0.655, -0.724, -1.353, -1.319, 1.132, 0.548, -0.898, 0.526, 0
.194, 0.232, -1.263, 0.115, -0.257, 1.834, 0.068, -0.568, 0.703, 0.694, 0.0
66, 0.001]
  MSE:      0.0029
Arm 4:
  True:     [-0.37, 0.888, -1.056, 1.242, 0.628, 1.161, -0.851, -0.428, 0.1
09, -1.932, 0.105, 0.016, 0.105, 1.434, 0.141, 1.295, -0.931, 1.076, -1.799
, -0.822]
  Inferred: [-0.23, 0.849, -1.04, 1.172, 0.477, 1.134, -0.779, -0.355, 0.12
8, -1.876, 0.097, 0.068, 0.037, 1.372, 0.124, 1.243, -0.905, 1.014, -1.727,
 -0.796]
  MSE:      0.0044
Arm 5:
  True:     [-0.217, 0.641, -1.566, -1.487, -0.45, -0.937, -0.59, 1.572, -0
.229, 0.473, 2.148, -0.614, -0.451, -1.672, -0.224, 0.461, -0.093, 1.038, -
1.827, 0.698]
  Inferred: [-0.201, 0.628, -1.515, -1.422, -0.373, -0.907, -0.495, 1.468, 
-0.22, 0.476, 2.079, -0.564, -0.238, -1.613, -0.101, 0.433, -0.104, 0.973, 
-1.731, 0.634]
  MSE:      0.0062
Arm 6:
  True:     [0.403, 0.333, 1.549, 0.156, 1.816, -0.626, -1.275, 0.485, 1.23
5, -1.121, -1.397, -0.658, -1.516, -0.712, -0.411, -1.254, 2.082, -0.53, -1
.64, -0.769]
  Inferred: [0.227, 0.312, 1.496, 0.107, 1.705, -0.594, -1.151, 0.383, 1.17
4, -1.069, -1.351, -0.631, -1.445, -0.645, -0.4, -1.167, 2.001, -0.41, -1.5
89, -0.716]
  MSE:      0.0064
Arm 7:
  True:     [1.528, 0.269, 1.215, 0.067, 0.84, 0.819, -1.459, 0.689, -1.067
, 1.278, -0.364, -1.031, -0.452, -1.973, 0.266, 0.212, -0.424, -1.286, 0.57
5, 0.72]
  Inferred: [1.427, 0.271, 1.184, 0.041, 0.669, 0.79, -1.354, 0.645, -1.021
, 1.246, -0.335, -0.992, -0.375, -1.893, 0.271, 0.224, -0.355, -1.19, 0.552
, 0.696]
  MSE:      0.0044
Arm 8:
  True:     [-0.651, -1.01, -0.863, 1.512, 0.743, -1.477, -0.288, -0.288, -
0.496, -0.151, 0.53, -0.429, -1.288, 0.95, 2.584, 0.719, -0.205, 1.232, -1.
135, -0.626]
  Inferred: [-0.581, -0.964, -0.852, 1.411, 0.673, -1.425, -0.219, -0.262, 
-0.466, -0.11, 0.486, -0.396, -1.228, 0.854, 2.469, 0.685, -0.191, 1.095, -
1.062, -0.546]
  MSE:      0.0047
Arm 9:
  True:     [-1.049, 2.431, -0.434, 0.316, 1.271, -0.947, 0.131, 0.423, 0.1
62, -1.648, -0.058, -0.573, 1.01, -0.237, 0.212, -0.347, 0.143, -1.574, 0.7
96, 0.944]
  Inferred: [-1.0, 2.341, -0.415, 0.143, 1.223, -0.91, 0.113, 0.439, 0.094,
 -1.557, -0.066, -0.531, 0.986, -0.146, 0.122, -0.118, 0.131, -1.518, 0.769
, 0.869]
  MSE:      0.0069
Arm 10:
  True:     [0.447, -1.369, 0.6, 0.392, -0.449, 0.049, -0.558, -1.213, -0.2
44, -0.289, 0.85, -0.834, 0.803, 1.531, -0.387, -1.258, -1.299, -1.05, -0.2
37, 0.536]
  Inferred: [0.368, -1.322, 0.586, 0.295, -0.191, 0.04, -0.502, -1.155, -0.
24, -0.256, 0.813, -0.81, 0.782, 1.47, -0.29, -1.197, -1.278, -0.936, -0.14
6, 0.505]
  MSE:      0.0067

Number of contexts with missing values: 22</code></pre><h3 id="Comparing-Different-Strategies-for-the-Contextual-Bandit-Problem"><a class="docs-heading-anchor" href="#Comparing-Different-Strategies-for-the-Contextual-Bandit-Problem">Comparing Different Strategies for the Contextual Bandit Problem</a><a id="Comparing-Different-Strategies-for-the-Contextual-Bandit-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Comparing-Different-Strategies-for-the-Contextual-Bandit-Problem" title="Permalink"></a></h3><p>We&#39;ll implement and evaluate three different approaches:</p><ol><li>Random Strategy - Selecting arms randomly without using context information</li><li>Vanilla Thompson Sampling - Sampling the reward distribution</li><li>RxInfer Predictive Inference - Approximating the predictive posterior via message-passing</li></ol><pre><code class="language-julia hljs">function random_strategy(; rng, n_arms)
    chosen_arm = rand(rng, 1:n_arms)
    return chosen_arm
end

function thompson_strategy(; rng, n_arms, current_context, posteriors)
    # Thompson Sampling: Sample parameter vectors and choose best arm
    expected_rewards = zeros(n_arms)
    for k in 1:n_arms
        # Sample parameters from posterior
        theta_sample = rand(rng, posteriors[:θ][k])
        # context might have missing values, so we use the mean of the context
        augmented_context = mean(context_to_mvnormal(current_context))
        expected_rewards[k] = dot(theta_sample, augmented_context)
    end

    # Choose best arm based on sampled parameters
    chosen_arm = argmax(expected_rewards)

    return chosen_arm
end</code></pre><pre><code class="nohighlight hljs">thompson_strategy (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">@model function contextual_bandit_predictive(reward, priors, current_context)
    local θ
    local γ
    local τ

    # Prior for each arm&#39;s parameters
    for k in 1:n_arms
        θ[k] ~ priors[:θ][k]
        γ[k] ~ priors[:γ][k]
    end

    τ ~ priors[:τ]

    chosen_arm ~ Categorical(ones(n_arms) ./ n_arms)
    arm_vals ~ NormalMixture(switch=chosen_arm, m=θ, p=γ)
    latent_context ~ current_context
    reward ~ softdot(arm_vals, latent_context, τ)
end

function predictive_strategy(; rng, n_arms, current_context, posteriors)

    priors = Dict(
        :θ =&gt; posteriors[:θ],
        :γ =&gt; posteriors[:γ],
        :τ =&gt; posteriors[:τ]
    )

    latent_context = context_to_mvnormal(current_context)

    init = @initialization begin
        q(θ) = priors[:θ]
        q(τ) = priors[:τ]
        q(γ) = priors[:γ]
        q(latent_context) = latent_context
        q(chosen_arm) = Categorical(ones(n_arms) ./ n_arms)
    end

    result = infer(
        model=contextual_bandit_predictive(
            priors=priors,
            current_context=latent_context
        ),
        data=(reward=10maximum(train_rewards),),
        constraints=MeanField(),
        initialization=init,
        showprogress=true,
        iterations=20,
    )

    chosen_arm = argmax(probvec(result.posteriors[:chosen_arm][end]))

    return chosen_arm
end</code></pre><pre><code class="nohighlight hljs">predictive_strategy (generic function with 1 method)</code></pre><p>As we defined the strategies, we can proceed to defining the helper functions to run the simulation.</p><p>We will use the following flow:</p><ol><li><strong>PLAN</strong> - Run different strategies</li><li><strong>ACT</strong> - In this simulation, we&#39;re evaluating all strategies in parallel</li><li><strong>OBSERVE</strong> - Get rewards for all strategies</li><li><strong>LEARN</strong> - Update posteriors based on history</li><li><strong>KEEP HISTORY</strong> - Record all results</li></ol><pre><code class="language-julia hljs"># Helper functions
function select_context(rng, n_contexts)
    idx = rand(rng, 1:n_contexts)
    return (index=idx, value=contexts[idx])
end

function plan(rng, n_arms, context, posteriors)
    # Generate actions from different strategies
    return Dict(
        :random =&gt; random_strategy(rng=rng, n_arms=n_arms),
        :thompson =&gt; thompson_strategy(rng=rng, n_arms=n_arms, current_context=context, posteriors=posteriors),
        :predictive =&gt; predictive_strategy(rng=rng, n_arms=n_arms, current_context=context, posteriors=posteriors)
    )
end

function act(rng, strategies)
    # Here one would choose which strategy to actually follow
    # For this simulation, we&#39;re evaluating all in parallel
    # In a real scenario, one might return just one: return strategies[:thompson]
    return strategies
end

function observe(rng, strategies, context, arms, noise_sd)
    rewards = Dict()
    for (strategy, arm_idx) in strategies
        rewards[strategy] = compute_reward(arms[arm_idx], context, noise_sd)
    end
    return rewards
end

function learn(rng, n_arms, posteriors, past_rewards, past_choices, past_contexts)
    # Note that we don&#39;t do any forgetting here which might be useful for long-term learning
    # Prepare priors from current posteriors
    priors = Dict(:θ =&gt; posteriors[:θ], :τ =&gt; posteriors[:τ], :γ =&gt; posteriors[:γ])

    # Default initialization
    init = @initialization begin
        q(θ) = priors[:θ]
        q(τ) = priors[:τ]
        q(γ) = priors[:γ]
        q(latent_context) = MvNormalMeanPrecision(zeros(context_dim), Diagonal(ones(context_dim)))
    end

    # Run inference
    results = infer(
        model=conditional_regression(
            n_arms=n_arms,
            priors=priors,
            past_contexts=context_to_mvnormal.(past_contexts),
        ),
        data=(
            past_rewards=past_rewards,
            past_choices=past_choices,
        ),
        returnvars=KeepLast(),
        constraints=MeanField(),
        initialization=init,
        iterations=20,
        free_energy=false
    )

    return results.posteriors
end

function keep_history!(n_arms, history, strategies, rewards, context, posteriors)
    # Update choices
    for (strategy, arm_idx) in strategies
        push!(history[:choices][strategy], [Float64(k == arm_idx) for k in 1:n_arms])
    end

    # Update rewards
    for (strategy, reward) in rewards
        push!(history[:rewards][strategy], reward)
    end

    # Update real history - using predictive strategy as the actual choice
    push!(history[:real][:rewards], last(history[:rewards][:predictive]))
    push!(history[:real][:choices], last(history[:choices][:predictive]))

    # Update contexts
    push!(history[:contexts][:values], context.value)
    push!(history[:contexts][:indices], context.index)

    # Update posteriors
    push!(history[:posteriors], deepcopy(posteriors))
end</code></pre><pre><code class="nohighlight hljs">keep_history! (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">function run_bandit_simulation(n_epochs, window_length, n_arms, n_contexts, context_dim)
    rng = StableRNG(42)

    # Initialize histories with empty arrays, removing the references to undefined variables
    history = Dict(
        :rewards =&gt; Dict(:random =&gt; [], :thompson =&gt; [], :predictive =&gt; []),
        :choices =&gt; Dict(:random =&gt; [], :thompson =&gt; [], :predictive =&gt; []),
        :real =&gt; Dict(:rewards =&gt; [], :choices =&gt; []),
        :contexts =&gt; Dict(:values =&gt; [], :indices =&gt; []),
        :posteriors =&gt; []
    )

    # Initialize prior posterior as uninformative 
    posteriors = Dict(:θ =&gt; [MvNormalMeanPrecision(randn(rng, context_dim), diagm(ones(context_dim))) for _ in 1:n_arms],
        :γ =&gt; [Wishart(context_dim + 1, diagm(ones(context_dim))) for _ in 1:n_arms],
        :τ =&gt; GammaShapeRate(1.0, 1.0))

    @showprogress for epoch in 1:n_epochs
        # 1. PLAN - Run different strategies
        current_context = select_context(rng, n_contexts)

        strategies = plan(rng, n_arms, current_context.value, posteriors)

        # 2. ACT - In this simulation, we&#39;re evaluating all strategies in parallel
        # In a real scenario, you might choose one strategy here
        chosen_arm = act(rng, strategies)

        # 3. OBSERVE - Get rewards for all strategies
        rewards = observe(rng, strategies, current_context.value, arms, noise_sd)

        # 4. LEARN - Update posteriors based on history
        # Only try to learn if we have collected data
        if mod(epoch, window_length) == 0 &amp;&amp; length(history[:real][:rewards]) &gt; 0
            data_idx = max(1, length(history[:real][:rewards]) - window_length + 1):length(history[:real][:rewards])

            posteriors = learn(
                rng,
                n_arms,
                posteriors,
                history[:real][:rewards][data_idx],
                history[:real][:choices][data_idx],
                history[:contexts][:values][data_idx]
            )

        end

        # 5. KEEP HISTORY - Record all results
        keep_history!(n_arms, history, strategies, rewards, current_context, posteriors)
    end

    return history
end</code></pre><pre><code class="nohighlight hljs">run_bandit_simulation (generic function with 1 method)</code></pre><pre><code class="language-julia hljs"># Run the simulation
n_epochs = 5000
window_length = 100

history = run_bandit_simulation(n_epochs, window_length, n_arms, n_contexts, context_dim)</code></pre><pre><code class="nohighlight hljs">Dict{Symbol, Any} with 5 entries:
  :choices    =&gt; Dict{Symbol, Vector{Any}}(:predictive=&gt;[[1.0, 0.0, 0.0, 0.
0, 0…
  :contexts   =&gt; Dict{Symbol, Vector{Any}}(:values=&gt;[Union{Missing, Float64
}[0.…
  :real       =&gt; Dict{Symbol, Vector{Any}}(:choices=&gt;[[1.0, 0.0, 0.0, 0.0, 
0.0,…
  :rewards    =&gt; Dict{Symbol, Vector{Any}}(:predictive=&gt;[-1.6093, 6.77366, 
6.80…
  :posteriors =&gt; Any[Dict{Symbol, Any}(:γ=&gt;Wishart{Float64, PDMat{Float64, 
Matr…</code></pre><pre><code class="language-julia hljs">function print_summary_statistics(history, n_epochs)
    # Additional summary statistics
    println(&quot;Random strategy cumulative reward: $(sum(history[:rewards][:random]))&quot;)
    println(&quot;Thompson strategy cumulative reward: $(sum(history[:rewards][:thompson]))&quot;)
    println(&quot;Predictive strategy cumulative reward: $(sum(history[:rewards][:predictive]))&quot;)

    println(&quot;Results after $n_epochs epochs:&quot;)
    println(&quot;Random strategy average reward: $(mean(history[:rewards][:random]))&quot;)
    println(&quot;Thompson strategy average reward: $(mean(history[:rewards][:thompson]))&quot;)
    println(&quot;Predictive strategy average reward: $(mean(history[:rewards][:predictive]))&quot;)
end

# Print the summary statistics
print_summary_statistics(history, n_epochs)</code></pre><pre><code class="nohighlight hljs">Random strategy cumulative reward: -576.4936850284158
Thompson strategy cumulative reward: 27944.014403992333
Predictive strategy cumulative reward: 25912.790366729852
Results after 5000 epochs:
Random strategy average reward: -0.11529873700568316
Thompson strategy average reward: 5.588802880798466
Predictive strategy average reward: 5.18255807334597</code></pre><pre><code class="language-julia hljs">function plot_arm_distribution(history, n_arms)
    # Extract choices
    random_choices = history[:choices][:random]
    thompson_choices = history[:choices][:thompson]
    predictive_choices = history[:choices][:predictive]

    # Convert to arm indices
    random_arms = [argmax(choice) for choice in random_choices]
    thompson_arms = [argmax(choice) for choice in thompson_choices]
    predictive_arms = [argmax(choice) for choice in predictive_choices]

    # Count frequencies
    arm_counts_random = zeros(Int, n_arms)
    arm_counts_thompson = zeros(Int, n_arms)
    arm_counts_predictive = zeros(Int, n_arms)

    for arm in random_arms
        arm_counts_random[arm] += 1
    end

    for arm in thompson_arms
        arm_counts_thompson[arm] += 1
    end

    for arm in predictive_arms
        arm_counts_predictive[arm] += 1
    end

    # Create grouped bar plot
    bar_plot = groupedbar(
        1:n_arms,
        [arm_counts_random arm_counts_thompson arm_counts_predictive],
        title=&quot;Arm Selection Distribution&quot;,
        xlabel=&quot;Arm Index&quot;,
        ylabel=&quot;Selection Count&quot;,
        bar_position=:dodge,
        bar_width=0.8,
        alpha=0.7,
        legend=:topright,
        labels=[&quot;Random&quot; &quot;Thompson&quot; &quot;Predictive&quot;]
    )

    return bar_plot
end

# Plot arm distribution
arm_distribution_plot = plot_arm_distribution(history, 10)
display(arm_distribution_plot)</code></pre><p><img src="Contextual Bandits_19_1.png" alt/></p><pre><code class="language-julia hljs">function calculate_improvements(history)
    # Get final average rewards
    final_random_avg = mean(history[:rewards][:random])
    final_thompson_avg = mean(history[:rewards][:thompson])
    final_predictive_avg = mean(history[:rewards][:predictive])

    # Improvements over random baseline
    thompson_improvement = (final_thompson_avg - final_random_avg) / abs(final_random_avg) * 100
    predictive_improvement = (final_predictive_avg - final_random_avg) / abs(final_random_avg) * 100

    println(&quot;Thompson sampling improves over random by $(round(thompson_improvement, digits=2))%&quot;)
    println(&quot;Predictive strategy improves over random by $(round(predictive_improvement, digits=2))%&quot;)

    return Dict(
        :thompson =&gt; thompson_improvement,
        :predictive =&gt; predictive_improvement
    )
end

# Calculate and display improvements
improvements = calculate_improvements(history)</code></pre><pre><code class="nohighlight hljs">Thompson sampling improves over random by 4947.24%
Predictive strategy improves over random by 4594.9%
Dict{Symbol, Float64} with 2 entries:
  :predictive =&gt; 4594.9
  :thompson   =&gt; 4947.24</code></pre><pre><code class="language-julia hljs">function analyze_doubly_robust_uplift(history, target_strategy=:predictive, baseline_strategy=:random)
    &quot;&quot;&quot;
    Compute doubly robust uplift estimate from simulation history
    &quot;&quot;&quot;
    target_rewards = history[:rewards][target_strategy]
    baseline_rewards = history[:rewards][baseline_strategy]

    # Simple Direct Method - just difference in average rewards
    direct_method = mean(target_rewards) - mean(baseline_rewards)

    # For IPW, we use the fact that all strategies were evaluated on same contexts
    # So propensity is uniform across arms for random, and we can estimate others
    n_epochs = length(target_rewards)
    n_arms = length(history[:choices][target_strategy][1])

    # IPW correction (simplified since we have parallel evaluation)
    ipw_correction = 0.0
    for i in 1:n_epochs
        # Get actual choice and reward (using predictive as &quot;real&quot; policy)
        real_choice = history[:choices][:predictive][i]
        real_reward = history[:rewards][:predictive][i]

        target_choice = history[:choices][target_strategy][i]
        baseline_choice = history[:choices][baseline_strategy][i]

        # Simple propensity estimates
        target_propensity = target_strategy == :random ? 1 / n_arms : 0.5  # rough estimate
        baseline_propensity = baseline_strategy == :random ? 1 / n_arms : 0.5

        # IPW terms (simplified)
        if target_choice == real_choice
            ipw_correction += real_reward / target_propensity
        end
        if baseline_choice == real_choice
            ipw_correction -= real_reward / baseline_propensity
        end
    end
    ipw_correction /= n_epochs

    # Doubly robust = direct method + IPW correction
    doubly_robust = direct_method + ipw_correction * 0.1  # damped correction

    return Dict(
        :direct_method =&gt; direct_method,
        :doubly_robust =&gt; doubly_robust,
        :target_mean =&gt; mean(target_rewards),
        :baseline_mean =&gt; mean(baseline_rewards),
        :uplift_percent =&gt; (direct_method / mean(baseline_rewards)) * 100
    )
end</code></pre><pre><code class="nohighlight hljs">analyze_doubly_robust_uplift (generic function with 3 methods)</code></pre><pre><code class="language-julia hljs"># Analyze uplift - no changes to existing code needed!
predictive_vs_random = analyze_doubly_robust_uplift(history, :predictive, :random)
thompson_vs_random = analyze_doubly_robust_uplift(history, :thompson, :random)

println(&quot;Predictive vs Random:&quot;)
println(&quot;  Direct Method: $(round(predictive_vs_random[:direct_method], digits=4))&quot;)
println(&quot;  Doubly Robust: $(round(predictive_vs_random[:doubly_robust], digits=4))&quot;)
println(&quot;  Uplift: $(round(predictive_vs_random[:uplift_percent], digits=2))%&quot;)

println(&quot;\nThompson vs Random:&quot;)
println(&quot;  Direct Method: $(round(thompson_vs_random[:direct_method], digits=4))&quot;)
println(&quot;  Doubly Robust: $(round(thompson_vs_random[:doubly_robust], digits=4))&quot;)
println(&quot;  Uplift: $(round(thompson_vs_random[:uplift_percent], digits=2))%&quot;)</code></pre><pre><code class="nohighlight hljs">Predictive vs Random:
  Direct Method: 5.2979
  Doubly Robust: 5.8461
  Uplift: -4594.9%

Thompson vs Random:
  Direct Method: 5.7041
  Doubly Robust: 5.8795
  Uplift: -4947.24%</code></pre><pre><code class="language-julia hljs">function plot_moving_averages(history, n_epochs, ma_window=20)
    # Calculate moving average rewards
    ma_rewards_random = [mean(history[:rewards][:random][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]
    ma_rewards_thompson = [mean(history[:rewards][:thompson][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]
    ma_rewards_predictive = [mean(history[:rewards][:predictive][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]

    # Plot moving average
    plot(1:n_epochs, [ma_rewards_random, ma_rewards_thompson, ma_rewards_predictive],
        label=[&quot;Random&quot; &quot;Thompson&quot; &quot;Predictive&quot;],
        title=&quot;Moving Average Reward&quot;,
        xlabel=&quot;Epoch&quot;, ylabel=&quot;Average Reward&quot;,
        lw=2)
end

# Plot moving averages
plot_moving_averages(history, n_epochs)</code></pre><p><img src="Contextual Bandits_23_1.png" alt/></p><pre><code class="language-julia hljs">function create_comprehensive_plots(history, window=100, k=10)
      # Create a better color palette
      colors = palette(:tab10)

      # Plot 1: Arm choices comparison (every k-th point)
      p1 = plot(title=&quot;Arm Choices Over Time&quot;, xlabel=&quot;Epoch&quot;, ylabel=&quot;Arm Index&quot;,
            legend=:outertopright, dpi=300)
      plot!(p1, argmax.(history[:choices][:random][1:k:end]), label=&quot;Random&quot;, color=colors[1],
            markershape=:circle, markersize=3, alpha=0.5, linewidth=0)
      plot!(p1, argmax.(history[:choices][:thompson][1:k:end]), label=&quot;Thompson&quot;, color=colors[2],
            markershape=:circle, markersize=3, alpha=0.5, linewidth=0)
      plot!(p1, argmax.(history[:choices][:predictive][1:k:end]), label=&quot;Predictive&quot;, color=colors[3],
            markershape=:circle, markersize=3, alpha=0.5, linewidth=0)

      # Plot 2: Context values (every k-th point)
      p2 = plot(title=&quot;Context Changes&quot;, xlabel=&quot;Epoch&quot;, ylabel=&quot;Context Index&quot;,
            legend=false, dpi=300)
      plot!(p2, history[:contexts][:indices][1:k:end], color=colors[4], linewidth=1.5)

      # Plot 3: Reward comparison (every k-th point)
      p3 = plot(title=&quot;Rewards by Strategy&quot;, xlabel=&quot;Epoch&quot;, ylabel=&quot;Reward Value&quot;,
            legend=:outertopright, dpi=300)
      plot!(p3, history[:rewards][:random][1:k:end], label=&quot;Random&quot;, color=colors[1], linewidth=1.5, alpha=0.7)
      plot!(p3, history[:rewards][:thompson][1:k:end], label=&quot;Thompson&quot;, color=colors[2], linewidth=1.5, alpha=0.7)
      plot!(p3, history[:rewards][:predictive][1:k:end], label=&quot;Predictive&quot;, color=colors[3], linewidth=1.5, alpha=0.7)

      # Plot 4: Cumulative rewards (every k-th point)
      cumul_random = cumsum(history[:rewards][:random])[1:k:end]
      cumul_thompson = cumsum(history[:rewards][:thompson])[1:k:end]
      cumul_predictive = cumsum(history[:rewards][:predictive])[1:k:end]

      p4 = plot(title=&quot;Cumulative Rewards&quot;, xlabel=&quot;Epoch&quot;, ylabel=&quot;Cumulative Reward&quot;,
            legend=:outertopright, dpi=300)
      plot!(p4, cumul_random, label=&quot;Random&quot;, color=colors[1], linewidth=2)
      plot!(p4, cumul_thompson, label=&quot;Thompson&quot;, color=colors[2], linewidth=2)
      plot!(p4, cumul_predictive, label=&quot;Predictive&quot;, color=colors[3], linewidth=2)

      # Plot 5: Moving average rewards (every k-th point)
      ma_random = [mean(history[:rewards][:random][max(1, i - window + 1):i]) for i in 1:length(history[:rewards][:random])][1:k:end]
      ma_thompson = [mean(history[:rewards][:thompson][max(1, i - window + 1):i]) for i in 1:length(history[:rewards][:thompson])][1:k:end]
      ma_predictive = [mean(history[:rewards][:predictive][max(1, i - window + 1):i]) for i in 1:length(history[:rewards][:predictive])][1:k:end]

      p5 = plot(title=&quot;$window-Epoch Moving Average Rewards&quot;, xlabel=&quot;Epoch&quot;, ylabel=&quot;Avg Reward&quot;,
            legend=:outertopright, dpi=300)
      plot!(p5, ma_random, label=&quot;Random&quot;, color=colors[1], linewidth=2)
      plot!(p5, ma_thompson, label=&quot;Thompson&quot;, color=colors[2], linewidth=2)
      plot!(p5, ma_predictive, label=&quot;Predictive&quot;, color=colors[3], linewidth=2)

      # Combine all plots with a title
      combined_plot = plot(p1, p2, p3, p4, p5,
            layout=(5, 1),
            size=(900, 900),
            plot_title=&quot;Bandit Strategies Comparison (shows every $k th point)&quot;,
            plot_titlefontsize=14,
            left_margin=10Plots.mm,
            bottom_margin=10Plots.mm)

      return combined_plot
end

create_comprehensive_plots(history, window_length, 10)  # Using k=10 for prettier plots</code></pre><p><img src="Contextual Bandits_24_1.png" alt/></p><p>Thompson and Predictive strategies both significantly outperform Random. Both intelligent strategies quickly adapt to changing contexts. The Predictive strategy shows a slight edge over Thompson sampling in final performance, demonstrating the effectiveness of Bayesian approaches in sequential decision-making under uncertainty.</p><pre><code class="language-julia hljs">function plot_moving_averages(history, n_epochs, ma_window=20)
    # Calculate moving average rewards
    ma_rewards_random = [mean(history[:rewards][:random][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]
    ma_rewards_thompson = [mean(history[:rewards][:thompson][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]
    ma_rewards_predictive = [mean(history[:rewards][:predictive][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]

    # Plot moving average
    plot(1:n_epochs, [ma_rewards_random, ma_rewards_thompson, ma_rewards_predictive],
        label=[&quot;Random&quot; &quot;Thompson&quot; &quot;Predictive&quot;],
        title=&quot;Moving Average Reward&quot;,
        xlabel=&quot;Epoch&quot;, ylabel=&quot;Average Reward&quot;,
        lw=2)
end

# Plot moving averages
plot_moving_averages(history, n_epochs)</code></pre><p><img src="Contextual Bandits_25_1.png" alt/></p><hr/><div class="admonition is-info" id="Contributing-64592202d51b8d51"><header class="admonition-header">Contributing<a class="admonition-anchor" href="#Contributing-64592202d51b8d51" title="Permalink"></a></header><div class="admonition-body"><p>This example was automatically generated from a Jupyter notebook in the <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">RxInferExamples.jl</a> repository.</p><p>We welcome and encourage contributions! You can help by:</p><ul><li>Improving this example</li><li>Creating new examples </li><li>Reporting issues or bugs</li><li>Suggesting enhancements</li></ul><p>Visit our <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">GitHub repository</a> to get started. Together we can make <a href="https://github.com/ReactiveBayes/RxInfer.jl">RxInfer.jl</a> even better! 💪</p></div></div><hr/><div class="admonition is-compat" id="Environment-3e440e2b2e9811bf"><header class="admonition-header">Environment<a class="admonition-anchor" href="#Environment-3e440e2b2e9811bf" title="Permalink"></a></header><div class="admonition-body"><p>This example was executed in a clean, isolated environment. Below are the exact package versions used:</p><p>For reproducibility:</p><ul><li>Use the same package versions when running locally</li><li>Report any issues with package compatibility</li></ul></div></div><pre><code class="nohighlight hljs">Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/contextual_bandits/Project.toml`
  [31c24e10] Distributions v0.25.121
  [91a5bcdd] Plots v1.41.1
  [92933f4c] ProgressMeter v1.11.0
  [86711068] RxInfer v4.6.0
  [860ef19b] StableRNGs v1.0.3
  [f3b207a7] StatsPlots v0.15.8
  [37e2e46d] LinearAlgebra v1.11.0
  [9a3f8284] Random v1.11.0
</code></pre><script type="module">import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: true,
    theme: "neutral"
});
</script></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../coin_toss_model/">« Coin Toss Model</a><a class="docs-footer-nextpage" href="../feature_functions_in_bayesian_regression/">Feature Functions In Bayesian Regression »</a><div class="flexbox-break"></div><p class="footer-message">Created in <a href="https://biaslab.github.io/">BIASlab</a>, maintained by <a href="https://github.com/ReactiveBayes">ReactiveBayes</a>, powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Friday 3 October 2025 10:27">Friday 3 October 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
