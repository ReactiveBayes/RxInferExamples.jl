<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Forgetting Factors For Online Inference Â· RxInfer.jl Examples</title><meta name="title" content="Forgetting Factors For Online Inference Â· RxInfer.jl Examples"/><meta property="og:title" content="Forgetting Factors For Online Inference Â· RxInfer.jl Examples"/><meta property="twitter:title" content="Forgetting Factors For Online Inference Â· RxInfer.jl Examples"/><meta name="description" content="Forgetting Factors for Online Inference with RxInfer.jl\nThis example demonstrates online Bayesian inference using forgetting factors to adaptively adjust the observation noise precision over time. \nForgetting factors help the model gradually &quot;forget&quot; old data, allowing it to better track non-stationary processes and adapt to changing noise levels.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="og:description" content="Forgetting Factors for Online Inference with RxInfer.jl\nThis example demonstrates online Bayesian inference using forgetting factors to adaptively adjust the observation noise precision over time. \nForgetting factors help the model gradually &quot;forget&quot; old data, allowing it to better track non-stationary processes and adapt to changing noise levels.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="twitter:description" content="Forgetting Factors for Online Inference with RxInfer.jl\nThis example demonstrates online Bayesian inference using forgetting factors to adaptively adjust the observation noise precision over time. \nForgetting factors help the model gradually &quot;forget&quot; old data, allowing it to better track non-stationary processes and adapt to changing noise levels.\n\nCheck more examples and tutorials at https://examples.rxinfer.com\n"/><meta property="og:url" content="https://examples.rxinfer.com/categories/basic_examples/forgetting_factors_for_online_inference/"/><meta property="twitter:url" content="https://examples.rxinfer.com/categories/basic_examples/forgetting_factors_for_online_inference/"/><link rel="canonical" href="https://examples.rxinfer.com/categories/basic_examples/forgetting_factors_for_online_inference/"/><script async src="https://www.googletagmanager.com/gtag/js?id=G-GMFX620VEP"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-GMFX620VEP', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/theme.css" rel="stylesheet" type="text/css"/><link href="../../../assets/header.css" rel="stylesheet" type="text/css"/><script src="../../../assets/header.js"></script><script src="../../../assets/chat.js"></script><link href="../../../assets/favicon.ico" rel="icon" type="image/x-icon"/>
    <meta property="og:title" content="Forgetting Factors for Online Inference - RxInfer Examples">
    <meta name="description" content="This example demonstrates online Bayesian inference using forgetting factors to adaptively adjust the observation noise precision over time. 
Forgetting factors help the model gradually "forget" old data, allowing it to better track non-stationary processes and adapt to changing noise levels.
">
    <meta property="og:description" content="This example demonstrates online Bayesian inference using forgetting factors to adaptively adjust the observation noise precision over time. 
Forgetting factors help the model gradually "forget" old data, allowing it to better track non-stationary processes and adapt to changing noise levels.
">
    <meta name="keywords" content="rxinfer, julia, bayesian inference, examples, probabilistic programming, message passing, probabilistic numerics, variational inference, belief propagation, online inference, forgetting factors, kalman filter">
    <link rel="sitemap" type="application/xml" title="Sitemap" href="https://examples.rxinfer.com/sitemap.xml">
    </head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.svg" alt="RxInfer.jl Examples logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.svg" alt="RxInfer.jl Examples logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">RxInfer.jl Examples</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><a class="tocitem" href="../../../how_to_contribute/">How to contribute</a></li><li><a class="tocitem" href="../../../autogenerated/list_of_examples/">List of Examples</a></li><li><span class="tocitem">Basic Examples</span><ul><li><a class="tocitem" href="../bayesian_binomial_regression/">Bayesian Binomial Regression</a></li><li><a class="tocitem" href="../bayesian_linear_regression/">Bayesian Linear Regression</a></li><li><a class="tocitem" href="../bayesian_multinomial_regression/">Bayesian Multinomial Regression</a></li><li><a class="tocitem" href="../bayesian_networks/">Bayesian Networks</a></li><li><a class="tocitem" href="../coin_toss_model/">Coin Toss Model</a></li><li><a class="tocitem" href="../contextual_bandits/">Contextual Bandits</a></li><li><a class="tocitem" href="../feature_functions_in_bayesian_regression/">Feature Functions In Bayesian Regression</a></li><li class="is-active"><a class="tocitem" href>Forgetting Factors For Online Inference</a><ul class="internal"><li><a class="tocitem" href="#The-Solution:-Forgetting-Factors"><span>The Solution: Forgetting Factors</span></a></li><li><a class="tocitem" href="#Setting-Up-the-Problem"><span>Setting Up the Problem</span></a></li><li><a class="tocitem" href="#Understanding-the-Generated-Data"><span>Understanding the Generated Data</span></a></li><li><a class="tocitem" href="#Introducing-Forgetting-Factors"><span>Introducing Forgetting Factors</span></a></li></ul></li><li><a class="tocitem" href="../hidden_markov_model/">Hidden Markov Model</a></li><li><a class="tocitem" href="../incomplete_data/">Incomplete Data</a></li><li><a class="tocitem" href="../kalman_filtering_and_smoothing/">Kalman Filtering And Smoothing</a></li><li><a class="tocitem" href="../pomdp_control/">Pomdp Control</a></li><li><a class="tocitem" href="../predicting_bike_rental_demand/">Predicting Bike Rental Demand</a></li></ul></li><li><span class="tocitem">Advanced Examples</span><ul><li><a class="tocitem" href="../../advanced_examples/active_inference_mountain_car/">Active Inference Mountain Car</a></li><li><a class="tocitem" href="../../advanced_examples/advanced_tutorial/">Advanced Tutorial</a></li><li><a class="tocitem" href="../../advanced_examples/assessing_people_skills/">Assessing People Skills</a></li><li><a class="tocitem" href="../../advanced_examples/bayesian_structured_time_series/">Bayesian Structured Time Series</a></li><li><a class="tocitem" href="../../advanced_examples/chance_constraints/">Chance Constraints</a></li><li><a class="tocitem" href="../../advanced_examples/conjugate-computational_variational_message_passing/">Conjugate-Computational Variational Message Passing</a></li><li><a class="tocitem" href="../../advanced_examples/drone_dynamics/">Drone Dynamics</a></li><li><a class="tocitem" href="../../advanced_examples/gp_regression_by_ssm/">Gp Regression By Ssm</a></li><li><a class="tocitem" href="../../advanced_examples/infinite_data_stream/">Infinite Data Stream</a></li><li><a class="tocitem" href="../../advanced_examples/integrating_neural_networks_with_flux.jl/">Integrating Neural Networks With Flux.Jl</a></li><li><a class="tocitem" href="../../advanced_examples/learning_dynamics_with_vaes/">Learning Dynamics With Vaes</a></li><li><a class="tocitem" href="../../advanced_examples/multi-agent_trajectory_planning/">Multi-Agent Trajectory Planning</a></li><li><a class="tocitem" href="../../advanced_examples/nonlinear_sensor_fusion/">Nonlinear Sensor Fusion</a></li><li><a class="tocitem" href="../../advanced_examples/parameter_optimisation_with_optim.jl/">Parameter Optimisation With Optim.Jl</a></li><li><a class="tocitem" href="../../advanced_examples/robotic_arm/">Robotic Arm</a></li></ul></li><li><span class="tocitem">Problem Specific</span><ul><li><a class="tocitem" href="../../problem_specific/autoregressive_models/">Autoregressive Models</a></li><li><a class="tocitem" href="../../problem_specific/gamma_mixture/">Gamma Mixture</a></li><li><a class="tocitem" href="../../problem_specific/gaussian_mixture/">Gaussian Mixture</a></li><li><a class="tocitem" href="../../problem_specific/hierarchical_gaussian_filter/">Hierarchical Gaussian Filter</a></li><li><a class="tocitem" href="../../problem_specific/invertible_neural_network_tutorial/">Invertible Neural Network Tutorial</a></li><li><a class="tocitem" href="../../problem_specific/ising_model/">Ising Model</a></li><li><a class="tocitem" href="../../problem_specific/litter_model/">Litter Model</a></li><li><a class="tocitem" href="../../problem_specific/ode_parameter_estimation/">Ode Parameter Estimation</a></li><li><a class="tocitem" href="../../problem_specific/probit_model/">Probit Model</a></li><li><a class="tocitem" href="../../problem_specific/rts_vs_bifm_smoothing/">Rts Vs Bifm Smoothing</a></li><li><a class="tocitem" href="../../problem_specific/simple_nonlinear_node/">Simple Nonlinear Node</a></li><li><a class="tocitem" href="../../problem_specific/structural_dynamics_with_augmented_kalman_filter/">Structural Dynamics With Augmented Kalman Filter</a></li><li><a class="tocitem" href="../../problem_specific/universal_mixtures/">Universal Mixtures</a></li></ul></li><li><span class="tocitem">Experimental Examples</span><ul><li><a class="tocitem" href="../../experimental_examples/bayesian_trust_learning/">Bayesian Trust Learning</a></li><li><a class="tocitem" href="../../experimental_examples/large_language_models/">Large Language Models</a></li><li><a class="tocitem" href="../../experimental_examples/latent_vector_autoregressive_model/">Latent Vector Autoregressive Model</a></li><li><a class="tocitem" href="../../experimental_examples/recurrent_switching_linear_dynamical_system/">Recurrent Switching Linear Dynamical System</a></li></ul></li><li><a class="tocitem" href="../../../how_build_works/">How we build the examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Basic Examples</a></li><li class="is-active"><a href>Forgetting Factors For Online Inference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Forgetting Factors For Online Inference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInferExamples.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ï‚›</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInferExamples.jl" title="View source on GitHub"><span class="docs-icon fa-solid">ï…œ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><div class="admonition is-info" id="Contributing-baba9dc142ba7ccb"><header class="admonition-header">Contributing<a class="admonition-anchor" href="#Contributing-baba9dc142ba7ccb" title="Permalink"></a></header><div class="admonition-body"><p>This example was automatically generated from a Jupyter notebook in the <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">RxInferExamples.jl</a> repository.</p><p>We welcome and encourage contributions! You can help by:</p><ul><li>Improving this example</li><li>Creating new examples </li><li>Reporting issues or bugs</li><li>Suggesting enhancements</li></ul><p>Visit our <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">GitHub repository</a> to get started. Together we can make <a href="https://github.com/ReactiveBayes/RxInfer.jl">RxInfer.jl</a> even better! ðŸ’ª</p></div></div><hr/><h1 id="Forgetting-Factors-for-Online-Inference"><a class="docs-heading-anchor" href="#Forgetting-Factors-for-Online-Inference">Forgetting Factors for Online Inference</a><a id="Forgetting-Factors-for-Online-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Forgetting-Factors-for-Online-Inference" title="Permalink"></a></h1><p>In this example, we explore a technique for online Bayesian inference which is called <strong>forgetting factors</strong>. When working with real-world data that changes over time, posterior distributions can become &quot;stuck&quot; in the past, leading to poor performance on recent observations. Forgetting factors provide an elegant solution to this problem. Imagine you&#39;re monitoring a sensor network, tracking financial markets, or analyzing social media trends. The underlying dynamics of these systems change over time. This can lead to the following problems when doing online inference:</p><ul><li>The model becomes overly confident in outdated parameters</li><li>Inability to track changing patterns in the data</li><li>Continuing to update parameters that are no longer relevant</li></ul><h2 id="The-Solution:-Forgetting-Factors"><a class="docs-heading-anchor" href="#The-Solution:-Forgetting-Factors">The Solution: Forgetting Factors</a><a id="The-Solution:-Forgetting-Factors-1"></a><a class="docs-heading-anchor-permalink" href="#The-Solution:-Forgetting-Factors" title="Permalink"></a></h2><p>Forgetting factors implement exponential forgetting, where older observations are gradually &quot;forgotten&quot; with a decay rate controlled by a parameter Î“ (gamma). This allows the model to <strong>adapt</strong> to recent changes in the data and <strong>maintain stability</strong> by not forgetting everything at once.</p><p>We will demonstrate the technique with a simple example: a Kalman filter tracking a non-stationary signal.</p><blockquote><p><strong>Note</strong>: For a related example that solves a similar non-stationary tracking problem using a different approach, see the <a href="https://examples.rxinfer.com/categories/problem_specific/hierarchical_gaussian_filter/">Hierarchical Gaussian Filter example</a>. This example specifically focuses on using Forgetting Factors.</p></blockquote><pre><code class="language-julia hljs">using RxInfer, StableRNGs, Plots</code></pre><h2 id="Setting-Up-the-Problem"><a class="docs-heading-anchor" href="#Setting-Up-the-Problem">Setting Up the Problem</a><a id="Setting-Up-the-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-Up-the-Problem" title="Permalink"></a></h2><p>Let&#39;s start by creating a realistic scenario with non-stationary noise. We&#39;ll generate a signal where the observation precision (inverse variance) changes sinusoidally over time, simulating real-world scenarios like varying sensor accuracy or changing market volatility.</p><p><strong>Key Insight</strong>: Notice how the true precision varies significantly over time (from ~20 to ~180). As we show later, a stationary model will fail to capture this variation while the forgetting factor model will adapt to it.</p><pre><code class="language-julia hljs">rng = StableRNG(1234)
time_points = 0.0:0.1:300.0
unstationary_noise = 100 .+ 80 .* sin.(0.075 * time_points)
data_points = map(d -&gt; rand(rng, NormalMeanPrecision(0.0, d[2])), zip(time_points, unstationary_noise))

p1 = plot(time_points, data_points, seriestype = :line, title = &quot;Signal&quot;, xlabel = &quot;Time&quot;, ylabel = &quot;Observation&quot;)
p2 = plot(time_points, unstationary_noise, seriestype = :line, title = &quot;True Precision of Observations&quot;, xlabel = &quot;Time&quot;, ylabel = &quot;Std&quot;)
plot(p1, p2, layout = (2, 1))</code></pre><p><img src="Forgetting Factors for Online Inference_2_1.png" alt/></p><h2 id="Understanding-the-Generated-Data"><a class="docs-heading-anchor" href="#Understanding-the-Generated-Data">Understanding the Generated Data</a><a id="Understanding-the-Generated-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-the-Generated-Data" title="Permalink"></a></h2><p>Let&#39;s examine the data we just generated:</p><p>The top plot shows our observations over time. These are random samples drawn from a normal distribution with mean 0 and a time-varying precision.</p><p>The bottom plot shows how the true precision (inverse variance) of the noise changes over time:</p><ul><li>It oscillates sinusoidally between approximately 20 and 180</li><li>The period of oscillation is relatively slow, allowing us to track the changes</li><li>This simulates real-world scenarios where noise characteristics drift over time</li></ul><p>This setup creates an interesting challenge for our inference: the model needs to adapt its beliefs about the noise precision as the true value changes. A static model would try to find a single &quot;best&quot; precision value, while we want our model to track these changes over time.</p><p>Let&#39;s create a simple model that tracks a non-stationary signal. We&#39;ll use a Kalman filter with a state variable and an unknown noise precision that we&#39;ll try to infer. The model assumes that the state evolves with small random changes (controlled by a fixed precision of 1000.0) and generates observations with unknown precision.</p><pre><code class="language-julia hljs">@model function kalman_filter(observation, state_prior_mean, state_prior_var, obs_noise_shape, obs_noise_rate)
    state_prior ~ Normal(mean = state_prior_mean, var = state_prior_var)
    next_state ~ Normal(mean = state_prior, precision = 1000.0)
    noise_precision ~ Gamma(shape = obs_noise_shape, rate = obs_noise_rate)
    observation ~ Normal(mean = next_state, precision = noise_precision)
end</code></pre><p>Now let&#39;s set up the inference process. First, we&#39;ll create an update rule that specifies how the model parameters should be updated after each observation. This includes updating our beliefs about both the state of the system and the noise precision. We&#39;ll also define an initialization that sets reasonable starting values for our variables.</p><pre><code class="language-julia hljs">autoupdates_without_forgetting = @autoupdates begin
    # Update the state prior mean and variance
    state_prior_mean = mean(q(next_state))
    state_prior_var = var(q(next_state))
    # Update the noise precision
    obs_noise_shape = shape(q(noise_precision))
    obs_noise_rate = rate(q(noise_precision))
end

initialization = @initialization begin
    q(next_state) = NormalMeanPrecision(0.0, 1000.0)
    q(noise_precision) = GammaShapeRate(1.0, 0.005)
end</code></pre><pre><code class="nohighlight hljs">Initial state: 
  q(next_state) = ExponentialFamily.NormalMeanPrecision{Float64}(Î¼=0.0, w=1
000.0)
  q(noise_precision) = ExponentialFamily.GammaShapeRate{Float64}(a=1.0, b=0
.005)</code></pre><p>Let&#39;s run the inference using our model and update rules. We&#39;ll use the <code>infer</code> function with our specified <code>kalman_filter</code> model and <code>autoupdates_without_forgetting</code> update rules. This will process our data points sequentially and track how the model&#39;s beliefs evolve over time.</p><pre><code class="language-julia hljs">result_without_forgetting = infer(
    model = kalman_filter(),
    data = (observation = data_points,),
    autoupdates = autoupdates_without_forgetting,
    initialization = initialization,
    autostart = true,
    constraints = MeanField(),
    historyvars = (noise_precision = KeepLast(), ),
    iterations = 50,
    keephistory = length(data_points)
)

inferred_without_forgetting = result_without_forgetting.history[:noise_precision]

plot(time_points, mean.(inferred_without_forgetting), ribbon = 3std.(inferred_without_forgetting), seriestype = :line, title = &quot;Inferred Noise Precision&quot;, xlabel = &quot;Time&quot;, ylabel = &quot;Std&quot;, label = &quot;Inferred (+/- 3 std)&quot;)
plot!(time_points, unstationary_noise, seriestype = :line, title = &quot;True Precision of Observations&quot;, xlabel = &quot;Time&quot;, ylabel = &quot;Std&quot;, label = &quot;True&quot;)</code></pre><p><img src="Forgetting Factors for Online Inference_5_1.png" alt/></p><p>The plot above shows how our model&#39;s inference of the noise precision compares to the true underlying precision. While the model successfully tracks the general magnitude of the precision, it fails to capture the non-stationary nature of the data. This is because our current inference approach assumes the noise precision is constant over time, leading to estimates that converge to a fixed value rather than adapting to the changing precision levels. This limitation motivates the need for a more flexible approach that can handle non-stationary dynamics.</p><h2 id="Introducing-Forgetting-Factors"><a class="docs-heading-anchor" href="#Introducing-Forgetting-Factors">Introducing Forgetting Factors</a><a id="Introducing-Forgetting-Factors-1"></a><a class="docs-heading-anchor-permalink" href="#Introducing-Forgetting-Factors" title="Permalink"></a></h2><p>To address the non-stationary nature of our data, we introduce a forgetting factor Î“. This factor controls how quickly the model forgets past observations, allowing it to focus more on recent data.</p><p>The forgetting factor Î“ is a parameter that ranges between 0 and 1. A value of Î“ = 1 means the model does not forget anything, while a value of Î“ = 0 means the model forgets everything. We will apply this technique to the parameters of the prior distribution of the noise precision. For this, we will create a callable structure that updates the parameters of the noise precision distribution based on the forgetting factor.</p><pre><code class="language-julia hljs">mutable struct UpdateParamsWithForgetting
    forgetting_factor::Float64
    previous_params::Union{Nothing, Tuple{Float64, Float64}}

    function UpdateParamsWithForgetting(; Î“ = 0.99)
        return new(Î“, nothing)
    end
end

# Create a callable structure
# https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects
# it allows us to have a local state, in this case `previous_params`
function (update::UpdateParamsWithForgetting)(gamma_posterior)

    if isnothing(update.previous_params)
        update.previous_params = (shape(gamma_posterior), rate(gamma_posterior))
        return (shape(gamma_posterior), rate(gamma_posterior))
    else 
        shape_prev, rate_prev = update.previous_params
        shape_current, rate_current = (shape(gamma_posterior), rate(gamma_posterior))
        shape_delta = shape_current - shape_prev
        rate_delta = rate_current - rate_prev

        @assert (shape_delta &gt; 0) &amp;&amp; (rate_delta &gt; 0) &quot;Shape and rate deltas must be strictly positive&quot;
        
        Î“ = update.forgetting_factor

        f_shape = shape_prev * Î“ + shape_delta
        f_rate = rate_prev * Î“ + rate_delta

        update.previous_params = (f_shape, f_rate)
        return (f_shape, f_rate)
    end

end</code></pre><p>Next, we define a function that implements the forgetting factor mechanism in our inference process. The function <code>autoupdates_with_forgetting</code> takes a forgetting factor parameter and creates an update procedure that maintains the state estimates while applying forgetting to the noise precision parameters. It first updates the state prior mean and variance from the current beliefs, then creates an instance of our <code>UpdateParamsWithForgetting</code> structure with the specified forgetting factor. This structure will handle the gradual forgetting of old noise precision information. Finally, it updates the noise precision parameters using this forgetting mechanism, returning both the shape and rate parameters of the gamma distribution.</p><pre><code class="language-julia hljs"># we make it a function because it must create `obs_update` every time it is called
@autoupdates function autoupdates_with_forgetting(; forgetting_factor)
    # Update the state prior mean and variance
    state_prior_mean = mean(q(next_state))
    state_prior_var = var(q(next_state))

    # Update the forgetting factor, callable structure
    obs_update = UpdateParamsWithForgetting(Î“ = forgetting_factor)

    # Update the noise precision, two at once
    obs_noise_shape, obs_noise_rate = obs_update(q(noise_precision))
end</code></pre><pre><code class="nohighlight hljs">autoupdates_with_forgetting (generic function with 1 method)</code></pre><p>Everything is ready to run the inference with the forgetting factor.</p><pre><code class="language-julia hljs">result_with_forgetting = infer(
    model = kalman_filter(),
    data = (observation = data_points,),
    autoupdates = autoupdates_with_forgetting(forgetting_factor = 0.97),
    initialization = initialization,
    autostart = true,
    constraints = MeanField(),
    historyvars = (noise_precision = KeepLast(), ),
    iterations = 100,
    keephistory = length(data_points)
)

inferred_with_forgetting = result_with_forgetting.history[:noise_precision]

plot(time_points, mean.(inferred_with_forgetting), ribbon = 3std.(inferred_with_forgetting), seriestype = :line, title = &quot;Inferred Noise Precision&quot;, xlabel = &quot;Time&quot;, ylabel = &quot;Std&quot;, label = &quot;Inferred (+/- 3 std)&quot;)
plot!(time_points, unstationary_noise, seriestype = :line, title = &quot;True Precision of Observations&quot;, xlabel = &quot;Time&quot;, ylabel = &quot;Precision&quot;, label = &quot;True&quot;)</code></pre><p><img src="Forgetting Factors for Online Inference_8_1.png" alt/></p><p>The plot shows how the forgetting factor mechanism allows the model to adapt to changes in the noise precision over time. The blue line and shaded area represent the inferred noise precision with uncertainty bounds (Â±3 standard deviations), while the orange line shows the true underlying noise precision that was used to generate the data. We can see that the model successfully tracks the changing noise levels, though with some lag due to the forgetting factor. The uncertainty bounds (shaded area) indicate the model&#39;s confidence in its estimates, which varies as it adapts to the non-stationary noise. This demonstrates that our forgetting factor approach effectively handles time-varying noise characteristics in the system.</p><p>The result depends on the choice of the forgetting factor. A higher forgetting factor (closer to 1) means the model has a longer memory and adapts more slowly to changes, while a lower value makes it more responsive but potentially more sensitive to noise. Here we try a forgetting factor of 0.99 instead of the previous 0.97 to see how it affects the inference.</p><pre><code class="language-julia hljs">result_with_forgetting = infer(
    model = kalman_filter(),
    data = (observation = data_points,),
    autoupdates = autoupdates_with_forgetting(forgetting_factor = 0.99),
    initialization = initialization,
    autostart = true,
    constraints = MeanField(),
    historyvars = (noise_precision = KeepLast(), ),
    iterations = 100,
    keephistory = length(data_points)
)

inferred_with_forgetting = result_with_forgetting.history[:noise_precision]

plot(time_points, mean.(inferred_with_forgetting), ribbon = 3std.(inferred_with_forgetting), seriestype = :line, title = &quot;Inferred Noise Precision&quot;, xlabel = &quot;Time&quot;, ylabel = &quot;Std&quot;, label = &quot;Inferred (+/- 3 std)&quot;)
plot!(time_points, unstationary_noise, seriestype = :line, title = &quot;True Precision of Observations&quot;, xlabel = &quot;Time&quot;, ylabel = &quot;Precision&quot;, label = &quot;True&quot;)</code></pre><p><img src="Forgetting Factors for Online Inference_9_1.png" alt/></p><p>With a higher forgetting factor of 0.99, we can observe that the model adapts more slowly to changes in the noise precision compared to the previous case with 0.97. This results in smoother estimates but increased lag in tracking sudden changes. The trade-off between responsiveness and stability is evident - while the 0.99 forgetting factor provides more stable estimates by being less sensitive to temporary fluctuations, it takes longer to adapt to genuine changes in the underlying noise characteristics.</p><p>This example demonstrates how forgetting factors can be effectively used in online inference to handle non-stationary data. By carefully choosing the forgetting factor, we can balance between the model&#39;s ability to retain historical information and its adaptability to changing conditions. This approach is particularly valuable in real-world applications where system characteristics evolve over time and require continuous adaptation of the inference process.</p><hr/><div class="admonition is-info" id="Contributing-baba9dc142ba7ccb"><header class="admonition-header">Contributing<a class="admonition-anchor" href="#Contributing-baba9dc142ba7ccb" title="Permalink"></a></header><div class="admonition-body"><p>This example was automatically generated from a Jupyter notebook in the <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">RxInferExamples.jl</a> repository.</p><p>We welcome and encourage contributions! You can help by:</p><ul><li>Improving this example</li><li>Creating new examples </li><li>Reporting issues or bugs</li><li>Suggesting enhancements</li></ul><p>Visit our <a href="https://github.com/ReactiveBayes/RxInferExamples.jl">GitHub repository</a> to get started. Together we can make <a href="https://github.com/ReactiveBayes/RxInfer.jl">RxInfer.jl</a> even better! ðŸ’ª</p></div></div><hr/><div class="admonition is-compat" id="Environment-ead41e814a894220"><header class="admonition-header">Environment<a class="admonition-anchor" href="#Environment-ead41e814a894220" title="Permalink"></a></header><div class="admonition-body"><p>This example was executed in a clean, isolated environment. Below are the exact package versions used:</p><p>For reproducibility:</p><ul><li>Use the same package versions when running locally</li><li>Report any issues with package compatibility</li></ul></div></div><pre><code class="nohighlight hljs">Status `/tmp/jl_A77yVv/Project.toml`
  [91a5bcdd] Plots v1.41.6
  [86711068] RxInfer v4.7.0
  [860ef19b] StableRNGs v1.0.4
  [9a3f8284] Random v1.11.0
</code></pre><script type="module">import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: true,
    theme: "neutral"
});
</script></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../feature_functions_in_bayesian_regression/">Â« Feature Functions In Bayesian Regression</a><a class="docs-footer-nextpage" href="../hidden_markov_model/">Hidden Markov Model Â»</a><div class="flexbox-break"></div><p class="footer-message">Created in <a href="https://biaslab.github.io/">BIASlab</a>, maintained by <a href="https://github.com/ReactiveBayes">ReactiveBayes</a>, powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.17.0 on <span class="colophon-date" title="Wednesday 25 February 2026 15:00">Wednesday 25 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
