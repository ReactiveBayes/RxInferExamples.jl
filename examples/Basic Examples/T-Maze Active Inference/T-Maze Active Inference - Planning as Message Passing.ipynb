{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19c0a273",
      "metadata": {},
      "source": [
        "# T-Maze Active Inference - Planning as Message Passing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eb13725",
      "metadata": {},
      "source": [
        "This tutorial demonstrates how to implement Active Inference using message passing on a factor graph with RxInfer.jl. We'll build an agent that solves a T-maze navigation task by minimizing Expected Free Energy (EFE) through variational message passing.\n",
        "\n",
        "### What is Active Inference?\n",
        "\n",
        "Active Inference is a framework that unifies perception, planning, and action under a single principle: minimizing variational free energy. Instead of separating \"what do I believe?\" (perception) from \"what should I do?\" (planning), Active Inference treats both as inference problems solved simultaneously through message passing.\n",
        "\n",
        "The key insight is that **planning is inference**: we infer the best actions by treating them as latent variables in a probabilistic model, just like we infer hidden states from observations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b08ba5b",
      "metadata": {},
      "source": [
        "## The T-Maze Problem\n",
        "\n",
        "Our agent starts at the middle of the trunk of a T-shaped maze and must navigate to find a reward that's either on the left or right arm. The challenge:\n",
        "\n",
        "- **Partial observability**: The agent doesn't know where the reward is initially\n",
        "- **Exploration vs. exploitation**: Should it explore to reduce uncertainty or exploit what it knows?\n",
        "- **Planning horizon**: The agent must plan several steps ahead\n",
        "\n",
        "The agent receives:\n",
        "- **Position observations**: Where it currently is\n",
        "- **Reward cues**: In the bottom of the T trunk is a reward cue that tells the agent, in which arm the reward is\n",
        "\n",
        "Let's set up the environment and required packages:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6dc119b",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, we activate the Julia environment and load required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "163bd7f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling RxInfer [86711068-29c9-4ff7-b620-ae75d7495b3d] (cache misses: wrong dep version loaded (12), wrong source (2), mismatched flags (2))\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling IJuliaExt [64482eec-cc57-5312-bea1-9f24eb636db7] (cache misses: wrong dep version loaded (6))\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling IJuliaExt [2f4121a4-3b3a-5ce6-9c5e-1f2673ce168a] (cache misses: wrong dep version loaded (6))\n"
          ]
        }
      ],
      "source": [
        "using RxInfer\n",
        "using Distributions\n",
        "using Plots\n",
        "using LinearAlgebra\n",
        "using Random\n",
        "using ProgressMeter\n",
        "using StableRNGs\n",
        "using ColorSchemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "37a1ad43",
      "metadata": {},
      "outputs": [],
      "source": [
        "### EXAMPLE_HIDDEN_BLOCK_START(Core Types and Configuration) ###\n",
        "\n",
        "\"\"\"\n",
        "    TMazeConfig\n",
        "\n",
        "Configuration for TMaze agent experiments.\n",
        "\n",
        "# Fields\n",
        "- `time_horizon::Int`: Planning horizon for the agent\n",
        "- `n_episodes::Int`: Number of episodes to run\n",
        "- `n_iterations::Int`: Number of inference iterations per step\n",
        "- `wait_time::Float64`: Time to wait between steps (for visualization)\n",
        "- `seed::Int`: Random seed\n",
        "- `experiment_name::String`: Name of the experiment (for saving results)\n",
        "\"\"\"\n",
        "Base.@kwdef struct TMazeConfig\n",
        "    time_horizon::Int\n",
        "    n_episodes::Int\n",
        "    n_iterations::Int\n",
        "    wait_time::Float64\n",
        "    seed::Int\n",
        "    experiment_name::String\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    TMazeBeliefs\n",
        "\n",
        "Container for agent's beliefs about the TMaze environment.\n",
        "\n",
        "# Fields\n",
        "- `location::Categorical{Float64}`: Belief about current location (5 possible states)\n",
        "- `reward_location::Categorical{Float64}`: Belief about reward location (left or right)\n",
        "- `action_posterior::Categorical{Float64}`: Posterior distribution over actions\n",
        "\"\"\"\n",
        "Base.@kwdef mutable struct TMazeBeliefs\n",
        "    location::Categorical{Float64}\n",
        "    reward_location::Categorical{Float64}\n",
        "    action_posterior::Categorical{Float64}\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    initialize_beliefs_tmaze()\n",
        "\n",
        "Initialize agent beliefs for the TMaze environment.\n",
        "\"\"\"\n",
        "function initialize_beliefs_tmaze()\n",
        "    # Initialize with uniform beliefs over states\n",
        "    return TMazeBeliefs(\n",
        "        location=Categorical(fill(1.0 / 5, 5)),\n",
        "        reward_location=Categorical([0.5, 0.5]),\n",
        "        action_posterior=Categorical(fill(0.25, 4))  # Uniform over 4 actions (North, East, South, West)\n",
        "    )\n",
        "end\n",
        "\n",
        "# Direction types for agent actions\n",
        "struct North end\n",
        "struct East end\n",
        "struct South end\n",
        "struct West end\n",
        "\n",
        "const DIRECTIONS = (North(), East(), South(), West())\n",
        "\n",
        "\"\"\"\n",
        "    MazeAgentAction\n",
        "\n",
        "Represents a directional action in the maze.\n",
        "\n",
        "# Fields\n",
        "- `direction`: One of North(), East(), South(), West()\n",
        "\"\"\"\n",
        "struct MazeAgentAction\n",
        "    direction::Union{North,East,South,West}\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "Convert action index to string representation.\n",
        "\"\"\"\n",
        "function action_to_string(idx::Int)\n",
        "    lookup = Dict(1=>\"North\", 2=>\"East\", 3=>\"South\", 4=>\"West\", 5=>\"Stay\")\n",
        "    return get(lookup, idx, \"Unknown\")\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    tmaze_convert_action(next_action::Int)\n",
        "\n",
        "Convert model action index to environment action.\n",
        "Action mapping: 1=North, 2=East, 3=South, 4=West\n",
        "\"\"\"\n",
        "function tmaze_convert_action(next_action::Int)\n",
        "    action_map = Dict(\n",
        "        1 => MazeAgentAction(North()),  # North\n",
        "        2 => MazeAgentAction(East()),   # East\n",
        "        3 => MazeAgentAction(South()),  # South\n",
        "        4 => MazeAgentAction(West())    # West\n",
        "    )\n",
        "    return get(action_map, next_action) do\n",
        "        error(\"Invalid action: $next_action\")\n",
        "    end\n",
        "end\n",
        "\n",
        "tmaze_convert_action(next_action::AbstractVector) = tmaze_convert_action(argmax(next_action))\n",
        "\n",
        "nothing # to suppress the output in the notebook\n",
        "### EXAMPLE_HIDDEN_BLOCK_END ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0cdd8a08",
      "metadata": {},
      "outputs": [],
      "source": [
        "### EXAMPLE_HIDDEN_BLOCK_START(Environment Dynamics and Tensors) ###\n",
        "\"\"\"\n",
        "    TMaze\n",
        "\n",
        "A T-shaped maze environment with 5 cells. The T has a stem cell at the bottom,\n",
        "a middle junction cell, and three cells at the top (left, middle, right).\n",
        "The reward can be either at the top-left or top-right cell.\n",
        "\n",
        "# Fields\n",
        "- `agent_position::Tuple{Int,Int}`: Current agent position as (x,y) tuple\n",
        "- `reward_position::Symbol`: Either :left or :right\n",
        "- `maze_structure::Matrix{UInt8}`: Binary encoding of walls for each cell\n",
        "\"\"\"\n",
        "mutable struct TMaze\n",
        "    agent_position::Tuple{Int,Int}\n",
        "    reward_position::Symbol\n",
        "    maze_structure::Matrix{UInt8}\n",
        "    reward_values::Dict{Tuple{Int,Int},Float64}\n",
        "\n",
        "    function TMaze(reward_position::Symbol=:left)\n",
        "        reward_position in [:left, :right] || throw(ArgumentError(\"reward_position must be :left or :right\"))\n",
        "\n",
        "        # Create the T-maze structure with 5 cells in T shape\n",
        "        # Using a 3×3 grid with T shape\n",
        "        # Binary encoding: North=1, West=2, South=4, East=8\n",
        "        structure = [\n",
        "            0x0B 0x09 0x0A; # Top row of T: left (0x0B), middle (0x09), right (0x0A)\n",
        "            0x0F 0x05 0x0F; # Middle row: only middle cell accessible (0x05)\n",
        "            0x0F 0x01 0x0F  # Bottom row: only middle cell accessible (0x01) - entry point\n",
        "        ]\n",
        "\n",
        "        # Define reward values\n",
        "        reward_values = Dict{Tuple{Int,Int},Float64}()\n",
        "        if reward_position == :left\n",
        "            reward_values[(1, 3)] = 1.0  # Top left with positive reward\n",
        "            reward_values[(3, 3)] = -1.0 # Top right with negative reward\n",
        "        else\n",
        "            reward_values[(1, 3)] = -1.0 # Top left with negative reward\n",
        "            reward_values[(3, 3)] = 1.0  # Top right with positive reward\n",
        "        end\n",
        "\n",
        "        # Start at the bottom of the T\n",
        "        agent_position = (2, 1)\n",
        "\n",
        "        return new(agent_position, reward_position, structure, reward_values)\n",
        "    end\n",
        "\n",
        "    function TMaze(reward_position::Symbol, start_position::Tuple{Int,Int})\n",
        "        reward_position in [:left, :right] || throw(ArgumentError(\"reward_position must be :left or :right\"))\n",
        "\n",
        "        # Validate start position\n",
        "        valid_positions = [(2, 1), (2, 2), (1, 3), (2, 3), (3, 3)]\n",
        "        start_position in valid_positions || throw(ArgumentError(\"Invalid start position. Must be one of: $valid_positions\"))\n",
        "\n",
        "        # Create the T-maze structure with 5 cells in T shape\n",
        "        structure = [\n",
        "            0x0B 0x09 0x0A; # Top row of T: left (0x0B), middle (0x09), right (0x0A)\n",
        "            0x0F 0x05 0x0F; # Middle row: only middle cell accessible (0x05)\n",
        "            0x0F 0x01 0x0F  # Bottom row: only middle cell accessible (0x01) - entry point\n",
        "        ]\n",
        "\n",
        "        # Define reward values\n",
        "        reward_values = Dict{Tuple{Int,Int},Float64}()\n",
        "        if reward_position == :left\n",
        "            reward_values[(1, 3)] = 1.0  # Top left with positive reward\n",
        "            reward_values[(3, 3)] = -1.0 # Top right with negative reward\n",
        "        else\n",
        "            reward_values[(1, 3)] = -1.0 # Top left with negative reward\n",
        "            reward_values[(3, 3)] = 1.0  # Top right with positive reward\n",
        "        end\n",
        "\n",
        "        return new(start_position, reward_position, structure, reward_values)\n",
        "    end\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    create_tmaze(reward_position::Symbol=rand([:left, :right]))\n",
        "\n",
        "Create a T-maze environment with a reward at the specified position.\n",
        "\"\"\"\n",
        "function create_tmaze(reward_position::Symbol=rand([:left, :right]))\n",
        "    return TMaze(reward_position)\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    create_tmaze(reward_position::Symbol, start_position::Tuple{Int,Int})\n",
        "\n",
        "Create a T-maze environment with a reward at the specified position and the agent\n",
        "starting at the specified position.\n",
        "\"\"\"\n",
        "function create_tmaze(reward_position::Symbol, start_position::Tuple{Int,Int})\n",
        "    return TMaze(reward_position, start_position)\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    boundaries(env::TMaze, pos::Tuple{Int,Int})\n",
        "\n",
        "Get the boundary encoding for a position in the maze.\n",
        "\"\"\"\n",
        "function boundaries(env::TMaze, pos::Tuple{Int,Int})\n",
        "    return env.maze_structure[pos[2], pos[1]]\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    step!(env::TMaze, action::MazeAgentAction)\n",
        "\n",
        "Take a step in the T-maze environment with the given action.\n",
        "Returns a tuple of (position_obs, reward_cue, reward).\n",
        "\"\"\"\n",
        "function step!(env::TMaze, action::MazeAgentAction)\n",
        "    # Update agent position based on action\n",
        "    env.agent_position = next_position(env, env.agent_position, action)\n",
        "\n",
        "    # Create observations\n",
        "    position_obs = create_position_observation(env)\n",
        "    reward_cue = create_reward_cue(env)\n",
        "    reward = get_reward(env)\n",
        "\n",
        "    return position_obs, reward_cue, reward\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    create_position_observation(env::TMaze)\n",
        "\n",
        "Create a one-hot encoded vector representing the agent's position.\n",
        "\"\"\"\n",
        "function create_position_observation(env::TMaze)\n",
        "    # Create vector for 5 positions\n",
        "    position_obs = zeros(Float64, 5)\n",
        "    position_idx = position_to_index(env.agent_position)\n",
        "    position_obs[position_idx] = 1.0\n",
        "    return position_obs\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    create_reward_cue(env::TMaze)\n",
        "\n",
        "Create a vector encoding information about the reward location.\n",
        "At the bottom position, it reveals the true reward location.\n",
        "At other positions, it provides uniform uncertainty.\n",
        "\"\"\"\n",
        "function create_reward_cue(env::TMaze)\n",
        "    reward_cue = zeros(Float64, 2)\n",
        "\n",
        "    # Only provide informative cue at the bottom position\n",
        "    if env.agent_position == (2, 1)\n",
        "        if env.reward_position == :left\n",
        "            reward_cue = [1.0, 0.0]  # Left reward\n",
        "        else\n",
        "            reward_cue = [0.0, 1.0]  # Right reward\n",
        "        end\n",
        "    else\n",
        "        # At other positions, provide uniform uncertainty\n",
        "        reward_cue = [0.5, 0.5]\n",
        "    end\n",
        "\n",
        "    return reward_cue\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    get_reward(env::TMaze)\n",
        "\n",
        "Get the reward at the current position, or 0 if no reward.\n",
        "\"\"\"\n",
        "function get_reward(env::TMaze)\n",
        "    # Return the reward at the current position, or 0 if there's no reward here\n",
        "    return get(env.reward_values, env.agent_position, 0.0)\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    next_position(env::TMaze, pos::Tuple{Int,Int}, action::MazeAgentAction)\n",
        "\n",
        "Calculate the next position based on current position and action.\n",
        "\"\"\"\n",
        "function next_position(env::TMaze, pos::Tuple{Int,Int}, action::MazeAgentAction)\n",
        "    # Handle each of the 5 valid positions and their possible movements\n",
        "\n",
        "    # Bottom of T (2,1)\n",
        "    if pos == (2, 1)\n",
        "        if action.direction isa North\n",
        "            return (2, 2)  # Move to middle junction\n",
        "        else\n",
        "            return pos     # Stay in place for all other directions (hitting walls)\n",
        "        end\n",
        "\n",
        "        # Middle junction (2,2)\n",
        "    elseif pos == (2, 2)\n",
        "        if action.direction isa North\n",
        "            return (2, 3)  # Move to top middle\n",
        "        elseif action.direction isa East\n",
        "            return (2, 2)  # Stay in place \n",
        "        elseif action.direction isa South\n",
        "            return (2, 1)  # Move to bottom\n",
        "        elseif action.direction isa West\n",
        "            return (2, 2)  # Stay in place\n",
        "        end\n",
        "\n",
        "        # Top left (1,3)\n",
        "    elseif pos == (1, 3)\n",
        "        if action.direction isa East\n",
        "            return (2, 3)  # Move to top middle\n",
        "        elseif action.direction isa South\n",
        "            return (2, 2)  # Move to middle junction\n",
        "        else\n",
        "            return pos     # Stay in place for other directions (hitting walls)\n",
        "        end\n",
        "\n",
        "        # Top middle (2,3)\n",
        "    elseif pos == (2, 3)\n",
        "        if action.direction isa East\n",
        "            return (3, 3)  # Move to top right\n",
        "        elseif action.direction isa South\n",
        "            return (2, 2)  # Move to middle junction\n",
        "        elseif action.direction isa West\n",
        "            return (1, 3)  # Move to top left\n",
        "        else\n",
        "            return pos     # Stay in place for North (hitting wall)\n",
        "        end\n",
        "\n",
        "        # Top right (3,3)\n",
        "    elseif pos == (3, 3)\n",
        "        if action.direction isa South\n",
        "            return (2, 2)  # Move to middle junction\n",
        "        elseif action.direction isa West\n",
        "            return (2, 3)  # Move to top middle\n",
        "        else\n",
        "            return pos     # Stay in place for other directions (hitting walls)\n",
        "        end\n",
        "\n",
        "        # Should not reach here with valid positions\n",
        "    else\n",
        "        error(\"Invalid position in T-maze: $pos\")\n",
        "    end\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    position_to_index(pos::Tuple{Int,Int})\n",
        "\n",
        "Convert position coordinates to state index (1-5).\n",
        "\"\"\"\n",
        "function position_to_index(pos::Tuple{Int,Int})\n",
        "    # Valid positions in the T-maze\n",
        "    position_mapping = Dict(\n",
        "        (2, 1) => 1,  # Bottom of T\n",
        "        (2, 2) => 2,  # Middle junction\n",
        "        (1, 3) => 3,  # Top left\n",
        "        (2, 3) => 4,  # Top middle\n",
        "        (3, 3) => 5   # Top right\n",
        "    )\n",
        "\n",
        "    if haskey(position_mapping, pos)\n",
        "        return position_mapping[pos]\n",
        "    else\n",
        "        error(\"Invalid T-maze position: $pos\")\n",
        "    end\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    create_reward_observation_tensor()\n",
        "\n",
        "Create the reward observation tensor for the T-maze environment.\n",
        "This tensor has dimensions (2×5×2) representing:\n",
        "- First dimension (2): Observation values [left_prob, right_prob]\n",
        "- Second dimension (5): Agent location states (1-5)\n",
        "- Third dimension (2): Reward location states (1=left, 2=right)\n",
        "\n",
        "Returns a 2×5×2 Float64 tensor.\n",
        "\"\"\"\n",
        "function create_reward_observation_tensor()\n",
        "    # Create reward observation tensor (2×5×2)\n",
        "    # Dimensions: (observation_values, agent_location, reward_location)\n",
        "    reward_obs_tensor = zeros(Float64, 2, 5, 2)\n",
        "\n",
        "    # Fill with default uncertainty [0.5, 0.5] at all non-bottom positions\n",
        "    for loc in 2:5, reward_loc in 1:2\n",
        "        reward_obs_tensor[:, loc, reward_loc] .= 0.5\n",
        "    end\n",
        "\n",
        "    # At bottom position (state 1), reveal true reward location\n",
        "    reward_obs_tensor[:, 1, 1] = [1.0, 0.0]  # Left reward\n",
        "    reward_obs_tensor[:, 1, 2] = [0.0, 1.0]  # Right reward\n",
        "\n",
        "    return reward_obs_tensor\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    create_location_transition_tensor()\n",
        "\n",
        "Create the location transition tensor for the T-maze environment.\n",
        "This tensor has dimensions (5×5×4) representing:\n",
        "- First dimension (5): Next location state\n",
        "- Second dimension (5): Current location state\n",
        "- Third dimension (4): Action (1=North, 2=East, 3=South, 4=West)\n",
        "\n",
        "Returns a 5×5×4 Float64 tensor.\n",
        "\"\"\"\n",
        "function create_location_transition_tensor()\n",
        "    # Create location transition tensor (5×5×4)\n",
        "    # Dimensions: (next_location, current_location, action)\n",
        "    transition_tensor = zeros(Float64, 5, 5, 4)\n",
        "\n",
        "    # Bottom of T (state 1)\n",
        "    transition_tensor[2, 1, 1] = 1.0  # North -> Middle junction\n",
        "    transition_tensor[1, 1, 2] = 1.0  # East -> Stay (wall)\n",
        "    transition_tensor[1, 1, 3] = 1.0  # South -> Stay (wall)\n",
        "    transition_tensor[1, 1, 4] = 1.0  # West -> Stay (wall)\n",
        "\n",
        "    # Middle junction (state 2)\n",
        "    transition_tensor[4, 2, 1] = 1.0  # North -> Top middle\n",
        "    transition_tensor[2, 2, 2] = 1.0  # East -> Stay (wall)\n",
        "    transition_tensor[1, 2, 3] = 1.0  # South -> Bottom\n",
        "    transition_tensor[2, 2, 4] = 1.0  # West -> Stay (wall)\n",
        "\n",
        "    # Top left (state 3)\n",
        "    transition_tensor[3, 3, 1] = 1.0  # North -> Stay (wall)\n",
        "    transition_tensor[4, 3, 2] = 1.0  # East -> Top middle\n",
        "    transition_tensor[2, 3, 3] = 1.0  # South -> Middle junction\n",
        "    transition_tensor[3, 3, 4] = 1.0  # West -> Stay (wall)\n",
        "\n",
        "    # Top middle (state 4)\n",
        "    transition_tensor[4, 4, 1] = 1.0  # North -> Stay (wall)\n",
        "    transition_tensor[5, 4, 2] = 1.0  # East -> Top right\n",
        "    transition_tensor[2, 4, 3] = 1.0  # South -> Middle junction\n",
        "    transition_tensor[3, 4, 4] = 1.0  # West -> Top left\n",
        "\n",
        "    # Top right (state 5)\n",
        "    transition_tensor[5, 5, 1] = 1.0  # North -> Stay (wall)\n",
        "    transition_tensor[5, 5, 2] = 1.0  # East -> Stay (wall)\n",
        "    transition_tensor[2, 5, 3] = 1.0  # South -> Middle junction\n",
        "    transition_tensor[4, 5, 4] = 1.0  # West -> Top middle\n",
        "\n",
        "    return transition_tensor\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    create_reward_to_location_mapping()\n",
        "\n",
        "Create the reward-to-location mapping tensor for the T-maze environment.\n",
        "This tensor has dimensions (5×2) representing:\n",
        "- First dimension (5): Location states\n",
        "- Second dimension (2): Reward location states (1=left, 2=right)\n",
        "\n",
        "Returns a 5×2 Float64 tensor.\n",
        "\"\"\"\n",
        "function create_reward_to_location_mapping()\n",
        "    # Create reward-to-location mapping tensor (5×2)\n",
        "    # Dimensions: (location, reward_location)\n",
        "    reward_mapping = zeros(Float64, 5, 2)\n",
        "\n",
        "    # Left reward (reward_location=1) is at top-left position (state 3)\n",
        "    reward_mapping[3, 1] = 1.0\n",
        "\n",
        "    # Right reward (reward_location=2) is at top-right position (state 5)\n",
        "    reward_mapping[5, 2] = 1.0\n",
        "\n",
        "    return reward_mapping\n",
        "end\n",
        "\n",
        "nothing # to suppress the output in the notebook\n",
        "### EXAMPLE_HIDDEN_BLOCK_END ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "82f3e96a",
      "metadata": {},
      "outputs": [],
      "source": [
        "### EXAMPLE_HIDDEN_BLOCK_START(Visualization and Plotting) ###\n",
        "#### Visualization ####\n",
        "scheme = colorschemes[:Paired_9]\n",
        "\n",
        "\"\"\"\n",
        "    MAZE_THEME\n",
        "\n",
        "A consistent color theme for maze environments.\n",
        "\n",
        "# Fields\n",
        "- `agent`: Color for the agent\n",
        "- `cue`: Color for cue indicators\n",
        "- `reward_positive`: Color for positive rewards\n",
        "- `reward_negative`: Color for negative rewards\n",
        "- `corridor`: Color for corridors/walkable areas\n",
        "- `wall`: Color for walls\n",
        "- `background`: Background color\n",
        "\"\"\"\n",
        "const MAZE_THEME = (\n",
        "    agent=scheme[2],\n",
        "    cue=scheme[7],\n",
        "    reward_positive=scheme[4],\n",
        "    reward_negative=scheme[6],\n",
        "    corridor=:white,\n",
        "    wall=:black,\n",
        "    background=:white\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "    plot_tmaze(env::TMaze)\n",
        "\n",
        "Create a visualization of the TMaze environment.\n",
        "Returns a Plots object showing the T-maze structure with corridors, walls,\n",
        "rewards, and agent position.\n",
        "\"\"\"\n",
        "function plot_tmaze(env::TMaze)\n",
        "    # Create a new plot with a clean appearance\n",
        "    p = Plots.plot(\n",
        "        aspect_ratio=:equal,\n",
        "        legend=false,\n",
        "        axis=false,\n",
        "        grid=false,\n",
        "        ticks=false,\n",
        "        background_color=MAZE_THEME.background,\n",
        "        size=(600, 600),\n",
        "        frame=:none,\n",
        "        margin=0Plots.mm,\n",
        "    )\n",
        "    scale = 20\n",
        "\n",
        "    # Vertical corridor\n",
        "    Plots.plot!(p, [1, 2], [1, 4], color=MAZE_THEME.corridor, linewidth=0, fill=true, fillcolor=MAZE_THEME.corridor)\n",
        "\n",
        "    # Horizontal corridor at top\n",
        "    Plots.plot!(p, [0, 3], [3, 4], color=MAZE_THEME.corridor, linewidth=0, fill=true, fillcolor=MAZE_THEME.corridor)\n",
        "\n",
        "    # Draw the outer walls of the maze (black)\n",
        "    # Vertical corridor walls\n",
        "    Plots.plot!(p, [1, 1], [1, 3], color=MAZE_THEME.wall, linewidth=2)  # Left vertical wall\n",
        "    Plots.plot!(p, [2, 2], [1, 3], color=MAZE_THEME.wall, linewidth=2)  # Right vertical wall\n",
        "\n",
        "    # Horizontal corridor top walls\n",
        "    Plots.plot!(p, [0, 3], [4, 4], color=MAZE_THEME.wall, linewidth=2)  # Top horizontal wall\n",
        "    Plots.plot!(p, [0, 1], [3, 3], color=MAZE_THEME.wall, linewidth=2)  # Bottom left horizontal wall\n",
        "    Plots.plot!(p, [2, 3], [3, 3], color=MAZE_THEME.wall, linewidth=2)  # Bottom right horizontal wall\n",
        "\n",
        "    # Bottom wall\n",
        "    Plots.plot!(p, [1, 2], [1, 1], color=MAZE_THEME.wall, linewidth=2)\n",
        "\n",
        "    # Left and right top walls\n",
        "    Plots.plot!(p, [0, 0], [3, 4], color=MAZE_THEME.wall, linewidth=2)  # Left wall\n",
        "    Plots.plot!(p, [3, 3], [3, 4], color=MAZE_THEME.wall, linewidth=2)  # Right wall\n",
        "\n",
        "    # Draw grid lines between cells\n",
        "    # Horizontal lines\n",
        "    Plots.plot!(p, [1, 2], [2, 2], color=MAZE_THEME.wall, linewidth=0.5, alpha=0.7)  # Bottom to middle\n",
        "    Plots.plot!(p, [1, 2], [3, 3], color=MAZE_THEME.wall, linewidth=0.5, alpha=0.7)  # Middle to top\n",
        "\n",
        "    # Vertical lines at top\n",
        "    Plots.plot!(p, [1, 1], [3, 4], color=MAZE_THEME.wall, linewidth=0.5, alpha=0.7)  # Top to top-left\n",
        "    Plots.plot!(p, [2, 2], [3, 4], color=MAZE_THEME.wall, linewidth=0.5, alpha=0.7)  # Top to top-right\n",
        "\n",
        "    # Draw reward locations with clear indicators\n",
        "    reward_position = env.reward_position\n",
        "\n",
        "    # The left reward location (left arm of the T)\n",
        "    left_color = reward_position == :left ? MAZE_THEME.reward_positive : MAZE_THEME.reward_negative\n",
        "    Plots.scatter!(p, [0.5], [3.5], markersize=ceil(Int, scale), color=left_color, alpha=0.7, markerstrokewidth=ceil(Int, scale / 15))\n",
        "\n",
        "    # The right reward location (right arm of the T)\n",
        "    right_color = reward_position == :right ? MAZE_THEME.reward_positive : MAZE_THEME.reward_negative\n",
        "    Plots.scatter!(p, [2.5], [3.5], markersize=ceil(Int, scale), color=right_color, alpha=0.7, markerstrokewidth=ceil(Int, scale / 15))\n",
        "\n",
        "    # Draw cue location (bottom middle)\n",
        "    Plots.scatter!(p, [1.5], [1.5], markersize=ceil(Int, scale), color=MAZE_THEME.cue, alpha=0.7, markerstrokewidth=ceil(Int, scale / 15))\n",
        "\n",
        "    # Convert agent position to plot coordinates\n",
        "    x, y = 0, 0\n",
        "    agent_pos = env.agent_position\n",
        "\n",
        "    if agent_pos != (2, 1)\n",
        "        Plots.annotate!(p, 1.5, 1.5, Plots.text(\"Cue\", :black, ceil(Int, scale / 2)))\n",
        "    end\n",
        "\n",
        "    if agent_pos == (2, 1)      # Bottom\n",
        "        x, y = 1.5, 1.5\n",
        "    elseif agent_pos == (2, 2)  # Middle\n",
        "        x, y = 1.5, 2.5\n",
        "    elseif agent_pos == (1, 3)  # Top left\n",
        "        x, y = 0.5, 3.5\n",
        "    elseif agent_pos == (2, 3)  # Top middle\n",
        "        x, y = 1.5, 3.5\n",
        "    elseif agent_pos == (3, 3)  # Top right\n",
        "        x, y = 2.5, 3.5\n",
        "    end\n",
        "\n",
        "    # Draw agent as a circle with a black border\n",
        "    Plots.scatter!(p, [x], [y], markersize=ceil(Int, (2 / 3) * scale), color=MAZE_THEME.agent, markerstrokewidth=ceil(Int, scale / 15), markerstrokecolor=MAZE_THEME.wall)\n",
        "    return p\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    plot_reward_location_belief(beliefs::TMazeBeliefs)\n",
        "\n",
        "Plot the belief distribution over reward location (left/right).\n",
        "\"\"\"\n",
        "function plot_reward_location_belief(beliefs::TMazeBeliefs)\n",
        "    probs = probvec(beliefs.reward_location)\n",
        "    p = bar([\"Left\", \"Right\"], probs, \n",
        "            title=\"Reward Location Belief\",\n",
        "            titlefontsize=10, # Decreased font size\n",
        "            ylabel=\"Probability\",\n",
        "            ylims=(0, 1),\n",
        "            color=:blue,\n",
        "            alpha=0.7,\n",
        "            legend=false)\n",
        "    return p\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    plot_action_posterior(beliefs::TMazeBeliefs, step_number::Int)\n",
        "\n",
        "Plot the posterior distribution over actions.\n",
        "\"\"\"\n",
        "function plot_action_posterior(beliefs::TMazeBeliefs, step_number::Int)\n",
        "    probs = probvec(beliefs.action_posterior)\n",
        "    action_names = [\"North\", \"East\", \"South\", \"West\"]\n",
        "    p = bar(action_names, probs,\n",
        "            title=\"Action Posterior (Step: $step_number)\", # Included step number\n",
        "            titlefontsize=10, # Decreased font size\n",
        "            ylabel=\"Probability\",\n",
        "            ylims=(0, 1),\n",
        "            color=:green,\n",
        "            alpha=0.7,\n",
        "            legend=false,\n",
        "            xrotation=45)\n",
        "    return p\n",
        "end\n",
        "\n",
        "nothing # to suppress the output in the notebook\n",
        "### EXAMPLE_HIDDEN_BLOCK_END ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e55a36",
      "metadata": {},
      "source": [
        "### Active Inference as Message Passing\n",
        "\n",
        "In traditional Active Inference, the agent selects actions by minimizing the Expected Free Energy (EFE), which balances two fundamental drives:\n",
        "\n",
        "$$G = \\underbrace{D_{KL}[Q(s|u) || P(s)]}_{\\text{Pragmatic value}} + \\underbrace{\\mathbb{E}_{Q(s|u)}[H[P(o|s)]]}_{\\text{Epistemic value}}$$\n",
        "\n",
        "* **Pragmatic value**: Drives the agent to visit states that align with its preferences (exploiting known rewards).\n",
        "* **Epistemic value**: Drives the agent to seek out informative observations (exploring to reduce uncertainty).\n",
        "\n",
        "Computing the EFE directly often involves evaluating all possible action sequences, which quickly becomes an intractable combinatorial search problem for extended planning horizons.\n",
        "\n",
        "However, following the approach in [\"A Message Passing Realization of Expected Free Energy Minimization\"](https://arxiv.org/abs/2508.02197), we can elegantly bypass this limitation. By reformulating EFE minimization directly as Variational Free Energy (VFE) minimization, we transform the combinatorial search into a tractable inference problem that can be solved using standard variational message passing techniques.\n",
        "\n",
        "We implement this in RxInfer.jl by introducing **two epistemic prior nodes** directly into our generative model's factor graph:\n",
        "\n",
        "1. **Epistemic Action Prior (Exploration Node)**: Encourages actions that maximize information gain and reduce uncertainty about the environment.\n",
        "2. **Epistemic State Prior (Ambiguity Node)**: Penalizes states with high observation noise, encouraging the agent to visit states where observations are reliable and informative.\n",
        "\n",
        "Crucially, in this tutorial, we focus on agents that **already know the dynamics of the environment** (the observation $A$ matrix and transition $B$ matrix). And where the injected epistemic priors are invariant to the agent's current beliefs about the state (see below). Consequently, these priors can be analytically reduced to the entropies of the $A$ and $B$ matrices. This allows us to implement it without requiring complex, belief-dependent iterative updates during inference.\n",
        "\n",
        "Generally these two priors naturally encode the epistemic drive for exploration, allowing our agent to seamlessly balance the acts of reading the cue and seeking the reward — all through reactive message passing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd47891f",
      "metadata": {},
      "source": [
        "## The Factor Graph Model\n",
        "\n",
        "Our model represents the agent's beliefs about:\n",
        "- **Current location**: Where the agent is now\n",
        "- **Reward location**: Whether the reward is left or right (unknown)\n",
        "- **Future locations**: Where the agent will be after taking planned actions\n",
        "- **Actions**: What actions to take at each future timestep\n",
        "\n",
        "The factor graph connects these variables through:\n",
        "- **Transition factors**: How actions change location\n",
        "- **Observation factors**: How locations generate observations\n",
        "- **Epistemic priors**: How uncertainty guides action selection\n",
        "\n",
        "Let's define the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2d5b58d5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "efe_tmaze_agent_initialization (generic function with 1 method)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@model function efe_tmaze_agent(reward_observation_tensor, location_transition_tensor, prior_location, prior_reward_location, reward_to_location_mapping, u_prev, T, reward_cue_observation, location_observation)\n",
        "    old_location ~ prior_location # =beliefs.location\n",
        "    reward_location ~ prior_reward_location # =beliefs.reward_location\n",
        "\n",
        "    current_location ~ DiscreteTransition(old_location, location_transition_tensor, u_prev)\n",
        "    location_observation ~ DiscreteTransition(current_location, diageye(5))\n",
        "    reward_cue_observation ~ DiscreteTransition(current_location, reward_observation_tensor, reward_location) # Reward observation tensor is 2x5x2 (reward location observation x agent location x reward location state)\n",
        "\n",
        "    previous_location = current_location\n",
        "    for t in 1:T\n",
        "        # Epistemic Action Prior (Exploration Node) - theoretically u[t] ~ Categorical([0.25, 0.25, 0.25, 0.25]) \n",
        "        u[t] ~ Categorical(calc_epis_action_prior_vec(reward_cue_observation, location_transition_tensor)) # needed since the other version doesnt get triggered in each VMP iteration: u[t] ~ Categorical([0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "        location[t] ~ DiscreteTransition(previous_location, location_transition_tensor, u[t])\n",
        "        # Epistemic State Prior (Ambiguity Node) - theoretically location[t] ~ Categorical([1/3, 1/6, 1/6, 1/6, 1/6])\n",
        "        location[t] ~ Categorical(calc_epis_loc_prior_vec(reward_observation_tensor))\n",
        "        previous_location = location[t]\n",
        "    end\n",
        "    location[end] ~ DiscreteTransition(reward_location, reward_to_location_mapping) # Reward tensor is 5x2 mapping current belief about reward location (left/right) to T-maze locations (5)\n",
        "end\n",
        "\n",
        "@constraints function efe_tmaze_agent_constraints()\n",
        "end\n",
        "\n",
        "@initialization function efe_tmaze_agent_initialization(prior_location, prior_reward_location, prior_future_locations)\n",
        "    μ(old_location) = prior_location\n",
        "    μ(reward_location) = prior_reward_location\n",
        "    μ(location) = prior_future_locations\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae215ca",
      "metadata": {},
      "source": [
        "### Understanding the Epistemic Priors\n",
        "\n",
        "In Active Inference, epistemic priors compel the agent to resolve uncertainty by targeting informative transitions and observations. In this `rxinfer.jl` implementation, we inject these priors directly into the categorical distributions governing actions (`u[t]`) and states (`location[t]`).\n",
        "\n",
        "Before diving into the mechanics, let's establish a clear notation. We will use capital letters (e.g., $X$) to denote random variables/distributions and lowercase letters (e.g., $x$) for specific samples or states. Furthermore, we use the subscripted entropy operator $H_q(\\cdot)$ to specify that the expectation is taken with respect to the variational distribution $q$.\n",
        "\n",
        "**1. Epistemic Action Prior (Exploration Node)**\n",
        "The action prior formalizes curiosity by favoring actions that maximize the entropy of predicted successor states—areas where the internal model is most uncertain. Assuming the Markov property, the local time-step prior is:\n",
        "\n",
        "$$\\tilde{p}(u_t) \\propto \\exp(H_q(\\mathbf{X}_t \\mid \\mathbf{X}_{t-1}, u_t))$$\n",
        "\n",
        "This conditional entropy evaluates specific actions $u_t$ across all possible previous states:\n",
        "\n",
        "$$H_q(\\mathbf{X}_t \\mid \\mathbf{X}_{t-1}, u_t) = -\\sum_{\\mathbf{x}_{t-1} \\in \\mathcal{X}} q(\\mathbf{x}_{t-1}) H_q(\\mathbf{X}_t \\mid \\mathbf{x}_{t-1}, u_t)$$\n",
        "\n",
        "Because the agent knows the transition matrix ($B$ matrix), the right-hand Shannon entropy term $H_q(\\mathbf{X}_t \\mid \\mathbf{x}_{t-1}, u_t)$ is fixed. If this entropy is **invariant** to the previous state $\\mathbf{x}_{t-1}$ (meaning environmental noise is uniform regardless of location), it becomes a constant $C_{u_t}$ that we can factor out. Since the variational distribution sums to 1 ($\\sum q(\\mathbf{x}_{t-1}) = 1$), the equation simplifies to $H_q(\\mathbf{X}_t \\mid \\mathbf{x}_{t-1}, u_t)$. \n",
        "\n",
        "This invariance allows `calc_epis_action_prior_vec` to skip dynamic dot products with the belief vector $q(\\mathbf{x}_{t-1})$ and efficiently compute the prior entirely from the $B$ matrix.\n",
        "\n",
        "**2. Epistemic State Prior (Ambiguity Node)**\n",
        "The state prior evaluates the information value of specific locations. In the T-maze, the hidden state factorizes into agent location and reward location: $x_t = \\{ x_t^{loc}, x_t^{rew} \\}$. The marginal epistemic value for an agent's location averages over possible reward locations:\n",
        "\n",
        "$$\\tilde{p}(x_t^{loc}) \\propto \\exp(H_q(Y_t \\mid x_t^{loc}, X_t^{rew}))$$\n",
        "\n",
        "Just like the epistemic action prior, this resolves to:\n",
        "\n",
        "$$H_q(Y_t \\mid x_t^{loc}, X_t^{rew}) = - \\sum_{x_t^{rew} \\in \\mathcal{X}^{rew}} q(x_t^{rew}) \\underbrace{H_q(Y_t \\mid x_t^{loc}, x_t^{rew})}_{\\text{Fixed value given } A}$$\n",
        "\n",
        "Since the agent fully knows the observation model (the $A$ matrix), the entropy term  is a fixed value.\n",
        "In our code, `calc_epis_loc_prior_vec` checks if the observation matrix $A$ is invariant to the agent's beliefs about the reward location. If it is invariant, we can safely compute the ambiguity resolution prior upfront — reducing it directly to the entropies of the  matrix — and pass it as a fixed, pre-computed parameter to the Categorical distribution in our `@model` macro.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8a746800",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "calc_epis_loc_prior_vec (generic function with 1 method)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "function compute_invariant_entropy_prior(\n",
        "    A::AbstractArray{T, 3}, \n",
        "    dim_keep::Int, \n",
        "    dim_check::Int; \n",
        "    atol=1e-8, \n",
        "    err_msg=\"Entropy depends on hidden state\"\n",
        ") where T <: Real\n",
        "\n",
        "    # 1. Calculate Entropy Map (Collapse Dim 1)\n",
        "    # Resulting H has dimensions (size(A, 2), size(A, 3))\n",
        "    dim_2, dim_3 = size(A, 2), size(A, 3)\n",
        "    H = zeros(T, dim_2, dim_3)\n",
        "\n",
        "    for k in 1:dim_3\n",
        "        for j in 1:dim_2\n",
        "            # Calculate negative entropy: Σ p * log(p)\n",
        "            # We use a view to avoid allocation\n",
        "            dist = @view A[:, j, k]\n",
        "            val = sum(p * log(max(p, 1e-12)) for p in dist)\n",
        "            H[j, k] = val\n",
        "        end\n",
        "    end\n",
        "\n",
        "    # Determine sizes for the output vector\n",
        "    # Note: H is 2D. \n",
        "    # If dim_keep is 2 (User's Logic for Location), we map it to dim 1 of H.\n",
        "    # If dim_keep is 3 (User's Logic for Action), we map it to dim 2 of H.\n",
        "    \n",
        "    # Map input tensor dims (2,3) to H dims (1,2)\n",
        "    h_keep_dim = (dim_keep == 2) ? 1 : 2\n",
        "    h_check_dim = (dim_check == 2) ? 1 : 2\n",
        "    \n",
        "    n_items = size(H, h_keep_dim)\n",
        "    entropy_vec = zeros(T, n_items)\n",
        "\n",
        "    # 2. Check Uniformity and Reduce\n",
        "    for i in 1:n_items\n",
        "        # slice = H[i, :] if keeping rows, or H[:, i] if keeping cols\n",
        "        # selectdim(A, d, i) fixes dimension d to index i\n",
        "        slice = selectdim(H, h_keep_dim, i)\n",
        "        first_h = slice[1]\n",
        "        \n",
        "        # Verify that entropy is invariant along the check dimension\n",
        "        if !all(x -> isapprox(x, first_h; atol=atol), slice)\n",
        "            throw(ArgumentError(err_msg))\n",
        "        end\n",
        "        entropy_vec[i] = first_h\n",
        "    end\n",
        "\n",
        "    # 3. Softmax with Log-Sum-Exp stability\n",
        "    exp_vals = exp.(entropy_vec .- maximum(entropy_vec))\n",
        "    return exp_vals ./ sum(exp_vals)\n",
        "end\n",
        "\n",
        "function calc_epis_action_prior_vec(message_trigger, B_matrix::AbstractArray{T, 3}; atol=1e-8) where T <: Real\n",
        "    return compute_invariant_entropy_prior(\n",
        "        B_matrix, \n",
        "        3, # Keep Dimension (Action/k)\n",
        "        2; # Check Dimension (Prev State/j)\n",
        "        atol=atol,\n",
        "        err_msg=\"Transition entropy depends on q(x_{t-1}), which RxInfer cannot currently handle.\"\n",
        "    )\n",
        "end\n",
        "\n",
        "function calc_epis_loc_prior_vec(A_matrix::AbstractArray{T, 3}; atol=1e-8) where T <: Real\n",
        "    return compute_invariant_entropy_prior(\n",
        "        A_matrix,\n",
        "        2, # Keep Dimension (Agent Location)\n",
        "        3; # Check/Marginalize Dimension (Reward Location)\n",
        "        atol=atol,\n",
        "        err_msg=\"Observation entropy depends on q(reward_location), which RxInfer cannot currently handle.\"\n",
        "    )\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd19a4c4",
      "metadata": {},
      "source": [
        "## Inference and Action Selection\n",
        "\n",
        "During inference, variational message passing updates beliefs about:\n",
        "- Current and future locations\n",
        "- Reward location\n",
        "- Optimal actions\n",
        "\n",
        "The epistemic priors guide the inference toward actions and states that reduce uncertainty. After inference, we select the action with highest posterior probability.\n",
        "\n",
        "Let's define the inference function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "94f45849",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "get_initialization_tmaze (generic function with 2 methods)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Initialize beliefs for inference, either from scratch or from previous results.\n",
        "This helps warm-start the inference procedure for faster convergence.\n",
        "\"\"\"\n",
        "function get_initialization_tmaze(initialization_fn, beliefs, previous_result::Nothing)\n",
        "    future_location_beliefs = vague(Categorical, 5)\n",
        "    return initialization_fn(beliefs.location, beliefs.reward_location, future_location_beliefs)\n",
        "end\n",
        "\n",
        "function get_initialization_tmaze(initialization_fn, beliefs, previous_result)\n",
        "    # Use posteriors from last VMP iteration to initialize next inference\n",
        "    current_location_belief = last(previous_result.posteriors[:location])[1]\n",
        "    future_location_beliefs = last(previous_result.posteriors[:location])[2:end]\n",
        "    reward_location_belief = last(previous_result.posteriors[:reward_location])\n",
        "    return initialization_fn(current_location_belief, reward_location_belief, future_location_beliefs)\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a000f7ea",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "execute_step_tmaze"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Execute a single inference step to determine the next action.\n",
        "Message passing updates beliefs about states and actions simultaneously.\n",
        "\"\"\"\n",
        "function execute_step_tmaze(\n",
        "    env, position_obs, reward_cue, beliefs, model, tensors, config, callbacks, \n",
        "    time_remaining, previous_result, previous_action;\n",
        "    constraints_fn, initialization_fn, inference_kwargs...\n",
        ")\n",
        "    # Convert previous action to one-hot encoding\n",
        "    previous_action_vec = Float64.(typeof(previous_action.direction) .== [North, East, South, West])\n",
        "\n",
        "    # Initialize inference from previous results (warm-start)\n",
        "    initialization = get_initialization_tmaze(initialization_fn, beliefs, previous_result)\n",
        "\n",
        "    # Run variational message passing inference\n",
        "    result = infer(\n",
        "        model=model(\n",
        "            reward_observation_tensor=tensors.reward_observation,\n",
        "            location_transition_tensor=tensors.location_transition,\n",
        "            prior_location=beliefs.location,\n",
        "            prior_reward_location=beliefs.reward_location,\n",
        "            reward_to_location_mapping=tensors.reward_to_location,\n",
        "            u_prev=previous_action_vec,\n",
        "            T=time_remaining\n",
        "        ),\n",
        "        data=(\n",
        "            location_observation=position_obs,\n",
        "            reward_cue_observation=reward_cue\n",
        "        ),\n",
        "        constraints=constraints_fn(),\n",
        "        callbacks=callbacks,\n",
        "        iterations=config.n_iterations,\n",
        "        initialization=initialization;\n",
        "        inference_kwargs...\n",
        "    )\n",
        "\n",
        "    # Select action with highest posterior probability\n",
        "    next_action_idx = Int(mode(first(last(result.posteriors[:u]))))\n",
        "    next_action = tmaze_convert_action(next_action_idx)\n",
        "\n",
        "    # Update beliefs based on inference results\n",
        "    beliefs.location = last(result.posteriors[:current_location])\n",
        "    beliefs.reward_location = last(result.posteriors[:reward_location])\n",
        "    beliefs.action_posterior = first(last(result.posteriors[:u]))\n",
        "\n",
        "    return next_action_idx, next_action, result\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f7f437",
      "metadata": {},
      "source": [
        "## Running the Agent\n",
        "\n",
        "Now let's set up the environment and run the agent. We'll create the required tensors (transition tensor B, observation tensor A) and configure the agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f1b3362b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TMazeConfig(4, 3, 1, 0.2, 42, \"efe_tmaze_demo\")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create transition and observation tensors\n",
        "tensors = (\n",
        "    reward_observation = create_reward_observation_tensor(), # A matrix\n",
        "    location_transition = create_location_transition_tensor(), # B matrix\n",
        "    reward_to_location = create_reward_to_location_mapping()\n",
        ")\n",
        "\n",
        "# Configure agent: plan 4 steps ahead\n",
        "config = TMazeConfig(\n",
        "    time_horizon = 4,          # Planning horizon\n",
        "    n_episodes = 3,            # Number of episodes\n",
        "    n_iterations = 1,         # VMP iterations per step\n",
        "    wait_time = 0.2,          # Visualization delay for gif generation\n",
        "    seed = 42,                 # Random seed\n",
        "    experiment_name = \"efe_tmaze_demo\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f5bfb4",
      "metadata": {},
      "source": [
        "Now let's run the agent and visualize its behavior. The agent will:\n",
        "1. Observe its current position and reward cues\n",
        "2. Perform inference to plan actions\n",
        "3. Execute the planned action\n",
        "4. Update beliefs based on new observations\n",
        "\n",
        "The visualization shows:\n",
        "- **Maze state**: Current agent position and reward location\n",
        "- **Reward belief**: Probability that reward is left vs. right\n",
        "- **Action posterior**: Probability distribution over actions\n",
        "\n",
        "We'll use a helper function to run the agent and generate an animated visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "33883b66",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "run_and_record_tmaze_gif"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Run the agent and record its behavior as an animated GIF.\n",
        "Shows the agent's beliefs and planned actions at each step.\n",
        "\"\"\"\n",
        "function run_and_record_tmaze_gif(model, tensors, config, seed; filename=\"tmaze_agent.gif\", kwargs...)\n",
        "    # Setup environment and agent\n",
        "    rng = StableRNG(seed)\n",
        "    reward_position = rand(rng, [:left, :right])\n",
        "    env = create_tmaze(reward_position, (2, 2)) \n",
        "    beliefs = initialize_beliefs_tmaze()\n",
        "    \n",
        "    previous_result = nothing\n",
        "    next_action = MazeAgentAction(East()) # Initial placeholder\n",
        "    next_action_idx = 2\n",
        "\n",
        "    # Initial observations\n",
        "    position_obs = convert.(Float64, create_position_observation(env))\n",
        "    reward_cue = convert.(Float64, create_reward_cue(env))\n",
        "    reward = 0.0\n",
        "\n",
        "    println(\"Recording T-Maze episode (Reward: $reward_position) to $filename...\")\n",
        "\n",
        "    # Animation loop\n",
        "    reached_goal = false\n",
        "    stop_next = false\n",
        "    current_step = 0\n",
        "    \n",
        "    anim = @animate for t in config.time_horizon:-1:-1\n",
        "        if stop_next\n",
        "            break\n",
        "        end\n",
        "\n",
        "        if reached_goal\n",
        "            # Final frame: goal reached\n",
        "            p_maze = plot_tmaze(env)\n",
        "            title!(p_maze, \"Goal Reached!\\nReward Loc: $reward_position | Reward: $reward\", titlefontsize=10)\n",
        "            \n",
        "            p_reward = plot_reward_location_belief(beliefs)\n",
        "            p_action = plot_action_posterior(beliefs, current_step)\n",
        "            \n",
        "            p = plot(p_maze, p_reward, p_action, \n",
        "                    layout=@layout([a{0.6w} [b; c]]),\n",
        "                    size=(500, 300))\n",
        "            \n",
        "            stop_next = true\n",
        "            \n",
        "        else\n",
        "            # Normal steps: perform inference and visualize\n",
        "            current_step = config.time_horizon - t + 1\n",
        "            \n",
        "            next_action_idx, next_action, result = execute_step_tmaze(\n",
        "                env, position_obs, reward_cue, beliefs, model, tensors, config,\n",
        "                nothing, t + 1, previous_result, next_action; \n",
        "                kwargs...\n",
        "            )\n",
        "            previous_result = result\n",
        "            action_str = action_to_string(next_action_idx)\n",
        "            \n",
        "            # Create visualization before executing action\n",
        "            p_maze = plot_tmaze(env)\n",
        "            title!(p_maze, \"Step: $current_step | Planned Action: $action_str\\nReward Loc: $reward_position | Prev Reward: $reward\", titlefontsize=10)\n",
        "            \n",
        "            p_reward = plot_reward_location_belief(beliefs)\n",
        "            p_action = plot_action_posterior(beliefs, current_step)\n",
        "            \n",
        "            p = plot(p_maze, p_reward, p_action,\n",
        "                    layout=@layout([a{0.6w} [b; c]]),\n",
        "                    size=(500, 300))\n",
        "            \n",
        "            # Execute action\n",
        "            position_obs, reward_cue, reward = step!(env, next_action)\n",
        "            position_obs = convert.(Float64, position_obs)\n",
        "            reward_cue = convert.(Float64, reward_cue)\n",
        "            \n",
        "            if reward == 1\n",
        "                reached_goal = true\n",
        "            end\n",
        "        end\n",
        "        \n",
        "        sleep(config.wait_time)\n",
        "        p \n",
        "    end\n",
        "\n",
        "    return gif(anim, filename, fps = 1)\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b8ec27dc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording T-Maze episode (Reward: right) to tmaze_agent.gif...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaved animation to c:\\Users\\BMW\\Documents\\activeInference\\rxinfer\\RxInferExamples.jl\\examples\\Basic Examples\\T-Maze Active Inference\\tmaze_agent.gif\n"
          ]
        }
      ],
      "source": [
        "# Run the agent and generate visualization\n",
        "run_and_record_tmaze_gif(\n",
        "    efe_tmaze_agent, \n",
        "    tensors, \n",
        "    config, \n",
        "    config.seed; \n",
        "    constraints_fn = efe_tmaze_agent_constraints, \n",
        "    initialization_fn = efe_tmaze_agent_initialization\n",
        ")\n",
        "nothing # supress double visual output in .ipynb file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83bd889f",
      "metadata": {},
      "source": [
        "![](tmaze_agent.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9902bd72",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This tutorial demonstrated how to implement Active Inference using message passing in RxInfer.jl:\n",
        "\n",
        "1. **Model Structure**: We built a factor graph that represents beliefs about states, actions, and observations\n",
        "2. **Epistemic Priors**: Two prior nodes encode the epistemic value:\n",
        "   - Action prior encourages exploration\n",
        "   - State prior encourages visiting informative locations\n",
        "3. **Message Passing**: Variational message passing simultaneously updates beliefs about states and actions\n",
        "4. **Action Selection**: Actions are selected by taking the mode of the action posterior\n",
        "\n",
        "The key insight is that **planning is inference**: by treating actions as latent variables in a probabilistic model, we can use standard inference algorithms to solve planning problems. The epistemic priors naturally encode the exploration-exploitation tradeoff through information-theoretic measures.\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- [\"A Message Passing Realization of Expected Free Energy Minimization\"](https://arxiv.org/abs/2508.02197) - The paper describing this approach for known parameters\n",
        "- [\"Expected Free Energy-based Planning as Variational Inference\"](https://arxiv.org/abs/2504.14898) - The foundational (theoretical) paper introducing EFE as VFE\n",
        "- [\"Active Inference is a Subtype of Variational Inference\"](https://arxiv.org/abs/2511.18955) - Further work, including parameter learning"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.11.3",
      "language": "julia",
      "name": "julia-1.11"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
