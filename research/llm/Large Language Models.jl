# This file was automatically generated from /Users/4d/Documents/GitHub/RxInferExamples.jl/examples/Experimental Examples/Large Language Models/Large Language Models.ipynb
# by notebooks_to_scripts.jl at 2025-08-18T16:08:28.628
# Do not edit by hand. Edit the notebook instead.
#
# Source notebook: Large Language Models.ipynb

using RxInfer, OpenAI, JSON

# Your OpenAI API key should be set in the environment variables
secret_key = ENV["OPENAI_KEY"];
llm_model = "gpt-4o-mini-2024-07-18";

observations = [
    "RxInfer.jl is confusing and frustrating to use. I wouldn't recommend it.",
    "RxInfer.jl made my Bayesian modeling workflow much easier and more efficient!",
    "Absolutely love RxInfer.jl! It's revolutionized my approach to probabilistic programming.",
    "I gave RxInfer.jl a try, but it just doesn't work for my needs at all.",
    "I prefer apples over oranges."  # üçé Wait, this one's different...
];

@model function language_mixture_model(c, context‚ÇÅ, context‚ÇÇ, task‚ÇÅ, task‚ÇÇ, likelihood_task)
    # Mixture probability (how much of each sentiment type)
    s ~ Beta(1.0, 1.0)
    
    # Two sentiment clusters with LLM-generated priors
    m[1] ~ LLMPrior(context‚ÇÅ, task‚ÇÅ)  # Negative sentiment prior
    w[1] ~ Gamma(shape = 0.01, rate = 0.01)
    
    m[2] ~ LLMPrior(context‚ÇÇ, task‚ÇÇ)  # Positive sentiment prior  
    w[2] ~ Gamma(shape = 0.01, rate = 0.01)
    
    for i in eachindex(c)
        z[i] ~ Bernoulli(s)  # Cluster assignment (0=negative, 1=positive)
        y[i] ~ NormalMixture(switch = z[i], m = m, p = w)  # Latent sentiment score
        c[i] ~ LLMObservation(y[i], likelihood_task)  # Observed text
    end
end

"""
    LLMPrior

Node that represents an LLM's prior beliefs about latent variables based on contextual information.
The LLM interprets the context and task to produce a probability distribution as a prior.

# Interfaces
- `belief` (b): Output distribution representing the LLM's prior belief
- `context` (c): Input text providing context for the prior
- `task` (t): Input text describing what distribution to generate
"""
struct LLMPrior end

@node LLMPrior Stochastic [ (b, aliases = [belief]), (c, aliases = [context]), (t, aliases = [task]) ]

@rule LLMPrior(:b, Marginalisation) (q_c::PointMass{<:String}, q_t::PointMass{<:String}) = begin
    # Build the conversation with the LLM
    messages = [
        Dict("role" => "system",
             "content" => """
                 You are an expert analyst who maps contextual cues to a
                 Normal(mean, variance) distribution.

                 ‚Ä¢ Think step-by-step internally.
                 ‚Ä¢ **Only** output a JSON object that conforms to the schema below.
                 ‚Ä¢ Do not wrap the JSON in markdown fences or add extra keys.
             """),

        Dict("role" => "assistant",
             "content" => """
                 ## CONTEXT
                 $(q_c.point)
             """),

        Dict("role" => "user",
             "content" => """
                 ## TASK
                 $(q_t.point)

                 Using the context above, infer a Normal distribution and return:
                   "analysis"  ‚Äì brief rationale (‚â§ 100 words)
                   "mean"      ‚Äì number in [0, 10]
                   "variance"  ‚Äì number in [1, 100]
             """)
    ]

    # Define strict JSON schema for consistent responses
    response_schema = Dict(
        "type" => "json_schema",
        "json_schema" => Dict(
            "name"   => "normal_estimate",
            "schema" => Dict(
                "type"       => "object",
                "properties" => Dict(
                    "analysis" => Dict("type" => "string"),
                    "mean"     => Dict("type" => "number", "minimum" => 0, "maximum" => 10),
                    "variance" => Dict("type" => "number", "minimum" => 1, "maximum" => 100)
                ),
                "required" => ["analysis", "mean", "variance"],
                "additionalProperties" => false
            )
        )
    )

    # Call the LLM and parse the response
    r = create_chat(secret_key, llm_model, messages; response_format = response_schema)
    obj = JSON.parse(r.response[:choices][1][:message][:content])

    return NormalMeanVariance(obj["mean"], obj["variance"])
end

"""
    LLMObservation

Node that represents an LLM's observation of data based on a latent belief and task description.
The LLM takes a latent belief and task description to produce corresponding observed data.

# Interfaces
- `out`: Output observation data generated by the LLM
- `belief` (b): Input latent variable/distribution that influences the observation
- `task` (t): Input text describing how to generate observations from beliefs
"""
struct LLMObservation end

@node LLMObservation Stochastic [ out, (b, aliases = [belief]), (t, aliases = [task]) ]

@rule LLMObservation(:b, Marginalisation) (q_out::PointMass{<:String}, q_t::PointMass{<:String}) = begin
    messages = [
        Dict("role" => "system",
             "content" => """
                 You are **LLMObservation**, a senior evaluator who maps a text to
                 a Normal(mean, variance) distribution.

                 ‚Ä¢ Think step-by-step internally, but **only** output a JSON object
                   that conforms to the provided schema.
                 ‚Ä¢ Do not wrap the JSON in markdown fences or add extra keys.
             """),

        Dict("role" => "assistant",
             "content" => """
                 ## TEXT
                 $(q_out.point)
             """),

        Dict("role" => "user",
             "content" => """
                 ## TASK
                 $(q_t.point)

                 Using the text above, infer a Gaussian distribution.
                 Return a JSON object with keys:
                   "analysis"  ‚Äì ‚â§ 100 words explaining your reasoning
                   "mean"      ‚Äì number in [0, 10]
                   "variance"  ‚Äì number in [0.1, 100]
             """)
    ]

    response_schema = Dict(
        "type" => "json_schema",
        "json_schema" => Dict(
            "name"   => "normal_estimate",
            "schema" => Dict(
                "type"       => "object",
                "properties" => Dict(
                    "analysis" => Dict("type" => "string"),
                    "mean"     => Dict("type" => "number", "minimum" => 0, "maximum" => 10),
                    "variance" => Dict("type" => "number", "minimum" => 0.1, "maximum" => 100)
                ),
                "required" => ["analysis", "mean", "variance"],
                "additionalProperties" => false
            )
        )
    )

    r = create_chat(secret_key, llm_model, messages; response_format = response_schema)
    obj = JSON.parse(r.response[:choices][1][:message][:content])

    return NormalMeanVariance(obj["mean"], obj["variance"])
end

# Priors
context‚ÇÅ = "RxInfer.jl is absolutely terrible."
context‚ÇÇ = "RxInfer.jl is a great tool for Bayesian Inference."

prior_task = """
Provide a distribution of the statement.
- **Mean**: Most likely satisfaction score (0-10 scale)  
- **Variance**: Uncertainty in your interpretation
    - Low variance (2.0-4.0): Very clear sentiment
    - Medium variance (4.1-6.0): Some ambiguity
    - High variance (6.0-10.0): Unclear or mixed signals
"""

# Likelihood  
likelihood_task = """
Evaluation of sentiment about RxInfer.jl and provide satisfaction score distribution.
If expression is not related to RxInfer.jl, return distribution with mean 5 and high variance of 100.
- **Mean**: Most likely satisfaction score (0-10 scale)
- **Variance**: Uncertainty in interpretation  
    - Low variance (0.1-1.0): Very clear sentiment, confident interpretation
    - Medium variance (1.1-5.0): Some ambiguity in the text
    - High variance (5.1-10.0): Unclear/mixed signals, or not related to RxInfer.jl
""";

# Some shennenigans to make inference work
n_iterations = 5 # number of variational iterations to run

# initial values for the variational distributions, we use uninformative distributions
# If this looks weird to you, please refer to the documentation for the @initialization macro
init = @initialization begin
    q(s) = vague(Beta)
    q(m) = [NormalMeanVariance(0.0, 1e2), NormalMeanVariance(10.0, 1e2)]
    q(y) = NormalMeanVariance(5.0, 1e2) # centered initialization with broad uncertainty
    q(w) = [GammaShapeRate(0.01, 0.01), GammaShapeRate(0.01, 0.01)]
end

import ReactiveMP: rule_nm_switch_k, softmax!

# Run Bayesian inference 
# Again, RxInfer is fast, LLMs are not, bare with inference
results_language = infer(
    model=language_mixture_model(context‚ÇÅ=context‚ÇÅ, context‚ÇÇ=context‚ÇÇ, task‚ÇÅ=prior_task, task‚ÇÇ=prior_task, likelihood_task=likelihood_task),
    constraints=MeanField(), # This is needed for the mixture node
    data=(c=observations,),
    initialization=init,
    iterations=n_iterations,
    free_energy=false,
    showprogress=true
)

using Plots

# Create the animation object
animation = @animate for i in 1:n_iterations

    # Get the data for visualization
    initial_means = [0.0, 10.0]
    initial_vars = [1e2, 1e2]
    posterior_means = [mean.(results_language.posteriors[:m][i])...]
    posterior_vars = inv.([mean.(results_language.posteriors[:w][i])...])

    x = -10:0.01:20

    plt = plot(
        title="RxLLM: Sentiment Clustering",
        xlabel="Sentiment Spectrum",
        ylabel="Density",
        size=(800, 500),
        dpi=300,
        background_color=:white,
        titlefontsize=14,
        legendfontsize=11
    )

    # Plot posteriors with fill
    plot!(plt, x, pdf.(Normal(posterior_means[1], sqrt(posterior_vars[1])), x),
        fillalpha=0.4, fillrange=0, fillcolor=:red,
        linewidth=3, linecolor=:darkred,
        label="Negative Sentiment")

    plot!(plt, x, pdf.(Normal(posterior_means[2], sqrt(posterior_vars[2])), x),
        fillalpha=0.4, fillrange=0, fillcolor=:blue,
        linewidth=3, linecolor=:darkblue,
        label="Positive Sentiment")

    # Plot priors as lighter background
    plot!(plt, x, pdf.(Normal(initial_means[1], sqrt(initial_vars[1])), x),
        linewidth=1, linestyle=:dash, linecolor=:gray, alpha=0.6,
        label="Initial Prior")

    plot!(plt, x, pdf.(Normal(initial_means[2], sqrt(initial_vars[2])), x),
        linewidth=1, linestyle=:dash, linecolor=:gray, alpha=0.6,
        label="")

    # Simple cluster probabilities visualization
    cluster_probs = probvec.(results_language.posteriors[:z][i])
    plt2 = bar(1:length(cluster_probs), [p[1] for p in cluster_probs],
        title="Positive Sentiment Probability", ylabel="P(Positive)", xlabel="Data Point")

    plot(plt, plt2)

end

# Now you can save the animation
gif(animation, "inference_process.gif", fps=1, show_msg=false);