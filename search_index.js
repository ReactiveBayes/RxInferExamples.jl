var documenterSearchIndex = {"docs":
[{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#Conjugate-Computational-Variational-Message-Passing-(CVI)","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"using RxInfer, Random, LinearAlgebra, Plots, Optimisers, StableRNGs, SpecialFunctions","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In this notebook, the usage of Conjugate-NonConjugate Variational Inference (CVI) will be described. The implementation of CVI follows the paper Probabilistic programming with stochastic variational message passing.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"This notebook will first describe an example in which CVI is used, then it discusses several limitations, followed by an explanation on how to extend upon CVI.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#An-example:-nonlinear-dynamical-system","page":"Conjugate-Computational Variational Message Passing","title":"An example: nonlinear dynamical system","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"A group of researchers is performing a tracking experiment on some moving object along a 1-dimensional trajectory. The object is moving at a constant velocity, meaning that its position increases constantly over time. However, the researchers do not have access to the absolute position z_t at time t. Instead they have access to the observed squared distance y_t between the object and some reference point s. Because of budget cuts, the servo moving the object and the measurement devices are quite outdated and therefore lead to noisy measurements:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# data generating process\nnr_observations = 50\nreference_point = 53\nhidden_location = collect(1:nr_observations) + rand(StableRNG(124), NormalMeanVariance(0.0, sqrt(5)), nr_observations)\nmeasurements = (hidden_location .- reference_point).^2 + rand(MersenneTwister(124), NormalMeanVariance(0.0, 5), nr_observations);","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# plot hidden location and reference frame\np1 = plot(1:nr_observations, hidden_location, linewidth=3, legend=:topleft, label=\"hidden location\")\nhline!([reference_point], linewidth=3, label=\"reference point\")\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot measurements\np2 = scatter(1:nr_observations, measurements, linewidth=3, label=\"measurements\")\nxlabel!(\"time [sec]\"), ylabel!(\"squared distance [cm2]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The researchers are interested in quantifying this noise and in tracking the unobserved location of the object. As a result of this uncertainty, the researchers employ a probabilistic modeling approach. They formulate the probabilistic model","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"beginaligned\n p(tau)  = Gamma(tau mid alpha_tau beta_tau)\n p(gamma)  = Gamma(gamma mid alpha_gamma beta_gamma)\n p(z_t mid z_t - 1 tau)  = mathcalN(z_t mid z_t - 1 + 1 tau^-1)\n p(y_t mid z_t gamma)  = mathcalN(y_t mid (z_t - s)^2 gamma^-1)\nendaligned","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"where the researchers put priors on the process and measurement noise parameters, tau and gamma, respectively. They do this, because they do not know the accuracy of their devices.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The researchers have recently heard of this cool probabilistic programming package RxInfer.jl. They decided to give it a try and create the above model as follows:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"function compute_squared_distance(z)\n    (z - reference_point)^2\nend;","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@model function measurement_model(y)\n\n    # set priors on precision parameters\n    τ ~ Gamma(shape = 0.01, rate = 0.01)\n    γ ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    # specify estimate of initial location\n    z[1] ~ Normal(mean = 0, precision = τ)\n    y[1] ~ Normal(mean = compute_squared_distance(z[1]), precision = γ)\n\n    # loop over observations\n    for t in 2:length(y)\n\n        # specify state transition model\n        z[t] ~ Normal(mean = z[t-1] + 1, precision = τ)\n\n        # specify non-linear observation model\n        y[t] ~ Normal(mean = compute_squared_distance(z[t]), precision = γ)\n        \n    end\n\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"But here is the problem, our compute_squared_distance function is already compelx enough such that the exact Bayesian inference is intractable in this model. But the researchers knew that the RxInfer.jl supports a various collection of approximation methods for exactly such cases. One of these approximations is called CVI. CVI allows us to perform probabilistic inference around the non-linear measurement function. In general, for any (non-)linear relationship y = f(x1, x2, ..., xN) CVI can be employed, by specifying the function f and by adding this relationship inside the @model macro as y ~ f(x1, x2, ...,xN). The @model macro will generate a factor node with node function p(y | x1, x2, ..., xN) = δ(y - f(x1, x2, ...,xN)).","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The use of this non-linearity requires us to specify that we would like to use CVI. This can be done by specifying the metadata using the @meta macro as:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@meta function measurement_meta(rng, nr_samples, nr_iterations, optimizer)\n    compute_squared_distance() -> CVI(rng, nr_samples, nr_iterations, optimizer)\nend;","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In general, for any (non-)linear function f(), CVI can be enabled with the @meta macro as:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@meta function model_meta(...)\n    f() -> CVI(args...)\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"See ?CVI for more information about the args....","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In our model, the z variables are connected to the non-linear node function. So in order to run probabilstic inference with CVI we need to enforce a constraint on the joint posterior distribution. Specifically, we need to create a factorization in which the variables that are directly connected to non-linearities are assumed to be independent from the rest of the variables.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In the above example, we will assume the following posterior factorization:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@constraints function measurement_constraints()\n    q(z, τ, γ) = q(z)q(τ)q(γ)\nend;","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"This constraint can be explained by the set of two constraints, one for getting CVI to run, and one for assuming a mean-field factorization around the normal node as ","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@constraints function posterior_constraints() begin\n    q(z, γ) = q(z)q(γ) # CVI\n    q(z, τ) = q(z)q(τ) # the mean-field assumption around normal node\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Because the engineers are using RxInfer.jl, they can automate the inference procedure. They track the inference performance using the Bethe free energy.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"initialization = @initialization begin\n    μ(z) = NormalMeanVariance(0, 5)\n    q(z) = NormalMeanVariance(0, 5)\n    q(τ) = GammaShapeRate(1e-12, 1e-3)\n    q(γ) = GammaShapeRate(1e-12, 1e-3)\nend\n\nresults = infer(\n    model = measurement_model(),\n    data = (y = measurements,),\n    iterations = 50,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = measurement_constraints(),\n    meta = measurement_meta(StableRNG(42), 1000, 1000, Optimisers.Descent(0.001)),\n    initialization = initialization\n)","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Inference results:\n  Posteriors       | available for (z)\n  Free Energy:     | Real[506.224, 327.426, 325.122, 320.962, 315.966, 312.\n922, 311.014, 309.621, 308.792, 308.776  …  306.958, 306.514, 306.427, 306.\n842, 306.876, 306.751, 306.631, 306.762, 306.91, 306.734]","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# plot estimates for location\np1 = plot(collect(1:nr_observations), hidden_location, label = \"hidden location\", legend=:topleft, linewidth=3, color = :red)\nplot!(map(mean, results.posteriors[:z]), label = \"estimated location (±2σ)\", ribbon = map(x -> 2*std(x), results.posteriors[:z]), fillalpha=0.5, linewidth=3, color = :orange)\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot Bethe free energy\np2 = plot(results.free_energy, linewidth=3, label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size = (1200, 500))","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#Requirements","page":"Conjugate-Computational Variational Message Passing","title":"Requirements","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"There are several main requirements for the CVI procedure to satisfy:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The out interface of the non-linearity must be independently factorized with respect to other variables in the model.\nThe messages on input interfaces (x1, x2, ..., xN) are required to be from the exponential family of distributions.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In RxInfer, you can satisfy the first requirement by using appropriate factor nodes (Normal, Gamma, Bernoulli, etc) and second requirement by specifying the @constraints macro. In general you can specify this procedure as","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@model function model(...)\n    ...\n    y ~ f(x1, x2, ..., xN)\n    ... ~ Node2(z1,..., y, zM) # some node that is using the out interface of the non-linearity\n    ... \nend\n\n@constraints function constraints_meta() begin\n    q(y, z1, ..., zn) = q(y)q(z1,...,zM)\n    ...\nend;\n\n@meta function model_meta(...)\n    f() -> CVI(rng, nr_samples, nr_iterations, optimizer))\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Note that not all exponential family distributions are implemented.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#Extensions","page":"Conjugate-Computational Variational Message Passing","title":"Extensions","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#Using-a-custom-optimizer","page":"Conjugate-Computational Variational Message Passing","title":"Using a custom optimizer","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"CVI only supports Optimisers optimizers out of the box.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Below an explanation on how to extend to it to a custom optimizer.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Suppose we have CustomDescent structure which we want to use inside CVI for optimization.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"To do so, we need to implement ReactiveMP.cvi_update!(opt::CustomDescent, λ, ∇).","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"ReactiveMP.cvi_update! incapsulates the gradient step:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"opt is used to select your optimizer structure\nλ is the current value\n∇ is a gradient value computed inside CVI.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"struct CustomDescent \n    learning_rate::Float64\nend\n\n# Must return an optimizer and its initial state\nfunction ReactiveMP.cvi_setup(opt::CustomDescent, q)\n     return (opt, nothing)\nend\n\n# Must return an updated (opt, state) and an updated λ (can use new_λ for inplace operation)\nfunction ReactiveMP.cvi_update!(opt_and_state::Tuple{CustomDescent, Nothing}, new_λ, λ, ∇)\n    opt, _ = opt_and_state\n    λ̂ = vec(λ) - (opt.learning_rate .* vec(∇))\n    copyto!(new_λ, λ̂)\n    return opt_and_state, new_λ\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Let's try to apply it to a model: beginaligned  p(x)  = mathcalN(0 1)\n p(y_imid x)  = mathcalN(y_i mid x^2 1)\nendaligned","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Let's generate some synthetic data for the model","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# generate data\ny = rand(StableRNG(123), NormalMeanVariance(19^2, 10), 1000)\nhistogram(y)","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Again we can create the corresponding model as:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# specify non-linearity\nf(x) = x ^ 2\n\n# specify model\n@model function normal_square_model(y)\n    # describe prior on latent state, we set an arbitrary prior \n    # in a positive domain\n    x ~ Normal(mean = 5, precision = 1e-3)\n    # transform latent state\n    mean := f(x)\n    # observation model\n    y .~ Normal(mean = mean, precision = 0.1)\nend\n\n# specify meta\n@meta function normal_square_meta(rng, nr_samples, nr_iterations, optimizer)\n    f() ->  CVI(rng, nr_samples, nr_iterations, optimizer)\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"normal_square_meta (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"We will use the inference function from ReactiveMP to run inference, where we provide an instance of the CustomDescent structure in our meta macro function:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"res = infer(\n    model = normal_square_model(),\n    data = (y = y,),\n    iterations = 5,\n    free_energy = true,\n    meta = normal_square_meta(StableRNG(123), 1000, 1000, CustomDescent(0.001)),\n    free_energy_diagnostics = nothing\n)\n\nmean(res.posteriors[:x][end])","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"18.992828536095285","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The mean inferred value of x is indeed close to 19, which was used to generate the data. Inference is working! ","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"p1 = plot(mean.(res.posteriors[:x]), ribbon = 3std.(res.posteriors[:x]), label = \"Posterior estimation\", ylim = (0, 40))\np2 = plot(res.free_energy, label = \"Bethe Free Energy\")\n\nplot(p1, p2, layout = @layout([ a b ]))","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Note: x^2 can not be inverted; the sign information can be lost: -19 and 19 are both equally good solutions.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/conjugate-computational_variational_message_passing/Project.toml`\n  [3bd65402] Optimisers v0.4.6\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [276daf66] SpecialFunctions v2.5.1\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Motion-Planning-of-Robotic-Arm-in-Joint-Space","page":"Robotic Arm","title":"Motion Planning of Robotic Arm in Joint Space","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"This example demonstrates motion planning for a robotic arm using RxInfer. It's important to understand that the probabilistic inference for motion planning occurs in joint space rather than Cartesian space:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Joint Space: The space of all possible joint angles (θ₁, θ₂, θ₃, ...) of the robotic arm. Our inference model directly plans trajectories in this space, finding optimal joint angle sequences and the control torques needed to achieve them.\nCartesian Space: The 3D space (x, y, z) where the end effector operates. While our targets are specified in Cartesian space, they are translated to joint space targets using inverse kinematics before planning begins.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"This approach has several advantages:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"It directly models the physical dynamics of the arm's joints\nIt respects the arm's natural degrees of freedom\nIt allows for more efficient inference in the space where control actually happens","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"The workflow is:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Specify target positions in Cartesian space (user-friendly)\nConvert targets to joint angles using inverse kinematics\nUse RxInfer to plan optimal trajectories between joint configurations\nVisualize the resulting motion in Cartesian space using forward kinematics","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Note: These examples demonstrate the use of RxInfer for motion planning for a robotic arm. The animations show the inferred trajectories from probabilistic inference, rather than simulated executions. For more realistic simulations the model would need to be extended with a reactive environment that responds to the robotic arm's actions during plan execution. If you're interested in collaborating on a more realistic implementation, please open a discussion and let's work on it together!","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"using RxInfer, LinearAlgebra, Plots","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"The next couple of blocks are spent on defining the structures that form the foundation of our 3D robotic arm simulation. This is boring but important stuff, as we need to define the state and environment of our robotic arm before doing any inference.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Defining-Structures","page":"Robotic Arm","title":"Defining Structures","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"The structures defined below form the foundation of our 3D robotic arm simulation:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Environment: Encapsulates physical properties of the world, such as gravity, that affect the arm's dynamics. This allows us to simulate different environmental conditions.\nRoboticArm3D{N}: Represents a robotic arm with N links in 3D space. The type parameter N ensures type safety and consistency across the codebase. Properties include:\nPhysical dimensions (link lengths)\nMass distribution (important for dynamics calculations)\nTorque limits (physical constraints of the motors)\nThe parametric type allows for compile-time optimizations and type checking.\nArmState3D: Captures the complete state of the arm at any moment, including:\nJoint angles (position)\nJoint velocities (motion)\nThis state representation is crucial for both forward dynamics (predicting motion) and inverse kinematics (planning motion) that we will define later.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"\"\"\"\n    Environment(; gravitational_constant::Float64 = 9.81)\n\nStructure containing environmental properties.\n\"\"\"\nBase.@kwdef struct Environment\n    gravitational_constant::Float64 = 9.81\nend\nget_gravity(env::Environment) = env.gravitational_constant\n\n\"\"\"\nRoboticArm3D(num_links, link_lengths, link_masses, joint_torque_limits)\n\nStructure containing properties of a 3D robotic arm.\n\"\"\"\nBase.@kwdef struct RoboticArm3D{N}\n    num_links::Int64 = N                    # Number of links in the arm\n    link_lengths::Vector{Float64}           # Length of each link\n    link_masses::Vector{Float64}            # Mass of each link\n    joint_torque_limits::Vector{Float64}    # Maximum torque for each joint\n    \n    function RoboticArm3D{N}(num_links, link_lengths, link_masses, joint_torque_limits) where {N}\n        @assert num_links == N \"Number of links must match type parameter\"\n        @assert length(link_lengths) == N \"Length of link_lengths must match number of links\"\n        @assert length(link_masses) == N \"Length of link_masses must match number of links\"\n        @assert length(joint_torque_limits) == 2*N \"Length of joint_torque_limits must match 2*number of links (2 DOF per joint)\"\n        new{N}(num_links, link_lengths, link_masses, joint_torque_limits)\n    end\nend\n\n# Constructor that infers N from the number of links\nfunction RoboticArm3D(;\n    num_links::Int64,\n    link_lengths::Vector{Float64},\n    link_masses::Vector{Float64},\n    joint_torque_limits::Vector{Float64}\n)\n    RoboticArm3D{num_links}(num_links, link_lengths, link_masses, joint_torque_limits)\nend\n\nfunction get_properties(arm::RoboticArm3D{N}) where {N}\n    return (arm.num_links, arm.link_lengths, arm.link_masses, arm.joint_torque_limits)\nend\n\n\"\"\"\nArmState3D(joint_angles, joint_velocities)\n\nStructure representing the state of a 3D robotic arm.\nEach joint has 2 angles (pitch and yaw).\n\"\"\"\nstruct ArmState3D\n    joint_angles::Vector{Float64}      # Angles of each joint (2 per joint: pitch, yaw)\n    joint_velocities::Vector{Float64}  # Angular velocities of each joint\nend\n\nfunction get_state(state::ArmState3D)\n    return (state.joint_angles, state.joint_velocities)\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"get_state (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Kinematics-and-Dynamics:-Working-Together","page":"Robotic Arm","title":"Kinematics and Dynamics: Working Together","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Robotic arm control requires three complementary mathematical tools:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#1.-Forward-Kinematics","page":"Robotic Arm","title":"1. Forward Kinematics","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Purpose: Maps joint angles to end effector position in Cartesian space\nInput: Joint angles (θ₁, θ₂, θ₃, ...)\nOutput: End effector position (x, y, z)\nUse cases: Visualization, collision detection, workspace analysis\nMathematical nature: Pure geometric transformation (no physics)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#2.-Inverse-Kinematics","page":"Robotic Arm","title":"2. Inverse Kinematics","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Purpose: Maps desired end effector position to required joint angles\nInput: Target position (x, y, z)\nOutput: Joint angles (θ₁, θ₂, θ₃, ...) that achieve this position\nUse cases: Goal specification, target translation, user interface\nMathematical nature: Solving geometric equations (often multiple solutions)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#3.-State-Transition-(Dynamics)","page":"Robotic Arm","title":"3. State Transition (Dynamics)","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Purpose: Models how the arm's state evolves over time when forces/torques are applied\nInput: Current state (angles, velocities) and control inputs (torques)\nOutput: Next state after a time step\nUse cases: Realistic motion simulation, control design, trajectory optimization\nMathematical nature: Physics-based differential equations (F=ma, τ=Iα)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Why-We-Need-All-Three","page":"Robotic Arm","title":"Why We Need All Three","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"These components work together in a complete robotic arm system:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Goal Translation: Inverse kinematics translates task-space goals (x,y,z positions) into joint-space goals (angles)\nMotion Generation: State transition models how to apply torques to move between joint configurations\nFeedback: Forward kinematics verifies the actual position achieved","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"The inference process (probabilistic planning) uses these components to determine optimal control policies:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"It uses the state transition to predict how controls affect future states\nIt uses inverse kinematics to define the target joint configuration\nIt uses forward kinematics to evaluate progress toward the goal","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Without inverse kinematics, we couldn't translate Cartesian targets into joint angles. Without state transition, we couldn't model realistic physical motion with inertia, gravity, etc. Without forward kinematics, we couldn't visualize the arm or verify its position.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"\"\"\"\n    forward_kinematics_3d(arm, joint_angles)\n\nA direct geometric approach to forward kinematics for a 2-link arm.\nAngles are interpreted as:\n- joint_angles[1]: yaw angle of the first joint (rotation around Z axis)\n- joint_angles[2]: pitch angle of the first joint (rotation around new Y axis)\n- joint_angles[3]: bend angle of the second joint (in the local XZ plane)\n\"\"\"\nfunction forward_kinematics_3d(arm::RoboticArm3D{2}, joint_angles::Vector{Float64})\n    # Extract arm lengths\n    l1, l2 = arm.link_lengths\n    \n    # Extract angles\n    yaw = joint_angles[1]    # Base rotation around Z\n    pitch = joint_angles[2]  # Shoulder pitch\n    bend = joint_angles[3]   # Elbow bend\n    \n    # Initialize positions\n    positions = zeros(Float64, 3, 3)  # Base, shoulder, elbow\n    \n    # Base position\n    positions[:, 1] = [0.0, 0.0, 0.0]\n    \n    # First, calculate the shoulder position after yaw and pitch\n    # The first link points in direction [cos(yaw)*cos(pitch), sin(yaw)*cos(pitch), sin(pitch)]\n    shoulder_dir = [cos(yaw)*cos(pitch), sin(yaw)*cos(pitch), sin(pitch)]\n    positions[:, 2] = positions[:, 1] + l1 * shoulder_dir\n    \n    # For the elbow, we need to bend in the plane perpendicular to the yaw rotation\n    # Create a coordinate system at the shoulder\n    z_axis = shoulder_dir  # Direction of first link\n    y_axis = [-sin(yaw), cos(yaw), 0.0]  # Perpendicular to xz-plane\n    x_axis = cross(y_axis, z_axis)  # Complete right-handed system\n    \n    # Calculate direction of second link after bend\n    elbow_dir = cos(bend) * z_axis + sin(bend) * x_axis\n    positions[:, 3] = positions[:, 2] + l2 * elbow_dir\n    \n    return positions\nend\n\n\"\"\"\n    inverse_kinematics_3d(arm, target_position)\n\nA direct geometric inverse kinematics solver for a 2-link arm.\n\"\"\"\nfunction inverse_kinematics_3d(arm::RoboticArm3D{2}, target_position)\n    # Extract arm lengths\n    l1, l2 = arm.link_lengths\n    \n    # Extract target coordinates\n    x, y, z = target_position\n    \n    # Calculate distance to target\n    dist = norm(target_position)\n    \n    # Special case: if target is exactly at origin or too close to it\n    if dist < 0.1\n        # Return a safe default position slightly away from the origin\n        return [0.0, 0.3, 0.3, 0.0]  # Small angles that position arm in a safe configuration\n    end\n    \n    # Check if target is reachable\n    if dist > l1 + l2\n        @warn \"Target is out of reach, using closest possible solution\"\n        # Scale target to be at maximum reach\n        scale_factor = (l1 + l2 * 0.99) / dist\n        x *= scale_factor\n        y *= scale_factor\n        z *= scale_factor\n        # Recalculate distance\n        dist = norm([x, y, z])\n    elseif dist < abs(l1 - l2) + 0.05  # Added small margin to prevent numerical issues\n        @warn \"Target is too close, using closest possible solution\"\n        # Scale target to minimum reach\n        scale_factor = (abs(l1 - l2) * 1.05) / dist  # Increased margin\n        x *= scale_factor\n        y *= scale_factor\n        z *= scale_factor\n        # Recalculate distance\n        dist = norm([x, y, z])\n    end\n    \n    # Calculate yaw angle (rotation in the XY plane)\n    # Handle the case where both x and y are close to zero\n    if abs(x) < 1e-6 && abs(y) < 1e-6\n        yaw = 0.0  # Default yaw when target is directly above/below\n    else\n        yaw = atan(y, x)\n    end\n    \n    # Project the target onto the plane defined by the yaw angle\n    # This gives us the radial distance in the direction of the yaw\n    r = sqrt(x^2 + y^2)\n    \n    # Now we have a 2D problem in the RZ plane (where R is the radial distance)\n    # Apply the law of cosines to find the elbow angle\n    cos_elbow = (r^2 + z^2 - l1^2 - l2^2) / (2 * l1 * l2)\n    # Ensure the value is within valid range for acos\n    cos_elbow = clamp(cos_elbow, -1.0, 1.0)\n    elbow = acos(cos_elbow)\n    \n    # Find the angle between the first link and the line to the target\n    # Handle case where r is very small\n    if r < 1e-6\n        if z >= 0\n            # Target is directly above, point straight up\n            pitch = π/2\n        else\n            # Target is directly below, point straight down\n            pitch = -π/2\n        end\n    else\n        cos_alpha = (l1^2 + r^2 + z^2 - l2^2) / (2 * l1 * sqrt(r^2 + z^2))\n        cos_alpha = clamp(cos_alpha, -1.0, 1.0)\n        alpha = acos(cos_alpha)\n        \n        # Calculate pitch angle (elevation from XY plane)\n        # It's the sum of the angle to the target and alpha\n        pitch = atan(z, r) + alpha\n    end\n    \n    # Return the joint angles: [yaw, pitch, elbow]\n    return [yaw, pitch, elbow, 0.0]\nend\n\n\"\"\"\n    state_transition_3d(state, action, arm, environment, dt)\n\nState transition function for the 3D robotic arm, modeling the physics of motion.\n\"\"\"\nfunction state_transition_3d(state, action, arm, environment, dt)\n    # Extract state components (angles and velocities)\n    n = length(state) ÷ 2\n    θ = state[1:n]\n    ω = state[n+1:end]\n    \n    # Extract physical parameters\n    g = get_gravity(environment)\n    num_links, link_lengths, link_masses, _ = get_properties(arm)\n    \n    # Initialize next state with current values\n    θ_next = copy(θ)\n    ω_next = copy(ω)\n    \n    # Apply simple physics for each joint\n    for i in 1:n\n        # Calculate acceleration: torque = I*α, so α = torque/I\n        # Using a simplified moment of inertia model\n        joint_idx = (i + 1) ÷ 2  # Convert to link index (1-indexed)\n        moment_of_inertia = link_masses[min(joint_idx, num_links)] * (link_lengths[min(joint_idx, num_links)]^2) / 3.0\n        \n        # Net torque = control torque - friction\n        # Gravity compensation is already in the action\n        friction = 0.1 * ω[i]\n        net_torque = action[i] - friction\n        \n        # Calculate angular acceleration\n        α = net_torque / moment_of_inertia\n        \n        # Update velocity and position using basic Euler integration\n        ω_next[i] = ω[i] + α * dt\n        θ_next[i] = θ[i] + ω_next[i] * dt\n    end\n    \n    # Combine angles and velocities\n    return vcat(θ_next, ω_next)\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Main.anonymous.state_transition_3d","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Visualization-Functions","page":"Robotic Arm","title":"Visualization Functions","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"This is the most boring part, but it's necessary to visualize the arm and its motion. This is where forward kinematics and inverse kinematics become handy.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"function plot_arm_3d!(p, arm::RoboticArm3D{2}, joint_angles; color=:black)\n    # Calculate positions using the kinematics\n    positions = forward_kinematics_3d(arm, joint_angles)\n    \n    # Add a more substantial base platform\n    θ = range(0, 2π, length=30)\n    base_radius = 0.25\n    base_height = 0.05\n    \n    # Base platform - top circle\n    base_x = base_radius .* cos.(θ)\n    base_y = base_radius .* sin.(θ)\n    base_z = zeros(length(θ)) .+ base_height\n    plot!(p, base_x, base_y, base_z, linewidth=2, color=:darkgray, \n          fill=true, fillcolor=:darkgray, fillalpha=0.7, label=false)\n    \n    # Base platform - bottom circle\n    base_x_bottom = base_radius .* cos.(θ)\n    base_y_bottom = base_radius .* sin.(θ)\n    base_z_bottom = zeros(length(θ))\n    plot!(p, base_x_bottom, base_y_bottom, base_z_bottom, linewidth=2, color=:darkgray, \n          fill=true, fillcolor=:darkgray, fillalpha=0.5, label=false)\n    \n    # Connect top and bottom circles to create cylinder\n    for i in 1:length(θ)\n        plot!(p, [base_x[i], base_x[i]], [base_y[i], base_y[i]], [base_z[i], base_z_bottom[i]],\n              linewidth=1, color=:darkgray, label=false)\n    end\n    \n    # Plot each link of the arm with improved appearance\n    # Link 1: Base to shoulder - create a tapered cylinder effect\n    num_segments = 8\n    for i in 1:num_segments\n        t1 = (i-1)/num_segments\n        t2 = i/num_segments\n        \n        # Interpolate positions\n        x1 = positions[1, 1] * (1-t1) + positions[1, 2] * t1\n        y1 = positions[2, 1] * (1-t1) + positions[2, 2] * t1\n        z1 = positions[3, 1] * (1-t1) + positions[3, 2] * t1\n        \n        x2 = positions[1, 1] * (1-t2) + positions[1, 2] * t2\n        y2 = positions[2, 1] * (1-t2) + positions[2, 2] * t2\n        z2 = positions[3, 1] * (1-t2) + positions[3, 2] * t2\n        \n        # Taper the width from thick to thin\n        width1 = 10 - (i-1) * 0.5\n        width2 = 10 - i * 0.5\n        \n        # Gradient color from dark to light blue\n        color1 = RGB(0.1, 0.3 + t1*0.3, 0.6 + t1*0.3)\n        color2 = RGB(0.1, 0.3 + t2*0.3, 0.6 + t2*0.3)\n        \n        # Draw segment\n        plot!(p, [x1, x2], [y1, y2], [z1, z2],\n              linewidth=width1, color=color1, label=false,\n              seriestype=:path3d, alpha=0.9)\n    end\n    \n    # Link 2: Shoulder to end effector - create a tapered cylinder effect\n    for i in 1:num_segments\n        t1 = (i-1)/num_segments\n        t2 = i/num_segments\n        \n        # Interpolate positions\n        x1 = positions[1, 2] * (1-t1) + positions[1, 3] * t1\n        y1 = positions[2, 2] * (1-t1) + positions[2, 3] * t1\n        z1 = positions[3, 2] * (1-t1) + positions[3, 3] * t1\n        \n        x2 = positions[1, 2] * (1-t2) + positions[1, 3] * t2\n        y2 = positions[2, 2] * (1-t2) + positions[2, 3] * t2\n        z2 = positions[3, 2] * (1-t2) + positions[3, 3] * t2\n        \n        # Taper the width from thick to thin\n        width1 = 8 - (i-1) * 0.5\n        width2 = 8 - i * 0.5\n        \n        # Gradient color from medium to light blue\n        color1 = RGB(0.1, 0.4 + t1*0.4, 0.7 + t1*0.2)\n        color2 = RGB(0.1, 0.4 + t2*0.4, 0.7 + t2*0.2)\n        \n        # Draw segment\n        plot!(p, [x1, x2], [y1, y2], [z1, z2],\n              linewidth=width1, color=color1, label=false,\n              seriestype=:path3d, alpha=0.9)\n    end\n    \n    # Add joint spheres with metallic appearance\n    # Base joint\n    scatter!(p, [positions[1, 1]], [positions[2, 1]], [positions[3, 1]],\n            markersize=12, color=:darkgray, markerstrokewidth=1, \n            markerstrokecolor=:black, label=false)\n    \n    # Middle joint (shoulder) with highlight effect\n    scatter!(p, [positions[1, 2]], [positions[2, 2]], [positions[3, 2]],\n            markersize=10, color=:silver, markerstrokewidth=1, \n            markerstrokecolor=:black, label=false)\n    # Add highlight to middle joint\n    scatter!(p, [positions[1, 2] + 0.02], [positions[2, 2] + 0.02], [positions[3, 2] + 0.02],\n            markersize=3, color=:white, markerstrokewidth=0, \n            label=false)\n    \n    # Plot end effector with a more interesting shape\n    # Main part\n    scatter!(p, [positions[1, 3]], [positions[2, 3]], [positions[3, 3]],\n            markersize=12, markershape=:diamond, color=:crimson, \n            markerstrokewidth=1, markerstrokecolor=:black, label=\"End Effector\")\n    \n    # Add \"gripper\" effect to end effector\n    gripper_length = 0.1\n    gripper_angle1 = atan(positions[2, 3] - positions[2, 2], positions[1, 3] - positions[1, 2])\n    gripper_angle2 = gripper_angle1 + π/2\n    \n    # Gripper part 1\n    plot!(p, [positions[1, 3], positions[1, 3] + gripper_length * cos(gripper_angle1 + π/4)],\n          [positions[2, 3], positions[2, 3] + gripper_length * sin(gripper_angle1 + π/4)],\n          [positions[3, 3], positions[3, 3]],\n          linewidth=3, color=:crimson, label=false)\n    \n    # Gripper part 2\n    plot!(p, [positions[1, 3], positions[1, 3] + gripper_length * cos(gripper_angle1 - π/4)],\n          [positions[2, 3], positions[2, 3] + gripper_length * sin(gripper_angle1 - π/4)],\n          [positions[3, 3], positions[3, 3]],\n          linewidth=3, color=:crimson, label=false)\n    \n    return p\nend\n\nfunction visualize_arm_and_target(arm::RoboticArm3D{N}, joint_angles, target_position) where {N}\n    # Calculate positions using forward kinematics\n    positions = forward_kinematics_3d(arm, joint_angles)\n    \n    # Create plot\n    p = plot(\n        title=\"3D Robotic Arm Visualization\",\n        xlabel=\"X\", ylabel=\"Y\", zlabel=\"Z\",\n        xlim=(-2, 2), ylim=(-2, 2), zlim=(-2, 2),\n        aspect_ratio=:equal,\n        legend=:topright\n    )\n    \n    # Plot the arm\n    for i in 1:arm.num_links\n        plot!(p, [positions[1, i], positions[1, i+1]], \n              [positions[2, i], positions[2, i+1]],\n              [positions[3, i], positions[3, i+1]],\n              linewidth=3, color=:blue, label=(i==1 ? \"Arm\" : false))\n        \n        scatter!(p, [positions[1, i]], [positions[2, i]], [positions[3, i]],\n                markersize=5, color=:black, label=(i==1 ? \"Joints\" : false))\n    end\n    \n    # Plot end effector\n    scatter!(p, [positions[1, end]], [positions[2, end]], [positions[3, end]],\n            markersize=6, color=:red, label=\"End Effector\")\n    \n    # Plot target\n    scatter!(p, [target_position[1]], [target_position[2]], [target_position[3]],\n            markersize=6, markershape=:star, color=:green, label=\"Target\")\n    \n    # Plot base\n    scatter!(p, [0], [0], [0], markersize=8, color=:black, label=\"Base\")\n    \n    # Calculate error\n    error = norm(positions[:, end] - target_position)\n    annotate!(p, 0, 0, 2, text(\"Error: $(round(error, digits=4))\", 10, :black))\n    \n    return p, positions, error\nend\n\n\"\"\"\n    animate_sequential_targets_3d(arm, all_states, all_targets)\n\nAnimate the arm's motion through a sequence of targets.\n\"\"\"\nfunction animate_sequential_targets_3d(arm::RoboticArm3D{N}, all_states::Vector, all_targets::Vector) where {N}\n    num_targets = length(all_targets)\n    \n    # Combine all trajectory segments into one continuous path\n    combined_states = hcat(all_states...)\n    total_frames = size(combined_states, 2)\n    \n    # Calculate the frame indices where we reach each target\n    target_reached_frames = zeros(Int, num_targets)\n    frame_count = 0\n    for i in 1:num_targets\n        frame_count += size(all_states[i], 2)\n        target_reached_frames[i] = frame_count\n    end\n    \n    animation = @animate for k in 1:total_frames\n        # Determine which target we're currently moving towards\n        current_target_idx = 1\n        for i in 1:num_targets\n            if k <= target_reached_frames[i]\n                current_target_idx = i\n                break\n            end\n        end\n        \n        # Get the current joint angles\n        joint_angles = combined_states[:, k]\n        \n        # Calculate camera angle that slowly rotates for better 3D perception\n        camera_angle_x = 30 + 20*sin(k/total_frames*2π)\n        camera_angle_y = 20 + 10*cos(k/total_frames*2π)\n        \n        # Calculate the current end effector position using the kinematics\n        positions = forward_kinematics_3d(arm, joint_angles)\n        current_pos = positions[:, 3]\n        \n        # Calculate distance to current target\n        distance = norm(current_pos - all_targets[current_target_idx])\n        \n        # Calculate overall progress\n        overall_progress = k / total_frames\n        \n        # Create a 3D plot with improved styling\n        p = plot(\n            xlabel=\"X\", ylabel=\"Y\", zlabel=\"Z\",\n            xlim=(-2, 2), ylim=(-2, 2), zlim=(0, 2),\n            title=\"Target: $current_target_idx/$num_targets | Progress: $(round(Int, overall_progress*100))% | Distance: $(round(distance, digits=2))\",\n            legend=:topright, size=(900, 700),\n            camera=(camera_angle_x, camera_angle_y),\n            grid=false,  # Remove grid for cleaner look\n            aspect_ratio=:equal,\n            background_color=:white,\n            foreground_color=:black,\n            guidefontsize=10,\n            titlefontsize=12\n        )\n        \n        # Add a more interesting ground plane with grid pattern\n        x_grid = range(-2, 2, length=20)\n        y_grid = range(-2, 2, length=20)\n        z_grid = zeros(length(x_grid), length(y_grid))\n        surface!(p, x_grid, y_grid, z_grid, color=:aliceblue, alpha=0.2, label=false)\n        \n        # Add grid lines on the ground for better depth perception\n        for x in range(-2, 2, step=0.5)\n            plot!(p, [x, x], [-2, 2], [0.01, 0.01], color=:lightgray, linewidth=1, label=false, alpha=0.3)\n        end\n        for y in range(-2, 2, step=0.5)\n            plot!(p, [-2, 2], [y, y], [0.01, 0.01], color=:lightgray, linewidth=1, label=false, alpha=0.3)\n        end\n        \n        # Plot targets with improved styling\n        for (i, target) in enumerate(all_targets)\n            if i < current_target_idx\n                # Completed targets - we've already reached these\n                scatter!(p, [target[1]], [target[2]], [target[3]],\n                        markersize=8, color=:darkgreen, markershape=:circle, \n                        label=(i==1 ? \"Completed\" : false))\n                \n                # Add a small vertical line connecting target to ground\n                plot!(p, [target[1], target[1]], [target[2], target[2]], [0, target[3]],\n                      linewidth=1, color=:darkgreen, linestyle=:dash, alpha=0.3, label=false)\n            elseif i == current_target_idx\n                # Current target with a glowing effect\n                scatter!(p, [target[1]], [target[2]], [target[3]],\n                        markersize=12, color=:green, markershape=:star, \n                        label=\"Current\")\n                \n                # Add a pulsing effect based on frame number\n                pulse_size = 6 + 3*sin(k/10)\n                scatter!(p, [target[1]], [target[2]], [target[3]],\n                        markersize=pulse_size, color=:green, markershape=:circle, \n                        alpha=0.3, label=false)\n                \n                # Add a vertical line connecting target to ground\n                plot!(p, [target[1], target[1]], [target[2], target[2]], [0, target[3]],\n                      linewidth=1, color=:green, linestyle=:dash, alpha=0.5, label=false)\n            elseif i == current_target_idx + 1\n                # Only show the next target\n                scatter!(p, [target[1]], [target[2]], [target[3]],\n                        markersize=8, color=:gray, markershape=:star, \n                        label=\"Next\")\n                \n                # Add a faint vertical line\n                plot!(p, [target[1], target[1]], [target[2], target[2]], [0, target[3]],\n                      linewidth=1, color=:gray, linestyle=:dash, alpha=0.2, label=false)\n            end\n        end\n        \n        # Add a trail of the end effector's path\n        if k > 1\n            # Get positions from previous frames to create a trail\n            trail_length = min(k-1, 15)  # Shorter trail for less clutter\n            trail_indices = max(1, k-trail_length):k-1\n            \n            # Extract end effector positions for each frame in the trail\n            trail_positions = []\n            for idx in trail_indices\n                trail_joint_angles = combined_states[:, idx]\n                trail_pos = forward_kinematics_3d(arm, trail_joint_angles)[:, 3]\n                push!(trail_positions, trail_pos)\n            end\n            \n            # Extract coordinates for the trail\n            trail_x = [pos[1] for pos in trail_positions]\n            trail_y = [pos[2] for pos in trail_positions]\n            trail_z = [pos[3] for pos in trail_positions]\n            \n            # Plot the trail with a gradient effect\n            if length(trail_x) > 1\n                for i in 1:length(trail_x)-1\n                    # Gradient color from orange to transparent\n                    alpha_val = 0.2 + 0.7 * i / length(trail_x)\n                    plot!(p, [trail_x[i], trail_x[i+1]], \n                          [trail_y[i], trail_y[i+1]],\n                          [trail_z[i], trail_z[i+1]],\n                          linewidth=2 + i/3, color=:orange, linestyle=:solid, \n                          label=false, alpha=alpha_val)\n                end\n            end\n        end\n        \n        # For visual reference, add a shadow of the arm on the XZ plane\n        for i in 1:size(positions, 2)-1\n            plot!(p, [positions[1, i], positions[1, i+1]], \n                  [0, 0],  # Fix Y coordinate to 0 (XZ plane)\n                  [positions[3, i], positions[3, i+1]],\n                  linewidth=2, color=:gray, linestyle=:dash, \n                  label=(i==1 ? \"Shadow\" : false), opacity=0.3)\n        end\n        \n        # Plot the arm with enhanced appearance\n        plot_arm_3d!(p, arm, joint_angles)\n    end\n    \n    gif(animation, \"sequential_targets_3d.gif\", fps=15, show_msg = false)\n    return nothing\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Main.anonymous.animate_sequential_targets_3d","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Model-specification","page":"Robotic Arm","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"@model function robotic_arm_3d_model(arm, environment, initial_state, goal, horizon, dt)\n    # Extract properties\n    g = get_gravity(environment)\n    num_links, _, link_masses, _ = get_properties(arm)\n    \n    # Initial state prior\n    s[1] ~ MvNormal(mean = initial_state, covariance = 1e-5 * I)\n    \n    for i in 1:horizon\n        # Prior on torques - compensate for gravity at each joint\n        # For 3D arm: first joint (yaw) not affected by gravity, \n        # pitch joints affected based on angle\n        gravity_compensation = zeros(2*num_links)\n        for j in 1:num_links\n            if j > 1  # Skip first joint (base yaw)\n                gravity_compensation[2*j-1] = link_masses[j] * g * 0.5  # Pitch compensation\n            end\n        end\n        \n        u[i] ~ MvNormal(μ = gravity_compensation, Σ = diageye(2*num_links))\n        \n        # State transition\n        s[i + 1] ~ MvNormal(\n            μ = state_transition_3d(s[i], u[i], arm, environment, dt),\n            Σ = 1e-10 * I\n        )\n    end\n    \n    # Final state constraint\n    s[end] ~ MvNormal(mean = goal, covariance = 1e-5 * diageye(4*num_links))\nend\n\n\n@meta function robotic_arm_meta()\n    # Approximate the state transition\n    state_transition_3d() -> Unscented()\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"robotic_arm_meta (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Integration-Possibilities","page":"Robotic Arm","title":"Integration Possibilities","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"While this example keeps kinematics separate from the probabilistic model, it's theoretically possible to integrate them directly:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Embedded Forward Kinematics: The model could include forward kinematics as part of its structure, allowing direct optimization in Cartesian space\nEmbedded Inverse Kinematics: The inference process could solve inverse kinematics simultaneously with trajectory optimization","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"For example, we could define a model that directly optimizes for reaching a Cartesian target:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"@model function direct_cartesian_model(arm, environment, initial_state, target_position, horizon, dt)\n    # Initial state prior\n    s[1] ~ MvNormal(mean = initial_state, covariance = 1e-5 * I)\n    \n    for i in 1:horizon\n        # Control priors\n        u[i] ~ MvNormal(μ = zeros(num_controls), Σ = diageye(num_controls))\n        \n        # State transition\n        s[i + 1] ~ MvNormal(\n            μ = state_transition(s[i], u[i], arm, environment, dt),\n            Σ = 1e-10 * I\n        )\n        \n        # Calculate end effector position using forward kinematics\n        ee_pos[i] := forward_kinematics(arm, s[i][1:num_joints])\n    end\n    \n    # Final position constraint directly in Cartesian space\n    ee_pos[horizon] ~ MvNormal(mean = target_position, covariance = 1e-5 * I)\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"This approach would eliminate the need for separate inverse kinematics calculations but would make the inference problem more complex. For clarity and computational efficiency, this example keeps these components separate.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Motion-Planning","page":"Robotic Arm","title":"Motion Planning","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"\"\"\"\n    move_to_target_3d(arm, env, start, target_position, horizon, dt)\n\nPlan motion to reach a target position in 3D space using the RxInfer model.\n\"\"\"\nfunction move_to_target_3d(arm::RoboticArm3D{N}, env::Environment, start::ArmState3D, target_position, horizon, dt) where {N}\n    # Convert ArmState3D to state vector\n    initial_state = vcat(start.joint_angles, start.joint_velocities)\n    \n    # Calculate target joint angles that would reach the target position\n    target_joint_angles = inverse_kinematics_3d(arm, target_position)\n    \n    # Create goal state (target angles and zero velocities)\n    goal_state = vcat(target_joint_angles, zeros(length(target_joint_angles)))\n        \n    # Create and run the inference using the correct API structure\n    results = infer(\n        model = robotic_arm_3d_model(\n            arm = arm,\n            environment = env,\n            horizon = horizon,\n            dt = dt\n        ),\n        data = (\n            initial_state = initial_state,\n            goal = goal_state,\n        ),\n        meta = robotic_arm_meta(),\n        returnvars = (s = KeepLast(), u = KeepLast())\n    )\n    \n    # Extract trajectories - FIXED to handle MvNormalWeightedMeanPrecision\n    states_distributions = results.posteriors[:s]\n    controls_distributions = results.posteriors[:u]\n    \n    # Extract means from the distributions\n    states = [mean(dist) for dist in states_distributions]\n    controls = [mean(dist) for dist in controls_distributions]\n    \n    # Convert to joint angles and velocities\n    n = length(states[1]) ÷ 2\n    joint_angles = [state[1:n] for state in states]\n    joint_velocities = [state[n+1:end] for state in states]\n    \n    return joint_angles, joint_velocities, controls\nend\n\n\n\n\"\"\"\n    run_3d_example()\n\nRun a complete example of 3D motion planning for a robotic arm.\n\"\"\"\nfunction run_3d_example()\n    # Create a 2-link 3D robotic arm\n    arm = RoboticArm3D{2}(\n        num_links = 2,                      # 2-link arm\n        link_lengths = [1.0, 0.8],          # Lengths of links\n        link_masses = [0.5, 0.3],           # Masses of links\n        joint_torque_limits = [5.0, 5.0, 3.0, 3.0]  # Maximum torques (2 per joint)\n    )\n    \n    # Create an environment\n    env = Environment(gravitational_constant = 9.81)\n    \n    # Define an expanded sequence of targets with more points\n    # Avoid the origin (0,0,0) which causes issues\n    targets = [\n        [1.5, 0.0, 0.3],     # Forward\n        [1.0, 1.0, 0.5],     # Forward-right and up\n        [0.0, 1.5, 0.3],     # Right\n        [-0.5, 1.0, 0.0],    # Back-right and down\n        [-1.0, 0.5, 0.8],    # Back and up\n        [-1.0, -0.5, 0.4],   # Back-left and mid-height\n        [-0.5, -1.0, 0.0],   # Back-left and down\n        [0.0, -1.5, 0.3],    # Left\n        [0.8, -0.8, 0.3],    # Forward-left\n        [0.5, 0.0, 1.5],     # Forward and up\n        [0.2, 0.2, 0.3]      # Near home position but not at origin\n    ]\n    \n    # Parameters for motion planning\n    horizon = 10   # Keep horizon at 10 as requested\n    dt = 0.1       # Time step\n    \n    # Initialize the arm state (all zeros)\n    initial_state = ArmState3D(\n        [0.0, 0.3, 0.3, 0.0],  # Start with a slight bend rather than all zeros\n        zeros(4)               # Joint velocities\n    )\n    \n    # Store the states, controls, and targets for later visualization\n    all_states = []\n    all_controls = []\n    current_state = initial_state\n    \n    # Plan motion for each target\n    for (i, target) in enumerate(targets)\n        println(\"\\nPlanning motion to target $i: $target\")\n        \n        # Plan motion to the target\n        θ_trajectory, ω_trajectory, u_trajectory = move_to_target_3d(arm, env, current_state, target, horizon, dt)\n        \n        # Combine all states into a single matrix for visualization\n        states_matrix = hcat(θ_trajectory...)\n        \n        # Update the current state for the next target\n        current_state = ArmState3D(\n            θ_trajectory[end],\n            ω_trajectory[end]\n        )\n        \n        # Store the results\n        push!(all_states, states_matrix)\n        push!(all_controls, hcat(u_trajectory...))\n    end\n    \n    # Animate the motion through all targets\n    animation = animate_sequential_targets_3d(arm, all_states, targets)\n    \n    return arm, all_states, targets, all_controls\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Main.anonymous.run_3d_example","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"arm, states, targets, controls = run_3d_example();","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Planning motion to target 1: [1.5, 0.0, 0.3]\n\nPlanning motion to target 2: [1.0, 1.0, 0.5]\n\nPlanning motion to target 3: [0.0, 1.5, 0.3]\n\nPlanning motion to target 4: [-0.5, 1.0, 0.0]\n\nPlanning motion to target 5: [-1.0, 0.5, 0.8]\n\nPlanning motion to target 6: [-1.0, -0.5, 0.4]\n\nPlanning motion to target 7: [-0.5, -1.0, 0.0]\n\nPlanning motion to target 8: [0.0, -1.5, 0.3]\n\nPlanning motion to target 9: [0.8, -0.8, 0.3]\n\nPlanning motion to target 10: [0.5, 0.0, 1.5]\n\nPlanning motion to target 11: [0.2, 0.2, 0.3]","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/robotic_arm/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [90137ffa] StaticArrays v1.9.15\n","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<style>\n    :root {\n        --tag-bg-color: #f3f6f9;\n        --tag-text-color: #476582;\n        --category-text-color: #2c3e50;\n        --card-border-color: #e9ecef;\n        --card-bg-color: transparent;\n        --text-color: inherit;\n        --resources-bg-color: #f8f9fa;\n        --text-muted-color: #666;\n        --description-text-color: #476582;\n    }\n    \n    .theme--documenter-dark {\n        --tag-bg-color: #2d2d2d;\n        --tag-text-color: #9ecbff;\n        --category-text-color: #e6e6e6;\n        --card-border-color: #404040;\n        --card-bg-color: #1f1f1f;\n        --text-color: #e6e6e6;\n        --resources-bg-color: #1f1f1f;\n        --text-muted-color: #999;\n        --description-text-color: #9ecbff;\n    }\n</style>","category":"page"},{"location":"autogenerated/list_of_examples/#List-of-Examples","page":"List of Examples","title":"List of Examples","text":"","category":"section"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Welcome to our curated collection of RxInfer.jl examples! Here you'll find a comprehensive set of tutorials, demonstrations, and real-world applications that showcase the power and flexibility of RxInfer.jl.","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Each example comes with:","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"A detailed description of concepts covered\nRelevant tags for easy filtering\nComplete source code and explanations\nVisualizations and results analysis","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"note: Contributing\nThis gallery is community-driven and automatically generated from our repository. We welcome your contributions!Report a bug\nSubmit a pull request\nRead contribution guide\nRxInfer.jl respository","category":"page"},{"location":"autogenerated/list_of_examples/#External-Resources","page":"List of Examples","title":"External Resources","text":"","category":"section"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1.2em; border-radius: 8px; \n    background-color: var(--resources-bg-color, #f8f9fa); \n    border: 1px solid var(--card-border-color, #e9ecef);\">\n    <h4 style=\"margin: 0 0 1em 0; color: var(--category-text-color, #333);\">Community Tutorials & Guides</h4>\n    <ul style=\"margin: 0; padding-left: 1.2em;\">\n        <li style=\"margin-bottom: 0.8em;\">\n            <strong>Active Inference with RxInfer.jl</strong><br/>\n            <span style=\"color: var(--text-muted-color, #666);\">An in-depth exploration of Active Inference principles guided by \n            <a href=\"https://www.linkedin.com/in/kobusesterhuysen/\">Kobus Esterhuysen</a> at \n            <a href=\"https://learnableloop.com/#category=RxInfer\">Learnable Loop</a>.</span>\n        </li>\n        <li style=\"margin-bottom: 0.8em;\">\n            <strong>Video Tutorial Series</strong><br/>\n            <span style=\"color: var(--text-muted-color, #666);\">Comprehensive video tutorials covering RxInfer.jl's core concepts and applications by \n            <a href=\"https://www.youtube.com/@doggodotjl/search?query=RxInfer\">@doggotodjl</a>.</span>\n        </li>\n        <li style=\"margin-bottom: 0.8em;\">\n            <strong>Victor Flores blogpost</strong><br/>\n            <span style=\"color: var(--text-muted-color, #666);\">A collection of projects and examples with RxInfer (but not limited to) at \n            <a href=\"https://vflores-io.github.io/\">vflores-io</a>.</span>\n        </li>\n    </ul>\n    \n    <h4 style=\"margin: 1.5em 0 1em 0; color: var(--category-text-color, #333);\">Python Integration & Server Infrastructure</h4>\n    <ul style=\"margin: 0; padding-left: 1.2em;\">\n        <li style=\"margin-bottom: 0.8em;\">\n            <strong>RxInferServer.jl</strong><br/>\n            <span style=\"color: var(--text-muted-color, #666);\">RESTful API service for deploying RxInfer models. \n            <a href=\"https://github.com/lazydynamics/RxInferServer\">GitHub Repository</a> | \n            <a href=\"https://server.rxinfer.com\">Documentation</a></span>\n        </li>\n        <li style=\"margin-bottom: 0.8em;\">\n            <strong>RxInferClient.py</strong><br/>\n            <span style=\"color: var(--text-muted-color, #666);\">Python SDK for interacting with RxInferServer. \n            <a href=\"https://github.com/lazydynamics/RxInferClient.py\">GitHub Repository</a> | \n            <a href=\"https://lazydynamics.github.io/RxInferClient.py/\">Documentation</a></span>\n        </li>\n    </ul>\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<h2 style=\"margin-top: 2em; margin-bottom: 1em; color: var(--category-text-color, #2c3e50);\">\n    Basic Examples\n</h2>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: -0.5em 0 2em 0; color: var(--description-text-color, #476582);\">\n    Fundamental concepts and introductory tutorials. Start here if you're new to RxInfer.jl.\nThese examples cover basic probabilistic models, inference techniques, and data processing.\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Bayesian Binomial Regression","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            An introductory tutorial to Bayesian binomial regression with RxInfer. \n    Learn how to model binary outcomes using logistic regression with proper Bayesian inference.\n    The example demonstrates the use of Expectation Propagation (EP) algorithm and Polya-Gamma augmentation.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    multivariate\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    expectation propagation\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    polya-gamma\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Bayesian Linear Regression","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An extensive tutorial on Bayesian linear regression with RxInfer with a lot of examples, including multivariate and hierarchical linear regression.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    tutorial\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hierarchical model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    multivariate\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Bayesian Multinomial Regression","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            An introductory tutorial to Bayesian multinomial regression with RxInfer. \n    Learn how to model categorical outcomes using multinomial regression with proper Bayesian inference.\n    The example demonstrates the use of Expectation Propagation (EP) algorithm and Polya-Gamma augmentation.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    multivariate\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    expectation propagation\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    polya-gamma\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Bayesian Networks: The Sprinkler Model","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            An introductory tutorial to Bayesian Networks with RxInfer using the classic sprinkler model.\n    Learn how to construct and perform inference in a simple Bayesian network given conditional probability tables.\n    The example demonstrates core concepts like conditional independence, belief propagation, and evidence-based inference.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    bayesian networks\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    discrete bayesian networks\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    sprinkler model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    conditional probability\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Coin toss model (Beta-Bernoulli)","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of Bayesian inference in Beta-Bernoulli model with IID observations.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    conjugate model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    iid observations\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    beta bernoulli\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Contextual Bandits","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This notebooks covers RxInfer usage for the Contextual Bandits problem.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    contextual bandits\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Feature Functions in Bayesian Regression","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of Bayesian inference in a parametric Gaussian regression model.\nBased on \"Probabilistic Numerics: Computation as Machine Learning\" by Hennig, Osborne and Kersting.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    probabilistic numerics\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basis functions\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    parametric\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Forgetting Factors for Online Inference","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example demonstrates online Bayesian inference using forgetting factors to adaptively adjust the observation noise precision over time. \nForgetting factors help the model gradually \"forget\" old data, allowing it to better track non-stationary processes and adapt to changing noise levels.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    online inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    forgetting factors\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    kalman filter\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"How to train your Hidden Markov Model","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of structured variational Bayesian inference in Hidden Markov Model with unknown transition and observational matrices.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hmm\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    structured inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    variational inference\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Incomplete Data","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This notebooks covers RxInfer usage for the Incomplete Data problem.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    incomplete data\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Kalman filtering and smoothing","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this demo, we are interested in Bayesian state estimation in different types of State-Space Models, including linear, nonlinear, and cases with missing observations\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    state space model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    kalman filter\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    missing data\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    nonlinear\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"POMDP Control with Reactive Inference","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example demonstrating how to perform control in Partially Observable Markov Decision Processes (POMDPs) using reactive message passing and variational inference.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    pomdp\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    control\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    structured inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    variational inference\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Predicting Bike Rental Demand","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An illustrative guide to implementing prediction mechanisms within RxInfer.jl, using bike rental demand forecasting as a contextual example.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    prediction\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    time series\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    real data\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<h2 style=\"margin-top: 2em; margin-bottom: 1em; color: var(--category-text-color, #2c3e50);\">\n    Advanced Examples\n</h2>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: -0.5em 0 2em 0; color: var(--description-text-color, #476582);\">\n    More complex applications and advanced inference techniques. These examples demonstrate\nsophisticated models, performance optimization, and integration with other Julia packages.\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Active Inference Mountain car","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This notebooks covers RxInfer usage in the Active Inference setting for the simple mountain car problem.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    active inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    control\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    reinforcement learning\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Advanced Tutorial","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This notebook covers the fundamentals and advanced usage of the `RxInfer.jl` package.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    tutorial\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    fundamentals\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Assessing People's Skills","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        The demo is inspired by the example from Chapter 2 of Bishop's Model-Based Machine Learning book. We are going to perform an exact inference to assess the skills of a student given the results of the test.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    exact inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    educational\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    skill assessment\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Chance-Constrained Active Inference","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This notebook applies reactive message passing for active inference in the context of chance-constraints.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    active inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    constraints\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    control\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Conjugate-Computational Variational Message Passing (CVI)","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example provides an extensive tutorial for the non-conjugate message-passing based inference by exploiting the local CVI approximation.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    cvi\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    non conjugate\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    variational inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    tutorial\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Drone Dynamics","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows how to use RxInfer.jl automated inference to simulate drone dynamics.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Solve GP regression by SDE","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this notebook, we solve a GP regression problem by using 'Stochastic Differential Equation' (SDE). This method is well described in the dissertation 'Stochastic differential equation methods for spatio-temporal Gaussian process regression.' by Arno Solin and 'Sequential Inference for Latent Temporal Gaussian Process Models' by Jouni Hartikainen.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    gaussian process\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    sde\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    state space model\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Infinite Data Stream","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows RxInfer capabilities of running inference for infinite time-series data.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    streaming\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    online inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    time series\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Integrating Neural Networks with Flux.jl","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows how to use RxInfer.jl together with Flux.jl to incorporate neural networks into probabilistic models.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    neural networks\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    deep learning\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    integration\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    Flux.jl\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Learning Dynamics with VAEs","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows how to use RxInfer.jl automated inference to learn dynamics in latent space of a VAE.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    vae\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    dynamics\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Multi-agent Trajectory Planning","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows how to plan multi-agents' trajectories while avoiding obstacles and collisions between agents.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Nonlinear Sensor Fusion","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        Nonlinear object position identification using a sparse set of sensors\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    sensor fusion\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    nonlinear\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    sparse data\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Parameter Optimisation with Optim.jl","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows how to use RxInfer.jl together with Optim.jl to perform parameter optimisation in probabilistic models.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    optimization\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    parameter estimation\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    integration\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    Optim.jl\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Robotic Arm","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example explores how RxInfer.jl's automated inference can be applied to path planning for a robotic arm and demonstrates how probabilistic inference enables smooth and efficient motion planning. Ideal for those interested in robotics, Bayesian inference, and intelligent control systems.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    robotics\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    probabilistic inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    path planning\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    Bayesian methods\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<h2 style=\"margin-top: 2em; margin-bottom: 1em; color: var(--category-text-color, #2c3e50);\">\n    Problem Specific\n</h2>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: -0.5em 0 2em 0; color: var(--description-text-color, #476582);\">\n    Real-world applications and domain-specific models. These examples show how RxInfer.jl\ncan be applied to specific problems like time series analysis and signal processing.\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Autoregressive Models","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of Bayesian treatment of latent AR and ARMA models. Reference: [Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models](https://www.mdpi.com/1099-4300/23/6/683).\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    time series\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    arma\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    latent variables\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Gamma Mixture Model","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example implements one of the Gamma mixture experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/ .\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    mixture model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    gamma distribution\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    clustering\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Gaussian Mixture","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example implements variational Bayesian inference in univariate and multivariate Gaussian mixture models with mean-field assumption.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    mixture model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    gaussian\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    mean field\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    clustering\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Hierarchical Gaussian Filter","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of online inference procedure for Hierarchical Gaussian Filter with univariate noisy observations using Variational Message Passing algorithm. Reference: [Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter](https://ieeexplore.ieee.org/document/9173980).\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hierarchical model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    online inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    filtering\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Invertible neural networks: a tutorial","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of variational Bayesian Inference with invertible neural networks. Reference: Bart van Erp, Hybrid Inference with Invertible Neural Networks in Factor Graphs.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    neural networks\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    invertible networks\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hybrid inference\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Litter Model","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        Using Bayesian Inference and RxInfer to estimate daily litter events (adapted from https://learnableloop.com/posts/LitterModel_PORT.html)\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    real data\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    time series\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    event modeling\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"ODE Parameter Estimation","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of solving Lotka Volterra ODE with RxInfer.jl. Reference: [Lotka Volterra ODE](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations).\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    ode\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    differential equations\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Probit Model (EP)","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    expectation propagation\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    probit\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    state space model\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"RTS vs BIFM Smoothing","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example performs BIFM Kalman smoother on a factor graph using message passing and compares it with the RTS implementation.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    smoothing\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    kalman filter\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    comparison\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Simple Nonlinear Node","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this example we create a non-conjugate model and use a nonlinear link function between variables. We show how to extend the functionality of `RxInfer` and to create a custom factor node with arbitrary message passing update rules.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    nonlinear\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    custom node\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    message passing\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Structural Dynamics with Augmented Kalman Filter","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this example, we estimate system states and unknown input forces for a simple **structural dynamical system** using the Augmented Kalman Filter (AKF) (https://www.sciencedirect.com/science/article/abs/pii/S0888327011003931) in **RxInfer**.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    kalman filter\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    structural dynamics\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    state estimation\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Universal Mixtures","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        Universal mixture modeling.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    mixture model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    universal approximation\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<h2 style=\"margin-top: 2em; margin-bottom: 1em; color: var(--category-text-color, #2c3e50);\">\n    Experimental Examples\n</h2>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: -0.5em 0 2em 0; color: var(--description-text-color, #476582);\">\n    Experimental examples and proof-of-concepts. These examples are not yet ready for\nproduction use but are useful for research and development. They also usually require \nadditional patches to RxInfer.jl to work.\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Bayesian Trust Learning","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            This is an experimental example of a Bayesian Trust Learning for LLM Routing.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    experimental examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    llm routing\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    bayesian trust learning\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hierarchical bayesian models\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    custom nodes\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    custom rules\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    online learning cycles\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Large Language Models","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            This is an experimental example of a Large Language Model (LLM) integration with RxInfer.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    experimental examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    large language models\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    llm\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Latent Vector Autoregressive Model","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            This is an experimental example of a Latent Vector Autoregressive Model (LVAR).\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    experimental examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    dynamical system\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    recurrent switching\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Recurrent Switching Linear Dynamical System","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            An experimental example of a Recurrent Switching Linear Dynamical System (RSLDS) model.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    experimental examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    dynamical system\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    recurrent switching\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"note: Contributing\nThis gallery is community-driven and automatically generated from our repository. We welcome your contributions!Report a bug\nSubmit a pull request\nRead contribution guide\nRxInfer.jl respository","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"","category":"page"},{"location":"categories/basic_examples/contextual_bandits/#Contextual-Bandits","page":"Contextual Bandits","title":"Contextual Bandits","text":"","category":"section"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"We will start this notebook with a motivating example.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Let’s face it—ads are annoying. But very often they’re also one of the few ways to keep the business running. Imagine a free-to-play game for example. The real question isn’t whether to show ads, but when and which kind to show, so that players don’t feel bombarded, leave the game frustrated, or stop spending altogether.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"It’s a balancing act between monetization and player retention.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"If you show ads too aggressively, players quit. If you show ads too cautiously, you leave money on the table. So, how do you decide what to do in each moment of a player’s session?","category":"page"},{"location":"categories/basic_examples/contextual_bandits/#Turning-Ad-Scheduling-Into-a-Learning-Problem","page":"Contextual Bandits","title":"Turning Ad Scheduling Into a Learning Problem","text":"","category":"section"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Every player session is different. Some players are engaged, some are rushing through, some just made a purchase, some are one level away from quitting.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"At every possible ad moment, you have choices like:","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Show a video ad (high revenue, high interruption)\nShow a banner ad (low revenue, low interruption)\nShow no ad (defer to later)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"And you have context—information about the session so far:","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Player’s level and progress\nTime since the last ad\nTime spent in the current session\nWhether the player just succeeded or failed at something\nRecent purchases or reward claims","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"What you want is a system that learns from player behavior over time, figuring out which actions perform best in which contexts—not just maximizing clicks or ad revenue right now, but balancing that with keeping players happy and playing longer.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/#Contextual-Multi-Armed-Bandits-to-the-Rescue","page":"Contextual Bandits","title":"Contextual Multi-Armed Bandits to the Rescue","text":"","category":"section"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"This is exactly where Contextual Multi-Armed Bandits (CMABs) come in.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Contextual Multi‑Armed Bandits (CMABs) extend the classic bandit setting by allowing the learner to observe a context—a feature vector that describes the current situation—before selecting an action (or arm). The learner’s aim is to maximise cumulative reward over time by repeatedly balancing:","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Exploration – trying arms whose pay‑offs are still uncertain in some contexts.\nExploitation – choosing the arm with the highest estimated reward in the current context.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"CMABs sit between simple A/B tests and full reinforcement‑learning problems and are the work‑horse behind many personalisation engines.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Acting identically for every context wastes opportunity; CMABs formalise how to adapt decisions on‑the‑fly.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/#Implementing-CMABs-in-**RxInfer.jl**","page":"Contextual Bandits","title":"Implementing CMABs in RxInfer.jl","text":"","category":"section"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"In this notebook we will implement a CMAB in RxInfer.jl way. A way to tackle CMAB in RxInfer requires expressing the generative model as a hierarchical Bayesian linear‑regression model and then message passing inference will do the rest.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/#Generative-model-(consistent-notation)","page":"Contextual Bandits","title":"Generative model (consistent notation)","text":"","category":"section"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Let","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"K\n– number of arms.\nd\n– dimension of the context vector.\nc_tinmathbbR^d\n– context observed at round $t$.\na_tin1dotsK\n– arm selected at round $t$.\nr_tinmathbbR\n– realised reward.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"The environment produces rewards according to:","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Global noise precision tausimmathcalG(alpha_tau beta_tau)\nArm‑specific regression parameters  k = 1dotsK theta_k sim mathcalN(m_0k V_0k) qquad  Lambda_k sim operatornameWishart(nu_0k W_0k)\nPer interaction latent coefficients  t = 1dotsT beta_t sim mathcalN(theta_a_t Lambda_a_t^-1)\nReward generation mu_t = c_t^top beta_t qquad  r_t sim mathcalN(mu_t tau^-1)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Interpretation. Each arm owns a distribution of weight vectors (theta_k), capturing how the context maps to reward for that arm. On every play we draw a concrete weight vector beta_t, compute the expected reward mu_t, and then observe a noisy realisation r_t.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/#Inference-and-decision‑making","page":"Contextual Bandits","title":"Inference & decision‑making","text":"","category":"section"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"With RxInfer we:","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Declare the model above with the @model macro. (To make the example simpler, we'll have two models: one for parameters and one for predictions. A more complex scenario would be to have a single model for both.)\nStream incoming tuples (c_t a_t r_t).\nCall infer to update the posterior over (theta_k Lambda_k tau).\nCompute predictive distributions of rewards in a new context c_t+1 via infer on the predictive model:\nChoose the next arm based on sampled from the predictive distribution.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Because both learning and prediction are expressed as probabilistic inference, we keep all uncertainty in closed form—ideal for principled exploration. The same pipeline generalises easily to non‑linear contexts (via feature maps) or non‑Gaussian rewards by swapping likelihood terms.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"using RxInfer, Distributions, LinearAlgebra, Plots, StatsPlots, ProgressMeter, StableRNGs, Random","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"At first let's generate synthetic data to simulate the CMAB problem.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"using StableRNGs\n\n# Random number generator \nrng = StableRNG(42)\n\n# Data generation parameters\nn_train_samples = 300\nn_test_samples = 100\nn_total_samples = n_train_samples + n_test_samples\nn_arms = 10\nn_contexts = 50\ncontext_dim = 20\nnoise_sd = 0.1\n\n# Generate true arm parameters (θ_k in the model description)\narms = [randn(rng, context_dim) for _ in 1:n_arms]\n\n# Generate context feature vectors with missing values\ncontexts = []\nfor i in 1:n_contexts\n    # Create a vector that can hold both Float64 and Missing values\n    context = Vector{Union{Float64,Missing}}(undef, context_dim)\n\n    # Fill with random values initially\n    for j in 1:context_dim\n        context[j] = randn(rng)\n    end\n\n    # Randomly introduce missing values in some contexts\n    if rand(rng) < 0.4  # 40% of contexts will have some missing values\n        n_missing = rand(rng, 1:2)  # 1-2 missing values per context\n\n        # Simple approach: randomly select indices for missing values\n        missing_indices = []\n        while length(missing_indices) < n_missing\n            idx = rand(rng, 1:context_dim)\n            if !(idx in missing_indices)\n                push!(missing_indices, idx)\n            end\n        end\n\n        for idx in missing_indices\n            context[idx] = missing\n        end\n    end\n\n    push!(contexts, context)\nend\n\n# Synthetic reward function (reusable)\nfunction compute_reward(arm_params, context_vec, noise_sd=0.0; rng=nothing)\n    \"\"\"\n    Compute reward for given arm parameters and context.\n\n    Args:\n        arm_params: Vector of arm parameters\n        context_vec: Context vector (may contain missing values)\n        noise_sd: Standard deviation of Gaussian noise to add\n        rng: Random number generator (optional, for reproducible noise)\n\n    Returns:\n        Scalar reward value\n    \"\"\"\n    # Calculate the deterministic part of the reward, handling missing values\n    mean_reward = 0.0\n    valid_dims = 0\n\n    for j in 1:length(context_vec)\n        if !ismissing(context_vec[j])\n            mean_reward += arm_params[j] * context_vec[j]\n            valid_dims += 1\n        end\n    end\n\n    # Normalize by the number of valid dimensions to maintain similar scale\n    if valid_dims > 0\n        mean_reward = mean_reward * (length(context_vec) / valid_dims)\n    end\n\n    # Add Gaussian noise if requested\n    if noise_sd > 0\n        if rng !== nothing\n            mean_reward += randn(rng) * noise_sd\n        else\n            mean_reward += randn() * noise_sd\n        end\n    end\n\n    return mean_reward\nend\n\n# Generate training and test data\nfunction generate_bandit_data(n_samples, arms, contexts, noise_sd; rng, start_idx=1)\n    \"\"\"\n    Generate bandit data for given number of samples.\n\n    Returns:\n        (arm_choices, context_choices, rewards)\n    \"\"\"\n    arm_choices = []\n    context_choices = []\n    rewards = []\n\n    for i in 1:n_samples\n        # Randomly select a context and an arm\n        push!(context_choices, rand(rng, 1:length(contexts)))\n        push!(arm_choices, rand(rng, 1:length(arms)))\n\n        # Get the selected context and arm\n        selected_context = contexts[context_choices[end]]\n        selected_arm = arms[arm_choices[end]]\n\n        # Compute reward using the synthetic reward function\n        reward = compute_reward(selected_arm, selected_context, noise_sd; rng=rng)\n        push!(rewards, reward)\n    end\n\n    return arm_choices, context_choices, rewards\nend\n\n# Generate training data\nprintln(\"Generating training data...\")\ntrain_arm_choices, train_context_choices, train_rewards = generate_bandit_data(\n    n_train_samples, arms, contexts, noise_sd; rng=rng\n)\n\n# Generate test data\nprintln(\"Generating test data...\")\ntest_arm_choices, test_context_choices, test_rewards = generate_bandit_data(\n    n_test_samples, arms, contexts, noise_sd; rng=rng\n)\n\n# Display information about the generated data\nprintln(\"\\nDataset Summary:\")\nprintln(\"Training samples: $n_train_samples\")\nprintln(\"Test samples: $n_test_samples\")\nprintln(\"Total contexts: $(length(contexts))\")\nprintln(\"Number of contexts with missing values: \", sum(any(ismissing, ctx) for ctx in contexts))\nprintln(\"Arms: $n_arms\")\nprintln(\"Context dimension: $context_dim\")\nprintln(\"Noise standard deviation: $noise_sd\")\n\n# Show examples of contexts with missing values\nprintln(\"\\nExamples of contexts with missing values:\")\ncount = 0\nfor (i, ctx) in enumerate(contexts)\n    if any(ismissing, ctx) && count < 3  # Show first 3 examples\n        println(\"Context $i: $ctx\")\n        global count += 1\n    end\nend\n\n# Show sample data\nprintln(\"\\nTraining data samples (first 5):\")\nfor i in 1:min(5, length(train_rewards))\n    println(\"Sample $i: Arm=$(train_arm_choices[i]), Context=$(train_context_choices[i]), Reward=$(round(train_rewards[i], digits=4))\")\nend\n\nprintln(\"\\nTest data samples (first 5):\")\nfor i in 1:min(5, length(test_rewards))\n    println(\"Sample $i: Arm=$(test_arm_choices[i]), Context=$(test_context_choices[i]), Reward=$(round(test_rewards[i], digits=4))\")\nend\n\n# Verify the reward function works correctly\nprintln(\"\\nTesting reward function:\")\ntest_context = contexts[1]\ntest_arm = arms[1]\ndeterministic_reward = compute_reward(test_arm, test_context, 0.0)  # No noise\nnoisy_reward = compute_reward(test_arm, test_context, noise_sd; rng=rng)  # With noise\nprintln(\"Deterministic reward: $(round(deterministic_reward, digits=4))\")\nprintln(\"Noisy reward: $(round(noisy_reward, digits=4))\")","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Generating training data...\nGenerating test data...\n\nDataset Summary:\nTraining samples: 300\nTest samples: 100\nTotal contexts: 50\nNumber of contexts with missing values: 22\nArms: 10\nContext dimension: 20\nNoise standard deviation: 0.1\n\nExamples of contexts with missing values:\nContext 3: Union{Missing, Float64}[missing, -0.4741385118651381, -1.0989041\n9081401, -1.079288892379018, 0.8184199040107111, -0.30409464242950546, -0.6\n709508997562322, -0.7469592378369052, 0.21407501633089995, -0.6139813001136\n504, 2.8170273653049507, -1.4362435690909499, -0.30112508107598307, -0.3868\n83090487843, 0.6563571763621648, 1.401591444397142, 0.6193863742347839, 0.1\n2760715013378465, -0.2758495479700435, 1.8822768045661076]\nContext 8: Union{Missing, Float64}[-2.3650237776747054, -1.1739461025984783\n, 1.128284045692684, -0.8690689832066373, 0.4497591893001418, -0.2617237965\n612964, 0.07868265261314639, missing, 1.9119126287271901, 0.719828217704402\n8, -2.708690227134825, -2.645555311022844, -0.3202946667428825, 1.398258591\n17344, 0.06974735851443013, 1.1639494445584129, -0.36687387238833036, 0.506\n2972107773495, -1.3675557327045547, missing]\nContext 9: Union{Missing, Float64}[-0.7928407885849085, 0.5230893424666117,\n 0.21944291871653826, -0.2951043978830045, missing, 0.6739591611416778, 0.6\n091981160535558, 0.37661376790321904, -0.08201072963796563, 0.6762611326141\n408, -1.7998621684347569, 0.7079334064897562, -1.5082653360123872, 0.423324\n15672698104, 1.380447484940245, -3.325041477189219, 1.1893655835458625, 0.9\n25128468276957, -1.62673585658528, -0.667629748025382]\n\nTraining data samples (first 5):\nSample 1: Arm=6, Context=11, Reward=2.7556\nSample 2: Arm=2, Context=14, Reward=-3.911\nSample 3: Arm=8, Context=14, Reward=-2.0472\nSample 4: Arm=6, Context=36, Reward=9.6666\nSample 5: Arm=3, Context=21, Reward=-4.1496\n\nTest data samples (first 5):\nSample 1: Arm=1, Context=18, Reward=0.3271\nSample 2: Arm=2, Context=27, Reward=-3.2327\nSample 3: Arm=4, Context=4, Reward=-2.1027\nSample 4: Arm=1, Context=7, Reward=4.7518\nSample 5: Arm=8, Context=24, Reward=3.4953\n\nTesting reward function:\nDeterministic reward: -2.2755\nNoisy reward: -2.2613","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function create_bandit_plots(arm_choices, context_choices, rewards, title_prefix, color_scheme)\n    p1 = scatter(1:length(context_choices), context_choices,\n        label=\"Context Choices\",\n        title=\"$title_prefix: Context Selection\",\n        xlabel=\"Sample\", ylabel=\"Context ID\",\n        marker=:circle, markersize=6,\n        color=color_scheme[:context], alpha=0.7)\n\n    p2 = scatter(1:length(arm_choices), arm_choices,\n        label=\"Arm Choices\",\n        title=\"$title_prefix: Arm Selection\",\n        xlabel=\"Sample\", ylabel=\"Arm ID\",\n        marker=:diamond, markersize=6,\n        color=color_scheme[:arm], alpha=0.7)\n\n    p3 = plot(1:length(rewards), rewards,\n        label=\"Rewards\",\n        title=\"$title_prefix: Rewards\",\n        xlabel=\"Sample\", ylabel=\"Reward Value\",\n        linewidth=2, marker=:star, markersize=4,\n        color=color_scheme[:reward], alpha=0.8)\n\n    hline!(p3, [mean(rewards)], label=\"Mean Reward\",\n        linestyle=:dash, linewidth=2, color=:black)\n\n    return plot(p1, p2, p3, layout=(3, 1), size=(800, 600))\nend\n\n# Create training plots\ntrain_colors = Dict(:context => :blue, :arm => :red, :reward => :green)\ntrain_plot = create_bandit_plots(train_arm_choices, train_context_choices, train_rewards,\n    \"Training Data\", train_colors)\n\n# Create test plots  \ntest_colors = Dict(:context => :lightblue, :arm => :pink, :reward => :lightgreen)\ntest_plot = create_bandit_plots(test_arm_choices, test_context_choices, test_rewards,\n    \"Test Data\", test_colors)\n\n# Display both\nplot(train_plot, test_plot, layout=(1, 2), size=(1600, 600),\n    plot_title=\"Contextual Bandit Experiment: Training and Test Data\")","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"@model function conditional_regression(n_arms, priors, past_rewards, past_choices, past_contexts)\n    local θ\n    local γ\n    local τ\n\n    # Prior for each arm's parameters\n    for k in 1:n_arms\n        θ[k] ~ priors[:θ][k]\n        γ[k] ~ priors[:γ][k]\n    end\n\n    # Prior for the noise precision\n    τ ~ priors[:τ]\n\n    # Model for past observations\n    for n in eachindex(past_rewards)\n        arm_vals[n] ~ NormalMixture(switch=past_choices[n], m=θ, p=γ)\n        latent_context[n] ~ past_contexts[n]\n        past_rewards[n] ~ softdot(arm_vals[n], latent_context[n], τ)\n    end\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Let's define the priors.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"priors_rng = StableRNG(42)\npriors = Dict(\n    :θ => [MvNormalMeanPrecision(randn(priors_rng, context_dim), diagm(ones(context_dim))) for _ in 1:n_arms],\n    :γ => [Wishart(context_dim + 1, diagm(ones(context_dim))) for _ in 1:n_arms],\n    :τ => GammaShapeRate(1.0, 1.0)\n)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Dict{Symbol, Any} with 3 entries:\n  :γ => Wishart{Float64, PDMat{Float64, Matrix{Float64}}, Int64}[Distributi\nons.…\n  :τ => ExponentialFamily.GammaShapeRate{Float64}(a=1.0, b=1.0)\n  :θ => MvNormalMeanPrecision{Float64, Vector{Float64}, Matrix{Float64}}[Mv\nNorm…","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"And finally run the inference.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function run_inference(; n_arms, priors, past_rewards, past_choices, past_contexts, iterations=50, free_energy=true)\n    init = @initialization begin\n        q(θ) = priors[:θ]\n        q(γ) = priors[:γ]\n        q(τ) = priors[:τ]\n        q(latent_context) = MvNormalMeanPrecision(zeros(context_dim), Diagonal(ones(context_dim)))\n    end\n\n    return infer(\n        model=conditional_regression(\n            n_arms=n_arms,\n            priors=priors,\n            past_contexts=past_contexts,\n        ),\n        data=(\n            past_rewards=past_rewards,\n            past_choices=past_choices,\n        ),\n        constraints=MeanField(),\n        initialization=init,\n        showprogress=true,\n        iterations=iterations,\n        free_energy=free_energy\n    )\n\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"run_inference (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"# Utility function to convert context with missing values to MvNormal distribution\nfunction context_to_mvnormal(context_vec; tiny_precision=1e-6, huge_precision=1e6)\n    \"\"\"\n    Convert a context vector (potentially with missing values) to MvNormal distribution.\n\n    Args:\n        context_vec: Vector that may contain missing values\n        tiny_v: Small variance for known values (high precision)\n        huge_var: Large variance for missing values (low precision)\n\n    Returns:\n        MvNormal distribution\n    \"\"\"\n    context_mean = Vector{Float64}(undef, length(context_vec))\n    context_precision = Vector{Float64}(undef, length(context_vec))\n\n    for j in 1:length(context_vec)\n        if ismissing(context_vec[j])\n            context_mean[j] = 0.0\n            context_precision[j] = tiny_precision\n        else\n            context_mean[j] = context_vec[j]\n            context_precision[j] = huge_precision\n        end\n    end\n\n    return MvNormalMeanPrecision(context_mean, Diagonal(context_precision))\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"context_to_mvnormal (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"# Convert to the required types for the model (TRAINING DATA ONLY)\nrewards_data = Float64.(train_rewards)\n\n# Parameters for the covariance matrix\ntiny_precision = 1e-6   # Very high precision (small variance) for known values\nhuge_precision = 1e6  # Very low precision (large variance) for missing values\n\ncontexts_data = [\n    let context = contexts[idx]\n        context_to_mvnormal(context; tiny_precision=tiny_precision, huge_precision=huge_precision)\n    end\n    for idx in train_context_choices  # Use training context choices\n]\n\narm_choices_data = [[Float64(k == chosen_arm) for k in 1:n_arms] for chosen_arm in train_arm_choices];  # Use training arm choices","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"result = run_inference(\n    n_arms=n_arms,\n    priors=priors,\n    past_rewards=rewards_data,\n    past_choices=arm_choices_data,\n    past_contexts=contexts_data,\n    iterations=20,\n    free_energy=false\n)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Inference results:\n  Posteriors       | available for (γ, arm_vals, τ, latent_context, θ)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"# Diagnostics of inferred arms\n\n# 1. MSE of inferred arms coefficients\ninferred_arms = mean.(result.posteriors[:θ][end])\nmse_arms = mean(mean((inferred_arms[i] .- arms[i]) .^ 2) for i in eachindex(arms))\nprintln(\"MSE of inferred arm coefficients: $mse_arms\")","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"MSE of inferred arm coefficients: 0.005138753286482585","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"# Function to compute predicted rewards using softdot rules\nfunction compute_predicted_rewards_with_variance(\n    arm_posteriors,\n    precision_posterior,\n    eval_arm_choices,\n    eval_context_choices,\n    eval_rewards\n)\n    predicted_rewards = []\n    reward_variances = []\n\n    for i in 1:length(eval_rewards)  # Evaluate on all samples in the evaluation set\n        arm_idx = eval_arm_choices[i]\n        ctx_idx = eval_context_choices[i]\n\n        # Get the posterior distributions\n        q_arm = arm_posteriors[arm_idx]  # Posterior over arm parameters\n        q_precision = precision_posterior  # Precision posterior\n\n        # Get the actual context and convert to MvNormal\n        actual_context = contexts[ctx_idx]\n        q_context = context_to_mvnormal(actual_context)\n\n        # Use softdot rule to compute predicted reward distribution\n        predicted_reward_dist = NormalMeanPrecision(\n            mean(q_arm)' * mean(q_context),\n            mean(q_precision)\n        )\n\n        push!(predicted_rewards, mean(predicted_reward_dist))\n        push!(reward_variances, var(predicted_reward_dist))\n    end\n\n    return predicted_rewards, reward_variances\nend\n\n# Function to display evaluation results\nfunction display_evaluation_results(predicted_rewards, reward_variances, actual_rewards,\n    arm_choices, context_choices, dataset_name)\n    println(\"\\n$dataset_name Evaluation Results:\")\n    println(\"Sample | Actual Reward | Predicted Mean | Predicted Std | Arm | Context\")\n    println(\"-------|---------------|----------------|---------------|-----|--------\")\n\n    for i in 1:min(10, length(predicted_rewards))\n        actual = actual_rewards[i]\n        pred_mean = predicted_rewards[i]\n        pred_std = sqrt(reward_variances[i])\n        arm_idx = arm_choices[i]\n        ctx_idx = context_choices[i]\n\n        println(\"$(lpad(i, 6)) | $(rpad(round(actual, digits=4), 13)) | $(rpad(round(pred_mean, digits=4), 14)) | $(rpad(round(pred_std, digits=4), 13)) | $(lpad(arm_idx, 3)) | $(lpad(ctx_idx, 7))\")\n    end\n\n    # Compute prediction metrics\n    prediction_mse = mean((predicted_rewards .- actual_rewards) .^ 2)\n    println(\"\\n$dataset_name Prediction MSE: $prediction_mse\")\n\n    # Compute log-likelihood of actual rewards under predicted distributions\n    log_likelihood = sum(\n        logpdf(Normal(predicted_rewards[i], sqrt(reward_variances[i])), actual_rewards[i])\n        for i in 1:length(predicted_rewards)\n    )\n    println(\"$dataset_name Average log-likelihood: $(log_likelihood / length(predicted_rewards))\")\n\n    return prediction_mse, log_likelihood / length(predicted_rewards)\nend\n\n# Evaluate on TRAINING data\nprintln(\"=== TRAINING DATA EVALUATION ===\")\ntrain_predicted_rewards, train_reward_variances = compute_predicted_rewards_with_variance(\n    result.posteriors[:θ][end],  # Use full posteriors\n    result.posteriors[:τ][end],  # Precision posterior\n    train_arm_choices,\n    train_context_choices,\n    train_rewards\n)\n\ntrain_mse, train_ll = display_evaluation_results(\n    train_predicted_rewards,\n    train_reward_variances,\n    train_rewards,\n    train_arm_choices,\n    train_context_choices,\n    \"Training\"\n)\n\n# Evaluate on TEST data\nprintln(\"\\n=== TEST DATA EVALUATION ===\")\ntest_predicted_rewards, test_reward_variances = compute_predicted_rewards_with_variance(\n    result.posteriors[:θ][end],  # Use full posteriors\n    result.posteriors[:τ][end],  # Precision posterior\n    test_arm_choices,\n    test_context_choices,\n    test_rewards\n)\n\ntest_mse, test_ll = display_evaluation_results(\n    test_predicted_rewards,\n    test_reward_variances,\n    test_rewards,\n    test_arm_choices,\n    test_context_choices,\n    \"Test\"\n)\n\n# Summary comparison\nprintln(\"\\n=== SUMMARY COMPARISON ===\")\nprintln(\"Dataset    | MSE      | Log-Likelihood\")\nprintln(\"-----------|----------|---------------\")\nprintln(\"Training   | $(rpad(round(train_mse, digits=4), 8)) | $(round(train_ll, digits=4))\")\nprintln(\"Test       | $(rpad(round(test_mse, digits=4), 8)) | $(round(test_ll, digits=4))\")\n\nif test_mse > train_mse * 1.2\n    println(\"\\nNote: Test MSE is significantly higher than training MSE - possible overfitting\")\nelseif test_mse < train_mse * 0.8\n    println(\"\\nNote: Test MSE is lower than training MSE - good generalization!\")\nelse\n    println(\"\\nNote: Test and training performance are similar - good generalization\")\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"=== TRAINING DATA EVALUATION ===\n\nTraining Evaluation Results:\nSample | Actual Reward | Predicted Mean | Predicted Std | Arm | Context\n-------|---------------|----------------|---------------|-----|--------\n     1 | 2.7556        | 2.7721         | 1.3082        |   6 |      11\n     2 | -3.911        | -3.535         | 1.3082        |   2 |      14\n     3 | -2.0472       | -1.8887        | 1.3082        |   8 |      14\n     4 | 9.6666        | 8.1026         | 1.3082        |   6 |      36\n     5 | -4.1496       | -3.8266        | 1.3082        |   3 |      21\n     6 | -4.7496       | -4.5237        | 1.3082        |   4 |      49\n     7 | -2.1116       | -2.1877        | 1.3082        |   5 |      34\n     8 | 0.3173        | 0.5183         | 1.3082        |   8 |      32\n     9 | -4.9155       | -4.1394        | 1.3082        |   8 |      49\n    10 | -3.0535       | -3.3429        | 1.3082        |   6 |      31\n\nTraining Prediction MSE: 0.16609598579011065\nTraining Average log-likelihood: -1.236149219320253\n\n=== TEST DATA EVALUATION ===\n\nTest Evaluation Results:\nSample | Actual Reward | Predicted Mean | Predicted Std | Arm | Context\n-------|---------------|----------------|---------------|-----|--------\n     1 | 0.3271        | 0.0431         | 1.3082        |   1 |      18\n     2 | -3.2327       | -2.6156        | 1.3082        |   2 |      27\n     3 | -2.1027       | -1.8232        | 1.3082        |   4 |       4\n     4 | 4.7518        | 4.465          | 1.3082        |   1 |       7\n     5 | 3.4953        | 2.9212         | 1.3082        |   8 |      24\n     6 | 1.0455        | 0.3028         | 1.3082        |   4 |      15\n     7 | -0.2608       | -0.2963        | 1.3082        |   9 |      38\n     8 | -6.0007       | -5.6078        | 1.3082        |   8 |       1\n     9 | -3.1662       | -2.8644        | 1.3082        |   7 |      36\n    10 | 0.9946        | 1.1453         | 1.3082        |  10 |      24\n\nTest Prediction MSE: 0.17808283508293526\nTest Average log-likelihood: -1.2396510580518532\n\n=== SUMMARY COMPARISON ===\nDataset    | MSE      | Log-Likelihood\n-----------|----------|---------------\nTraining   | 0.1661   | -1.2361\nTest       | 0.1781   | -1.2397\n\nNote: Test and training performance are similar - good generalization","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"# Additional diagnostics\nprintln(\"\\n=== ADDITIONAL DIAGNOSTICS ===\")\n\n# Show precision/variance information\nprecision_posterior = result.posteriors[:τ][end]\nprintln(\"Inferred noise precision: mean=$(round(mean(precision_posterior), digits=4)), \" *\n        \"std=$(round(std(precision_posterior), digits=4))\")\nprintln(\"Inferred noise variance: $(round(1/mean(precision_posterior), digits=4))\")\nprintln(\"True noise variance: $(round(noise_sd^2, digits=4))\")\n\n# Show arm coefficient statistics\nprintln(\"\\nArm coefficient comparison:\")\nfor i in eachindex(arms)\n    true_arm = arms[i]\n    inferred_arm_posterior = result.posteriors[:θ][end][i]\n    inferred_arm_mean = mean(inferred_arm_posterior)\n    println(\"Arm $i:\")\n    println(\"  True:     $(round.(true_arm, digits=3))\")\n    println(\"  Inferred: $(round.(inferred_arm_mean, digits=3))\")\n    println(\"  MSE:      $(round(mean((inferred_arm_mean .- true_arm).^2), digits=4))\")\nend\n\nprintln(\"\\nNumber of contexts with missing values: \", sum(any(ismissing, ctx) for ctx in contexts))","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"=== ADDITIONAL DIAGNOSTICS ===\nInferred noise precision: mean=0.5843, std=0.0475\nInferred noise variance: 1.7115\nTrue noise variance: 0.01\n\nArm coefficient comparison:\nArm 1:\n  True:     [-0.67, 0.447, 1.374, 1.31, 0.126, 0.684, -1.019, -0.794, 1.775\n, 1.297, -1.644, 0.794, -1.31, -0.037, 1.072, -0.397, -0.239, -0.651, 1.134\n, -0.84]\n  Inferred: [-0.604, 0.429, 1.306, 1.212, 0.094, 0.665, -0.972, -0.744, 1.6\n85, 1.264, -1.578, 0.765, -1.279, -0.01, 1.043, -0.381, -0.209, -0.582, 1.0\n56, -0.76]\n  MSE:      0.003\nArm 2:\n  True:     [2.085, -1.801, 0.483, -0.57, -0.665, 2.243, -1.464, -1.012, -2\n.042, -0.787, 0.591, 0.642, 0.455, 0.054, 0.288, 0.587, -1.694, -0.696, -0.\n301, 2.101]\n  Inferred: [1.997, -1.708, 0.451, -0.542, -0.636, 2.161, -1.364, -0.965, -\n1.964, -0.754, 0.49, 0.614, 0.459, 0.044, 0.171, 0.557, -1.624, -0.59, -0.1\n67, 2.001]\n  MSE:      0.0057\nArm 3:\n  True:     [-0.69, -0.73, -1.417, -1.383, 1.201, 0.576, -0.987, 0.626, 0.1\n87, 0.239, -1.287, 0.147, -0.345, 1.909, 0.093, -0.643, 0.743, 0.725, 0.077\n, -0.008]\n  Inferred: [-0.655, -0.724, -1.353, -1.319, 1.132, 0.548, -0.898, 0.526, 0\n.194, 0.232, -1.263, 0.115, -0.257, 1.834, 0.068, -0.568, 0.703, 0.694, 0.0\n66, 0.001]\n  MSE:      0.0029\nArm 4:\n  True:     [-0.37, 0.888, -1.056, 1.242, 0.628, 1.161, -0.851, -0.428, 0.1\n09, -1.932, 0.105, 0.016, 0.105, 1.434, 0.141, 1.295, -0.931, 1.076, -1.799\n, -0.822]\n  Inferred: [-0.23, 0.849, -1.04, 1.172, 0.477, 1.134, -0.779, -0.355, 0.12\n8, -1.876, 0.097, 0.068, 0.037, 1.372, 0.124, 1.243, -0.905, 1.014, -1.727,\n -0.796]\n  MSE:      0.0044\nArm 5:\n  True:     [-0.217, 0.641, -1.566, -1.487, -0.45, -0.937, -0.59, 1.572, -0\n.229, 0.473, 2.148, -0.614, -0.451, -1.672, -0.224, 0.461, -0.093, 1.038, -\n1.827, 0.698]\n  Inferred: [-0.201, 0.628, -1.515, -1.422, -0.373, -0.907, -0.495, 1.468, \n-0.22, 0.476, 2.079, -0.564, -0.238, -1.613, -0.101, 0.433, -0.104, 0.973, \n-1.731, 0.634]\n  MSE:      0.0062\nArm 6:\n  True:     [0.403, 0.333, 1.549, 0.156, 1.816, -0.626, -1.275, 0.485, 1.23\n5, -1.121, -1.397, -0.658, -1.516, -0.712, -0.411, -1.254, 2.082, -0.53, -1\n.64, -0.769]\n  Inferred: [0.227, 0.312, 1.496, 0.107, 1.705, -0.594, -1.151, 0.383, 1.17\n4, -1.069, -1.351, -0.631, -1.445, -0.645, -0.4, -1.167, 2.001, -0.41, -1.5\n89, -0.716]\n  MSE:      0.0064\nArm 7:\n  True:     [1.528, 0.269, 1.215, 0.067, 0.84, 0.819, -1.459, 0.689, -1.067\n, 1.278, -0.364, -1.031, -0.452, -1.973, 0.266, 0.212, -0.424, -1.286, 0.57\n5, 0.72]\n  Inferred: [1.427, 0.271, 1.184, 0.041, 0.669, 0.79, -1.354, 0.645, -1.021\n, 1.246, -0.335, -0.992, -0.375, -1.893, 0.271, 0.224, -0.355, -1.19, 0.552\n, 0.696]\n  MSE:      0.0044\nArm 8:\n  True:     [-0.651, -1.01, -0.863, 1.512, 0.743, -1.477, -0.288, -0.288, -\n0.496, -0.151, 0.53, -0.429, -1.288, 0.95, 2.584, 0.719, -0.205, 1.232, -1.\n135, -0.626]\n  Inferred: [-0.581, -0.964, -0.852, 1.411, 0.673, -1.425, -0.219, -0.262, \n-0.466, -0.11, 0.486, -0.396, -1.228, 0.854, 2.469, 0.685, -0.191, 1.095, -\n1.062, -0.546]\n  MSE:      0.0047\nArm 9:\n  True:     [-1.049, 2.431, -0.434, 0.316, 1.271, -0.947, 0.131, 0.423, 0.1\n62, -1.648, -0.058, -0.573, 1.01, -0.237, 0.212, -0.347, 0.143, -1.574, 0.7\n96, 0.944]\n  Inferred: [-1.0, 2.341, -0.415, 0.143, 1.223, -0.91, 0.113, 0.439, 0.094,\n -1.557, -0.066, -0.531, 0.986, -0.146, 0.122, -0.118, 0.131, -1.518, 0.769\n, 0.869]\n  MSE:      0.0069\nArm 10:\n  True:     [0.447, -1.369, 0.6, 0.392, -0.449, 0.049, -0.558, -1.213, -0.2\n44, -0.289, 0.85, -0.834, 0.803, 1.531, -0.387, -1.258, -1.299, -1.05, -0.2\n37, 0.536]\n  Inferred: [0.368, -1.322, 0.586, 0.295, -0.191, 0.04, -0.502, -1.155, -0.\n24, -0.256, 0.813, -0.81, 0.782, 1.47, -0.29, -1.197, -1.278, -0.936, -0.14\n6, 0.505]\n  MSE:      0.0067\n\nNumber of contexts with missing values: 22","category":"page"},{"location":"categories/basic_examples/contextual_bandits/#Comparing-Different-Strategies-for-the-Contextual-Bandit-Problem","page":"Contextual Bandits","title":"Comparing Different Strategies for the Contextual Bandit Problem","text":"","category":"section"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"We'll implement and evaluate three different approaches:","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Random Strategy - Selecting arms randomly without using context information\nVanilla Thompson Sampling - Sampling the reward distribution\nRxInfer Predictive Inference - Approximating the predictive posterior via message-passing","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function random_strategy(; rng, n_arms)\n    chosen_arm = rand(rng, 1:n_arms)\n    return chosen_arm\nend\n\nfunction thompson_strategy(; rng, n_arms, current_context, posteriors)\n    # Thompson Sampling: Sample parameter vectors and choose best arm\n    expected_rewards = zeros(n_arms)\n    for k in 1:n_arms\n        # Sample parameters from posterior\n        theta_sample = rand(rng, posteriors[:θ][k])\n        # context might have missing values, so we use the mean of the context\n        augmented_context = mean(context_to_mvnormal(current_context))\n        expected_rewards[k] = dot(theta_sample, augmented_context)\n    end\n\n    # Choose best arm based on sampled parameters\n    chosen_arm = argmax(expected_rewards)\n\n    return chosen_arm\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"thompson_strategy (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"@model function contextual_bandit_predictive(reward, priors, current_context)\n    local θ\n    local γ\n    local τ\n\n    # Prior for each arm's parameters\n    for k in 1:n_arms\n        θ[k] ~ priors[:θ][k]\n        γ[k] ~ priors[:γ][k]\n    end\n\n    τ ~ priors[:τ]\n\n    chosen_arm ~ Categorical(ones(n_arms) ./ n_arms)\n    arm_vals ~ NormalMixture(switch=chosen_arm, m=θ, p=γ)\n    latent_context ~ current_context\n    reward ~ softdot(arm_vals, latent_context, τ)\nend\n\nfunction predictive_strategy(; rng, n_arms, current_context, posteriors)\n\n    priors = Dict(\n        :θ => posteriors[:θ],\n        :γ => posteriors[:γ],\n        :τ => posteriors[:τ]\n    )\n\n    latent_context = context_to_mvnormal(current_context)\n\n    init = @initialization begin\n        q(θ) = priors[:θ]\n        q(τ) = priors[:τ]\n        q(γ) = priors[:γ]\n        q(latent_context) = latent_context\n        q(chosen_arm) = Categorical(ones(n_arms) ./ n_arms)\n    end\n\n    result = infer(\n        model=contextual_bandit_predictive(\n            priors=priors,\n            current_context=latent_context\n        ),\n        data=(reward=10maximum(train_rewards),),\n        constraints=MeanField(),\n        initialization=init,\n        showprogress=true,\n        iterations=20,\n    )\n\n    chosen_arm = argmax(probvec(result.posteriors[:chosen_arm][end]))\n\n    return chosen_arm\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"predictive_strategy (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"As we defined the strategies, we can proceed to defining the helper functions to run the simulation.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"We will use the following flow:","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"PLAN - Run different strategies\nACT - In this simulation, we're evaluating all strategies in parallel\nOBSERVE - Get rewards for all strategies\nLEARN - Update posteriors based on history\nKEEP HISTORY - Record all results","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"# Helper functions\nfunction select_context(rng, n_contexts)\n    idx = rand(rng, 1:n_contexts)\n    return (index=idx, value=contexts[idx])\nend\n\nfunction plan(rng, n_arms, context, posteriors)\n    # Generate actions from different strategies\n    return Dict(\n        :random => random_strategy(rng=rng, n_arms=n_arms),\n        :thompson => thompson_strategy(rng=rng, n_arms=n_arms, current_context=context, posteriors=posteriors),\n        :predictive => predictive_strategy(rng=rng, n_arms=n_arms, current_context=context, posteriors=posteriors)\n    )\nend\n\nfunction act(rng, strategies)\n    # Here one would choose which strategy to actually follow\n    # For this simulation, we're evaluating all in parallel\n    # In a real scenario, one might return just one: return strategies[:thompson]\n    return strategies\nend\n\nfunction observe(rng, strategies, context, arms, noise_sd)\n    rewards = Dict()\n    for (strategy, arm_idx) in strategies\n        rewards[strategy] = compute_reward(arms[arm_idx], context, noise_sd)\n    end\n    return rewards\nend\n\nfunction learn(rng, n_arms, posteriors, past_rewards, past_choices, past_contexts)\n    # Note that we don't do any forgetting here which might be useful for long-term learning\n    # Prepare priors from current posteriors\n    priors = Dict(:θ => posteriors[:θ], :τ => posteriors[:τ], :γ => posteriors[:γ])\n\n    # Default initialization\n    init = @initialization begin\n        q(θ) = priors[:θ]\n        q(τ) = priors[:τ]\n        q(γ) = priors[:γ]\n        q(latent_context) = MvNormalMeanPrecision(zeros(context_dim), Diagonal(ones(context_dim)))\n    end\n\n    # Run inference\n    results = infer(\n        model=conditional_regression(\n            n_arms=n_arms,\n            priors=priors,\n            past_contexts=context_to_mvnormal.(past_contexts),\n        ),\n        data=(\n            past_rewards=past_rewards,\n            past_choices=past_choices,\n        ),\n        returnvars=KeepLast(),\n        constraints=MeanField(),\n        initialization=init,\n        iterations=20,\n        free_energy=false\n    )\n\n    return results.posteriors\nend\n\nfunction keep_history!(n_arms, history, strategies, rewards, context, posteriors)\n    # Update choices\n    for (strategy, arm_idx) in strategies\n        push!(history[:choices][strategy], [Float64(k == arm_idx) for k in 1:n_arms])\n    end\n\n    # Update rewards\n    for (strategy, reward) in rewards\n        push!(history[:rewards][strategy], reward)\n    end\n\n    # Update real history - using predictive strategy as the actual choice\n    push!(history[:real][:rewards], last(history[:rewards][:predictive]))\n    push!(history[:real][:choices], last(history[:choices][:predictive]))\n\n    # Update contexts\n    push!(history[:contexts][:values], context.value)\n    push!(history[:contexts][:indices], context.index)\n\n    # Update posteriors\n    push!(history[:posteriors], deepcopy(posteriors))\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"keep_history! (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function run_bandit_simulation(n_epochs, window_length, n_arms, n_contexts, context_dim)\n    rng = StableRNG(42)\n\n    # Initialize histories with empty arrays, removing the references to undefined variables\n    history = Dict(\n        :rewards => Dict(:random => [], :thompson => [], :predictive => []),\n        :choices => Dict(:random => [], :thompson => [], :predictive => []),\n        :real => Dict(:rewards => [], :choices => []),\n        :contexts => Dict(:values => [], :indices => []),\n        :posteriors => []\n    )\n\n    # Initialize prior posterior as uninformative \n    posteriors = Dict(:θ => [MvNormalMeanPrecision(randn(rng, context_dim), diagm(ones(context_dim))) for _ in 1:n_arms],\n        :γ => [Wishart(context_dim + 1, diagm(ones(context_dim))) for _ in 1:n_arms],\n        :τ => GammaShapeRate(1.0, 1.0))\n\n    @showprogress for epoch in 1:n_epochs\n        # 1. PLAN - Run different strategies\n        current_context = select_context(rng, n_contexts)\n\n        strategies = plan(rng, n_arms, current_context.value, posteriors)\n\n        # 2. ACT - In this simulation, we're evaluating all strategies in parallel\n        # In a real scenario, you might choose one strategy here\n        chosen_arm = act(rng, strategies)\n\n        # 3. OBSERVE - Get rewards for all strategies\n        rewards = observe(rng, strategies, current_context.value, arms, noise_sd)\n\n        # 4. LEARN - Update posteriors based on history\n        # Only try to learn if we have collected data\n        if mod(epoch, window_length) == 0 && length(history[:real][:rewards]) > 0\n            data_idx = max(1, length(history[:real][:rewards]) - window_length + 1):length(history[:real][:rewards])\n\n            posteriors = learn(\n                rng,\n                n_arms,\n                posteriors,\n                history[:real][:rewards][data_idx],\n                history[:real][:choices][data_idx],\n                history[:contexts][:values][data_idx]\n            )\n\n        end\n\n        # 5. KEEP HISTORY - Record all results\n        keep_history!(n_arms, history, strategies, rewards, current_context, posteriors)\n    end\n\n    return history\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"run_bandit_simulation (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"# Run the simulation\nn_epochs = 5000\nwindow_length = 100\n\nhistory = run_bandit_simulation(n_epochs, window_length, n_arms, n_contexts, context_dim)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Dict{Symbol, Any} with 5 entries:\n  :choices    => Dict{Symbol, Vector{Any}}(:predictive=>[[1.0, 0.0, 0.0, 0.\n0, 0…\n  :contexts   => Dict{Symbol, Vector{Any}}(:values=>[Union{Missing, Float64\n}[0.…\n  :real       => Dict{Symbol, Vector{Any}}(:choices=>[[1.0, 0.0, 0.0, 0.0, \n0.0,…\n  :rewards    => Dict{Symbol, Vector{Any}}(:predictive=>[-1.6093, 6.77366, \n6.80…\n  :posteriors => Any[Dict{Symbol, Any}(:γ=>Wishart{Float64, PDMat{Float64, \nMatr…","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function print_summary_statistics(history, n_epochs)\n    # Additional summary statistics\n    println(\"Random strategy cumulative reward: $(sum(history[:rewards][:random]))\")\n    println(\"Thompson strategy cumulative reward: $(sum(history[:rewards][:thompson]))\")\n    println(\"Predictive strategy cumulative reward: $(sum(history[:rewards][:predictive]))\")\n\n    println(\"Results after $n_epochs epochs:\")\n    println(\"Random strategy average reward: $(mean(history[:rewards][:random]))\")\n    println(\"Thompson strategy average reward: $(mean(history[:rewards][:thompson]))\")\n    println(\"Predictive strategy average reward: $(mean(history[:rewards][:predictive]))\")\nend\n\n# Print the summary statistics\nprint_summary_statistics(history, n_epochs)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Random strategy cumulative reward: -576.4936850284158\nThompson strategy cumulative reward: 27944.014403992333\nPredictive strategy cumulative reward: 25912.790366729852\nResults after 5000 epochs:\nRandom strategy average reward: -0.11529873700568316\nThompson strategy average reward: 5.588802880798466\nPredictive strategy average reward: 5.18255807334597","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function plot_arm_distribution(history, n_arms)\n    # Extract choices\n    random_choices = history[:choices][:random]\n    thompson_choices = history[:choices][:thompson]\n    predictive_choices = history[:choices][:predictive]\n\n    # Convert to arm indices\n    random_arms = [argmax(choice) for choice in random_choices]\n    thompson_arms = [argmax(choice) for choice in thompson_choices]\n    predictive_arms = [argmax(choice) for choice in predictive_choices]\n\n    # Count frequencies\n    arm_counts_random = zeros(Int, n_arms)\n    arm_counts_thompson = zeros(Int, n_arms)\n    arm_counts_predictive = zeros(Int, n_arms)\n\n    for arm in random_arms\n        arm_counts_random[arm] += 1\n    end\n\n    for arm in thompson_arms\n        arm_counts_thompson[arm] += 1\n    end\n\n    for arm in predictive_arms\n        arm_counts_predictive[arm] += 1\n    end\n\n    # Create grouped bar plot\n    bar_plot = groupedbar(\n        1:n_arms,\n        [arm_counts_random arm_counts_thompson arm_counts_predictive],\n        title=\"Arm Selection Distribution\",\n        xlabel=\"Arm Index\",\n        ylabel=\"Selection Count\",\n        bar_position=:dodge,\n        bar_width=0.8,\n        alpha=0.7,\n        legend=:topright,\n        labels=[\"Random\" \"Thompson\" \"Predictive\"]\n    )\n\n    return bar_plot\nend\n\n# Plot arm distribution\narm_distribution_plot = plot_arm_distribution(history, 10)\ndisplay(arm_distribution_plot)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function calculate_improvements(history)\n    # Get final average rewards\n    final_random_avg = mean(history[:rewards][:random])\n    final_thompson_avg = mean(history[:rewards][:thompson])\n    final_predictive_avg = mean(history[:rewards][:predictive])\n\n    # Improvements over random baseline\n    thompson_improvement = (final_thompson_avg - final_random_avg) / abs(final_random_avg) * 100\n    predictive_improvement = (final_predictive_avg - final_random_avg) / abs(final_random_avg) * 100\n\n    println(\"Thompson sampling improves over random by $(round(thompson_improvement, digits=2))%\")\n    println(\"Predictive strategy improves over random by $(round(predictive_improvement, digits=2))%\")\n\n    return Dict(\n        :thompson => thompson_improvement,\n        :predictive => predictive_improvement\n    )\nend\n\n# Calculate and display improvements\nimprovements = calculate_improvements(history)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Thompson sampling improves over random by 4947.24%\nPredictive strategy improves over random by 4594.9%\nDict{Symbol, Float64} with 2 entries:\n  :predictive => 4594.9\n  :thompson   => 4947.24","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function analyze_doubly_robust_uplift(history, target_strategy=:predictive, baseline_strategy=:random)\n    \"\"\"\n    Compute doubly robust uplift estimate from simulation history\n    \"\"\"\n    target_rewards = history[:rewards][target_strategy]\n    baseline_rewards = history[:rewards][baseline_strategy]\n\n    # Simple Direct Method - just difference in average rewards\n    direct_method = mean(target_rewards) - mean(baseline_rewards)\n\n    # For IPW, we use the fact that all strategies were evaluated on same contexts\n    # So propensity is uniform across arms for random, and we can estimate others\n    n_epochs = length(target_rewards)\n    n_arms = length(history[:choices][target_strategy][1])\n\n    # IPW correction (simplified since we have parallel evaluation)\n    ipw_correction = 0.0\n    for i in 1:n_epochs\n        # Get actual choice and reward (using predictive as \"real\" policy)\n        real_choice = history[:choices][:predictive][i]\n        real_reward = history[:rewards][:predictive][i]\n\n        target_choice = history[:choices][target_strategy][i]\n        baseline_choice = history[:choices][baseline_strategy][i]\n\n        # Simple propensity estimates\n        target_propensity = target_strategy == :random ? 1 / n_arms : 0.5  # rough estimate\n        baseline_propensity = baseline_strategy == :random ? 1 / n_arms : 0.5\n\n        # IPW terms (simplified)\n        if target_choice == real_choice\n            ipw_correction += real_reward / target_propensity\n        end\n        if baseline_choice == real_choice\n            ipw_correction -= real_reward / baseline_propensity\n        end\n    end\n    ipw_correction /= n_epochs\n\n    # Doubly robust = direct method + IPW correction\n    doubly_robust = direct_method + ipw_correction * 0.1  # damped correction\n\n    return Dict(\n        :direct_method => direct_method,\n        :doubly_robust => doubly_robust,\n        :target_mean => mean(target_rewards),\n        :baseline_mean => mean(baseline_rewards),\n        :uplift_percent => (direct_method / mean(baseline_rewards)) * 100\n    )\nend","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"analyze_doubly_robust_uplift (generic function with 3 methods)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"# Analyze uplift - no changes to existing code needed!\npredictive_vs_random = analyze_doubly_robust_uplift(history, :predictive, :random)\nthompson_vs_random = analyze_doubly_robust_uplift(history, :thompson, :random)\n\nprintln(\"Predictive vs Random:\")\nprintln(\"  Direct Method: $(round(predictive_vs_random[:direct_method], digits=4))\")\nprintln(\"  Doubly Robust: $(round(predictive_vs_random[:doubly_robust], digits=4))\")\nprintln(\"  Uplift: $(round(predictive_vs_random[:uplift_percent], digits=2))%\")\n\nprintln(\"\\nThompson vs Random:\")\nprintln(\"  Direct Method: $(round(thompson_vs_random[:direct_method], digits=4))\")\nprintln(\"  Doubly Robust: $(round(thompson_vs_random[:doubly_robust], digits=4))\")\nprintln(\"  Uplift: $(round(thompson_vs_random[:uplift_percent], digits=2))%\")","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Predictive vs Random:\n  Direct Method: 5.2979\n  Doubly Robust: 5.8461\n  Uplift: -4594.9%\n\nThompson vs Random:\n  Direct Method: 5.7041\n  Doubly Robust: 5.8795\n  Uplift: -4947.24%","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function plot_moving_averages(history, n_epochs, ma_window=20)\n    # Calculate moving average rewards\n    ma_rewards_random = [mean(history[:rewards][:random][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]\n    ma_rewards_thompson = [mean(history[:rewards][:thompson][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]\n    ma_rewards_predictive = [mean(history[:rewards][:predictive][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]\n\n    # Plot moving average\n    plot(1:n_epochs, [ma_rewards_random, ma_rewards_thompson, ma_rewards_predictive],\n        label=[\"Random\" \"Thompson\" \"Predictive\"],\n        title=\"Moving Average Reward\",\n        xlabel=\"Epoch\", ylabel=\"Average Reward\",\n        lw=2)\nend\n\n# Plot moving averages\nplot_moving_averages(history, n_epochs)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function create_comprehensive_plots(history, window=100, k=10)\n      # Create a better color palette\n      colors = palette(:tab10)\n\n      # Plot 1: Arm choices comparison (every k-th point)\n      p1 = plot(title=\"Arm Choices Over Time\", xlabel=\"Epoch\", ylabel=\"Arm Index\",\n            legend=:outertopright, dpi=300)\n      plot!(p1, argmax.(history[:choices][:random][1:k:end]), label=\"Random\", color=colors[1],\n            markershape=:circle, markersize=3, alpha=0.5, linewidth=0)\n      plot!(p1, argmax.(history[:choices][:thompson][1:k:end]), label=\"Thompson\", color=colors[2],\n            markershape=:circle, markersize=3, alpha=0.5, linewidth=0)\n      plot!(p1, argmax.(history[:choices][:predictive][1:k:end]), label=\"Predictive\", color=colors[3],\n            markershape=:circle, markersize=3, alpha=0.5, linewidth=0)\n\n      # Plot 2: Context values (every k-th point)\n      p2 = plot(title=\"Context Changes\", xlabel=\"Epoch\", ylabel=\"Context Index\",\n            legend=false, dpi=300)\n      plot!(p2, history[:contexts][:indices][1:k:end], color=colors[4], linewidth=1.5)\n\n      # Plot 3: Reward comparison (every k-th point)\n      p3 = plot(title=\"Rewards by Strategy\", xlabel=\"Epoch\", ylabel=\"Reward Value\",\n            legend=:outertopright, dpi=300)\n      plot!(p3, history[:rewards][:random][1:k:end], label=\"Random\", color=colors[1], linewidth=1.5, alpha=0.7)\n      plot!(p3, history[:rewards][:thompson][1:k:end], label=\"Thompson\", color=colors[2], linewidth=1.5, alpha=0.7)\n      plot!(p3, history[:rewards][:predictive][1:k:end], label=\"Predictive\", color=colors[3], linewidth=1.5, alpha=0.7)\n\n      # Plot 4: Cumulative rewards (every k-th point)\n      cumul_random = cumsum(history[:rewards][:random])[1:k:end]\n      cumul_thompson = cumsum(history[:rewards][:thompson])[1:k:end]\n      cumul_predictive = cumsum(history[:rewards][:predictive])[1:k:end]\n\n      p4 = plot(title=\"Cumulative Rewards\", xlabel=\"Epoch\", ylabel=\"Cumulative Reward\",\n            legend=:outertopright, dpi=300)\n      plot!(p4, cumul_random, label=\"Random\", color=colors[1], linewidth=2)\n      plot!(p4, cumul_thompson, label=\"Thompson\", color=colors[2], linewidth=2)\n      plot!(p4, cumul_predictive, label=\"Predictive\", color=colors[3], linewidth=2)\n\n      # Plot 5: Moving average rewards (every k-th point)\n      ma_random = [mean(history[:rewards][:random][max(1, i - window + 1):i]) for i in 1:length(history[:rewards][:random])][1:k:end]\n      ma_thompson = [mean(history[:rewards][:thompson][max(1, i - window + 1):i]) for i in 1:length(history[:rewards][:thompson])][1:k:end]\n      ma_predictive = [mean(history[:rewards][:predictive][max(1, i - window + 1):i]) for i in 1:length(history[:rewards][:predictive])][1:k:end]\n\n      p5 = plot(title=\"$window-Epoch Moving Average Rewards\", xlabel=\"Epoch\", ylabel=\"Avg Reward\",\n            legend=:outertopright, dpi=300)\n      plot!(p5, ma_random, label=\"Random\", color=colors[1], linewidth=2)\n      plot!(p5, ma_thompson, label=\"Thompson\", color=colors[2], linewidth=2)\n      plot!(p5, ma_predictive, label=\"Predictive\", color=colors[3], linewidth=2)\n\n      # Combine all plots with a title\n      combined_plot = plot(p1, p2, p3, p4, p5,\n            layout=(5, 1),\n            size=(900, 900),\n            plot_title=\"Bandit Strategies Comparison (shows every $k th point)\",\n            plot_titlefontsize=14,\n            left_margin=10Plots.mm,\n            bottom_margin=10Plots.mm)\n\n      return combined_plot\nend\n\ncreate_comprehensive_plots(history, window_length, 10)  # Using k=10 for prettier plots","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Thompson and Predictive strategies both significantly outperform Random. Both intelligent strategies quickly adapt to changing contexts. The Predictive strategy shows a slight edge over Thompson sampling in final performance, demonstrating the effectiveness of Bayesian approaches in sequential decision-making under uncertainty.","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"function plot_moving_averages(history, n_epochs, ma_window=20)\n    # Calculate moving average rewards\n    ma_rewards_random = [mean(history[:rewards][:random][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]\n    ma_rewards_thompson = [mean(history[:rewards][:thompson][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]\n    ma_rewards_predictive = [mean(history[:rewards][:predictive][max(1, i - ma_window + 1):i]) for i in 1:n_epochs]\n\n    # Plot moving average\n    plot(1:n_epochs, [ma_rewards_random, ma_rewards_thompson, ma_rewards_predictive],\n        label=[\"Random\" \"Thompson\" \"Predictive\"],\n        title=\"Moving Average Reward\",\n        xlabel=\"Epoch\", ylabel=\"Average Reward\",\n        lw=2)\nend\n\n# Plot moving averages\nplot_moving_averages(history, n_epochs)","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/contextual_bandits/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [92933f4c] ProgressMeter v1.11.0\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [f3b207a7] StatsPlots v0.15.8\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/contextual_bandits/","page":"Contextual Bandits","title":"Contextual Bandits","text":"","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Advanced-Tutorial","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"using RxInfer, Plots","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This notebook covers the fundamentals and advanced usage of the RxInfer.jl package.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#General-model-specification-syntax","page":"Advanced Tutorial","title":"General model specification syntax","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We use the @model macro from the GraphPPL.jl package to create a probabilistic model p(s y) and we also specify extra constraints on the variational family of distributions mathcalQ, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of GraphPPL.jl.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `@model` macro accepts a regular Julia function\n@model function test_model1(s_mean, s_precision, y)\n    \n    # the `tilde` operator creates a functional dependency\n    # between variables in our model and can be read as \n    # `sampled from` or `is modeled by`\n    s ~ Normal(mean = s_mean, precision = s_precision)\n    y ~ Normal(mean = s, precision = 1.0)\n    \n    # It is possible to return something from the model specification (including variables and nodes)\n    return \"Hello world\"\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @model macro creates a function with the same name and with the same set of input arguments as the original function (test_model1(s_mean, s_precision, y) in this example). The arguments are however converted to the keyword arguments. The @model macro does not support positional arguments.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use control flow statements such as if or for blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the @model block. As an example consider the following (valid!) model:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model2(y)\n    \n    if length(y) <= 1\n        error(\"The `length` of `y` argument must be greater than one.\")\n    end\n    \n    s[1] ~ Normal(mean = 0.0, precision = 0.1)\n    y[1] ~ Normal(mean = s[1], precision = 1.0)\n    \n    for i in eachindex(y)\n        s[i] ~ Normal(mean = s[i - 1], precision = 1.0)\n        y[i] ~ Normal(mean = s[i], precision = 1.0)\n    end\n    \nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use complex expressions inside the functional dependency expressions","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y ~ Normal(mean = 2.0 * (s + 1.0), precision = 1.0)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `~` creates random variables automatically\ns ~ Normal(mean = 0.0, precision1.0)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Probabilistic-inference-in-RxInfer.jl","page":"Advanced Tutorial","title":"Probabilistic inference in RxInfer.jl","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl uses the Rocket.jl package API for inference routines. Rocket.jl is a reactive programming extension for Julia that is higly inspired by RxJS and similar libraries from the Rx ecosystem. It consists of observables, actors, subscriptions and operators. For more information and rigorous examples see Rocket.jl github page.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Observables","page":"Advanced Tutorial","title":"Observables","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Observables are lazy push-based collections and they deliver their values over time.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Timer that emits a new value every second and has an initial one second delay \nobservable = timer(300, 300)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerObservable(300, 300)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"actor = (value) -> println(value)\nsubscription1 = subscribe!(observable, actor)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We always need to unsubscribe from some observables\nunsubscribe!(subscription1)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We can modify our observables\nmodified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ProxyObservable(Int64, MapProxy(Int64))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscription2 = subscribe!(modified, (value) -> println(value))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(subscription2)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Coin-Toss-Model","page":"Advanced Tutorial","title":"Coin Toss Model","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model(y)\n    # We endow θ parameter of our model with some prior\n    θ  ~ Beta(2.0, 7.0)\n    # We assume that the outcome of each coin flip \n    # is modeled by a Bernoulli distribution\n    y .~ Bernoulli(θ)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can call the infer function to run inference in such model:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"p = 0.75 # Bias of a coin\n\ndataset = float.(rand(Bernoulli(p), 500));\n\nresult = infer(\n    model = coin_toss_model(),\n    data  = (y = dataset, )\n)\n\nprintln(\"Inferred bias is \", mean(result.posteriors[:θ]), \" with standard deviation is \", std(result.posteriors[:θ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inferred bias is 0.75049115913556 with standard deviation is 0.019161551535\n43022","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can see that the inferred bias is quite close to the actual value we used in the dataset generation with a low standard deviation.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Reactive-Online-Inference","page":"Advanced Tutorial","title":"Reactive Online Inference","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function online_coin_toss_model(θ_a, θ_b, y)\n    θ ~ Beta(θ_a, θ_b)\n    y ~ Bernoulli(θ)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"autoupdates = @autoupdates begin \n    θ_a, θ_b = params(q(θ))\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@autoupdates begin\n    (θ_a, θ_b) = params(q(θ))\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"init = @initialization begin\n    q(θ) = vague(Beta)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Initial state: \n  q(θ) = Distributions.Beta{Float64}(α=1.0, β=1.0)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"rxresult = infer(\n    model = online_coin_toss_model(),\n    data  = (y = dataset, ),\n    autoupdates = autoupdates,\n    historyvars = (θ = KeepLast(), ),\n    keephistory = length(dataset),\n    initialization = init,\n    autostart = true\n);","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"animation = @animate for i in 1:length(dataset)\n    plot(mean.(rxresult.history[:θ][1:i]), ribbon = std.(rxresult.history[:θ][1:i]), title = \"Online coin bias inference\", label = \"Inferred bias\", legend = :bottomright)\n    hline!([ p ], label = \"Real bias\", size = (600, 200))\nend\n\ngif(animation, \"online-coin-bias-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we used static dataset and the history field of the reactive inference result, but the rxinference function also supports any real-time reactive stream and can run indefinitely.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, RxInfer is not limited to only the sum-product algorithm but it also supports variational message passing with Constrained Bethe Free Energy Minimisation.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Variational-inference","page":"Advanced Tutorial","title":"Variational inference","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"On a very high-level, RxInfer is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions q in mathcalQ. Often this involves assuming some factorization over q. ","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6(y)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we want to specify extra constraints for q_a for Bethe factorisation:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)\nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl package exports @constraints macro to simplify factorisation and form constraints specification. Read more about @constraints macro in the corresponding documentation section, here we show a simple example of the same factorisation constraints specification, but with @constraints macro:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints6 = @constraints begin\n     q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints: \n  q(μ, τ) = q(μ)q(τ)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"init = @initialization begin\n    q(μ) = vague(NormalMeanPrecision)\n    q(τ) = vague(GammaShapeRate)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Initial state: \n  q(μ) = ExponentialFamily.NormalMeanPrecision{Float64}(μ=0.0, w=1.0e-12)\n  q(τ) = ExponentialFamily.GammaShapeRate{Float64}(a=1.0, b=1.0e-12)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Inference","page":"Advanced Tutorial","title":"Inference","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To run inference in this model we again need to create a synthetic dataset and call the infer function.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);\nresult = infer(\n    model          = test_model6(),\n    data           = (y = dataset, ),\n    constraints    = constraints6, \n    initialization = init,\n    returnvars     = (μ = KeepLast(), τ = KeepLast()),\n    iterations     = 10,\n    free_energy    = true,\n    showprogress   = true\n)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inference results:\n  Posteriors       | available for (μ, τ)\n  Free Energy:     | Real[14763.3, 3275.03, 677.388, 638.643, 638.643, 638.\n643, 638.643, 638.643, 638.643, 638.643]","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -2.9700093849073688, std = 0.014380585796886015","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 4.835550763488422, std = 0.21603647575082605","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Form-constraints","page":"Advanced Tutorial","title":"Form constraints","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to support form constraints, the @constraints macro supports additional type specifications for posterior marginals.  For example, here how we can perform the EM algorithm with PointMass form constraint.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7(y)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"As in the previous example we can use @constraints macro to achieve the same goal with a nicer syntax:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints7 = @constraints begin \n    q(μ) :: PointMassFormConstraint()\n    \n    q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints: \n  q(μ, τ) = q(μ)q(τ)\n  q(μ) :: PointMassFormConstraint()","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);\nresult = infer(\n    model          = test_model7(),\n    data           = (y = dataset, ),\n    constraints    = constraints7, \n    initialization = init,\n    returnvars     = (μ = KeepLast(), τ = KeepLast()),\n    iterations     = 10,\n    free_energy    = true,\n    showprogress   = true\n)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inference results:\n  Posteriors       | available for (μ, τ)\n  Free Energy:     | Real[14766.5, 2055.24, 630.968, 630.968, 630.968, 630.\n968, 630.968, 630.968, 630.968, 630.968]","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -3.0269364027239596, std = 0.0","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 4.882623721575535, std = 0.21813954042033434","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Meta-data-specification","page":"Advanced Tutorial","title":"Meta data specification","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During model specification some functional dependencies may accept an optional meta object in the where { ... } clause. The purpose of the meta object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The meta object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n# the autoregressive process and its order. In addition it specifies that the message computation rules should\n# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n# by cost of possible numerical instabilities during an inference procedure\ns[i] ~ AR(s[i - 1], θ, γ) where { meta = ARMeta(Multivariate, order, ARsafe()) }\n...\ns[i] ~ AR(s[i - 1], θ, γ) where { meta = ARMeta(Univariate, order, ARunsafe()) }","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Another example with GaussianControlledVariance, or simply GCV [see Hierarchical Gaussian Filter], node:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\nxt ~ GCV(xt_min, zt, real_k, real_w) where { meta = GCVMetadata(GaussHermiteCubature(21)) }","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#GraphPPL.jl-@meta-macro","page":"Advanced Tutorial","title":"GraphPPL.jl @meta macro","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Users can use @meta macro from the GraphPPL.jl package to achieve the same goal. Read more about @meta macro in the corresponding documentation section. Here is a simple example of the same meta specification:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@meta begin \n     AR(s, θ, γ) -> ARMeta(Multivariate, 5, ARsafe())\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Meta: \n  ReactiveMP.AR(s, θ, γ) -> ReactiveMP.ARMeta{Distributions.Multivariate, R\neactiveMP.ARsafe}(5, ReactiveMP.ARsafe())","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Creating-custom-nodes-and-message-computation-rules","page":"Advanced Tutorial","title":"Creating custom nodes and message computation rules","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/#Custom-nodes","page":"Advanced Tutorial","title":"Custom nodes","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To create a custom functional form and to make it available during model specification the ReactiveMP inference engine exports the @node macro:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n\n# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\nstruct NormalMeanVariance end \n\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# It is also possible to use function objects as a node functional form\nfunction dot end\n\n# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n# out = dot(x, a)\n@node typeof(dot) Deterministic [ out, x, a ]","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"After that it is possible to use the newly created node during model specification:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n    ...\n    y ~ dot(x, a)\n    ...\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Custom-messages-computation-rules","page":"Advanced Tutorial","title":"Custom messages computation rules","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl exports the @rule macro to create custom message computation rules. For example let us create a simple + node to be available for usage in the model specification usage. We refer to A Factor Graph Approach to Signal Modelling , System Identification and Filtering [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the + node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for + node is the following:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu_z = mu_x + mu_y\nV_z = V_x + V_y\nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To specify this in RxInfer.jl we use the @node and @rule macros:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@node typeof(+) Deterministic  [ z, x, y ]\n\n@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n    x_mean, x_var = mean_var(m_x)\n    y_mean, y_var = mean_var(m_y)\n    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example, for the @rule macro, we specify a type of our functional form: typeof(+). Next, we specify an edge we are going to compute an outbound message for. Marginalisation indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(z) = int q(z x y) mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: ) (Image: )","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu(z) = int f(x y z)mu(x)mu(y)mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nnu(z) = exp int log f(x y z)q(x)q(y)mathrmdxmathrmdy \nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @rule macro supports both cases with special prefixes during rule specification:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"m_ prefix corresponds to the incoming message on a specific edge\nq_ prefix corresponds to the posterior marginal of a specific edge","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Sum-Product rule with m_ messages used:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n    m_out_mean, m_out_cov = mean_cov(m_out)\n    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Variational rule with Mean-Field assumption with q_ posteriors used:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n    return NormalMeanPrecision(mean(q_out), mean(q_τ))\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl also supports structured rules. It is possible to obtain joint marginal over a set of edges:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n    m, V = mean_cov(q_out_μ)\n    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n    α = convert(typeof(θ), 1.5)\n    return Gamma(α, θ)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"NOTE: In the @rule specification the messages or marginals arguments must be in order with interfaces specification from @node macro:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference backend expects arguments in `@rule` macro to be in the same order\n@node NormalMeanPrecision Stochastic [ out, μ, τ ]","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Any rule always has access to the meta information with hidden the meta::Any variable:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n    ...\n    println(meta)\n    ...\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to dispatch on a specific type of a meta object:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n    ...\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"or","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n    ...\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Customizing-messages-computational-pipeline","page":"Advanced Tutorial","title":"Customizing messages computational pipeline","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In certain situations it might be convenient to customize the default message computational pipeline. RxInfer.jl supports the pipeline keyword in the where { ... } clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Logs all outbound messages\ny[i] ~ Normal(mean = x[i], precision = 1.0) where { pipeline = LoggerPipelineStage() }\n# In principle, it is possible to approximate outbound messages with Laplace Approximation (this is not an implemented feature, but a concept)\ny[i] ~ Normal(mean = x[i], precision = 1.0) where { pipeline = LaplaceApproximation() }","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let us return to the coin toss model, but this time we want to print flowing messages:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model_log(y)\n    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n    end\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = float.(rand(Bernoulli(p), 5));\nresult = infer(\n    model = coin_toss_model_log(),\n    data  = (y = dataset, )\n)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"[θ]: [Distributions.Beta][out]: DeferredMessage([ use `as_message` to compu\nte the message ])\n[y[1]]: [Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to\n compute the message ])\n[y[2]]: [Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to\n compute the message ])\n[y[3]]: [Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to\n compute the message ])\n[y[4]]: [Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to\n compute the message ])\n[y[5]]: [Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to\n compute the message ])\nInference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/advanced_tutorial/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Multi-agent-Trajectory-Planning","page":"Multi-Agent Trajectory Planning","title":"Multi-agent Trajectory Planning","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"These examples demonstrate the use of RxInfer for trajectory planning in multi-agent situations. The animations show the inferred trajectories from probabilistic inference. The examples shown in this notebook are based on https://github.com/biaslab/MultiAgentTrajectoryPlanning/blob/main/door.jl, prepared by Michi-Tsubaki, extended by bvdmitri. The original code is a part of the paper Multi-Agent Trajectory Planning with NUV Priors by Bart van Erp.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Introduction","page":"Multi-Agent Trajectory Planning","title":"Introduction","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"This notebook demonstrates multi-agent trajectory planning using probabilistic inference with RxInfer.jl. In this example, we model multiple agents navigating through an environment with obstacles while trying to reach their respective goals. The planning problem is formulated as Bayesian inference, where:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Agent states evolve according to linear dynamics\nCollision avoidance between agents and obstacles is encoded as probabilistic constraints\nGoal-seeking behavior is represented as prior distributions","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"By performing inference on this probabilistic model, we can compute optimal trajectories that balance goal-reaching with collision avoidance. The visualization shows how agents coordinate their movements to navigate efficiently through the environment.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"using LinearAlgebra, RxInfer, Plots, LogExpFunctions, StableRNGs","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Environment-setup","page":"Multi-Agent Trajectory Planning","title":"Environment setup","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"To test our ideas, we need an environment to work with. We are going to create a simple environment consisting of a plane with boxes as obstacles. These boxes can be placed anywhere we want on the plane, allowing us to experiment with different configurations and scenarios. This flexible setup will help us evaluate how our multi-agent trajectory planning algorithms perform under various conditions and obstacle arrangements.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# A simple struct to represent a rectangle, which is defined by its center (x, y) and size (width, height)\nBase.@kwdef struct Rectangle\n    center::Tuple{Float64, Float64}\n    size::Tuple{Float64, Float64}\nend\n\nfunction plot_rectangle!(p, rect::Rectangle)\n    # Calculate the x-coordinates of the four corners\n    x_coords = rect.center[1] .+ rect.size[1]/2 * [-1, 1, 1, -1, -1]\n    # Calculate the y-coordinates of the four corners\n    y_coords = rect.center[2] .+ rect.size[2]/2 * [-1, -1, 1, 1, -1]\n    \n    # Plot the rectangle with a black fill\n    plot!(p, Shape(x_coords, y_coords), \n          label = \"\", \n          color = :black, \n          alpha = 0.5,\n          linewidth = 1.5,\n          fillalpha = 0.3)\nend\n\n# A simple struct to represent an environment, which is defined by a list of obctales,\n# and in this demo the obstacles are just rectangles\nBase.@kwdef struct Environment\n    obstacles::Vector{Rectangle}\nend\n\nfunction plot_environment!(p, env::Environment)\n    for obstacle in env.obstacles\n        plot_rectangle!(p, obstacle)\n    end\n    return p\nend\n\nfunction plot_environment(env::Environment)\n    p = plot(size = (800, 400), xlims = (-20, 20), ylims = (-20, 20), aspect_ratio = :equal)\n    plot_environment!(p, env)\n    return p\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"plot_environment (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In the code above, we've defined two key structures for our environment:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Rectangle: A simple structure representing rectangular obstacles, defined by:\ncenter: The (x,y) coordinates of the rectangle's center\nsize: The (width, height) of the rectangle\nEnvironment: A structure that contains a collection of obstacles (rectangles)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"We've also defined several plotting functions:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"plot_rectangle!: Adds a rectangle to an existing plot\nplot_environment!: Adds all obstacles in an environment to an existing plot\nplot_environment: Creates a new plot and displays the environment","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"These structures and functions provide the foundation for visualizing our 2D environment where multi-agent trajectory planning will take place.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Let's create a couple of different environments to demonstrate multi-agent trajectory planning. You can experiment with different obstacle configurations by modifying the rectangle positions, sizes, and quantities. This will allow you to test how the agents navigate around various obstacle arrangements and interact with each other in different scenarios.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Door-environment","page":"Multi-Agent Trajectory Planning","title":"Door environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In this environment, we'll create a scenario resembling a doorway that agents must navigate through. The environment will consist of two wall-like obstacles with a narrow passage between them, simulating a door or gateway. This setup will test the agents' ability to coordinate when passing through a constrained space, which is a common challenge in multi-agent path planning. The narrow passage will force agents to negotiate the right-of-way and potentially wait for each other to pass through, demonstrating emergent cooperative behaviors.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"door_environment = Environment(obstacles = [\n    Rectangle(center = (-40, 0), size = (70, 5)),\n    Rectangle(center = (40, 0), size = (70, 5))\n])\n\nplot_environment(door_environment)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Wall-environment","page":"Multi-Agent Trajectory Planning","title":"Wall environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In this environment, we'll create a scenario with a wall in the center that agents must navigate around. The environment will consist of a single elongated obstacle positioned in the middle of the space, forcing agents to choose whether to go above or below the wall. This setup will test the agents' ability to find efficient paths around obstacles and coordinate with each other to avoid congestion on either side of the wall. It represents a common scenario in multi-agent navigation where agents must make decisions about which route to take when faced with a barrier.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"wall_environment = Environment(obstacles = [\n    Rectangle(center = (0, 0), size = (10, 5))\n])\n\nplot_environment(wall_environment)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Combined-environment","page":"Multi-Agent Trajectory Planning","title":"Combined environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In this environment, we'll combine the door and wall scenarios to create a more complex navigation challenge. This environment will feature both a narrow doorway that agents must pass through and a wall obstacle they need to navigate around. This combined setup will test the agents' ability to handle multiple types of obstacles in sequence, requiring more sophisticated path planning and coordination. Agents will need to negotiate the doorway and then decide which path to take around the wall, or vice versa depending on their starting and goal positions. This represents a more realistic scenario where environments often contain various types of obstacles that require different navigation strategies.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"combined_environment = Environment(obstacles = [\n    Rectangle(center = (-50, 0), size = (70, 2)),\n    Rectangle(center = (50, -0), size = (70, 2)),\n    Rectangle(center = (5, -1), size = (3, 10))\n])\n\nplot_environment(combined_environment)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Agent-state","page":"Multi-Agent Trajectory Planning","title":"Agent state","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In this section, we define states and goals for our agents. Each agent has a initial position and target end position. These states will be used to drive agent movement through the environment. The trajectory planning algorithm will use this information to generate paths from start to destination while avoiding obstacles. We start by first defining the necessary structures and functions for the goals.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"\n# Agent plan, encodes start and goal states\nBase.@kwdef struct Agent\n    radius::Float64\n    initial_position::Tuple{Float64, Float64}\n    target_position::Tuple{Float64, Float64}\nend\n\nfunction plot_marker_at_position!(p, radius, position; color=\"red\", markersize=10.0, alpha=1.0, label=\"\")\n    # Draw the agent as a circle with the given radius\n    θ = range(0, 2π, 100)\n    \n    x_coords = position[1] .+ radius .* cos.(θ)\n    y_coords = position[2] .+ radius .* sin.(θ)\n    \n    plot!(p, Shape(x_coords, y_coords); color=color, label=label, alpha=alpha)\n    return p\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"plot_marker_at_position! (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Let see how does one of configurations for a single agent might look like in the first door environment. For this we will use two agents with different radius, as well as different initial and taget positions.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"function plot_agent_naive_plan!(p, agent; color = \"blue\")\n    plot_marker_at_position!(p, agent.radius, agent.initial_position, color = color)\n    plot_marker_at_position!(p, agent.radius, agent.target_position, color = color, alpha = 0.1)\n    quiver!(p, [ agent.initial_position[1] ], [ agent.initial_position[2] ], quiver = ([ agent.target_position[1] - agent.initial_position[1] ], [ agent.target_position[2] -  agent.initial_position[2] ]))\nend\n\nlet pe = plot_environment(door_environment)\n    agents = [ \n        Agent(radius = 2.5, initial_position = (-4, 10), target_position = (-10, -10)),\n        Agent(radius = 1.5, initial_position = (-10, 5), target_position = (10, -15)),\n        Agent(radius = 1.0, initial_position = (-15, -10), target_position = (10, 10)),\n        Agent(radius = 2.5, initial_position = (0, -10), target_position = (-10, 15))\n    ]\n    \n    colors = Plots.palette(:tab10)\n    \n    for (k, agent) in enumerate(agents)\n        plot_agent_naive_plan!(pe, agent, color = colors[k])\n    end\n    \n    pe\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The plot above illustrates that naive trajectory from initial to target position will obviously not work and the agents will hit either the wall or each other while trying to execute their plan. Thus we need to come up with a better plan and simultaneously take into account multiple agents in the same environment.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Next-Steps","page":"Multi-Agent Trajectory Planning","title":"Next Steps","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Now that we have set up our environment, defined our agents, and created utility functions, we are ready to build an RxInfer model to solve this multi-agent trajectory planning problem. In the following sections, we will:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Define a probabilistic model that captures the dynamics of our agents\nIncorporate collision avoidance constraints between agents and obstacles using NUV priors\nUse message passing to infer optimal trajectories\nVisualize the resulting paths","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"This will demonstrate how probabilistic programming with RxInfer can elegantly solve complex planning problems while handling uncertainty and constraints in a principled way.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Half-space-prior-implementation","page":"Multi-Agent Trajectory Planning","title":"Half space prior implementation","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"For our multi-agent trajectory planning model, we need to implement half-space priors to handle collision avoidance constraints. These priors allow us to model the requirement that agents must stay outside of obstacles and maintain safe distances from each other. The mathematical details and theoretical foundation of these half-space priors can be found in the paper referenced at the beginning of this notebook. The implementation below defines the necessary node and message-passing rules for incorporating these constraints into our probabilistic model.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# Define the probabilistic model for obstacles using halfspace constraints\nstruct Halfspace end\n\n@node Halfspace Stochastic [out, a, σ2, γ]\n\n# rule specification\n@rule Halfspace(:out, Marginalisation) (q_a::Any, q_σ2::Any, q_γ::Any) = begin\n    return NormalMeanVariance(mean(q_a) + mean(q_γ) * mean(q_σ2), mean(q_σ2))\nend\n\n@rule Halfspace(:σ2, Marginalisation) (q_out::Any, q_a::Any, q_γ::Any, ) = begin\n    # `BayesBase.TerminalProdArgument` is used to ensure that the result of the posterior computation is equal to this value\n    return BayesBase.TerminalProdArgument(PointMass( 1 / mean(q_γ) * sqrt(abs2(mean(q_out) - mean(q_a)) + var(q_out))))\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Distance-functions-for-collision-avoidance","page":"Multi-Agent Trajectory Planning","title":"Distance functions for collision avoidance","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In addition to the halfspace priors, we need to implement distance functions to properly handle collision avoidance between agents and obstacles. These functions will calculate the distance between agents and obstacles, which is essential for determining when collision avoidance constraints should be activated. The distance functions will be used to ensure that agents maintain safe distances from each other and from obstacles in the environment. In the next section, we'll define utility functions that include these distance calculations for different geometric shapes like rectangles and circles.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"softmin(x; l=10) = -logsumexp(-l .* x) / l\n\n# state here is a 4-dimensional vector [x, y, vx, vy]\nfunction distance(r::Rectangle, state)\n    if abs(state[1] - r.center[1]) > r.size[1] / 2 || abs(state[2] - r.center[2]) > r.size[2] / 2\n        # outside of rectangle\n        dx = max(abs(state[1] - r.center[1]) - r.size[1] / 2, 0)\n        dy = max(abs(state[2] - r.center[2]) - r.size[2] / 2, 0)\n        return sqrt(dx^2 + dy^2)\n    else\n        # inside rectangle\n        return max(abs(state[1] - r.center[1]) - r.size[1] / 2, abs(state[2] - r.center[2]) - r.size[2] / 2)\n    end\nend\n\nfunction distance(env::Environment, state)\n    return softmin([distance(obstacle, state) for obstacle in env.obstacles])\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"distance (generic function with 2 methods)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"We use the softmin function to create a smooth approximation of the minimum distance between an agent and multiple obstacles. Unlike the regular min function which returns the exact minimum value, softmin produces a differentiable approximation that considers all distances with a weighted average, heavily biased toward the smallest values.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The parameter l controls the \"sharpness\" of the approximation - with larger values making the function behave more like the true minimum. This smoothness is particularly valuable in optimization contexts as it:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Avoids discontinuities that could cause numerical issues during inference\nProvides gradient information from all obstacles, not just the closest one\nCreates a more stable optimization landscape for trajectory planning","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"When calculating the distance between an agent and the environment, softmin helps create a continuous repulsive field around all obstacles, allowing for more natural avoidance behaviors.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Model-specification","page":"Multi-Agent Trajectory Planning","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"We use RxInfer's @model macro to specify the model. In the current example we fix our model to exactly 4 agents to simplify the model creation and construction. We also define auxiliary functions g and h which computes distance with agent's radius offset and minimum distance between all agents pairwise.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# Helper function, distance with radius offset\nfunction g(environment, radius, state)\n    return distance(environment, state) - radius\nend\n\n# Helper function, finds minimum distances between agents pairwise\nfunction h(environment, radiuses, states...)\n    # Calculate pairwise distances between all agents\n    distances = Real[]\n    n = length(states)\n\n    for i in 1:n\n        for j in (i+1):n\n            push!(distances, norm(states[i] - states[j]) - radiuses[i] - radiuses[j])\n        end\n    end\n\n    return softmin(distances)\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"h (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# For more details about the model, please refer to the original paper\n@model function path_planning_model(environment, agents, goals, nr_steps)\n\n    # Model's parameters are fixed, refer to the original \n    # paper's implementation for more details about these parameters\n    local dt = 1\n    local A  = [1 dt 0 0; 0 1 0 0; 0 0 1 dt; 0 0 0 1]\n    local B  = [0 0; dt 0; 0 0; 0 dt]\n    local C  = [1 0 0 0; 0 0 1 0]\n    local γ  = 1\n\n    local control\n    local state\n    local path   \n    \n    # Extract radiuses of each agent in a separate collection\n    local rs = map((a) -> a.radius, agents)\n\n    # Model is fixed for 4 agents\n    for k in 1:4\n\n        # Prior on state, the state structure is 4 dimensional, where\n        # [ x_position, x_velocity, y_position, y_velocity ]\n        state[k, 1] ~ MvNormal(mean = zeros(4), covariance = 1e2I)\n\n        for t in 1:nr_steps\n\n            # Prior on controls\n            control[k, t] ~ MvNormal(mean = zeros(2), covariance = 1e-1I)\n\n            # State transition\n            state[k, t+1] ~ A * state[k, t] + B * control[k, t]\n\n            # Path model, the path structure is 2 dimensional, where \n            # [ x_position, y_position ]\n            path[k, t] ~ C * state[k, t+1]\n\n            # Environmental distance\n            zσ2[k, t] ~ GammaShapeRate(3 / 2, γ^2 / 2)\n            z[k, t]   ~ g(environment, rs[k], path[k, t])\n            \n            # Halfspase priors were defined previousle in this experiment\n            z[k, t] ~ Halfspace(0, zσ2[k, t], γ)\n\n        end\n\n        # goal priors (indexing reverse due to definition)\n        goals[1, k] ~ MvNormal(mean = state[k, 1], covariance = 1e-5I)\n        goals[2, k] ~ MvNormal(mean = state[k, nr_steps+1], covariance = 1e-5I)\n\n    end\n\n    for t = 1:nr_steps\n\n        # observation constraint\n        dσ2[t] ~ GammaShapeRate(3 / 2, γ^2 / 2)\n        d[t] ~ h(environment, rs, path[1, t], path[2, t], path[3, t], path[4, t])\n        d[t] ~ Halfspace(0, dσ2[t], γ)\n\n    end\n\nend\n\n@constraints function path_planning_constraints()\n    # Mean-field variational constraints on the parameters\n    q(d, dσ2) = q(d)q(dσ2)\n    q(z, zσ2) = q(z)q(zσ2)\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"path_planning_constraints (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Constraint-specification","page":"Multi-Agent Trajectory Planning","title":"Constraint specification","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"function path_planning(; environment, agents, nr_iterations = 350, nr_steps = 40, seed = 42)\n    # Fixed number of agents\n    nr_agents = 4\n\n    # Form goals compatible with the model\n    goals = hcat(\n        map(agents) do agent\n            return [\n                [ agent.initial_position[1], 0, agent.initial_position[2], 0 ],\n                [ agent.target_position[1], 0, agent.target_position[2], 0 ]\n            ]\n        end...\n    )\n    \n    rng = StableRNG(seed)\n    \n    # Initialize variables, more details about initialization \n    # can be found in the original paper\n    init = @initialization begin\n\n        q(dσ2) = repeat([PointMass(1)], nr_steps)\n        q(zσ2) = repeat([PointMass(1)], nr_agents, nr_steps)\n        q(control) = repeat([PointMass(0)], nr_steps)\n\n        μ(state) = MvNormalMeanCovariance(randn(rng, 4), 100I)\n        μ(path) = MvNormalMeanCovariance(randn(rng, 2), 100I)\n\n    end\n\n    # Define approximation methods for the non-linear functions used in the model\n    # `Linearization` is a simple and fast approximation method, but it is not\n    # the most accurate one. For more details about the approximation methods,\n    # please refer to the RxInfer documentation\n    door_meta = @meta begin \n        h() -> Linearization()\n        g() -> Linearization()\n    end\n\n    results = infer(\n        model \t\t\t= path_planning_model(environment = environment, agents = agents, nr_steps = nr_steps),\n        data  \t\t\t= (goals = goals, ),\n        initialization  = init,\n        constraints \t= path_planning_constraints(),\n        meta \t\t\t= door_meta,\n        iterations \t\t= nr_iterations,\n        returnvars \t\t= KeepLast(), \n        options         = (limit_stack_depth = 300, )\n    )\n\n    return results\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"path_planning (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"function execute_and_save_animation(environment, agents; gifname = \"result.gif\", kwargs...)\n    result = path_planning(environment = environment, agents = agents; kwargs...)\n    paths  = mean.(result.posteriors[:path])\n    \n    nr_agents, nr_steps = size(paths)\n    colors = Plots.palette(:tab10)\n\n    animation = @animate for t in 1:nr_steps\n        frame = plot_environment(environment)\n    \n        for k in 1:nr_agents\n            position = paths[k, t]          \n            path = paths[k, 1:t]\n            \n            plot_marker_at_position!(frame, agents[k].radius, position, color = colors[k])\n            plot_marker_at_position!(frame, agents[k].radius, agents[k].target_position, color = colors[k], alpha = 0.2)\n            plot!(frame, getindex.(path, 1), getindex.(path, 2); linestyle=:dash, label=\"\", color=colors[k])\n        end\n\n        frame\n    end\n\n    # assign the path to save the image\n    gif(animation, gifname, fps=15, show_msg = false)\n    \n    return nothing\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# These are the same agents as in the beginning of the notebook, but copy-pasted here \n# for easier experimentation, closer to the actual experiments\nagents = [\n    Agent(radius = 2.5, initial_position = (-4, 10), target_position = (-10, -10)),\n    Agent(radius = 1.5, initial_position = (-10, 5), target_position = (10, -15)),\n    Agent(radius = 1.0, initial_position = (-15, -10), target_position = (10, 10)),\n    Agent(radius = 2.5, initial_position = (0, -10), target_position = (-10, 15))\n]","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"4-element Vector{Main.anonymous.Agent}:\n Main.anonymous.Agent(2.5, (-4.0, 10.0), (-10.0, -10.0))\n Main.anonymous.Agent(1.5, (-10.0, 5.0), (10.0, -15.0))\n Main.anonymous.Agent(1.0, (-15.0, -10.0), (10.0, 10.0))\n Main.anonymous.Agent(2.5, (0.0, -10.0), (-10.0, 15.0))","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Experiments-and-visualizations","page":"Multi-Agent Trajectory Planning","title":"Experiments and visualizations","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The experiments and animations below demonstrate the power of probabilistic inference for multi-agent trajectory planning in different environments. Let's analyze what we can observe in each scenario:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Door-environment-2","page":"Multi-Agent Trajectory Planning","title":"Door environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In the door environment, we see four agents with different sizes navigating through a narrow passage. The agents demonstrate several interesting behaviors:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"When multiple agents approach the doorway simultaneously, they naturally form a queue, with some agents waiting for others to pass through first\nAgents slow down or speed up based on the presence of other agents near the doorway\nLarger agents (with bigger radius) effectively have precedence in tight spaces, as smaller agents can more easily find alternative paths","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The two different seeds (42 and 123) show how small changes in initialization can lead to different coordination patterns, highlighting the inherent variability in multi-agent systems.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(door_environment, agents; seed = 42, gifname = \"door_42.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(door_environment, agents; seed = 123, gifname = \"door_123.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Wall-environment-2","page":"Multi-Agent Trajectory Planning","title":"Wall environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The wall environment forces agents to choose whether to go above or below the obstacle and distribute themselves between the two possible paths to avoid congestion The choice of path (above or below) appears to be influenced by the agent's initial position.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(wall_environment, agents; seed = 42, gifname = \"wall_42.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(wall_environment, agents; seed = 123, gifname = \"wall_123.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Combined-environment-2","page":"Multi-Agent Trajectory Planning","title":"Combined environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The combined environment presents the most complex challenge, requiring agents to navigate both a doorway and a wall. This environment best showcases the power of the approach, as traditional reactive navigation methods would struggle with the compounding complexity of multiple obstacle types.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(combined_environment, agents; seed = 42, gifname = \"combined_42.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(combined_environment, agents; seed = 123, gifname = \"combined_123.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Practical-Applications-and-Potential-Improvements","page":"Multi-Agent Trajectory Planning","title":"Practical Applications and Potential Improvements","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The multi-agent trajectory planning approach demonstrated in this notebook has numerous real-world applications:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Warehouse robotics: Coordinating multiple robots in fulfillment centers to avoid collisions while efficiently picking and delivering items\nTraffic management: Planning trajectories for autonomous vehicles at intersections or in congested areas\nCrowd simulation: Modeling realistic human movement patterns in architectural design or emergency evacuation planning\nDrone swarms: Coordinating groups of UAVs for tasks like search and rescue, surveillance, or package delivery","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Future extensions to this work could include:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Handling dynamic obstacles that move or change over time\nIncorporating uncertainty in agent dynamics and sensing\nScaling to much larger numbers of agents through more efficient inference algorithms\nAdding communication constraints between agents for more realistic modeling\nIncorporating heterogeneous agent types with different dynamics and capabilities","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Conclusion","page":"Multi-Agent Trajectory Planning","title":"Conclusion","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"This notebook has demonstrated how probabilistic inference can be used to solve the complex problem of multi-agent trajectory planning. By formulating the planning problem as Bayesian inference, we've shown how agents can coordinate their movements to navigate through various challenging environments while avoiding collisions with obstacles and each other.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/multi-agent_trajectory_planning/Project.toml`\n  [2ab3a3ac] LogExpFunctions v0.3.29\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Bayesian-Linear-Regression-Tutorial","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression Tutorial","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"This notebook is an extensive tutorial on Bayesian linear regression with RxInfer and consists of two major parts:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The first part uses a regular Bayesian Linear Regression on a simple application of fuel consumption for a car with synthetic data.\nThe second part is an adaptation of a tutorial from NumPyro and uses Hierarchical Bayesian linear regression on the OSIC pulmonary fibrosis progression dataset from Kaggle.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"using RxInfer, Random, Plots, StableRNGs, LinearAlgebra, StatsPlots, LaTeXStrings, DataFrames, CSV, GLM","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Part-1.-Bayesian-Linear-Regression","page":"Bayesian Linear Regression","title":"Part 1. Bayesian Linear Regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"John recently purchased a new car and is interested in its fuel consumption rate. He believes that this rate has a linear relationship with speed, and as such, he wants to conduct tests by driving his car on different types of roads, recording both the fuel usage and speed. In order to determine the fuel consumption rate, John employs Bayesian linear regression.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Univariate-regression-with-known-noise","page":"Bayesian Linear Regression","title":"Univariate regression with known noise","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"First, he drives the car on a urban road. John enjoys driving on the well-built, wide, and flat urban roads. Urban roads also offer the advantage of precise fuel consumption measurement with minimal noise. Therefore John models the fuel consumption y_ninmathbbR as a normal distribution and treats x_n as a fixed hyperparameter:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginaligned\np(y_n mid a b) = mathcalN(y_n mid a x_n + b  1)\nendaligned","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The recorded speed is denoted as x_n in mathbbR and the recorded fuel consumption as y_n in mathbbR. Prior beliefs on a and b are informed by the vehicle manual.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginaligned\n    p(a) = mathcalN(a mid m_a v_a) \n    p(b) = mathcalN(b mid m_b v_b) \nendaligned","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Together they form the probabilistic model p(y a b) = p(a)p(b) prod_N=1^N p(y_n mid a b) where the goal is to infer the posterior distributions p(a mid y) and p(bmid y).","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He records the speed and fuel consumption for the urban road which is the xdata and ydata.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function generate_data(a, b, v, nr_samples; rng=StableRNG(1234))\n    x = float.(collect(1:nr_samples))\n    y = a .* x .+ b .+ randn(rng, nr_samples) .* sqrt(v)\n    return x, y\nend;","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"x_data, y_data = generate_data(0.5, 25.0, 1.0, 250)\n\nscatter(x_data, y_data, title = \"Dataset (City road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In order to estimate the two parameters with the recorded data, he uses a RxInfer.jl to create the above described model.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression(x, y)\n    a ~ Normal(mean = 0.0, variance = 1.0)\n    b ~ Normal(mean = 0.0, variance = 100.0)    \n    y .~ Normal(mean = a .* x .+ b, variance = 1.0)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He is delighted that he can utilize the inference function from this package, saving him the effort of starting from scratch and enabling him to obtain the desired results for this road. He does note that there is a loop in his model, namely all a and b variables are connected over all observations, therefore he needs to initialize one of the messages and run multiple iterations for the loopy belief propagation algorithm (see detailed explanation). It is worth noting that loopy belief propagation is not guaranteed to converge in general and might be highly influenced by the choice of the initial messages in the initialization argument. He is going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"results = infer(\n    model          = linear_regression(), \n    data           = (y = y_data, x = x_data), \n    initialization = @initialization(μ(b) = NormalMeanVariance(0.0, 100.0)), \n    returnvars     = (a = KeepLast(), b = KeepLast()),\n    iterations     = 20,\n    free_energy    = true\n)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (a, b)\n  Free Energy:     | Real[450.062, 8526.84, 4960.42, 2949.02, 1819.14, 1184\n.44, 827.897, 627.595, 515.064, 451.839, 416.313, 396.349, 385.129, 378.821\n, 375.274, 373.279, 372.156, 371.524, 371.167, 370.966]","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He knows the theoretical coefficients and noise for this car from the manual. He is going to compare the experimental solution with theoretical results.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=L\"Prior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a)$\", c=1,)\npra = vline!(pra, [ 0.5 ], label=L\"True $a$\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results.posteriors[:a], x), title=L\"Posterior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a\\mid y)$\", c=2,)\npsa = vline!(psa, [ 0.5 ], label=L\"True $a$\", c = 3)\n\nplot(pra, psa, size = (1000, 200), xlabel=L\"$a$\", ylabel=L\"$p(a)$\", ylims=[0,Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=L\"Prior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"p(b)\", c=1, legend = :topleft)\nprb = vline!(prb, [ 25 ], label=L\"True $b$\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results.posteriors[:b], x), title=L\"Posterior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"p(b\\mid y)\", c=2, legend = :topleft)\npsb = vline!(psb, [ 25 ], label=L\"True $b$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$b$\", ylabel=L\"$p(b)$\", ylims=[0, Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"a = results.posteriors[:a]\nb = results.posteriors[:b]\n\nprintln(\"Real a: \", 0.5, \" | Estimated a: \", mean_var(a), \" | Error: \", abs(mean(a) - 0.5))\nprintln(\"Real b: \", 25.0, \" | Estimated b: \", mean_var(b), \" | Error: \", abs(mean(b) - 25.0))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Real a: 0.5 | Estimated a: (0.501490188462706, 1.9162284531300301e-7) | Err\nor: 0.001490188462705988\nReal b: 25.0 | Estimated b: (24.81264210195605, 0.0040159675312827) | Error\n: 0.18735789804394898","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Based on the Bethe free energy below, John knows that the loopy belief propagation has actually converged after 20 iterations:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# drop first iteration, which is influenced by the `initmessages`\nplot(2:20, results.free_energy[2:end], title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Univariate-regression-with-unknown-noise","page":"Bayesian Linear Regression","title":"Univariate regression with unknown noise","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Afterwards, he plans to test the car on a mountain road. However, mountain roads are typically narrow and filled with small stones, which makes it more difficult to establish a clear relationship between fuel consumption and speed, leading to an unknown level of noise in the regression model. Therefore, he design a model with unknown Inverse-Gamma distribution on the variance. beginaligned p(y_n mid a b s) = mathcalN(y_n mid ax_n + b s)\np(s) = mathcalIG(smidalpha theta)\np(a) = mathcalN(a mid m_a v_a) \np(b) = mathcalN(b mid m_b v_b)  endaligned","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression_unknown_noise(x, y)\n    a ~ Normal(mean = 0.0, variance = 1.0)\n    b ~ Normal(mean = 0.0, variance = 100.0)\n    s ~ InverseGamma(1.0, 1.0)\n    y .~ Normal(mean = a .* x .+ b, variance = s)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"x_data_un, y_data_un = generate_data(0.5, 25.0, 400.0, 250)\n\nscatter(x_data_un, y_data_un, title = \"Dateset with unknown noise (mountain road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"To solve this problem in closed-from we need to resort to a variational approximation. The procedure will be a combination of variational inference and loopy belief propagation. He chooses constraints = MeanField() as a global variational approximation and provides initial marginals with the initialization argument. He is, again, going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"init_unknown_noise = @initialization begin \n    μ(b) = NormalMeanVariance(0.0, 100.0)\n    q(s) = vague(InverseGamma)\nend\n\nresults_unknown_noise = infer(\n    model           = linear_regression_unknown_noise(), \n    data            = (y = y_data_un, x = x_data_un), \n    initialization  = init_unknown_noise, \n    returnvars      = (a = KeepLast(), b = KeepLast(), s = KeepLast()), \n    iterations      = 20,\n    constraints     = MeanField(),\n    free_energy     = true\n)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (a, b, s)\n  Free Energy:     | Real[1657.49, 1192.08, 1142.31, 1135.43, 1129.19, 1125\n.47, 1123.34, 1122.13, 1121.44, 1121.05, 1120.82, 1120.69, 1120.61, 1120.56\n, 1120.53, 1120.52, 1120.5, 1120.5, 1120.49, 1120.49]","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Based on the Bethe free energy below, John knows that his algorithm has converged after 20 iterations:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"plot(results_unknown_noise.free_energy, title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Below he visualizes the obtained posterior distributions for parameters:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=L\"Prior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a)$\", c=1,)\npra = vline!(pra, [ 0.5 ], label=L\"True $a$\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:a], x), title=L\"Posterior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(a)$\", c=2,)\npsa = vline!(psa, [ 0.5 ], label=L\"True $a$\", c = 3)\n\nplot(pra, psa, size = (1000, 200), xlabel=L\"$a$\", ylabel=L\"$p(a)$\", ylims=[0, Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=L\"Prior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(b)$\", c=1, legend = :topleft)\nprb = vline!(prb, [ 25.0 ], label=L\"True $b$\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:b], x), title=L\"Posterior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(b)$\", c=2, legend = :topleft)\npsb = vline!(psb, [ 25.0 ], label=L\"True $b$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$b$\", ylabel=L\"$p(b)$\", ylims=[0, Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(0.001, 400, length = 1000), (x) -> pdf(InverseGamma(1.0, 1.0), x), title=L\"Prior for $s$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(s)$\", c=1, legend = :topleft)\nprb = vline!(prb, [ 200 ], label=L\"True $s$\", c = 3)\npsb = plot(range(0.001, 400, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:s], x), title=L\"Posterior for $s$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(s)$\", c=2, legend = :topleft)\npsb = vline!(psb, [ 200 ], label=L\"True $s$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$s$\", ylabel=L\"$p(s)$\", ylims=[0, Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He sees that in the presence of more noise the inference result is more uncertain about the actual values for a and b parameters.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"John samples a and b and plot many possible regression lines on the same plot:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"as = rand(results_unknown_noise.posteriors[:a], 100)\nbs = rand(results_unknown_noise.posteriors[:b], 100)\np = scatter(x_data_un, y_data_un, title = \"Linear regression with more noise\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")\nfor (a, b) in zip(as, bs)\n    global p = plot!(p, x_data_un, a .* x_data_un .+ b, alpha = 0.05, color = :red)\nend\n\nplot(p, size = (900, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"From this plot John can see that many lines do fit the data well and there is no definite \"best\" answer to the regression coefficients. He realize that most of these lines, however, resemble a similar angle and shift.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Multivariate-linear-regression","page":"Bayesian Linear Regression","title":"Multivariate linear regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In addition to fuel consumption, he is also interested in evaluating the car's power performance, braking performance, handling stability, smoothness, and other factors. To investigate the car's performance, he includes additional measurements. Essentially, this approach involves performing multiple linear regression tasks simultaneously, using multiple data vectors for x and y with different levels of noise. As in the previous example, he assumes the level of noise to be unknown.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression_multivariate(dim, x, y)\n    a ~ MvNormal(mean = zeros(dim), covariance = 100 * diageye(dim))\n    b ~ MvNormal(mean = ones(dim), covariance = 100 * diageye(dim))\n    W ~ InverseWishart(dim + 2, 100 * diageye(dim))\n    y .~ MvNormal(mean = x .* a .+ b, covariance = W)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"After received all the measurement records, he plots the measurements and performance index:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"dim_mv = 6\nnr_samples_mv = 50\nrng_mv = StableRNG(42)\na_mv = randn(rng_mv, dim_mv)\nb_mv = 10 * randn(rng_mv, dim_mv)\nv_mv = 100 * rand(rng_mv, dim_mv)\n\nx_data_mv, y_data_mv = collect(zip(generate_data.(a_mv, b_mv, v_mv, nr_samples_mv)...));","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p = plot(title = \"Multivariate linear regression\", legend = :topleft)\n\nplt = palette(:tab10)\n\ndata_set_label = [\"\"]\n\nfor k in 1:dim_mv\n    global p = scatter!(p, x_data_mv[k], y_data_mv[k], label = \"Measurement #$k\", ms = 2, color = plt[k])\nend\nxlabel!(L\"$x$\")\nylabel!(L\"$y$\")\np","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Before this data can be used to perform inference, John needs to change its format slightly.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"x_data_mv_processed = map(i -> Diagonal([getindex.(x_data_mv, i)...]), 1:nr_samples_mv)\ny_data_mv_processed = map(i -> [getindex.(y_data_mv, i)...], 1:nr_samples_mv);","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"init = @initialization begin \n    q(W) = InverseWishart(dim_mv + 2, 10 * diageye(dim_mv))\n    μ(b) = MvNormalMeanCovariance(ones(dim_mv), 10 * diageye(dim_mv))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Initial state: \n  q(W) = Distributions.InverseWishart{Float64, PDMats.PDMat{Float64, Matrix\n{Float64}}}(\ndf: 8.0\nΨ: [10.0 0.0 … 0.0 0.0; 0.0 10.0 … 0.0 0.0; … ; 0.0 0.0 … 10.0 0.0; 0.0 0.0\n … 0.0 10.0]\n)\n\n  μ(b) = MvNormalMeanCovariance(\nμ: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\nΣ: [10.0 0.0 … 0.0 0.0; 0.0 10.0 … 0.0 0.0; … ; 0.0 0.0 … 10.0 0.0; 0.0 0.0\n … 0.0 10.0]\n)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"results_mv = infer(\n    model           = linear_regression_multivariate(dim = dim_mv),\n    data            = (y = y_data_mv_processed, x = x_data_mv_processed),\n    initialization  = init,\n    returnvars      = (a = KeepLast(), b = KeepLast(), W = KeepLast()),\n    free_energy     = true,\n    iterations      = 50,\n    constraints     = MeanField()\n)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (a, b, W)\n  Free Energy:     | Real[864.485, 789.026, 769.094, 750.865, 737.67, 724.7\n22, 712.341, 700.865, 690.782, 682.505  …  664.434, 664.434, 664.434, 664.4\n34, 664.434, 664.434, 664.434, 664.434, 664.434, 664.434]","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Again, the algorithm nicely converged, because the Bethe free energy reached a plateau. John also draws the results for the linear regression parameters and sees that the lines very nicely follow the provided data.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p = plot(title = \"Multivariate linear regression\", legend = :topleft, xlabel=L\"$x$\", ylabel=L\"$y$\")\n\n# how many lines to plot\nr = 50\n\ni_a = collect.(eachcol(rand(results_mv.posteriors[:a], r)))\ni_b = collect.(eachcol(rand(results_mv.posteriors[:b], r)))\n\nplt = palette(:tab10)\n\nfor k in 1:dim_mv\n    x_mv_k = x_data_mv[k]\n    y_mv_k = y_data_mv[k]\n\n    for i in 1:r\n        global p = plot!(p, x_mv_k, x_mv_k .* i_a[i][k] .+ i_b[i][k], label = nothing, alpha = 0.05, color = plt[k])\n    end\n\n    global p = scatter!(p, x_mv_k, y_mv_k, label = \"Measurement #$k\", ms = 2, color = plt[k])\nend\n\n# truncate the init step\nf = plot(results_mv.free_energy[2:end], title =\"Bethe free energy convergence\", label = nothing, xlabel = \"Iteration\", ylabel = \"Bethe free energy [nats]\") \n\nplot(p, f, size = (1000, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He needs more iterations to converge in comparison to the very first example, but that is expected since the problem became multivariate and, hence, more difficult.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"i_a_mv = results_mv.posteriors[:a]\n\nps_a = []\n\nfor k in 1:dim_mv\n    \n    local _p = plot(title = L\"Estimated $a_{%$k}$\", xlabel=L\"$a_{%$k}$\", ylabel=L\"$p(a_{%$k})$\", xlims = (-1.5,1.5), xticks=[-1.5, 0, 1.5], ylims=[0, Inf])\n\n    local m_a_mv_k = mean(i_a_mv)[k]\n    local v_a_mv_k = std(i_a_mv)[k, k]\n    \n    _p = plot!(_p, Normal(m_a_mv_k, v_a_mv_k), fillalpha=0.3, fillrange = 0, label=L\"$q(a_{%$k})$\", c=2,)\n    _p = vline!(_p, [ a_mv[k] ], label=L\"True $a_{%$k}$\", c = 3)\n           \n    push!(ps_a, _p)\nend\n\nplot(ps_a...)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"i_b_mv = results_mv.posteriors[:b]\n\nps_b = []\n\nfor k in 1:dim_mv\n    \n    local _p = plot(title = L\"Estimated $b_{%$k}$\", xlabel=L\"$b_{%$k}$\", ylabel=L\"$p(b_{%$k})$\", xlims = (-20,20), xticks=[-20, 0, 20], ylims =[0, Inf])\n    local m_b_mv_k = mean(i_b_mv)[k]\n    local v_b_mv_k = std(i_b_mv)[k, k]\n\n    _p = plot!(_p, Normal(m_b_mv_k, v_b_mv_k), fillalpha=0.3, fillrange = 0, label=L\"$q(b_{%$k})$\", c=2,)\n    _p = vline!(_p, [ b_mv[k] ], label=L\"Real $b_{%$k}$\", c = 3)\n           \n    push!(ps_b, _p)\nend\n\nplot(ps_b...)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He also checks the noise estimation procedure and sees that the noise variance are currently a bit underestimated. Note here that he neglects the covariance terms between the individual elements, which might result in this kind of behaviour.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"scatter(1:dim_mv, v_mv, ylims=(0, 100), label=L\"True $s_d$\")\nscatter!(1:dim_mv, diag(mean(results_mv.posteriors[:W])); yerror=sqrt.(diag(var(results_mv.posteriors[:W]))), label=L\"$\\mathrm{E}[s_d] \\pm \\sigma$\")\nplot!(; xlabel=L\"Dimension $d$\", ylabel=\"Variance\", title=\"Estimated variance of the noise\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Part-2.-Hierarchical-Bayesian-Linear-Regression","page":"Bayesian Linear Regression","title":"Part 2. Hierarchical Bayesian Linear Regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Disclaimer The tutorial below is an adaptation of the Bayesian Hierarchical Linear Regression tutorial implemented in NumPyro. ","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The original author in NumPyro is Carlos Souza. Updated by Chris Stoafer in NumPyro. Adapted to RxInfer by Dmitry Bagaev.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Probabilistic Machine Learning models can not only make predictions about future data but also model uncertainty. In areas such as personalized medicine, there might be a large amount of data, but there is still a relatively small amount available for each patient. To customize predictions for each person, it becomes necessary to build a model for each individual — considering its inherent uncertainties — and then couple these models together in a hierarchy so that information can be borrowed from other similar individuals [1].","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The purpose of this tutorial is to demonstrate how to implement a Bayesian Hierarchical Linear Regression model using RxInfer. To provide motivation for the tutorial, I will use the OSIC Pulmonary Fibrosis Progression competition, hosted on Kaggle.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# https://www.machinelearningplus.com/linear-regression-in-julia/\n# https://nbviewer.org/github/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_hierarchical_linear_regression.ipynb","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Understanding-the-Task","page":"Bayesian Linear Regression","title":"Understanding the Task","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Pulmonary fibrosis is a disorder characterized by scarring of the lungs, and its cause and cure are currently unknown. In this competition, the objective was to predict the severity of decline in lung function for patients. Lung function is assessed based on the output from a spirometer, which measures the forced vital capacity (FVC), representing the volume of air exhaled.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In medical applications, it is valuable to evaluate a model's confidence in its decisions. As a result, the metric used to rank the teams was designed to reflect both the accuracy and certainty of each prediction. This metric is a modified version of the Laplace Log Likelihood (further details will be provided later).","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Now, let's explore the data and dig deeper into the problem involved.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"dataset = CSV.read(\"hbr/osic_pulmonary_fibrosis.csv\", DataFrame);","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"describe(dataset)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"7×7 DataFrame\n Row │ variable       mean     min                        median   max     \n    ⋯\n     │ Symbol         Union…   Any                        Union…   Any     \n    ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ Patient                 ID00007637202177411956430           ID004266\n372 ⋯\n   2 │ Weeks          31.8618  -5                         28.0     133\n   3 │ FVC            2690.48  827                        2641.0   6399\n   4 │ Percent        77.6727  28.8776                    75.6769  153.145\n   5 │ Age            67.1885  49                         68.0     88      \n    ⋯\n   6 │ Sex                     Female                              Male\n   7 │ SmokingStatus           Currently smokes                    Never sm\noke\n                                                               3 columns om\nitted","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"first(dataset, 5)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"5×7 DataFrame\n Row │ Patient                    Weeks  FVC    Percent  Age    Sex      Sm\noki ⋯\n     │ String31                   Int64  Int64  Float64  Int64  String7  St\nrin ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ ID00007637202177411956430     -4   2315  58.2536     79  Male     Ex\n-sm ⋯\n   2 │ ID00007637202177411956430      5   2214  55.7121     79  Male     Ex\n-sm\n   3 │ ID00007637202177411956430      7   2061  51.8621     79  Male     Ex\n-sm\n   4 │ ID00007637202177411956430      9   2144  53.9507     79  Male     Ex\n-sm\n   5 │ ID00007637202177411956430     11   2069  52.0634     79  Male     Ex\n-sm ⋯\n                                                                1 column om\nitted","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The dataset provided us with a baseline chest CT scan and relevant clinical information for a group of patients. Each patient has an image taken at Week = 0, and they undergo numerous follow-up visits over approximately 1-2 years, during which their Forced Vital Capacity (FVC) is measured. For the purpose of this tutorial, we will only consider the Patient ID, the weeks, and the FVC measurements, discarding all other information. Restricting our analysis to these specific columns allowed our team to achieve a competitive score, highlighting the effectiveness of Bayesian hierarchical linear regression models, especially when dealing with uncertainty, which is a crucial aspect of the problem.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Since this is real medical data, the relative timing of FVC measurements varies widely, as shown in the 3 sample patients below:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patientinfo(dataset, patient_id) = filter(:Patient => ==(patient_id), dataset)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patientinfo (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function patientchart(dataset, patient_id; line_kws = true)\n    info = patientinfo(dataset, patient_id)\n    x = info[!, \"Weeks\"]\n    y = info[!, \"FVC\"]\n\n    p = plot(tickfontsize = 10, margin = 1Plots.cm, size = (400, 400), titlefontsize = 11)\n    p = scatter!(p, x, y, title = patient_id, legend = false, xlabel = \"Weeks\", ylabel = \"FVC\")\n    \n    if line_kws\n        # Use the `GLM.jl` package to estimate linear regression\n        linearFormulae = @formula(FVC ~ Weeks)\n        linearRegressor = lm(linearFormulae, patientinfo(dataset, patient_id))\n        linearPredicted = predict(linearRegressor)\n        p = plot!(p, x, linearPredicted, color = :red, lw = 3)\n    end\n\n    return p\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patientchart (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p1 = patientchart(dataset, \"ID00007637202177411956430\")\np2 = patientchart(dataset, \"ID00009637202177434476278\")\np3 = patientchart(dataset, \"ID00010637202177584971671\")\n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"On average, each of the 176 patients provided in the dataset had 9 visits during which their FVC was measured. These visits occurred at specific weeks within the interval [-12, 133]. The decline in lung capacity is evident, but it also varies significantly from one patient to another.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Our task was to predict the FVC measurements for each patient at every possible week within the [-12, 133] interval, along with providing a confidence score for each prediction. In other words, we were required to fill a matrix, as shown below, with the predicted values and their corresponding confidence scores:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The task was ideal for applying Bayesian inference. However, the vast majority of solutions shared within the Kaggle community utilized discriminative machine learning models, disregarding the fact that most discriminative methods struggle to provide realistic uncertainty estimates. This limitation stems from their typical training process, which aims to optimize parameters to minimize certain loss criteria (such as predictive error). As a result, these models do not inherently incorporate uncertainty into their parameters or subsequent predictions. While some methods may produce uncertainty estimates as a by-product or through post-processing steps, these are often heuristic-based and lack a statistically principled approach to estimate the target uncertainty distribution [2].","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Modelling:-Bayesian-Hierarchical-Linear-Regression-with-Partial-Pooling","page":"Bayesian Linear Regression","title":"Modelling: Bayesian Hierarchical Linear Regression with Partial Pooling","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In a basic linear regression, which is not hierarchical, the assumption is that all FVC decline curves share the same α and β values. This model is known as the \"pooled model.\" On the other extreme, we could assume a model where each patient has a personalized FVC decline curve, and these curves are entirely independent of one another. This model is referred to as the \"unpooled model,\" where each patient has completely separate regression lines.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In this analysis, we will adopt a middle ground approach known as \"Partial pooling.\" Specifically, we will assume that while α's and β's are different for each patient, as in the unpooled case, these coefficients share some similarities. This partial pooling will be achieved by modeling each individual coefficient as being drawn from a common group distribution.:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Mathematically, the model is described by the following equations:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginequation\n    beginaligned\n        mu_alpha sim mathcalN(mathrmmean = 00 mathrmvariance = 2500000) \n        sigma_alpha sim mathcalGamma(mathrmshape = 175 mathrmscale = 4554) \n        mu_beta sim mathcalN(mathrmmean = 00 mathrmvariance = 90) \n        sigma_beta sim mathcalGamma(mathrmshape = 175 mathrmscale = 136) \n        alpha_i sim mathcalN(mathrmmean = mu_alpha mathrmprecision = sigma_alpha) \n        beta sim mathcalN(mathrmmean = mu_beta mathrmprecision = sigma_beta) \n        sigma sim mathcalGamma(mathrmshape = 175 mathrmscale = 4554) \n        mathrmFVC_ij sim mathcalN(mathrmmean = alpha_i + t beta_i mathrmprecision = sigma)\n    endaligned\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"where t is the time in weeks. Those are very uninformative priors, but that's ok: our model will converge!","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Implementing this model in RxInfer is pretty straightforward:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function partially_pooled(patient_codes, weeks, data)\n    μ_α ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of α (intercept)\n    μ_β ~ Normal(mean = 0.0, var = 9.0)      # Prior for the mean of β (slope)\n    σ_α ~ Gamma(shape = 1.75, scale = 45.54) # Prior for the precision of α (intercept)\n    σ_β ~ Gamma(shape = 1.75, scale = 1.36)  # Prior for the precision of β (slope)\n\n    n_codes = length(patient_codes)            # Total number of data points\n    n_patients = length(unique(patient_codes)) # Number of unique patients in the data\n\n    local α # Individual intercepts for each patient\n    local β # Individual slopes for each patient\n\n    for i in 1:n_patients\n        α[i] ~ Normal(mean = μ_α, precision = σ_α) # Sample the intercept α from a Normal distribution\n        β[i] ~ Normal(mean = μ_β, precision = σ_β) # Sample the slope β from a Normal distribution\n    end\n\n    σ ~ Gamma(shape = 1.75, scale = 45.54)   # Prior for the standard deviation of the error term\n    \n    local FVC_est\n\n    for i in 1:n_codes\n        FVC_est[i] ~ α[patient_codes[i]] + β[patient_codes[i]] * weeks[i] # FVC estimation using patient-specific α and β\n        data[i] ~ Normal(mean = FVC_est[i], precision = σ)                # Likelihood of the observed FVC data\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Variational constraints are used in variational methods to restrict the set of functions or probability distributions that the method can explore during optimization. These constraints help guide the optimization process towards more meaningful and tractable solutions. We need variational constraints to ensure that the optimization converges to valid and interpretable solutions, avoiding solutions that might not be meaningful or appropriate for the given problem. By incorporating constraints, we can control the complexity and shape of the solutions, making them more useful for practical applications. We use the @constraints macro from RxInfer to define approriate variational constraints.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@constraints function partially_pooled_constraints()\n    # Assume that `μ_α`, `σ_α`, `μ_β`, `σ_β` and `σ` are jointly independent\n    q(μ_α, σ_α, μ_β, σ_β, σ) = q(μ_α)q(σ_α)q(μ_β)q(σ_β)q(σ)\n    # Assume that `μ_α`, `σ_α`, `α` are jointly independent\n    q(μ_α, σ_α, α) = q(μ_α, α)q(σ_α)\n    # Assume that `μ_β`, `σ_β`, `β` are jointly independent\n    q(μ_β, σ_β, β) = q(μ_β, β)q(σ_β)\n    # Assume that `FVC_est`, `σ` are jointly independent\n    q(FVC_est, σ) = q(FVC_est)q(σ) \nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_constraints (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"These @constraints assume some structural independencies in the resulting variational approximation. For simplicity we can also use constraints = MeanField() in the inference function below. That's all for modelling!","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Inference-in-the-model","page":"Bayesian Linear Regression","title":"Inference in the model","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"A significant achievement of Probabilistic Programming Languages, like RxInfer, is the ability to separate model specification and inference. Once I define my generative model with priors, condition statements, and data likelihoods, I can delegate the challenging inference tasks to RxInfer's inference engine.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Calling the inference engine only takes a few lines of code. Before proceeding, let's assign a numerical Patient ID to each patient code, a task that can be easily accomplished using label encoding.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patient_ids          = dataset[!, \"Patient\"] # get the column of all patients\npatient_code_encoder = Dict(map(((id, patient), ) -> patient => id, enumerate(unique(patient_ids))));\npatient_code_column  = map(patient -> patient_code_encoder[patient], patient_ids)\n\ndataset[!, :PatientCode] = patient_code_column\n\nfirst(patient_code_encoder, 5)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"5-element Vector{Pair{InlineStrings.String31, Int64}}:\n \"ID00197637202246865691526\" => 85\n \"ID00388637202301028491611\" => 160\n \"ID00341637202287410878488\" => 142\n \"ID00020637202178344345685\" => 9\n \"ID00305637202281772703145\" => 127","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function partially_pooled_inference(dataset)\n\n    patient_codes = values(dataset[!, \"PatientCode\"])\n    weeks = values(dataset[!, \"Weeks\"])\n    FVC_obs = values(dataset[!, \"FVC\"]);\n\n    init = @initialization begin \n        μ(α) = vague(NormalMeanVariance)\n        μ(β) = vague(NormalMeanVariance)\n        q(α) = vague(NormalMeanVariance)\n        q(β) = vague(NormalMeanVariance)\n        q(σ) = vague(Gamma)\n        q(σ_α) = vague(Gamma)\n        q(σ_β) = vague(Gamma)\n    end\n\n    results = infer(\n        model = partially_pooled(patient_codes = patient_codes, weeks = weeks),\n        data = (data = FVC_obs, ),\n        options = (limit_stack_depth = 500, ),\n        constraints = partially_pooled_constraints(),\n        initialization = init,\n        returnvars = KeepLast(),\n        iterations = 100\n    )\n    \nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_inference (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We use a hybrid message passing approach combining exact and variational inference. In loopy models, where there are cycles or feedback loops in the graphical model, we need to initialize messages to kick-start the message passing process. Messages are passed between connected nodes in the model to exchange information and update beliefs iteratively. Initializing messages provides a starting point for the iterative process and ensures that the model converges to a meaningful solution.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In variational inference procedures, we need to initialize marginals because variational methods aim to approximate the true posterior distribution with a simpler, tractable distribution. Initializing marginals involves providing initial estimates for the parameters of this approximating distribution. These initial estimates serve as a starting point for the optimization process, allowing the algorithm to iteratively refine the approximation until it converges to a close approximation of the true posterior distribution. ","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_inference_results = partially_pooled_inference(dataset)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (α, σ_α, σ_β, σ, FVC_est, μ_β, μ_α, β)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Checking-the-model","page":"Bayesian Linear Regression","title":"Checking the model","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/#Inspecting-the-learned-parameters","page":"Bayesian Linear Regression","title":"Inspecting the learned parameters","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# Convert to `Normal` since it supports easy plotting with `StatsPlots`\nlet \n    local μ_α = Normal(mean_std(partially_pooled_inference_results.posteriors[:μ_α])...)\n    local μ_β = Normal(mean_std(partially_pooled_inference_results.posteriors[:μ_β])...)\n    local α = map(d -> Normal(mean_std(d)...), partially_pooled_inference_results.posteriors[:α])\n    local β = map(d -> Normal(mean_std(d)...), partially_pooled_inference_results.posteriors[:β])\n    \n    local p1 = plot(μ_α, title = \"q(μ_α)\", fill = 0, fillalpha = 0.2, label = false)\n    local p2 = plot(μ_β, title = \"q(μ_β)\", fill = 0, fillalpha = 0.2, label = false)\n    \n    local p3 = plot(title = \"q(α)...\", legend = false)\n    local p4 = plot(title = \"q(β)...\", legend = false)\n    \n    foreach(d -> plot!(p3, d), α) # Add each individual `α` on plot `p3`\n    foreach(d -> plot!(p4, d), β) # Add each individual `β` on plot `p4`\n    \n    plot(p1, p2, p3, p4, size = (1200, 400), layout = @layout([ a b; c d ]))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Looks like our model learned personalized alphas and betas for each patient!","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Visualizing-FVC-decline-curves-for-some-patients","page":"Bayesian Linear Regression","title":"Visualizing FVC decline curves for some patients","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Now, let's visually inspect the FVC decline curves predicted by our model. We will complete the FVC table by predicting all the missing values. To do this, we need to create a table to accommodate the predictions.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function patientchart_bayesian(results, dataset, encoder, patient_id; kwargs...)\n    info            = patientinfo(dataset, patient_id)\n    patient_code_id = encoder[patient_id]\n\n    patient_α = results.posteriors[:α][patient_code_id]\n    patient_β = results.posteriors[:β][patient_code_id]\n\n    estimated_σ = inv(mean(results.posteriors[:σ]))\n    \n    predict_weeks = range(-12, 134)\n\n    predicted = map(predict_weeks) do week\n        pm = mean(patient_α) + mean(patient_β) * week\n        pv = var(patient_α) + var(patient_β) * week ^ 2 + estimated_σ\n        return pm, sqrt(pv)\n    end\n    \n    p = patientchart(dataset, patient_id; kwargs...)\n    \n    return plot!(p, predict_weeks, getindex.(predicted, 1), ribbon = getindex.(predicted, 2), color = :orange)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patientchart_bayesian (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p1 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00007637202177411956430\")\np2 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00009637202177434476278\")\np3 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00011637202177653955184\")\n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The results match our expectations perfectly! Let's highlight the observations:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The model successfully learned Bayesian Linear Regressions! The orange line representing the learned predicted FVC mean closely aligns with the red line representing the deterministic linear regression. More importantly, the model effectively predicts uncertainty, demonstrated by the light orange region surrounding the mean FVC line.\nThe model predicts higher uncertainty in cases where the data points are more dispersed, such as in the 1st and 3rd patients. In contrast, when data points are closely grouped together, as seen in the 2nd patient, the model predicts higher confidence, resulting in a narrower light orange region.\nAdditionally, across all patients, we observe that the uncertainty increases as we look further into the future. The light orange region widens as the number of weeks increases, reflecting the growth of uncertainty over time.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Computing-the-modified-Laplace-Log-Likelihood-and-RMSE","page":"Bayesian Linear Regression","title":"Computing the modified Laplace Log Likelihood and RMSE","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"As mentioned earlier, the competition evaluated models using a modified version of the Laplace Log Likelihood, which takes into account both the accuracy and certainty of each prediction—a valuable feature in medical applications.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"To compute the metric, we predicted both the FVC value and its associated confidence measure (standard deviation σ). The metric is given by the formula:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginequation\n    beginaligned\n        sigma_mathrmclipped = max(sigma 70) \n        delta = min(vert mathrmFVC_mathrmtrue - mathrmFVC_mathrmpredvert 1000) \n        mathrmmetric = -fracsqrt2deltasigma_mathrmclipped - mathrmln(sqrt2sigma_mathrmclipped) \n    endaligned\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"To prevent large errors from disproportionately penalizing results, errors were thresholded at 1000 ml. Additionally, confidence values were clipped at 70 ml to account for the approximate measurement uncertainty in FVC. The final score was determined by averaging the metric across all (Patient, Week) pairs. It is worth noting that metric values will be negative, and a higher score indicates better model performance.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function FVC_predict(results)\n    return broadcast(results.posteriors[:FVC_est], Ref(results.posteriors[:σ])) do f, s\n        return @call_rule NormalMeanPrecision(:out, Marginalisation) (m_μ = f, q_τ = s)\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"FVC_predict (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function compute_rmse(results, dataset)\n    FVC_predicted = FVC_predict(results)\n    return mean((dataset[!, \"FVC\"] .- mean.(FVC_predicted)) .^ 2) ^ (1/2)\nend\n\nfunction compute_laplace_log_likelihood(results, dataset)\n    FVC_predicted = FVC_predict(results)\n    sigma_c = std.(FVC_predicted)\n    sigma_c[sigma_c .< 70] .= 70\n    delta = abs.(mean.(FVC_predicted) .- dataset[!, \"FVC\"])\n    delta[delta .> 1000] .= 1000\n    return mean(-sqrt(2) .* delta ./ sigma_c .- log.(sqrt(2) .* sigma_c))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"compute_laplace_log_likelihood (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"println(\"RMSE: $(compute_rmse(partially_pooled_inference_results, dataset))\")\nprintln(\"Laplace Log Likelihood: $(compute_laplace_log_likelihood(partially_pooled_inference_results, dataset))\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"RMSE: 124.01306287551854\nLaplace Log Likelihood: -6.156795747473707","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"What do these numbers signify? They indicate that adopting this approach would lead to outperforming the majority of public solutions in the competition. In several seconds of inference!","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Interestingly, most public solutions rely on a standard deterministic Neural Network and attempt to model uncertainty through a quantile loss, adhering to a frequentist approach. The importance of uncertainty in single predictions is growing in the field of machine learning, becoming a crucial requirement. Especially when the consequences of an inaccurate prediction are significant, knowing the probability distribution of individual predictions becomes essential.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Add-layer-to-model-hierarchy:-Smoking-Status","page":"Bayesian Linear Regression","title":"Add layer to model hierarchy: Smoking Status","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We can enhance the model by incorporating the column \"SmokingStatus\" as a pooling level, where model parameters will be partially pooled within the groups \"Never smoked,\" \"Ex-smoker,\" and \"Currently smokes.\" To achieve this, we need to:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Encode the \"SmokingStatus\" column. Map the patient encoding to the corresponding \"SmokingStatus\" encodings. Refine and retrain the model with the additional hierarchical structure.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"combine(groupby(dataset, \"SmokingStatus\"), nrow)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"3×2 DataFrame\n Row │ SmokingStatus     nrow\n     │ String31          Int64\n─────┼─────────────────────────\n   1 │ Ex-smoker          1038\n   2 │ Never smoked        429\n   3 │ Currently smokes     82","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"smoking_id_mapping   = Dict(map(((code, smoking_status), ) -> smoking_status => code, enumerate(unique(dataset[!, \"SmokingStatus\"]))))\nsmoking_code_encoder = Dict(map(unique(values(patient_ids))) do patient_id\n    smoking_status = first(unique(patientinfo(dataset, patient_id)[!, \"SmokingStatus\"]))\n    return patient_code_encoder[patient_id] => smoking_id_mapping[smoking_status]\nend)\n\nsmoking_status_patient_mapping = map(id -> smoking_code_encoder[id], 1:length(unique(patient_ids)));","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function partially_pooled_with_smoking(patient_codes, smoking_status_patient_mapping, weeks, data)\n    μ_α_global ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of α (intercept)\n    μ_β_global ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of β (slope)\n    σ_α_global ~ Gamma(shape = 1.75, scale = 45.54) # Corresponds to half-normal with scale 100.0\n    σ_β_global ~ Gamma(shape = 1.75, scale = 1.36)  # Corresponds to half-normal with scale 3.0\n\n    n_codes = length(patient_codes) # Total number of data points\n    n_smoking_statuses = length(unique(smoking_status_patient_mapping)) # Number of different smoking patterns\n    n_patients = length(unique(patient_codes)) # Number of unique patients in the data\n\n    local μ_α_smoking_status  # Individual intercepts for smoking pattern\n    local μ_β_smoking_status  # Individual slopes for smoking pattern\n    \n    for i in 1:n_smoking_statuses\n        μ_α_smoking_status[i] ~ Normal(mean = μ_α_global, precision = σ_α_global)\n        μ_β_smoking_status[i] ~ Normal(mean = μ_β_global, precision = σ_β_global)\n    end\n    \n    local α # Individual intercepts for each patient\n    local β # Individual slopes for each patient\n\n    for i in 1:n_patients\n        α[i] ~ Normal(mean = μ_α_smoking_status[smoking_status_patient_mapping[i]], precision = σ_α_global)\n        β[i] ~ Normal(mean = μ_β_smoking_status[smoking_status_patient_mapping[i]], precision = σ_β_global)\n    end\n\n    σ ~ Gamma(shape = 1.75, scale = 45.54) # Corresponds to half-normal with scale 100.0\n\n    local FVC_est\n\n    for i in 1:n_codes\n        FVC_est[i] ~ α[patient_codes[i]] + β[patient_codes[i]] * weeks[i] # FVC estimation using patient-specific α and β\n        data[i] ~ Normal(mean = FVC_est[i], precision = σ)              # Likelihood of the observed FVC data\n    end\n    \nend\n\n@constraints function partially_pooled_with_smooking_constraints()\n    q(μ_α_global, σ_α_global, μ_β_global, σ_β_global) = q(μ_α_global)q(σ_α_global)q(μ_β_global)q(σ_β_global)\n    q(μ_α_smoking_status, μ_β_smoking_status, σ_α_global, σ_β_global) = q(μ_α_smoking_status)q(μ_β_smoking_status)q(σ_α_global)q(σ_β_global)\n    q(μ_α_global, σ_α_global, μ_β_global, σ_β_global, σ) = q(μ_α_global)q(σ_α_global)q(μ_β_global)q(σ_β_global)q(σ)\n    q(μ_α_global, σ_α_global, α) = q(μ_α_global, α)q(σ_α_global)\n    q(μ_β_global, σ_β_global, β) = q(μ_β_global, β)q(σ_β_global)\n    q(FVC_est, σ) = q(FVC_est)q(σ) \nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_with_smooking_constraints (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function partially_pooled_with_smoking(dataset, smoking_status_patient_mapping)\n    patient_codes = values(dataset[!, \"PatientCode\"])\n    weeks = values(dataset[!, \"Weeks\"])\n    FVC_obs = values(dataset[!, \"FVC\"]);\n\n    init = @initialization begin \n        μ(α) = vague(NormalMeanVariance)\n        μ(β) = vague(NormalMeanVariance)\n        q(σ) = Gamma(1.75, 45.54)\n        q(σ_α_global) = Gamma(1.75, 45.54)\n        q(σ_β_global) = Gamma(1.75, 1.36)\n    end\n    \n    return infer(\n        model = partially_pooled_with_smoking(\n            patient_codes = patient_codes, \n            smoking_status_patient_mapping = smoking_status_patient_mapping, \n            weeks = weeks\n        ),\n        data = (data = FVC_obs, ),\n        options = (limit_stack_depth = 500, ),\n        constraints = partially_pooled_with_smooking_constraints(),\n        initialization = init,\n        returnvars = KeepLast(),\n        iterations = 100,\n    )\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_with_smoking (generic function with 3 methods)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_with_smoking_inference_results = partially_pooled_with_smoking(dataset, smoking_status_patient_mapping);","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Inspect-the-learned-parameters","page":"Bayesian Linear Regression","title":"Inspect the learned parameters","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# Convert to `Normal` since it supports easy plotting with `StatsPlots`\nlet \n    local μ_α = Normal(mean_std(partially_pooled_with_smoking_inference_results.posteriors[:μ_α_global])...)\n    local μ_β = Normal(mean_std(partially_pooled_with_smoking_inference_results.posteriors[:μ_β_global])...)\n    local αsmoking = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:μ_α_smoking_status])\n    local βsmoking = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:μ_β_smoking_status])\n    local α = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:α])\n    local β = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:β])\n    \n    local p1 = plot(μ_α, title = \"q(μ_α_global)\", fill = 0, fillalpha = 0.2, label = false)\n    local p2 = plot(μ_β, title = \"q(μ_β_global)\", fill = 0, fillalpha = 0.2, label = false)\n    \n    local p3 = plot(title = \"q(α)...\", legend = false)\n    local p4 = plot(title = \"q(β)...\", legend = false)\n    \n    foreach(d -> plot!(p3, d), α) # Add each individual `α` on plot `p3`\n    foreach(d -> plot!(p4, d), β) # Add each individual `β` on plot `p4`\n    \n    local p5 = plot(title = \"q(μ_α_smoking_status)...\", legend = false)\n    local p6 = plot(title = \"q(μ_β_smoking_status)...\", legend = false)\n    \n    foreach(d -> plot!(p5, d, fill = 0, fillalpha = 0.2), αsmoking) \n    foreach(d -> plot!(p6, d, fill = 0, fillalpha = 0.2), βsmoking)\n    \n    plot(p1, p2, p3, p4, p5, p6, size = (1200, 600), layout = @layout([ a b; c d; e f ]))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Interpret-smoking-status-model-parameters","page":"Bayesian Linear Regression","title":"Interpret smoking status model parameters","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The model parameters for each smoking status reveal intriguing findings, particularly concerning the trend, μ_β_smoking_status. In the summary below, it is evident that the trend for current smokers has a positive mean, while the trend for ex-smokers and those who have never smoked is negative.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"smoking_id_mapping","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Dict{InlineStrings.String31, Int64} with 3 entries:\n  \"Currently smokes\" => 3\n  \"Ex-smoker\"        => 1\n  \"Never smoked\"     => 2","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"posteriors_μ_β_smoking_status = partially_pooled_with_smoking_inference_results.posteriors[:μ_β_smoking_status]\n\nprintln(\"Trend for\")\nforeach(pairs(smoking_id_mapping)) do (key, id)\n    println(\"  $key: $(mean(posteriors_μ_β_smoking_status[id]))\")\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Trend for\n  Currently smokes: 1.8147143381168995\n  Ex-smoker: -4.572743474510769\n  Never smoked: -4.447769588035918","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Let's look at these curves for individual patients to help interpret these model results.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# Never smoked\np1 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00007637202177411956430\") \n# Ex-smoker\np2 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00009637202177434476278\") \n# Currently smokes\np3 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00011637202177653955184\") \n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Review-patients-that-currently-smoke","page":"Bayesian Linear Regression","title":"Review patients that currently smoke","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"When plotting each patient with the smoking status \"Currently smokes,\" we observe different trends. Some patients show a clear positive trend, while others do not exhibit a clear trend or even have a negative trend. Compared to the unpooled trend lines, the trend lines with partial pooling are less prone to overfitting and display greater uncertainty in both slope and intercept.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Depending on the purpose of the model, we can proceed in different ways:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"If our goal is to gain insights into how different attributes relate to a patient's FVC over time, we can stop here and understand that current smokers might experience an increase in FVC over time when monitored for Pulmonary Fibrosis. We may then formulate hypotheses to explore the reasons behind this observation and design new experiments for further testing.\nHowever, if our aim is to develop a model for generating predictions to treat patients, it becomes crucial to ensure that the model does not overfit and can be trusted with new patients. To achieve this, we could adjust model parameters to shrink the \"Currently smokes\" group's parameters closer to the global parameters, or even consider merging the group with \"Ex-smokers.\" Additionally, collecting more data for current smokers could help in ensuring the model's robustness and preventing overfitting.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"let \n    local plots = []\n\n    for (i, patient) in enumerate(unique(filter(:SmokingStatus => ==(\"Currently smokes\"), dataset)[!, \"Patient\"]))\n        push!(plots, patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, patient))\n    end\n\n    plot(plots..., size = (1200, 1200))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Modified-Laplace-Log-Likelihood-and-RMSE-for-model-with-Smoking-Status-Level","page":"Bayesian Linear Regression","title":"Modified Laplace Log Likelihood and RMSE for model with Smoking Status Level","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We calculate the metrics for the updated model and compare to the original model.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"println(\"RMSE: $(compute_rmse(partially_pooled_with_smoking_inference_results, dataset))\")\nprintln(\"Laplace Log Likelihood: $(compute_laplace_log_likelihood(partially_pooled_with_smoking_inference_results, dataset))\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"RMSE: 124.81042940540623\nLaplace Log Likelihood: -6.165660148605184","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Both the Laplace Log Likelihood and RMSE indicate slightly worse performance for the smoking status model. Adding this hierarchy level as it is did not improve the model's performance significantly. However, we did discover some interesting results from the smoking status level that might warrant further investigation. Additionally, we could attempt to enhance model performance by adjusting priors or exploring different hierarchy levels, such as gender.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#References","page":"Bayesian Linear Regression","title":"References","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"[1] Ghahramani, Z. Probabilistic machine learning and artificial intelligence. Nature 521, 452–459 (2015). https://doi.org/10.1038/nature14541","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"[2] Rainforth, Thomas William Gamlen. Automating Inference, Learning, and Design Using Probabilistic Programming. University of Oxford, 2017.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/bayesian_linear_regression/Project.toml`\n  [336ed68f] CSV v0.10.15\n  [a93c6f00] DataFrames v1.8.0\n  [38e38edf] GLM v1.9.0\n  [b964fa9f] LaTeXStrings v1.4.0\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [f3b207a7] StatsPlots v0.15.8\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Invertible-neural-networks:-a-tutorial","page":"Invertible Neural Network Tutorial","title":"Invertible neural networks: a tutorial","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Table of contents","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Introduction\nModel specification\nModel compilation\nProbabilistic inference\nParameter estimation","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Introduction","page":"Invertible Neural Network Tutorial","title":"Introduction","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Load-required-packages","page":"Invertible Neural Network Tutorial","title":"Load required packages","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Before we can start, we need to import some packages:","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"using RxInfer\nusing Random\nusing StableRNGs\n\nusing ReactiveMP        # ReactiveMP is included in RxInfer, but we explicitly use some of its functionality\nusing LinearAlgebra     # only used for some matrix specifics\nusing Plots             # only used for visualisation\nusing Distributions     # only used for sampling from multivariate distributions\nusing Optim             # only used for parameter optimisation","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Model-specification","page":"Invertible Neural Network Tutorial","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Specifying an invertible neural network model is easy. The general recipe looks like follows: model = FlowModel(input_dim, (layer1(options), layer2(options), ...)). Here the first argument corresponds to the input dimension of the model and the second argument is a tuple of layers. An example model can be defined as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"model = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Alternatively, the input_dim can also be passed as an InputLayer layer as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"In the above AdditiveCouplingLayer layers the input bfx = x_1 x_2 ldots x_N is partitioned into chunks of unit length. These partitions are additively coupled to an output bfy = y_1 y_2 ldots y_N as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"beginaligned\n    y_1 = x_1 \n    y_2 = x_2 + f_1(x_1) \n    vdots \n    y_N = x_N + f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Importantly, this structure can easily be converted as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"beginaligned\n    x_1 = y_1 \n    x_2 = y_2 - f_1(x_1) \n    vdots \n    x_N = y_N - f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"f_n","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"is an arbitrarily complex function, here chosen to be a PlanarFlow, but this can be interchanged for any function or neural network. The permute keyword argument (which defaults to true) specifies whether the output of this layer should be randomly permuted or shuffled. This makes sure that the first element is also transformed in consecutive layers.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"A permutation layer can also be added by itself as a PermutationLayer layer with a custom permutation matrix if desired.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false),\n        PermutationLayer(PermutationMatrix(2)),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Model-compilation","page":"Invertible Neural Network Tutorial","title":"Model compilation","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"In the current models, the layers are setup to work with the passed input dimension. This means that the function f_n is repeated input_dim-1 times for each of the partitions. Furthermore the permutation layers are set up with proper permutation matrices. If we print the model we get","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"model","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"ReactiveMP.FlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayerEmpty{Tuple{R\neactiveMP.PlanarFlowEmpty{1}}}, ReactiveMP.PermutationLayer{Int64}, Reactiv\neMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}}}(2, (\nReactiveMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}\n(2, (ReactiveMP.PlanarFlowEmpty{1}(),), 1), ReactiveMP.PermutationLayer{Int\n64}(2, [0 1; 1 0]), ReactiveMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.\nPlanarFlowEmpty{1}}}(2, (ReactiveMP.PlanarFlowEmpty{1}(),), 1)))","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"The text below describes the terms above. Please note the distinction in typing and elements, i.e. FlowModel{types}(elements):","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"FlowModel - specifies that we are dealing with a flow model.\n3 - Number of layers.\nTuple{AdditiveCouplingLayerEmpty{...},PermutationLayer{Int64},AdditiveCouplingLayerEmpty{...}} - tuple of layer types.\nTuple{ReactiveMP.PlanarFlowEmpty{1},ReactiveMP.PlanarFlowEmpty{1}} - tuple of functions f_n.\nPermutationLayer{Int64}(2, [0 1; 1 0]) - permutation layer with input dimension 2 and permutation matrix [0 1; 1 0].","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"From inspection we can see that the AdditiveCouplingLayerEmpty and PlanarFlowEmpty objects are different than before. They are initialized for the correct dimension, but they do not have any parameters registered to them. This is by design to allow for separating the model specification from potential optimization procedures. Before we perform inference in this model, the parameters should be initialized. We can randomly initialize the parameters as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"compiled_model = compile(model)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"ReactiveMP.CompiledFlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayer{Tupl\ne{ReactiveMP.PlanarFlow{Float64, Float64}}}, ReactiveMP.PermutationLayer{In\nt64}, ReactiveMP.AdditiveCouplingLayer{Tuple{ReactiveMP.PlanarFlow{Float64,\n Float64}}}}}(2, (ReactiveMP.AdditiveCouplingLayer{Tuple{ReactiveMP.PlanarF\nlow{Float64, Float64}}}(2, (ReactiveMP.PlanarFlow{Float64, Float64}(-0.0267\n64523737459973, 1.011996647538435, -1.0115814360310365),), 1), ReactiveMP.P\nermutationLayer{Int64}(2, [0 1; 1 0]), ReactiveMP.AdditiveCouplingLayer{Tup\nle{ReactiveMP.PlanarFlow{Float64, Float64}}}(2, (ReactiveMP.PlanarFlow{Floa\nt64, Float64}(-0.18712184678799276, -0.8640695971228994, 0.265551136938963)\n,), 1)))","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Now we can see that random parameters have been assigned to the individual functions inside of our model. Alternatively if we would like to pass our own parameters, then this is also possible. You can easily find the required number of parameters using the nr_params(model) function.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"ReactiveMP.CompiledFlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayer{Tupl\ne{ReactiveMP.PlanarFlow{Float64, Float64}}}, ReactiveMP.PermutationLayer{In\nt64}, ReactiveMP.AdditiveCouplingLayer{Tuple{ReactiveMP.PlanarFlow{Float64,\n Float64}}}}}(2, (ReactiveMP.AdditiveCouplingLayer{Tuple{ReactiveMP.PlanarF\nlow{Float64, Float64}}}(2, (ReactiveMP.PlanarFlow{Float64, Float64}(0.72964\n12319250487, -0.9767336128037319, -0.4749869451771002),), 1), ReactiveMP.Pe\nrmutationLayer{Int64}(2, [0 1; 1 0]), ReactiveMP.AdditiveCouplingLayer{Tupl\ne{ReactiveMP.PlanarFlow{Float64, Float64}}}(2, (ReactiveMP.PlanarFlow{Float\n64, Float64}(0.3490911082645933, -0.8184067956921087, -1.4578214732352386),\n), 1)))","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Probabilistic-inference","page":"Invertible Neural Network Tutorial","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"We can perform inference in our compiled model through standard usage of RxInfer and its underlying ReactiveMP inference engine. Let's first generate some random 2D data which has been sampled from a standard normal distribution and is consecutively passed through an invertible neural network. Using the forward(model, data) function we can propagate data in the forward direction.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"function generate_data(nr_samples::Int64, model::CompiledFlowModel; seed = 123)\n\n    rng = StableRNG(seed)\n    \n    # specify latent sampling distribution\n    dist = MvNormal([1.5, 0.5], I)\n\n    # sample from the latent distribution\n    x = rand(rng, dist, nr_samples)\n\n    # transform data\n    y = zeros(Float64, size(x))\n    for k = 1:nr_samples\n        y[:,k] .= ReactiveMP.forward(model, x[:,k])\n    end\n\n    # return data\n    return y, x\n\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"# generate data\ny, x = generate_data(1000, compiled_model)\n\n# plot generated data\np1 = scatter(x[1,:], x[2,:], alpha=0.3, title=\"Original data\", size=(800,400))\np2 = scatter(y[1,:], y[2,:], alpha=0.3, title=\"Transformed data\", size=(800,400))\nplot(p1, p2, legend = false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"The probabilistic model for doing inference can be described as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"@model function invertible_neural_network(y)\n\n    # specify prior\n    z_μ ~ MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    z_Λ ~ Wishart(2.0, tiny*diagm(ones(2)))\n\n    # specify observations\n    for k in eachindex(y)\n\n        # specify latent state\n        x[k] ~ MvNormalMeanPrecision(z_μ, z_Λ)\n\n        # specify transformed latent value\n        y_lat[k] ~ Flow(x[k])\n\n        # specify observations\n        y[k] ~ MvNormalMeanCovariance(y_lat[k], tiny*diagm(ones(2)))\n\n    end\n\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Here the model is passed inside a meta data object of the flow node. Inference then resorts to","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"observations = [y[:,k] for k=1:size(y,2)]\n\nfmodel         = invertible_neural_network()\ndata           = (y = observations, )\ninitialization = @initialization begin \n    q(z_μ) = MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    q(z_Λ) = Wishart(2.0, tiny*diagm(ones(2)))\nend\nreturnvars     = (z_μ = KeepLast(), z_Λ = KeepLast(), x = KeepLast(), y_lat = KeepLast())\n\nconstraints = @constraints begin\n    q(z_μ, x, z_Λ) = q(z_μ)q(z_Λ)q(x)\nend\n\n@meta function fmeta(model)\n    compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))\n    Flow(y_lat, x) -> FlowMeta(compiled_model) # defaults to FlowMeta(compiled_model; approximation=Linearization()). \n                                               # other approximation methods can be e.g. FlowMeta(compiled_model; approximation=Unscented(input_dim))\nend\n\n# First execution is slow due to Julia's initial compilation \nresult = infer(\n    model          = fmodel, \n    data           = data,\n    constraints    = constraints,\n    meta           = fmeta(model),\n    initialization = initialization,\n    returnvars     = returnvars,\n    free_energy    = true,\n    iterations     = 10, \n    showprogress   = false\n)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Inference results:\n  Posteriors       | available for (z_μ, z_Λ, y_lat, x)\n  Free Energy:     | Real[29485.3, 23762.9, 23570.6, 23570.6, 23570.6, 2357\n0.6, 23570.6, 23570.6, 23570.6, 23570.6]","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"fe_flow = result.free_energy\nzμ_flow = result.posteriors[:z_μ]\nzΛ_flow = result.posteriors[:z_Λ]\nx_flow  = result.posteriors[:x]\ny_flow  = result.posteriors[:y_lat];","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"As we can see, the variational free energy decreases inside of our model.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"plot(1:10, fe_flow/size(y,2), xlabel=\"iteration\", ylabel=\"normalized variational free energy [nats/sample]\", legend=false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"If we plot a random noisy observation and its approximated transformed uncertainty we obtain:","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"# pick a random observation\nid = rand(StableRNG(321), 1:size(y,2))\nrand_observation = MvNormal(y[:,id], 5e-1*diagm(ones(2)))\nwarped_observation = MvNormal(ReactiveMP.backward(compiled_model, y[:,id]), ReactiveMP.inv_jacobian(compiled_model, y[:,id])*5e-1*diagm(ones(2))*ReactiveMP.inv_jacobian(compiled_model, y[:,id])');\n\np1 = scatter(x[1,:], x[2,:], alpha=0.1, title=\"Latent distribution\", size=(1200,500), label=\"generated data\")\ncontour!(-5:0.1:5, -5:0.1:5, (x, y) -> pdf(MvNormal([1.5, 0.5], I), [x, y]), c=:viridis, colorbar=false, linewidth=2)\nscatter!([mean(zμ_flow)[1]], [mean(zμ_flow)[2]], color=\"red\", markershape=:x, markersize=5, label=\"inferred mean\")\ncontour!(-5:0.01:5, -5:0.01:5, (x, y) -> pdf(warped_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, colorbar=false)\nscatter!([mean(warped_observation)[1]], [mean(warped_observation)[2]], color=\"red\", label=\"transformed noisy observation\")\np2 = scatter(y[1,:], y[2,:], alpha=0.1, label=\"generated data\")\nscatter!([ReactiveMP.forward(compiled_model, mean(zμ_flow))[1]], [ReactiveMP.forward(compiled_model, mean(zμ_flow))[2]], color=\"red\", marker=:x, label=\"inferred mean\")\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(MvNormal([1.5, 0.5], I), ReactiveMP.backward(compiled_model, [x, y])), c=:viridis, colorbar=false, linewidth=2)\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(rand_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, label=\"random noisy observation\", colorba=false)\nscatter!([mean(rand_observation)[1]], [mean(rand_observation)[2]], color=\"red\", label=\"random noisy observation\")\nplot(p1, p2, legend = true)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Parameter-estimation","page":"Invertible Neural Network Tutorial","title":"Parameter estimation","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"The flow model is often used to learn unknown probabilistic mappings. Here we will demonstrate it as follows for a binary classification task with the following data:","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n\n    # sample weights\n    w = rand(rng, nr_samples, 2)\n\n    # sample appraisal\n    y = zeros(Float64, nr_samples)\n    for k = 1:nr_samples\n        y[k] = 1.0*(w[k,1] > 0.5)*(w[k,2] < 0.5)\n    end\n\n    # return data\n    return y, w\n\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"data_y, data_x = generate_data(50);\nscatter(data_x[:,1], data_x[:,2], marker_z=data_y, xlabel=\"w1\", ylabel=\"w2\", colorbar=false, legend=false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"We will then specify a possible model as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"# specify flow model\nmodel = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()), # defaults to AdditiveCouplingLayer(PlanarFlow(); permute=true)\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"The corresponding probabilistic model for the binary classification task can be created as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"@model function invertible_neural_network_classifier(x, y)\n\n    # specify observations\n    for k in eachindex(x)\n\n        # specify latent state\n        x_lat[k] ~ MvNormalMeanPrecision(x[k], 1e3*diagm(ones(2)))\n\n        # specify transformed latent value\n        y_lat1[k] ~ Flow(x_lat[k])\n        y_lat2[k] ~ dot(y_lat1[k], [1, 1])\n\n        # specify observations\n        y[k] ~ Probit(y_lat2[k]) # default: where { pipeline = RequireMessage(in = NormalMeanPrecision(0, 1.0)) }\n\n    end\n\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"fcmodel       = invertible_neural_network_classifier()\ndata          = (y = data_y, x = [data_x[k,:] for k=1:size(data_x,1)], )\n\n@meta function fmeta(model, params)\n    compiled_model = compile(model, params)\n    Flow(y_lat1, x_lat) -> FlowMeta(compiled_model)\nend","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"fmeta (generic function with 2 methods)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Here we see that the compilation occurs inside of our probabilistic model. As a result we can pass parameters (and a model) to this function which we wish to opmize for some criterium, such as the variational free energy. Inference can be described as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"For the optimization procedure, we will simplify our inference loop, such that it only accepts parameters as an argument (which is wishes to optimize) and outputs a performance metric.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"function f(params)\n    Random.seed!(123) # Flow uses random permutation matrices, which is not good for the optimisation procedure\n    result = infer(\n        model                   = fcmodel, \n        data                    = data,\n        meta                    = fmeta(model, params),\n        free_energy             = true,\n        free_energy_diagnostics = nothing, # Free Energy can be set to NaN due to optimization procedure\n        iterations              = 10, \n        showprogress            = false\n    );\n    \n    result.free_energy[end]\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Optimization can be performed using the Optim package. Alternatively, other (custom) optimizers can be implemented, such as:","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(store_trace = true, show_trace = true, show_every = 50), autodiff=:forward)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"uses finitediff and is slower/less accurate.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"or","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"# create gradient function\ng = (x) -> ForwardDiff.gradient(f, x);\n\n# specify initial params\nparams = randn(nr_params(model))\n\n# create custom optimizer (here Adam)\noptimizer = Adam(params; λ=1e-1)\n\n# allocate space for gradient\n∇ = zeros(nr_params(model))\n\n# perform optimization\nfor it = 1:10000\n\n    # backward pass\n    ∇ .= ForwardDiff.gradient(f, optimizer.x)\n\n    # gradient update\n    ReactiveMP.update!(optimizer, ∇)\n\nend\n","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(f_tol = 1e-3, store_trace = true, show_trace = true, show_every = 100), autodiff=:forward)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Iter     Function value   Gradient norm \n     0     5.927695e+02     8.826085e+02\n * time: 0.04146003723144531\n * Status: success\n\n * Candidate solution\n    Final objective value:     5.579987e+01\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 0.00e+00 ≤ 0.0e+00\n    |x - x'|/|x'|          = 0.00e+00 ≤ 0.0e+00\n    |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 1.0e-03\n    |g(x)|                 = 2.22e+01 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   13  (vs limit Inf)\n    Iterations:    5\n    f(x) calls:    211\n    ∇f(x) calls:   211","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"optimization results are then given as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"params = Optim.minimizer(res)\ninferred_model = compile(model, params)\ntrans_data_x_1 = hcat(map((x) -> ReactiveMP.forward(inferred_model, x), [data_x[k,:] for k=1:size(data_x,1)])...)'\ntrans_data_x_2 = map((x) -> dot([1, 1], x), [trans_data_x_1[k,:] for k=1:size(data_x,1)])\ntrans_data_x_2_split = [trans_data_x_2[data_y .== 1.0], trans_data_x_2[data_y .== 0.0]]\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, size=(1200,400), c=:viridis, colorbar=false, title=\"original data\")\np2 = scatter(trans_data_x_1[:,1], trans_data_x_1[:,2], marker_z = data_y, c=:viridis, size=(1200,400), colorbar=false, title=\"|> warp\")\np3 = histogram(trans_data_x_2_split; stacked=true, bins=50, size=(1200,400), title=\"|> dot\")\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"using StatsFuns: normcdf\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, title=\"original labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np2 = scatter(data_x[:,1], data_x[:,2], marker_z = normcdf.(trans_data_x_2), title=\"predicted labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np3 = contour(0:0.01:1, 0:0.01:1, (x, y) -> normcdf(dot([1,1], ReactiveMP.forward(inferred_model, [x,y]))), title=\"Classification map\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/invertible_neural_network_tutorial/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [429524aa] Optim v1.13.2\n  [91a5bcdd] Plots v1.41.1\n  [a194aa59] ReactiveMP v5.6.0\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [4c63d2b9] StatsFuns v1.5.0\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/#Simple-Nonlinear-Node","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"section"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"using RxInfer, Random, StableRNGs","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Here is an example of creating custom node with nonlinear function approximation with samplelist.","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/#Custom-node-creation","page":"Simple Nonlinear Node","title":"Custom node creation","text":"","category":"section"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"struct NonlinearNode end # Dummy structure just to make Julia happy\n\nstruct NonlinearMeta{R, F}\n    rng      :: R\n    fn       :: F   # Nonlinear function, we assume 1 float input - 1 float output\n    nsamples :: Int # Number of samples used in approximation\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@node NonlinearNode Deterministic [ out, in ]","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"We need to define two Sum-product message computation rules for our new custom node","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Rule for outbound message on out edge given inbound message on in edge\nRule for outbound message on in edge given inbound message on out edge\nBoth rules accept optional meta object","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Rule for outbound message on `out` edge given inbound message on `in` edge\n@rule NonlinearNode(:out, Marginalisation) (m_in::NormalMeanVariance, meta::NonlinearMeta) = begin \n    samples = rand(meta.rng, m_in, meta.nsamples)\n    return SampleList(map(meta.fn, samples))\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Rule for outbound message on `in` edge given inbound message on `out` edge\n@rule NonlinearNode(:in, Marginalisation) (m_out::Gamma, meta::NonlinearMeta) = begin     \n    return ContinuousUnivariateLogPdf((x) -> logpdf(m_out, meta.fn(x)))\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/#Model-specification","page":"Simple Nonlinear Node","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"After we have defined our custom node with custom rules we may proceed with a model specification:","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"beginaligned\np(theta) = mathcalN(thetamu_theta sigma_theta)\np(m) = mathcalN(thetamu_m sigma_m)\np(w) = f(theta)\np(y_im w) = mathcalN(y_im w)\nendaligned","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Given this IID model, we aim to estimate the precision of a Gaussian distribution. We pass a random variable theta through a non-linear transformation f to make it positive and suitable for a precision parameter of a Gaussian distribution. We, later on, will estimate the posterior of theta. ","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@model function nonlinear_estimation(y, θ_μ, m_μ, θ_σ, m_σ)\n    \n    # define a distribution for the two variables\n    θ ~ Normal(mean = θ_μ, variance = θ_σ)\n    m ~ Normal(mean = m_μ, variance = m_σ)\n\n    # define a nonlinear node\n    w ~ NonlinearNode(θ)\n\n    # We consider the outcome to be normally distributed\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = m, precision = w)\n    end\n    \nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@constraints function nconstsraints(nsamples)\n    q(θ) :: SampleListFormConstraint(nsamples, LeftProposal())\n    q(w) :: SampleListFormConstraint(nsamples, RightProposal())\n    \n    q(θ, w, m) = q(θ)q(m)q(w)\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nconstsraints (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@meta function nmeta(fn, nsamples)\n    NonlinearNode(θ, w) -> NonlinearMeta(StableRNG(123), fn, nsamples)\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nmeta (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@initialization function ninit()\n    q(m) = vague(NormalMeanPrecision)\n    q(w) = vague(Gamma)\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"ninit (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Here we generate some data","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nonlinear_fn(x) = abs(exp(x) * sin(x))","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nonlinear_fn (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"seed = 123\nrng  = StableRNG(seed)\n\nniters   = 15 # Number of VMP iterations\nnsamples = 5_000 # Number of samples in approximation\n\nn = 500 # Number of IID samples\nμ = -10.0\nθ = -1.0\nw = nonlinear_fn(θ)\n\ndata = rand(rng, NormalMeanPrecision(μ, w), n);","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"now that synthetic data/constriants/model is defined, lets infer:","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"result = infer(\n    model = nonlinear_estimation(θ_μ = 0.0, m_μ = 0.0, θ_σ=100.0, m_σ=1.0),\n    meta =  nmeta(nonlinear_fn, nsamples),\n    constraints = nconstsraints(nsamples),\n    data = (y = data, ), \n    initialization = ninit(),\n    returnvars = (θ = KeepLast(), ),\n    iterations = niters,  \n    showprogress = true\n)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"we can check the posterior now","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"θposterior = result.posteriors[:θ]","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"SampleList(Univariate, 5000)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Let's us visualise the results","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"using Plots, StatsPlots\n\nestimated = Normal(mean_std(θposterior)...)\n\nplot(estimated, title=\"Posterior for θ\", label = \"Estimated\", legend = :bottomright, fill = true, fillopacity = 0.2, xlim = (-3, 3), ylim = (0, 2))\nvline!([ θ ], label = \"Real value of θ\")","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/simple_nonlinear_node/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [f3b207a7] StatsPlots v0.15.8\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/#Learning-Dynamics-with-VAEs","page":"Learning Dynamics With Vaes","title":"Learning Dynamics with VAEs","text":"","category":"section"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Bayesian inference often struggles in high-dimensional spaces (not with RxInfer.jl, it actually happens rarely) - a challenge known as the \"curse of dimensionality.\" ","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces that do not occur in low-dimensional settings. In Bayesian inference context, the curse of dimensionality refers to the exponential increase in computational difficulty and statistical challenges as the number of parameters (dimensions) in a model increases.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"There are several approaches to tackle this problem, among them:","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Hierarchical models - Breaking down complex problems into nested, simpler components\n\"Compromise\" approaches - Being \"partially Bayesian\" by combining Bayesian and non-fully Bayesian methods","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"In this notebook, we explore the compromise approach. We'll use a Variational Autoencoder (VAE) to learn a low-dimensional latent representation of MNIST digits, and then perform Bayesian inference on the dynamics of rotating digits in this latent space.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Variational Autoencoder (VAE): A type of neural network architecture that learns to encode data into a compressed latent representation and then decode it back to the original form. Unlike traditional autoencoders, VAEs encode data as probability distributions rather than fixed points, making them generative models capable of creating new data samples.Latent representation: A compressed, lower-dimensional encoding of data that captures its essential features. Think of it as finding the most important aspects of complex data while discarding noise and redundancy.MNIST digits: A standard dataset in machine learning consisting of 28×28 pixel grayscale images of handwritten digits (0-9), commonly used for training image processing systems.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Note: This is a simple example with simplified data that serves as a template for more interesting applications.Throughout this demonstration we will use CPU for computation; however, the model is fully GPU compatible. See AutoEncoderToolkit.jl for more details.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"This demonstrates how we can leverage the strengths of both worlds:","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"VAEs for efficient dimensionality reduction (non-fully Bayesian)\nBayesian inference for modeling dynamics in the reduced space","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Dimensionality reduction: The process of reducing the number of variables under consideration by obtaining a set of principal variables that still preserves the essential information in the data.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Dynamics: In this context, refers to how the latent representations change over time or with certain transformations (like rotation of digits).","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"import AutoEncoderToolkit\nimport Flux\nusing MLDatasets: MNIST\nimport JLD2\nusing Plots\nusing Statistics\nusing RxInfer, ReactiveMP, LinearAlgebra","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/#Part-1:-Creating-a-Dataset-of-Rotating-Digits","page":"Learning Dynamics With Vaes","title":"Part 1: Creating a Dataset of Rotating Digits","text":"","category":"section"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"First, we need data to work with. Instead of using static MNIST digits, we'll create a dataset of rotating digits. This will give us a clear transformation to model in the latent space.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"We'll select a few examples of digits 0 and 1, then generate multiple rotated versions of each. This will create a dataset where we know exactly how the images are related to each other through the rotation transformation.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"\"\"\"\n    generate_rotated_sequence(image, n_frames=36, max_angle=360)\n\nGenerate a sequence of rotated versions of an input image.\nReturns the sequence of rotated images.\n\"\"\"\nfunction generate_rotated_sequence(image, n_frames=36, max_angle=360)\n    img_2d = image[:, :, 1, 1]\n    \n    # Get image dimensions\n    height, width = size(img_2d)\n    \n    # Create array to store rotated images\n    rotated_images = []\n    \n    for i in 1:n_frames\n        # Calculate rotation angle for this frame\n        angle = (i-1) * max_angle / n_frames\n        \n        # Create rotation matrix\n        rotation = [cosd(angle) -sind(angle); sind(angle) cosd(angle)]\n        \n        # Create empty image for the rotated result\n        rotated = zeros(Float32, height, width)\n        \n        # Center of the image\n        center_y, center_x = (height+1)/2, (width+1)/2\n        \n        # Apply rotation to each pixel\n        for y in 1:height, x in 1:width\n            # Convert to coordinates relative to center\n            y_centered = y - center_y\n            x_centered = x - center_x\n            \n            # Apply rotation\n            new_coords = rotation * [x_centered, y_centered]\n            \n            # Convert back to image coordinates\n            new_x = round(Int, new_coords[1] + center_x)\n            new_y = round(Int, new_coords[2] + center_y)\n            \n            # Check if the new coordinates are within bounds\n            if 1 <= new_x <= width && 1 <= new_y <= height\n                rotated[y, x] = img_2d[new_y, new_x]\n            end\n        end\n        \n        # Store the rotated image\n        push!(rotated_images, rotated)\n    end\n    \n    return rotated_images\nend","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Main.anonymous.generate_rotated_sequence","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"In order to download datasets without having to manually confirm the download, we can set to true the DATADEPS_ALWAYS_ACCEPT environmental variable. Read more about MLDatasets.jl here.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"true","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"\"\"\"\n    create_rotated_mnist_dataset(n_samples_per_digit=10, n_rotations=36)\n\nCreate a dataset of rotated MNIST digits (0s and 1s).\nReturns the dataset and labels.\n\"\"\"\nfunction create_rotated_mnist_dataset(n_samples_per_digit=10, n_rotations=36)\n    # Load MNIST data\n    train_dataset = MNIST(split=:train)\n    \n    # Find indices of 0s and 1s\n    indices_0 = findall(x -> x == 0, train_dataset.targets)\n    indices_1 = findall(x -> x == 1, train_dataset.targets)\n    \n    # Select a subset of each digit\n    selected_indices_0 = indices_0[1:n_samples_per_digit]\n    selected_indices_1 = indices_1[1:n_samples_per_digit]\n    \n    # Combine indices\n    selected_indices = vcat(selected_indices_0, selected_indices_1)\n    \n    # Create arrays to store rotated images and labels\n    rotated_images = []\n    rotated_labels = []\n    \n    # For each selected digit\n    for (i, idx) in enumerate(selected_indices)\n        # Get the original image\n        original = Float32.(reshape(train_dataset.features[:, :, idx], 28, 28))\n        \n        # Get the label\n        label = train_dataset.targets[idx]\n        \n        # Generate rotated versions\n        rotations = generate_rotated_sequence(original, n_rotations)\n        \n        # Add to dataset\n        for rotated in rotations\n            push!(rotated_images, rotated)\n            push!(rotated_labels, label)\n        end\n    end\n    \n    # Convert to arrays\n    X = zeros(Float32, 28, 28, 1, length(rotated_images))\n    for i in 1:length(rotated_images)\n        X[:, :, 1, i] = rotated_images[i]\n    end\n    \n    y = rotated_labels\n    \n    # we will use a binarized version of the MNIST datase\n    threshold = 0.5 * maximum(X)\n    X = Float32.(X .> threshold)\n    \n    return X, y\nend","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Main.anonymous.create_rotated_mnist_dataset","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Create rotated MNIST dataset\nprintln(\"Creating rotated MNIST dataset...\")\nn_samples_per_digit = 10  # Number of original digits to use\nn_rotations = 36  # Number of rotations per digit\n\nrotated_train_data, rotated_train_labels = create_rotated_mnist_dataset(n_samples_per_digit, n_rotations)\nprintln(\"Rotated dataset created with $(size(rotated_train_data, 4)) images\")\n\n# Create an animation of sample rotated digits\nsample_indices = rand(1:n_samples_per_digit*2, 5)  # 5 random digits from our dataset\nsample_animation = @animate for angle_idx in 1:n_rotations\n    # Create a plot with 5 random digits at the same rotation angle\n    p = plot(layout=(1, 5), size=(1000, 200))\n    for (i, digit_idx) in enumerate(sample_indices)\n        # Calculate the index in the full dataset\n        idx = (digit_idx-1) * n_rotations + angle_idx\n        # Use binary colors (white background, black digits)\n        heatmap!(p, rotated_train_data[:, :, 1, idx], \n                color=[:black, :white], \n                colorbar=false,\n                title=\"Digit: $(rotated_train_labels[idx])\", \n                subplot=i, axis=false)\n    end\n    plot!(p, title=\"Rotation: $(round((angle_idx-1)*360/n_rotations, digits=1))°\")\nend\n\n# Save as GIF with 10 frames per second\ngif(sample_animation, \"rotated_digits_samples.gif\", fps=10, show_msg=false);","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Creating rotated MNIST dataset...\nRotated dataset created with 720 images","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/#Part-2:-Building-the-VAE-Model","page":"Learning Dynamics With Vaes","title":"Part 2: Building the VAE Model","text":"","category":"section"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Now we'll define our Variational Autoencoder. The VAE will learn to compress the high-dimensional image data (28×28 = 784 dimensions) into a much lower-dimensional latent space (just 2 dimensions in this example).","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Variational Autoencoder (VAE) architecture: A VAE consists of two main components:An encoder network that compresses input data into a probability distribution in latent space\nA decoder network that reconstructs the original data from samples in the latent spaceUnlike traditional autoencoders, VAEs don't encode to exact points but to probability distributions, making them generative models.Latent space regularization: VAEs use a special loss function that includes both reconstruction error and a regularization term (KL divergence) that encourages the latent space to be well-structured and continuous, making it suitable for interpolation and generation.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"This is the \"non-Bayesian\" part of our approach - we're using a neural network for dimensionality reduction rather than a fully Bayesian model, which can be computationally intractable for high-dimensional image data.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Computational intractability: A fully Bayesian approach would require modeling the joint probability distribution of all 784 pixels, which would involve an enormous number of parameters and complex dependencies. This becomes computationally prohibitive as the dimensionality increases - a manifestation of the \"curse of dimensionality\" mentioned earlier.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"graph LR\n    A[Input 784d] --> B\n    subgraph Encoder\n    B[Neural Network]\n    end\n    B --> C[Latent Space 2d]\n    C --> D\n    subgraph Decoder\n    D[Neural Network]\n    end\n    D --> E[Output 784d]","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"\"\"\"\n    create_vae(n_latent=2, n_channels_init=32)\n\nCreate a VAE model with convolutional encoder and decoder.\nReturns the VAE model with a 2D latent space by default.\n\"\"\"\nfunction create_vae(n_latent=2, n_channels_init=32)\n    # Define convolutional layers for encoder\n    conv_layers = Flux.Chain(\n        Flux.Conv((4, 4), 1 => n_channels_init, Flux.relu; stride=2, pad=1),\n        Flux.Conv((4, 4), n_channels_init => n_channels_init * 2, Flux.relu; stride=2, pad=1),\n        AutoEncoderToolkit.Flatten(),\n        Flux.Dense(n_channels_init * 2 * 7 * 7 => 256, Flux.relu),\n        Flux.Dense(256 => 256, Flux.relu),\n    )\n\n    # Define layers for μ and log(σ)\n    μ_layer = Flux.Dense(256, n_latent, Flux.identity)\n    logσ_layer = Flux.Dense(256, n_latent, Flux.identity, bias=fill(-1.0f0, n_latent))\n\n    # Build encoder\n    encoder = AutoEncoderToolkit.JointGaussianLogEncoder(conv_layers, μ_layer, logσ_layer)\n\n    # Define deconvolutional layers for decoder\n    deconv_layers = Flux.Chain(\n        Flux.Dense(n_latent => 256, Flux.identity),\n        Flux.Dense(256 => 256, Flux.relu),\n        Flux.Dense(256 => n_channels_init * 2 * 7 * 7, Flux.relu),\n        AutoEncoderToolkit.Reshape(7, 7, n_channels_init * 2, :),\n        Flux.ConvTranspose((4, 4), n_channels_init * 2 => n_channels_init, Flux.relu; stride=2, pad=1),\n        Flux.ConvTranspose((4, 4), n_channels_init => 1, x -> Flux.sigmoid_fast(x * 5.0); stride=2, pad=1),\n    )\n\n    # Define decoder - use BernoulliDecoder for binarized data\n    decoder = AutoEncoderToolkit.BernoulliDecoder(deconv_layers)\n\n    # Define VAE model\n    vae = encoder * decoder |> Flux.cpu\n    \n    return vae\nend;","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/#Part-3:-Training-the-VAE","page":"Learning Dynamics With Vaes","title":"Part 3: Training the VAE","text":"","category":"section"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Now we'll train the VAE on our rotated digit dataset. The VAE learns to:","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Encode images into a 2D latent space (encoder)\nDecode points from the latent space back to images (decoder)","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"The training process and function implementation used here are adapted from the AutoEncoderToolkit.jl documentation, which provides comprehensive examples for implementing various autoencoder architectures in Julia.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"\"\"\"\n    train_vae(vae, train_data; n_epoch=50, batch_size=64, learning_rate=1e-3)\n\nTrain a VAE model on the provided data.\nReturns the trained model and training metrics.\n\"\"\"\nfunction train_vae(vae, train_data; n_epoch=50, batch_size=64, learning_rate=1e-3)\n    # Create data loader\n    train_loader = Flux.DataLoader(train_data, batchsize=batch_size, shuffle=true)\n    \n    # Setup optimizer\n    opt_vae = Flux.Train.setup(Flux.Optimisers.Adam(learning_rate), vae)\n    \n    # Initialize arrays to save metrics\n    train_losses = Array{Float32}(undef, n_epoch)\n    \n    # Loop through epochs\n    for epoch in 1:n_epoch\n        batch_losses = Float32[]\n        \n        # Calculate β value with annealing scheme\n        β_value = epoch <= 15 ? 0.1f0 + (epoch - 1) / 15 * 0.9f0 : 1.0f0\n        \n        # Loop through batches\n        for (i, x) in enumerate(train_loader)\n            # Train VAE with current β value (doesn't return a value)\n            AutoEncoderToolkit.VAEs.train!(vae, x, opt_vae; loss_kwargs=(β=β_value,))\n            \n            # Calculate loss separately\n            batch_loss = AutoEncoderToolkit.VAEs.loss(vae, x; β=β_value)\n            push!(batch_losses, batch_loss)\n            \n            # Print progress for every 10 batches\n            if i % 10 == 0\n                println(\"Epoch: $epoch/$n_epoch | Batch: $i/$(length(train_loader)) | Loss: $(round(mean(batch_losses), digits=4))\")\n            end\n        end\n        \n        # Record average loss for epoch\n        train_losses[epoch] = mean(batch_losses)\n        \n        # Print epoch summary\n        println(\"Epoch $epoch/$n_epoch completed | Avg Loss: $(round(train_losses[epoch], digits=4))\")\n    end\n    \n    return vae, train_losses\nend","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Main.anonymous.train_vae","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# # Create and train the VAE model on the rotated dataset\n# # We will skip the training step within the notebook and will load the pre-trained model instead, feel free to uncomment the following lines to train the model yourself.\n# println(\"Creating VAE model...\")\n# vae = create_vae()\n\n# println(\"Training VAE on rotated MNIST dataset...\")\n# vae, losses = train_vae(vae, rotated_train_data, n_epoch=100)\n\n# # Save the trained model parameters\n# model_params = Dict()\n# for (i, p) in enumerate(Flux.params(vae))\n#     model_params[\"param_$i\"] = Flux.cpu(p)  # Move to CPU before saving\n# end\n\n# # Save the parameters dictionary\n# JLD2.save(\"rotated_mnist_vae_params.jld2\", model_params)\n# println(\"Model parameters saved to: rotated_mnist_vae_params.jld2\")\n\n# # Plot training loss\n# loss_plot = plot(losses, title=\"VAE Training Loss (Rotated MNIST)\", xlabel=\"Epoch\", ylabel=\"Loss\", \n#                  legend=false, linewidth=2, color=:blue)","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Create an empty VAE with the same architecture as the one you saved\nfunction load_vae(filepath; n_latent=2, n_channels_init=32)\n    # Create a new VAE with the same architecture\n    vae = create_vae(n_latent, n_channels_init)\n    \n    # Load the saved parameters\n    model_params = JLD2.load(filepath)\n    \n    # Get the parameters of the current model\n    ps = Flux.params(vae)\n    \n    # Replace the parameters with the loaded ones\n    for (i, p) in enumerate(ps)\n        param_key = \"param_$i\"\n        if haskey(model_params, param_key)\n            # Copy values from saved parameters to the model\n            copyto!(p, model_params[param_key])\n        else\n            @warn \"Parameter $param_key not found in saved model\"\n        end\n    end\n    \n    return vae\nend\n\nvae = load_vae(\"rotated_mnist_vae_params.jld2\");","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Display original and reconstructed images\nimg = rotated_train_data[:, :, :, 42]\np1 = heatmap(img[:, :, 1, 1] |> Flux.cpu, color=[:black, :white], colorbar=false, title=\"Original Image\")\n# Reshape to add channel dimension (H×W → H×W×C) for network input\nencoded_img = vae.encoder(reshape(img, size(img)..., 1))\n# p represents pixel-wise probabilities (Bernoulli parameters) since we're using binarized images\ndecoded_img = vae.decoder(encoded_img.μ)\np2 = heatmap(decoded_img.p[:, :, 1, 1], color=:grays, title=\"Reconstructed Image\")\ndisplay(plot(p1, p2, layout=(1, 2), size=(800, 400)))","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/#Part-4:-Integrating-VAE-with-RxInfer-for-Bayesian-Inference","page":"Learning Dynamics With Vaes","title":"Part 4: Integrating VAE with RxInfer for Bayesian Inference","text":"","category":"section"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Now we'll explore the Bayesian component of our approach. Using RxInfer, we'll perform probabilistic inference on the latent space dynamics of rotating digits, enabling us to simultaneously model the underlying rotation process and generate predictions of future images.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"First, we need to create a custom node that connects our VAE to the RxInfer framework. This is really simple, we just need to define a node and a couple of rules. ","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"We will create a VAENode with input and output interfaces. Nodes in RxInfer require messages to be passed between them during inference. In this case, we have the luxury of not needing to solve any tough integrals:","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"We'll use the encoder as our message passing function toward the input interface (from images to latent space)\nThe forward message for the output interface will be our decoder (from latent space to images)","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Create a VAE wrapper to help with RxInfer integration\nstruct VAENode end\n\n# Meta struct to store the VAE model for use in message passing rules\nstruct VAEMeta{F}\n    vae::F\nend\n\n# Define a custom VAE node for RxInfer\n@node VAENode Stochastic [out, x]\n\n# Define the backward message passing rule (z ← out)\n@rule VAENode(:x, Marginalisation) (q_out::PointMass, meta::VAEMeta) = begin\n    # Get the value from the output message\n    x_val = mean(q_out)\n    \n    # Pass through encoder to get latent distribution\n    # Need to reshape x_val to match expected input shape\n    x_reshaped = reshape(x_val, 28, 28, 1, 1)\n    encoded = meta.vae.encoder(x_reshaped)\n    \n    z_mean = vec(encoded.μ) |> Flux.cpu\n    z_var = exp.(vec(encoded.logσ)) .^ 2 |> Flux.cpu\n    \n    # Return multivariate normal distribution\n    return MvNormalMeanCovariance(z_mean, Diagonal(z_var))\nend\n\n@rule VAENode(:out, Marginalisation) (q_x::MultivariateNormalDistributionsFamily, meta::VAEMeta) = begin \n    # from the latent distribution, sample a value\n    z_val = mean(q_x)\n    # Convert to Float32 to match the decoder's parameter type\n    z_val = Float32.(z_val)\n    # pass through decoder to get image distribution\n    decoder_output = meta.vae.decoder(z_val)\n    \n    # return the image distribution\n    return decoder_output.p\nend\n\n# DONE!","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Now we'll define a state space model to capture the dynamics of rotation in the latent space. This model assumes:","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"The latent state evolves according to a linear dynamical system with unknown transition matrix and noise\nThe observed images are generated from the latent states through the VAE","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Define the state space model using the VAE node\n@model function ssm_vae(y)\n    Λₛ ~ Wishart(4, diageye(2)) # Precision matrix for the transition matrix\n    Hₛ ~ MvNormal(μ = zeros(4), Λ = diageye(4)) # Vectorized 2×2 transition matrix prior\n\n    # Initial state\n    x[1] ~ MvNormal(μ = zeros(2), Σ = diageye(2)) \n    y[1] ~ VAENode(x[1])\n\n    # State space model evolution\n    for t in 2:length(y)\n        x[t] ~ ContinuousTransition(x[t-1], Hₛ, Λₛ)  # equivalent to x[t] := Hₛ * x[t-1] + ϵ[t]\n        y[t] ~ VAENode(x[t])\n    end\nend","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"After specifying our model, we need to set up several components that will guide the inference process:","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Constraints: We'll specify factorization constraints on the approximate posterior distribution, assuming independence between variables transition matrix and precision matrix, q(x, Hₛ, Λₛ) = q(x)q(Hₛ)q(Λₛ).\nTransition function: This reshapes our vector representation of the transition matrix into the 2×2 matrix form needed for the state equation.\nMetadata: We provide RxInfer with information about our custom VAE node and continuous transition function.\nInitial marginals: Starting distributions for our unknown parameters (transition matrix and precision matrix).","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"These components define how our inference algorithm will learn the dynamics of rotation in the latent space.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Define constraints for inference\nconstraints = @constraints begin\n    q(x, Hₛ, Λₛ) = q(x)q(Hₛ)q(Λₛ)\nend\n\n# Define transition function\ntransition(h) = reshape(h, (2, 2))\n\n# Define metadata for inference\nmeta = @meta begin\n    ContinuousTransition() ->  CTMeta(transition)\n    VAENode() -> VAEMeta(vae)\nend\n\n# Define initial marginals\ninitmarginals = @initialization begin\n    q(Hₛ) = MvNormalMeanCovariance(zeros(4), 1e2diagm(ones(4)))\n    q(Λₛ) = Wishart(4, diageye(2))\nend\n\n# Create model\nssm_vae_model = ssm_vae();","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Let's generate a sequence of rotated images to use for inference.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Select a sample image from the dataset (e.g., the first digit 1)\nprintln(\"Generating rotated image sequence...\")\ndigit_indices = findall(rotated_train_labels .== 1)\nsample_image = rotated_train_data[:, :, :, digit_indices[1]]\n\n# Generate and display the rotation sequence\nrotation_anim = generate_rotated_sequence(sample_image, 100, 360)\n\n# Create an animation of the rotating sequence used for learning dynamics\nprintln(\"Creating rotation animation for learning dynamics...\")\nrotation_animation = @animate for i in 1:length(rotation_anim)\n    heatmap(rotation_anim[i], color=[:black, :white], colorbar=false, \n            title=\"Observed Rotations $(round((i-1)*360/length(rotation_anim), digits=1))°\",\n            axis=false, aspect_ratio=:equal, size=(400, 400))\nend\n\n# Save as GIF with 10 frames per second\ngif(rotation_animation, \"rotation_learning_animation.gif\", fps=10, show_msg=false);","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Generating rotated image sequence...\nCreating rotation animation for learning dynamics...","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Now, we are going to do something cool here. We will not only pass 100 images to our model to infer the dynamics, but we will also have RxInfer predict the next 100 images. This means we'll simultaneously learn the dynamics and generate predictions for future images.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Prepare rotated sequence for inference\n# Convert the rotated images to the format expected by the VAE node\nrotated_data = []\nfor img in rotation_anim\n    img_4d = reshape(img, size(img)..., 1, 1)\n    push!(rotated_data, img_4d)\nend\nn_obs = length(rotated_data);\nn_pred = 100;\n# Create data structure for inference\ndata = (y = [rotated_data; repeat([missing], n_pred)],);","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"println(\"Running inference...\")\nresult = infer(\n    model = ssm_vae_model, \n    meta = meta, \n    initialization = initmarginals, \n    data = data, \n    constraints = constraints, \n    iterations = 50, \n    showprogress = true, \n    free_energy = false, \n    free_energy_diagnostics = nothing, \n    options = (limit_stack_depth = 100,)\n)","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Running inference...\nInference results:\n  Posteriors       | available for (x, Λₛ, Hₛ)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Create an animation that distinguishes between observations and predictions\ny_pred = result.predictions[:y][end];\nprintln(\"Creating observation-prediction animation...\")\ncontinuation_animation = @animate for i in 1:length(y_pred)\n    if i == 1\n        # First frame gets a more descriptive title\n        heatmap(y_pred[i][:, :, 1, 1], color=:grays, \n                title=\"Rotating Digit: $n_obs observations → $n_pred predictions\",\n                axis=false, aspect_ratio=:equal, size=(400, 400))\n    elseif i <= n_obs\n        # Rest of observations\n        heatmap(y_pred[i][:, :, 1, 1], color=:grays, \n                title=\"Observation #$i of $n_obs\",\n                axis=false, colorbar=false, aspect_ratio=:equal, size=(400, 400))\n    else\n        # Predictions - with red title text\n        heatmap(y_pred[i][:, :, 1, 1], color=:inferno, \n                title=(\"Prediction #$(i-n_obs) of $n_pred ahead\"),\n                title_location=:center, titlefontcolor=:red,\n                axis=false, aspect_ratio=:equal, size=(400, 400),\n                border=:red, colorbar=false, borderwidth=3)\n    end\nend\n\n# Save as GIF with 10 frames per second\ngif(continuation_animation, \"observation_prediction_animation.gif\", fps=24, show_msg=false);","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Creating observation-prediction animation...","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Let's visualize the observations and predictions side by side.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"function create_digit_strips_with_titles(y_pred, n_obs=100, n_pred_to_show=50)\n    # Get image dimensions\n    img_height, img_width = size(y_pred[1][:,:,1,1])\n    \n    # Create a single large image for observations (top row)\n    obs_strip = zeros(img_height, img_width * n_obs)\n    for i in 1:n_obs\n        obs_strip[:, ((i-1)*img_width+1):(i*img_width)] = y_pred[i][:,:,1,1]\n    end\n    \n    # For predictions, use the first 50 predictions (101-150)\n    pred_indices = (n_obs+1):(n_obs+n_pred_to_show)\n    \n    # Create a single large image for predictions (bottom row)\n    # Make each prediction twice as wide to fill the same space\n    pred_strip = zeros(img_height, img_width * n_pred_to_show * 2)\n    for i in 1:n_pred_to_show\n        # Fill a 2x wider space for each prediction\n        start_col = ((i-1)*img_width*2+1)\n        end_col = (i*img_width*2)\n        \n        # Repeat each column to double the width\n        for j in 1:img_width\n            pred_strip[:, start_col+2*(j-1):start_col+2*(j-1)+1] .= y_pred[pred_indices[i]][:, j, 1, 1]\n        end\n    end\n    \n    # Create tick positions and labels\n    obs_tick_pos = range(1, img_width*n_obs, length=11)\n    obs_tick_labels = string.(range(1, n_obs, length=11))\n    \n    # For predictions, show ticks from 101 to 150\n    pred_tick_pos = range(1, img_width*n_pred_to_show*2, length=6)\n    pred_tick_labels = string.(range(n_obs+1, n_obs+n_pred_to_show, length=6))\n    \n    # Plot observations (1-100)\n    p1 = heatmap(\n        obs_strip,\n        color=:grays, \n        colorbar=false,\n        yticks=false,\n        xticks=(obs_tick_pos, obs_tick_labels),\n        framestyle=:box,\n        title=\"Observation Sequence\",\n        titlefontsize=12,\n        titlefontcolor=:blue,\n        size=(1000, 100)\n    )\n    \n    # Plot predictions (101-150)\n    p2 = heatmap(\n        pred_strip,\n        color=:inferno, \n        colorbar=false,\n        yticks=false,\n        xticks=(pred_tick_pos, pred_tick_labels),\n        framestyle=:box,\n        title=\"Prediction Sequence\",\n        titlefontsize=12,\n        titlefontcolor=:red,\n        size=(1000, 150)  # Make this row taller\n    )\n    \n    # Create a small plot for the simultaneous learning message\n    message_plot = plot(\n        grid=false,\n        showaxis=false,\n        ticks=false,\n        framestyle=:none,\n        size=(1000, 30),\n        margin=0Plots.mm,\n        bottom_margin=-5Plots.mm,\n        top_margin=-5Plots.mm\n    )\n    \n    # Combine the plots\n    p = plot(\n        p1, message_plot, p2,\n        layout=grid(3, 1, heights=[0.35, 0.1, 0.55]),\n        size=(1000, 280),\n        margin=5Plots.mm\n    )\n    \n    return p\nend\n\n# Create the strips with titles and message\nprintln(\"Creating digit strips with titles...\")\nn_obs = 100  # Number of observations\nn_pred_to_show = 50  # Number of predictions to show (101-150)\n\nfinal_strips = create_digit_strips_with_titles(\n    result.predictions[:y][end], n_obs, n_pred_to_show\n)\ndisplay(final_strips)","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Creating digit strips with titles...","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Let's also plot the predicted trajectories with uncertainty ribbons.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Extract the real latent trajectories from the data\nreal_latent_dim1 = first.(first.(vae.encoder.(rotated_data)))\nreal_latent_dim2 = last.(first.(vae.encoder.(rotated_data)))\n\n# Create improved plots with both inferred and real trajectories\np1 = plot(1:length(mean.(result.posteriors[:x][end])), \n          first.(mean.(result.posteriors[:x][end])), \n          ribbon=first.(std.(result.posteriors[:x][end])), \n          xlabel=\"Time Step\", ylabel=\"Latent Dimension 1\", \n          label=\"Predicted\", \n          linewidth=2, alpha=0.7)\n\n# Add the real trajectory to dimension 1 plot\nplot!(p1, 1:length(real_latent_dim1), real_latent_dim1, \n      label=\"Real trajectory\", \n      linewidth=2, linestyle=:dash, color=:red)\n\n# Create dimension 2 plot\np2 = plot(1:length(mean.(result.posteriors[:x][end])), \n          last.(mean.(result.posteriors[:x][end])), \n          ribbon=last.(std.(result.posteriors[:x][end])),\n          xlabel=\"Time Step\", ylabel=\"Latent Dimension 2\", \n          label=\"Predicted\", \n          linewidth=2, alpha=0.7)\n\n# Add the real trajectory to dimension 2 plot\nplot!(p2, 1:length(real_latent_dim2), real_latent_dim2, \n      label=\"Real trajectory\", \n      linewidth=2, linestyle=:dash, color=:red)\n\n# Combine the plots\nplot(p1, p2, layout=(1,2), size=(1000, 300), \n     title=\"Latent Space Trajectory\")","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Lastly, let's analyze the learned rotation matrix.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"# Analyze the learned rotation matrix\nH_matrix = reshape(mean(result.posteriors[:Hₛ][end]), (2, 2))\nprintln(\"Learned rotation matrix:\")\ndisplay(H_matrix)\n\n# Calculate eigenvalues and eigenvectors\neigen_vals, eigen_vecs = eigen(H_matrix)\n\n# Calculate the rotation angle from the matrix\nrotation_angle = atan(H_matrix[2,1], H_matrix[1,1]) * 180 / π\n\n# Calculate the determinant (should be close to 1 for a rotation)\ndet_H = det(H_matrix)\n\n# Calculate the matrix norm (measure of scaling)\nnorm_H = norm(H_matrix)\n\n# Print analysis results\nprintln(\"\\n=== Rotation Matrix Analysis ===\")\nprintln(\"Determinant: $(round(det_H, digits=5)) (ideal for pure rotation: 1.0)\")\nprintln(\"Matrix norm: $(round(norm_H, digits=5))\")\nprintln(\"Eigenvalues: $(round.(abs.(eigen_vals), digits=5)) ∠ $(round.(angle.(eigen_vals) .* 180/π, digits=2))°\")\nprintln(\"Estimated rotation angle per step: $(round(rotation_angle, digits=2))°\")\nprintln(\"Expected rotation angle per step: $(round(360/100, digits=2))°\")\n\n# Check if it's close to a pure rotation\nis_pure_rotation = isapprox(det_H, 1.0, atol=0.05) && \n                   all(isapprox.(abs.(eigen_vals), 1.0, atol=0.05))\n\nprintln(\"\\n=== Interpretation ===\")\nif is_pure_rotation\n    println(\"✓ The matrix is very close to a pure rotation matrix.\")\nelse\n    println(\"⚠ The matrix includes some scaling or shearing in addition to rotation.\")\nend\n\n# Check if eigenvalues are complex conjugates (as expected for rotation)\nif all(isapprox.(real(eigen_vals[1]), real(eigen_vals[2]), atol=1e-10)) && \n   isapprox(imag(eigen_vals[1]), -imag(eigen_vals[2]), atol=1e-10)\n    println(\"✓ Eigenvalues form a complex conjugate pair, as expected for rotation.\")\nelse\n    println(\"⚠ Eigenvalues don't form a perfect complex conjugate pair.\")\nend\n\n# Check if the rotation angle matches expectation\nangle_error = abs(rotation_angle - 360/100)\nif angle_error < 1.0\n    println(\"✓ Rotation angle matches expected value very closely (error < 1°).\")\nelseif angle_error < 2.0\n    println(\"✓ Rotation angle is reasonably close to expected value (error < 2°).\")\nelse\n    println(\"⚠ Rotation angle differs from expected value by $(round(angle_error, digits=2))°.\")\nend\n\n# Overall assessment\nprintln(\"\\n=== Overall Assessment ===\")\nif is_pure_rotation && angle_error < 2.0\n    println(\"The learned matrix is an excellent approximation of the expected rotation.\")\nelseif angle_error < 5.0\n    println(\"The learned matrix captures the rotation well, with some minor deviations.\")\nelse\n    println(\"The learned matrix approximates the rotation, but has significant deviations.\")\nend","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Learned rotation matrix:\n2×2 Matrix{Float64}:\n 0.985487   -0.048242\n 0.0721243   0.97395\n\n=== Rotation Matrix Analysis ===\nDeterminant: 0.96329 (ideal for pure rotation: 1.0)\nMatrix norm: 1.38827\nEigenvalues: [0.98148, 0.98148] ∠ [-3.43, 3.43]°\nEstimated rotation angle per step: 4.19°\nExpected rotation angle per step: 3.6°\n\n=== Interpretation ===\n✓ The matrix is very close to a pure rotation matrix.\n✓ Eigenvalues form a complex conjugate pair, as expected for rotation.\n✓ Rotation angle matches expected value very closely (error < 1°).\n\n=== Overall Assessment ===\nThe learned matrix is an excellent approximation of the expected rotation.","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/learning_dynamics_with_vaes/Project.toml`\n  [1575904b] AutoEncoderToolkit v0.1.2\n⌅ [587475ba] Flux v0.14.25\n⌅ [033835bb] JLD2 v0.5.15\n  [eb30cadb] MLDatasets v0.7.18\n  [91a5bcdd] Plots v1.41.1\n  [a194aa59] ReactiveMP v5.6.0\n  [86711068] RxInfer v4.6.0\n  [10745b16] Statistics v1.11.1\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n","category":"page"},{"location":"categories/advanced_examples/learning_dynamics_with_vaes/","page":"Learning Dynamics With Vaes","title":"Learning Dynamics With Vaes","text":"","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Structural-Dynamics-with-Augmented-Kalman-Filter","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics with Augmented Kalman Filter","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This example demonstrates state and input force estimation for structural dynamical systems with Augmented Kalman Filter (AKF) implemented in RxInfer.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"NOTE: This example was originally featured in this blog post. Check it out for additional insights! The notebook has been prepared by Víctor Flores and adapted by Dmitry Bagaev.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#State-and-Input-Estimation","page":"Structural Dynamics With Augmented Kalman Filter","title":"State and Input Estimation","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"State-space models are fundamental tools in control theory and signal processing that allow us to analyze complex dynamical systems by breaking them down into first-order differential equations. They are particularly important for structural dynamics problems because they can capture both the internal states (like position and velocity) and external influences (like forces) in a unified mathematical framework. A typical state-space model formulation might look like this:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"xk+1 sim mathcalN(A xk + B pk Q)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"yk sim mathcalN(G xk + J pk R)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"xk\nrepresents the system states at time-step k\npk\nrepresents the unknown input forces at time-step k\nyk\nrepresents our noisy measurements at time-step k\nA\nis the state transition matrix that describes how the system evolves from one time step to the next\nB\nis the input matrix that maps the external forces to their effects on the states\nQ\nis the process noise covariance matrix that captures uncertainties in the system dynamics\nR\nis the measurement noise covariance matrix that represents uncertainties in sensor measurements","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#4-floor-shear-building-model","page":"Structural Dynamics With Augmented Kalman Filter","title":"4-floor shear building model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"For this example, we consider a simplified 4-floor shear building model with 4 degrees of freedom (DOF). This system is depicted below:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this example, the dynamics of a structural system are governed by its mass (M), stiffness (K), and damping (C) matrices, leading to the equation of motion:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"M ddotx(t) + C dotx(t) + K x(t) = p(t)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where x(t) represents the displacements at each degree of freedom, and p(t) is the external force applied to the system.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This model captures the essential dynamics of a multi-story structure while remaining computationally manageable. The system matrices are defined as follows:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"M\nis the diagonal mass matrix representing the lumped masses at each floor,  \nK\nis the stiffness matrix representing inter-floor lateral stiffness, and  \nC\nis the proportional damping matrix reflecting energy dissipation.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Lets begin the experiment! To start, we import the necessary packages.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"using LinearAlgebra, Statistics, Random, Plots","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"To keep our analysis organized, we'll use a custom StructuralModelData data structure. This structure serves as a central repository for model parameters, simulation settings, system matrices, results, and outputs.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# define a data structure for the structural model environment\nstruct StructuralModelData\n    t::Union{Nothing,Any}\n    ndof::Union{Nothing,Int64}\n    nf::Union{Nothing,Int64}\n    N_data::Union{Nothing,Int64}\n    y_meas::Union{Nothing,Vector{Vector{Float64}}}\n    A_aug::Union{Nothing,Matrix{Float64}}\n    G_aug::Union{Nothing,Matrix{Float64}}\n    G_aug_fullfield::Union{Nothing,Matrix{Float64}}\n    Q_akf::Union{Nothing,Matrix{Float64}}\n    R::Union{Nothing,LinearAlgebra.Diagonal{Float64,Vector{Float64}}}\n    x_real::Union{Nothing,Matrix{Float64}}\n    y_real::Union{Nothing,Matrix{Float64}}\n    p_real::Union{Nothing,Matrix{Float64}}\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We also define a structure for the system matrices.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# define the structural system matrices\nstruct StructuralMatrices\n    M::Union{Nothing,Matrix{Float64}}\n    K::Union{Nothing,Matrix{Float64}}\n    C::Union{Nothing,Matrix{Float64}}\nend\n\n\nM = I(4)\n\n\nK = [\n    2 -1 0 0;\n    -1 2 -1 0;\n    0 -1 2 -1;\n    0 0 -1 1\n] * 1e3\n\nC = [\n    2 -1 0 0;\n    -1 2 -1 0;\n    0 -1 2 -1;\n    0 0 -1 1\n]\n\nStructuralModel = StructuralMatrices(M, K, C);","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Constructing-the-State-Space-Model","page":"Structural Dynamics With Augmented Kalman Filter","title":"Constructing the State-Space Model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We convert the structural system into its discrete-time state-space form for numerical simulation. Starting from the equation of motion:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"M ddotx(t) + C dotx(t) + K x(t) = F(t)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"we introduce the state variable:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"z(t) = beginbmatrix x(t)  dotx(t) endbmatrix","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"which allows us to express the system as:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"dotz(t) = A_textc z(t) + B_textc p(t)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"A_textc = beginbmatrix 0  I  -(M^-1 K)  -(M^-1 C) endbmatrix\nB_textc = beginbmatrix 0  M^-1 S_p endbmatrix\nS_p\nis the input selection matrix that determines where the external forces p(t) are applied.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"To perform simulations, the system is discretized using a time step Delta t as:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"zk+1 = A zk + B pk","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"A = e^A_textc Delta t\nis the state transition matrix.\nB = (A - I) A_textc^-1 B_textc\nis the input matrix, obtained by integrating the continuous-time system.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This state-space representation forms the basis for propagating the system states during simulation.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to construct the state space model\nfunction construct_ssm(StructuralModel, dt, ndof, nf)\n    # unpack the structural model\n    M = StructuralModel.M\n    K = StructuralModel.K\n    C = StructuralModel.C\n\n\n    Sp = zeros(ndof, nf)\n    Sp[4, 1] = 1\n\n    Z = zeros(ndof, ndof)\n    Id = I(ndof)\n\n    A_continuous = [Z Id;\n        -(M \\ K) -(M \\ C)]\n    B_continuous = [Z; Id \\ M] * Sp\n\n    A = exp(dt * A_continuous)\n    B = (A - I(2 * ndof)) * A_continuous \\ B_continuous\n\n    return A, B, Sp\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"construct_ssm (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Generating-Input-Forces","page":"Structural Dynamics With Augmented Kalman Filter","title":"Generating Input Forces","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"External forces pk acting on the system are modeled as Gaussian white noise:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"pk sim mathcalN(mu sigma^2)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where mu is the mean and sigma controls the intensity of the force.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this example, the inputs are generated independently at each time step k and across input channels to simulate random excitations, such as wind or seismic forces.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to generate random input noise\nfunction generate_input(N_data::Int, nf::Int; input_mu::Float64, input_std::Float64)\n    Random.seed!(42)\n    p_real = input_mu .+ randn(N_data, nf) .* input_std\n    return p_real\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"generate_input (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Observation-Model","page":"Structural Dynamics With Augmented Kalman Filter","title":"Observation Model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"System responses, such as accelerations, are often measured at specific locations using sensors. The measurements are simulated using the equation:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"yk = G xk + J pk + vk","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"G\nmaps the system states xk to measured outputs.\nJ\nmaps the input forces pk to the measurements.\nvk sim mathcalN(0 sigma_y^2 I)\nis Gaussian noise representing sensor inaccuracies.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The noise variance sigma_y^2 is chosen as a fraction of the true system response variance for realism.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this example, accelerations are measured at selected degrees of freedom (e.g., nodes 1 and 4).","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to generate the measurements and noise\nfunction generate_measurements(ndof, na, nv, nd, N_data, x_real, y_real, p_real, StructuralModel, Sp)\n    # unpack the structural model\n    M = StructuralModel.M\n    K = StructuralModel.K\n    C = StructuralModel.C\n\n    Sa = zeros(na, ndof)            # selection matrix\n    Sa[1, 1] = 1                    # acceleration at node 1\n    Sa[2, 4] = 1                    # acceleration at node 4\n    G = Sa * [-(M \\ K) -(M \\ C)]\n    J = Sa * (I \\ M) * Sp\n\n    ry = Statistics.var(y_real[2*ndof+1, :],) * (0.1^2)        # simulate noise as 1% RMS of the noise-free acceleration response\n\n    nm = na + nv + nd\n\n    R = I(nm) .* ry\n\n    y_meas = zeros(nm, N_data)\n    y_noise = sqrt(ry) .* randn(nm, N_data)\n\n    # reconstruct the measurements\n    y_meas = Vector{Vector{Float64}}(undef, N_data)\n    for i in 1:N_data\n        y_meas[i] = G * x_real[:, i] + J * p_real[i, :] + y_noise[:, i]\n    end\n\n    return y_meas, G, J, R\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"generate_measurements (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Simulating-the-Structural-Response","page":"Structural Dynamics With Augmented Kalman Filter","title":"Simulating the Structural Response","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The structural response under applied forces is governed by the state-space equations:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"beginaligned\nxk+1  = A xk + B pk \nyk    = G_textfull xk + J_textfull pk\nendaligned","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where xk are the system states, pk are the input forces, and yk are the full-field responses, i.e., the response at every degree of freedom in our structure.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The function below returns:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"True States: x_textreal, propagated using $ A $ and $ B $.\nFull-Field Responses: y_textreal, incorporating both states and inputs.\nInput Forces: p_textreal, generated as stochastic excitations.\nResponse Matrices: G_textfull (state-to-response) and J_textfull (input-to-response).","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"These outputs simulate the physical behavior of the system and serve as the basis for inference. We keep the matrices because they will be used later when analyzing our results.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to simulate the structural response\nfunction simulate_response(A, B, StructuralModel, Sp, nf, ndof, N_data)\n    # unpack the structural model\n    M = StructuralModel.M\n    K = StructuralModel.K\n    C = StructuralModel.C\n\n    p_real = generate_input(N_data, nf, input_mu=0.0, input_std=0.05)\n\n    Z = zeros(ndof, ndof)\n    Id = I(ndof)\n\n    G_full = [\n        Id Z;\n        Z Id;\n        -(M \\ K) -(M \\ C)\n    ]\n\n    J_full = [\n        Z;\n        Z;\n        Id \\ M\n    ] * Sp\n\n    # preallocate matrices\n    x_real = zeros(2 * ndof, N_data)\n    y_real = zeros(3 * ndof, N_data)\n\n    for i in 2:N_data\n        x_real[:, i] = A * x_real[:, i-1] + B * p_real[i-1, :]\n        y_real[:, i] = G_full * x_real[:, i-1] + J_full * p_real[i-1, :]\n    end\n\n    return x_real, y_real, p_real, G_full, J_full\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"simulate_response (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Augmented-State-Space-Model","page":"Structural Dynamics With Augmented Kalman Filter","title":"Augmented State-Space Model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In structural health monitoring, external input forces pk acting on a structure, such as environmental loads or unknown excitations, are often not directly measurable. To estimate both the system states xk and these unknown input forces, we augment the state vector as follows:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"tildexk = \nbeginbmatrix\nxk \npk\nendbmatrix","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This approach allows us to simultaneously infer the internal system states (e.g., displacements and velocities) and the unknown inputs using available measurements.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The augmented system dynamics are then expressed as:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"beginaligned\ntildexk+1  = A_textaug tildexk + wk \nyk  = G_textaug tildexk + vk\nendaligned","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"A_textaug\n: Augmented state transition matrix.  \nG_textaug\n: Augmented measurement matrix.  \nQ_textakf\n: Augmented process noise covariance, capturing uncertainties in both states and inputs.  \nwk\n, vk: Process and measurement noise.  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Full-Field-vs.-Measurement-Space","page":"Structural Dynamics With Augmented Kalman Filter","title":"Full-Field vs. Measurement Space","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"To avoid confusion, we define two augmented measurement matrices:  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"G_textaug\n: Projects the augmented state vector tildexk to the observed sensor measurements (e.g., accelerations at specific nodes).  \nG^*\n: The augmented full-field measurement matrix, which projects the augmented state vector to the full-field system response. This includes all degrees of freedom (displacements, velocities, and accelerations).  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The distinction is critical:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"G_textaug\nis used directly in the smoother to estimate states and inputs from limited measurements.  \nG^*\nis used later to reconstruct the full response field for visualization and validation.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"For clarity, we will refer to the augmented full-field matrix as G^* throughout the rest of this example, whereas, in the code, this will be the G_aug_fullfield object.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Noise-Covariances","page":"Structural Dynamics With Augmented Kalman Filter","title":"Noise Covariances","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this step, the process and measurement noise covariances are assumed to be known or pre-calibrated. For example:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The input force uncertainty Q_p is set to reflect significant variability.  \nState noise covariance Q_x is chosen to reflect uncertainty in the model.  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The augmented noise covariance matrix Q_textakf combines these quantities:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Q_textakf =\nbeginaligned\nbeginbmatrix\nQ_x  0 \n0  Q_p\nendbmatrix\nendaligned","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to construct the augmented model\nfunction construct_augmented_model(A, B, G, J, G_full, J_full, nf, ndof)\n    Z_aug = zeros(nf, 2 * ndof)\n    A_aug = [\n        A B;\n        Z_aug I(nf)\n    ]\n    G_aug = [G J]\n\n    G_aug_fullfield = [G_full J_full]                               # full-field augmented matrix\n\n    Qp_aug = I(nf) * 1e-2                                           # assumed known or pre-callibrated\n\n    # The `Q` matrix here has zero entries on the diagonal, which in turn \n    # leads to a non-symmetric matrices in the computation. \n    # This is acceptable for this example\n    ENV[\"JULIA_FASTCHOLESKY_NO_WARN_NON_SYMMETRIC\"] = \"1\"\n    if haskey(ENV, \"JULIA_FASTCHOLESKY_THROW_ERROR_NON_SYMMETRIC\")\n        delete!(ENV, \"JULIA_FASTCHOLESKY_THROW_ERROR_NON_SYMMETRIC\")\n    end\n\n    Qx_aug = zeros(2 * ndof, 2 * ndof)\n    Qx_aug[(ndof+1):end, (ndof+1):end] = I(ndof) * 1e-1             # assumed known or pre-callibrated\n\n    Q_akf = [\n        Qx_aug Z_aug';\n        Z_aug Qp_aug\n    ]\n\n    return A_aug, G_aug, Q_akf, G_aug_fullfield\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"construct_augmented_model (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Finally, we combine all the key steps into a single workflow to generate the system dynamics, responses, measurements, and the augmented state-space model.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The results are stored in a StructuralModelData object for convenient access:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"function get_structural_model(StructuralModel, simulation_time, dt)\n\n    # intialize\n    ndof = size(StructuralModel.M)[1]                               # number of degrees of freedom\n    nf = 1                                                          # number of inputs\n    na, nv, nd = 2, 0, 0                                            # number of oberved accelerations, velocities, and displacements\n    N_data = Int(simulation_time / dt) + 1\n    t = range(0, stop=simulation_time, length=N_data)\n\n    # construct state-space model from structural matrices\n    A, B, Sp = construct_ssm(StructuralModel, dt, ndof, nf)\n\n    # Generate input and simulate response\n    x_real, y_real, p_real, G_full, J_full = simulate_response(A, B, StructuralModel, Sp, nf, ndof, N_data)\n\n    # Generate measurements\n    y_meas, G, J, R = generate_measurements(ndof, na, nv, nd, N_data, x_real, y_real, p_real, StructuralModel, Sp)\n\n    # Construct augmented model\n    A_aug, G_aug, Q_akf, G_aug_fullfield = construct_augmented_model(A, B, G, J, G_full, J_full, nf, ndof)\n\n    return StructuralModelData(t, ndof, nf, N_data, y_meas, A_aug, G_aug, G_aug_fullfield, Q_akf, R, x_real, y_real, p_real)\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"get_structural_model (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We define the simulation time and time step, then run the workflow to generate the structural model:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"simulation_time = 5.0\ndt = 0.001\n\nmodel_data = get_structural_model(StructuralModel, simulation_time, dt);","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#State-and-Input-Estimation-with-RxInfer","page":"Structural Dynamics With Augmented Kalman Filter","title":"State and Input Estimation with RxInfer","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this section, we use RxInfer to estimate the system states and unknown input forces from the simulated noisy measurements using the Augmented State Space Model discussed.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"using RxInfer","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Defining-the-AKF-Smoother-Model","page":"Structural Dynamics With Augmented Kalman Filter","title":"Defining the AKF Smoother Model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Here, we define our Augmented Kalman Filter (AKF) smoother using RxInfer. This probabilistic model estimates the system states and unknown input forces based on the measurements.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"State Prior: We start with a prior belief about the initial state, x0.  \nState Transition: At each time step, the system state evolves based on the transition matrix A and process noise covariance Q: xk sim mathcalN(A xk-1 Q)\nMeasurements: The observations (sensor data) are modeled as noisy measurements of the states: yk sim mathcalN(G xk R) where G maps the states to the measurements, and R is the measurement noise covariance.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"@model function smoother_model(y, x0, A, G, Q, R)\n\n    x_prior ~ x0\n    x_prev = x_prior  # initialize previous state with x_prior\n\n    for i in 1:length(y)\n        x[i] ~ MvNormal(mean=A * x_prev, cov=Q)\n        y[i] ~ MvNormal(mean=G * x[i], cov=R)\n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Running-the-AKF-Smoother","page":"Structural Dynamics With Augmented Kalman Filter","title":"Running the AKF Smoother","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Now that we have our system set up, it's time to estimate the system states and unknown input forces using RxInfer. We'll run the Augmented Kalman Filter (AKF) smoother to make sense of the noisy measurements.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Here’s the game plan:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Unpack the Data:   We grab everything we need from the model_data object – time, matrices, measurements, and noise covariances.\nSet the Initial State:   We start with a prior belief about the first state, assuming it's zero with some process noise:   x_0 sim mathcalN(0 Q_textakf)\nRun the Smoother:   We define a helper function to keep things tidy. This function calls RxInfer’s infer method, which does the heavy lifting for us.\nExtract and Reconstruct:  \nRxInfer gives us state marginals, which are the posterior estimates of the states.  \nUsing a helper function, we reconstruct the full-field responses (displacements, velocities, and accelerations).  \nWe also extract the estimated input forces, which are part of the augmented state.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"That’s it! With just a few lines of code, RxInfer takes care of the math behind the scenes and delivers smooth, reliable estimates of what’s happening inside the system.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# RxInfer returns the result in its own structure. \n# Here we wrap the results in a different struct for the example's convenience\nstruct InferenceResults\n    state_marginals\n    y_full_means\n    y_full_stds\n    p_means\n    p_stds\nend\n\nfunction run_smoother(model_data)\n    # unpack the model data\n    t = model_data.t\n    N_data = model_data.N_data\n    A_aug = model_data.A_aug\n    G_aug = model_data.G_aug\n    G_aug_fullfield = model_data.G_aug_fullfield\n    Q_akf = model_data.Q_akf\n    R = model_data.R\n    y_meas = model_data.y_meas\n\n    # initialize the state - required when doing smoothing\n    x0 = MvNormalMeanCovariance(zeros(size(A_aug, 1)), Q_akf)\n\n    # define the smoother engine\n    function smoother_engine(y_meas, A, G, Q, R)\n        # run the akf smoother\n        result_smoother = infer(\n            model=smoother_model(x0=x0, A=A, G=G, Q=Q, R=R),\n            data=(y=y_meas,),\n            options=(limit_stack_depth=500,) # This setting is required for large models\n        )\n\n        # return posteriors as this inference task returns the results as posteriors\n        # because inference is done over the full graph\n        return result_smoother.posteriors[:x]\n    end\n\n    # get the marginals of x\n    state_marginals = smoother_engine(y_meas, A_aug, G_aug, Q_akf, R)\n\n    # reconstructing the full-field response:\n    # use helper function to reconstruct the full-field response\n    y_full_means, y_full_stds = reconstruct_full_field(state_marginals, G_aug_fullfield, N_data)\n\n    # extract the estimated input (input modeled as an augmentation state)\n    p_results_means = getindex.(mean.(state_marginals), length(state_marginals[1]))\n    p_results_stds = getindex.(std.(state_marginals), length(state_marginals[1]))\n\n    return InferenceResults(state_marginals, y_full_means, y_full_stds, p_results_means, p_results_stds)\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"run_smoother (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Mapping-States-to-Full-Field-Responses","page":"Structural Dynamics With Augmented Kalman Filter","title":"Mapping States to Full-Field Responses","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In the run_smoother function, we used a helper function to map the state estimates from the AKF smoother back to the full-field responses (e.g., displacements, velocities, and accelerations).  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Why is this important?","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"While the smoother estimates the system states, we often care about physical quantities like accelerations or displacements across the entire structure.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Using the augmented full-field matrix G^*, we compute:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Response means from state means:   mu_yi = G^* mu_xi  \nResponse uncertainties from state covariances:   sigma_yi = sqrttextdiag(G^* Sigma_xi G^*^top)  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This gives us both the expected responses and their uncertainties at each time step.  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In other words, this function connects the smoother’s internal state estimates to meaningful, physical quantities, making it easy to visualize the system’s behavior.  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# helper function to reconstruct the full field response from the state posteriors\nfunction reconstruct_full_field(\n    x_marginals,\n    G_aug_fullfield,\n    N_data::Int\n)\n\n    # preallocate the full field response\n    y_means = Vector{Vector{Float64}}(undef, N_data)        # vector of vectors\n    y_stds = Vector{Vector{Float64}}(undef, N_data)\n\n    # reconstruct the full-field response using G_aug_fullfield\n    for i in 1:N_data\n        # extract the mean and covariance of the state posterior\n        state_mean = mean(x_marginals[i])       # each index is a vector\n        state_cov = cov(x_marginals[i])\n\n        # project mean and covariance onto the full-field response space\n        y_means[i] = G_aug_fullfield * state_mean\n        y_stds[i] = sqrt.(diag(G_aug_fullfield * state_cov * G_aug_fullfield'))\n    end\n\n    return y_means, y_stds\n\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"reconstruct_full_field (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We now run the AKF smoother using the structural model data to estimate the system states, reconstruct the full-field responses, and extract the input forces along with their uncertainties.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Let’s fire up that RxInfer!","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# run the smoother\nsmoother_results = run_smoother(model_data);","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We first write a helper function and then plot the true states, full-field response, input, their estimates, and the associated uncertainty:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# helper function\nfunction plot_with_uncertainty(\n    t,\n    true_values,\n    estimated_means,\n    estimated_uncertainties,\n    ylabel_text,\n    title_text,\n    label_suffix=\"\";\n    plot_size=(700, 300),)\n    # plot true values\n    plt = plot(\n        t,\n        true_values,\n        label=\"true ($label_suffix)\",\n        lw=2,\n        color=:blue,\n        size=plot_size,\n        left_margin=5Plots.mm,\n        top_margin=5Plots.mm,\n        bottom_margin=5Plots.mm\n    )\n\n    # plot estimated values with uncertainty ribbon\n    plot!(\n        plt,\n        t,\n        estimated_means,\n        ribbon=estimated_uncertainties,\n        fillalpha=0.3,\n        label=\"estimated ($label_suffix)\",\n        lw=2,\n        color=:orange,\n        linestyle=:dash\n    )\n\n    # add labels and title\n    xlabel!(\"time (s)\")\n    ylabel!(ylabel_text)\n    title!(title_text)\n\n    return plt\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"plot_with_uncertainty (generic function with 2 methods)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# select some DOFs to plot\nndof = size(StructuralModel.M)[1]\n\ndisplay_state_dof = 4                # dof 1:4 displacements, dof 5:8 velocities\ndisplay_response_dof = 2 * ndof + 1       # dof 1:4 displacements, dof 5:8 velocities, dof 9:12 accelerations\ndisplay_input_dof = 1                # the only one really\n\n# plot the states\nstate_plot = plot_with_uncertainty(\n    model_data.t,\n    model_data.x_real[display_state_dof, :],\n    getindex.(mean.(smoother_results.state_marginals), display_state_dof),\n    getindex.(std.(smoother_results.state_marginals), display_state_dof),\n    \"state value\",\n    \"state estimate (dof $(display_state_dof))\",\n    \"state dof $(display_state_dof)\"\n);\n\n# plot the responses\nresponse_plot = plot_with_uncertainty(\n    model_data.t,\n    model_data.y_real[display_response_dof, :],\n    getindex.(smoother_results.y_full_means, display_response_dof),\n    getindex.(smoother_results.y_full_stds, display_response_dof),\n    \"response value\",\n    \"reconstructed response (dof $(display_response_dof))\",\n    \"response dof $(display_response_dof)\"\n);\n\n# plot the inputs\ninput_plot = plot_with_uncertainty(\n    model_data.t,\n    model_data.p_real[:, display_input_dof],\n    smoother_results.p_means,\n    smoother_results.p_stds,\n    \"force value\",\n    \"input estimate (applied at dof $(display_input_dof))\",\n    \"input force $(display_input_dof)\"\n);\n\ndisplay(state_plot)\ndisplay(response_plot)\ndisplay(input_plot)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"(Image: ) (Image: ) (Image: )","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Let's quickly go over these results now:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"State estimation: The true state and the estimated state show excellent agreement, demonstrating the accuracy of the smoother model implemented via RxInfer. The uncertainty bounds around the estimated states are noticeable, especially early in the domain. This reflects the natural uncertainty in state estimation since only accelerations are observed, whereas displacements and velocities are inferred through integration.\nReconstructed response: the real response and the reconstructed response align well across the domain, confirming that the filter captures the dynamics quite nicely. The uncertainty bounds here are narrower, showing that the confidence improves as the filter incorporates observations of these quantities of interest (i.e. accelerations).\nInput force reconstruction: The input force and its reconstructed counterpart show significant high frequency variations with very narrow uncertainty bounds. This is expected because accelerations, being the directly observed quantities, are estimated with higher confidence. Plus, we gave ourselves a small advantage by using a well-calibrated prior on this quantity of interest (Q_p).","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The results demonstrate how well the smoother model, implemented with RxInfer, performs in capturing the system dynamics and reconstructing hidden states and inputs. Notably, setting up the probabilistic model was straightforward and intuitive—much easier than dealing with the rest of the structural modeling! This highlights the power of RxInfer for quickly building and solving complex inference problems while keeping the implementation clean and efficient.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"With just a few lines of code, we were able to estimate states, reconstruct responses, and confidently quantify uncertainties—a win for both accuracy and usability. 🚀","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [10745b16] Statistics v1.11.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Gaussian-Mixture","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"This notebook illustrates how to use the NormalMixture node in RxInfer.jl for both univariate and multivariate observations.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Load-packages","page":"Gaussian Mixture","title":"Load packages","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"using RxInfer, Plots, Random, LinearAlgebra, StableRNGs, LaTeXStrings","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Univariate-Gaussian-Mixture-Model","page":"Gaussian Mixture","title":"Univariate Gaussian Mixture Model","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Consider the data set of length N observed below.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"function generate_univariate_data(nr_samples; rng = MersenneTwister(123))\n\n    # data generating parameters\n    class        = [1/3, 2/3]\n    mean1, mean2 = -10, 10\n    precision    = 1.777\n\n    # generate data\n    z = rand(rng, Categorical(class), nr_samples)\n    y = zeros(nr_samples)\n    for k in 1:nr_samples\n        y[k] = rand(rng, Normal(z[k] == 1 ? mean1 : mean2, 1/sqrt(precision)))\n    end\n\n    return y\n\nend;","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"data_univariate = generate_univariate_data(100)\nhistogram(data_univariate, bins=50, label=\"data\", normed=true)\nxlims!(minimum(data_univariate), maximum(data_univariate))\nylims!(0, Inf)\nylabel!(\"relative occurrence [%]\")\nxlabel!(\"y\")","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Model-specification","page":"Gaussian Mixture","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The goal here is to create a model for the data set above. In this case a Gaussian mixture model with K components seems to suite the situation well. We specify the factorized model as  p(y z s m w) = prod_n=1^N bigg(p(y_n mid m w z_n) p(z_n mid s) bigg)prod_k=1^K bigg(p(m_k) p(w_k) bigg) p(s) where the individual terms are specified as beginaligned     p(s)                    = mathrmBeta(s mid alpha_s beta_s) \n    p(m_k)                = mathcalN(m_k mid mu_k sigma_k^2)          p(w_k)                = Gamma(w_k mid alpha_k beta_k) \n    p(z_n mid s)           = mathrmBer(z_n mid s) \n    p(y_n mid m w z_n)   = prod_k=1^K mathcalNleft(y_n mid m_k w_kright)^z_nk endaligned","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The set of observations y = y_1 y_2 ldots y_N is modeled by a mixture of Gaussian distributions, parameterized by means m = m_1 m_2 ldots m_K and precisions w =  w_1 w_2 ldots w_K, where k denotes the component index. This component is selected per observation by the indicator variable z_n, which is a one-of-K encoded vector satisfying sum_k=1^K z_nk = 1 and z_nk in 0 1 forall k. We put a hyperprior on these variables, termed s, which represents the relative occurrence of the different realizations of z_n.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Here we implement the following model with uninformative values for the hyperparameters as","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"@model function univariate_gaussian_mixture_model(y)\n    \n    s ~ Beta(1.0, 1.0)\n\n    m[1] ~ Normal(mean = -2.0, variance = 1e3)\n    w[1] ~ Gamma(shape = 0.01, rate = 0.01)\n\n    m[2] ~ Normal(mean = 2.0, variance = 1e3)\n    w[2] ~ Gamma(shape = 0.01, rate = 0.01)\n\n    for i in eachindex(y)\n        z[i] ~ Bernoulli(s)\n        y[i] ~ NormalMixture(switch = z[i], m = m, p = w)\n    end\n    \nend","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Probabilistic-inference","page":"Gaussian Mixture","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"In order to fit the model to the data, we are interested in computing the posterior distribution p(z s m w mid y) However, computation of this term is intractable. Therefore, it is approximated by a naive mean-field approximation, specified as  p(z s m w mid y) approx prod_n=1^N q(z_n) prod_k=1^K bigg(q(m_k) q(w_k)bigg) q(s) with the functional forms beginaligned     q(s)   = mathrmBeta(s mid hatalpha_s hatbeta_s) \n    q(m_k) = mathcalN(m_k mid hatmu_k hatsigma^2_k) \n    q(w_k) = Gamma (w_k mid hatalpha_k hatbeta_k) \n    q(z_n) = mathrmBer(z_n mid hatp_n) endaligned In order to get the inference procedure started, these marginal distribution need to be initialized.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"n_iterations = 10\n\ninit = @initialization begin\n    q(s) = vague(Beta)\n    q(m) = [NormalMeanVariance(-2.0, 1e3), NormalMeanVariance(2.0, 1e3)]\n    q(w) = [vague(GammaShapeRate), vague(GammaShapeRate)]\nend\n\nresults_univariate = infer(\n    model = univariate_gaussian_mixture_model(), \n    constraints = MeanField(),\n    data  = (y = data_univariate,), \n    initialization = init, \n    iterations  = n_iterations, \n    free_energy = true\n)","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Inference results:\n  Posteriors       | available for (m, w, s, z)\n  Free Energy:     | Real[262.985, 206.795, 183.824, 140.402, 138.188, 138.\n184, 138.184, 138.184, 138.184, 138.184]","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Results","page":"Gaussian Mixture","title":"Results","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Below the inference results can be seen as a function of the iterations","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"m1 = [results_univariate.posteriors[:m][i][1] for i in 1:n_iterations]\nm2 = [results_univariate.posteriors[:m][i][2] for i in 1:n_iterations]\nw1 = [results_univariate.posteriors[:w][i][1] for i in 1:n_iterations]\nw2 = [results_univariate.posteriors[:w][i][2] for i in 1:n_iterations];","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"mp = plot(mean.(m1), ribbon = std.(m1) .|> sqrt, label = L\"posterior $m_1$\")\nmp = plot!(mean.(m2), ribbon = std.(m2) .|> sqrt, label = L\"posterior $m_2$\")\nmp = plot!(mp, [ -10 ], seriestype = :hline, label = L\"true $m_1$\")\nmp = plot!(mp, [ 10 ], seriestype = :hline, label = L\"true $m_2$\")\n\nwp = plot(mean.(w1), ribbon = std.(w1) .|> sqrt, label = L\"posterior $w_1$\", legend = :bottomright, ylim = (-1, 3))\nwp = plot!(wp, mean.(w2), ribbon = std.(w2) .|> sqrt, label = L\"posterior $w_2$\")\nwp = plot!(wp, [ 1.777 ], seriestype = :hline, label = L\"true $w_1$\")\nwp = plot!(wp, [ 1.777 ], seriestype = :hline, label = L\"true $w_2$\")\n\nswp = plot(mean.(results_univariate.posteriors[:s]), ribbon = std.(results_univariate.posteriors[:s]) .|> sqrt, label = L\"posterior $s$\")\nswp = plot!(swp, [ 2/3 ], seriestype = :hline, label = L\"true $s$\")\n\nfep = plot(results_univariate.free_energy, label = \"Free Energy\", legend = :topright)\n\nplot(mp, wp, swp, fep, layout = @layout([ a b; c d ]), size = (800, 400))\nxlabel!(\"iteration\")","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Multivariate-Gaussian-Mixture-Model","page":"Gaussian Mixture","title":"Multivariate Gaussian Mixture Model","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The above example can also be extended to the multivariate case. Consider the data set below","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"function generate_multivariate_data(nr_samples; rng = MersenneTwister(123))\n\n    L         = 50.0\n    nr_mixtures = 6\n\n    probvec = normalize!(ones(nr_mixtures), 1)\n\n    switch = Categorical(probvec)\n\n    gaussians = map(1:nr_mixtures) do index\n        angle      = 2π / nr_mixtures * (index - 1)\n        basis_v    = L * [ 1.0, 0.0 ]\n        R          = [ cos(angle) -sin(angle); sin(angle) cos(angle) ]\n        mean       = R * basis_v \n        covariance = Matrix(Hermitian(R * [ 10.0 0.0; 0.0 20.0 ] * transpose(R)))\n        return MvNormal(mean, covariance)\n    end\n\n    z = rand(rng, switch, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n\n    for n in 1:nr_samples\n        y[n] = rand(rng, gaussians[z[n]])\n    end\n\n    return y\n\nend;","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"data_multivariate = generate_multivariate_data(500)\n\nsdim(n) = (a) -> map(d -> d[n], a) # helper function\nscatter(data_multivariate |> sdim(1), data_multivariate |> sdim(2), ms = 2, alpha = 0.4, size = (600, 400), legend=false)\nxlabel!(L\"y_1\")\nylabel!(L\"y_2\")","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Model-specification-2","page":"Gaussian Mixture","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The goal here is to create a model for the data set above. In this case a Gaussian mixture model with K components seems to suite the situation well. We specify the factorized model as  p(y z s m w) = prod_n=1^N bigg(p(y_n mid m W z_n) p(z_n mid s) bigg)prod_k=1^K bigg(p(m_k) p(W_k) bigg) p(s) where the individual terms are specified as beginaligned     p(s)                    = mathrmDir(s mid alpha_s) \n    p(m_k)                = mathcalN(m_k mid mu_k Sigma_k)          p(W_k)                = mathcalW(W_k mid V_k nu_k) \n    p(z_n mid s)           = mathrmCat(z_n mid s) \n    p(y_n mid m W z_n)   = prod_k=1^K mathcalNleft(y_n mid m_k W_kright)^z_nk endaligned","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The set of observations y = y_1 y_2 ldots y_N is modeled by a mixture of Gaussian distributions, parameterized by means m = m_1 m_2 ldots m_K and precisions W =  W_1 W_2 ldots W_K, where k denotes the component index. This component is selected per observation by the indicator variable z_n, which is a one-of-K encoded vector satisfying sum_k=1^K z_nk = 1 and z_nk in 0 1 forall k. We put a hyperprior on these variables, termed s, which represents the relative occurrence of the different realizations of z_n.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"@model function multivariate_gaussian_mixture_model(nr_mixtures, priors, y)\n    local m\n    local w\n\n    for k in 1:nr_mixtures        \n        m[k] ~ priors[k]\n        w[k] ~ Wishart(3, 1e2*diagm(ones(2)))\n    end\n    \n    s ~ Dirichlet(ones(nr_mixtures))\n    \n    for n in eachindex(y)\n        z[n] ~ Categorical(s) \n        y[n] ~ NormalMixture(switch = z[n], m = m, p = w)\n    end\n    \nend","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Probabilistic-inference-2","page":"Gaussian Mixture","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"In order to fit the model to the data, we are interested in computing the posterior distribution p(z s m W mid y) However, computation of this term is intractable. Therefore, it is approximated by a naive mean-field approximation, specified as  p(z s m W mid y) approx prod_n=1^N q(z_n) prod_k=1^K bigg(q(m_k) q(W_k)bigg) q(s) with the functional forms beginaligned     q(s)   = mathrmDir(s mid hatalpha_s) \n    q(m_k) = mathcalN(m_k mid hatmu_k hatSigma_k) \n    q(w_k) = mathcalW(W_k mid hatV_k hatnu_k) \n    q(z_n) = mathrmCat(z_n mid hatp_n) endaligned In order to get the inference procedure started, these marginal distribution need to be initialized.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"rng = MersenneTwister(121)\npriors = [MvNormal([cos(k*2π/6), sin(k*2π/6)], diagm(1e2 * ones(2))) for k in 1:6]\ninit = @initialization begin\n    q(s) = vague(Dirichlet, 6)\n    q(m) = priors\n    q(w) = Wishart(3, diagm(1e2 * ones(2)))\nend\n\nresults_multivariate = infer(\n    model = multivariate_gaussian_mixture_model(\n        nr_mixtures = 6, \n        priors = priors,\n    ), \n    data  = (y = data_multivariate,), \n    constraints   = MeanField(),\n    initialization = init, \n    iterations  = 50, \n    free_energy = true\n)","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Inference results:\n  Posteriors       | available for (w, m, s, z)\n  Free Energy:     | Real[3927.95, 3884.37, 3884.37, 3884.37, 3884.37, 3884\n.37, 3884.37, 3884.37, 3884.37, 3884.37  …  3884.37, 3884.37, 3884.37, 3884\n.37, 3884.37, 3884.37, 3884.37, 3884.37, 3884.37, 3884.37]","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Results-2","page":"Gaussian Mixture","title":"Results","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Below the inference results can be seen","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"p_data = scatter(data_multivariate |> sdim(1), data_multivariate |> sdim(2), ms = 2, alpha = 0.4, legend=false, title=\"Data\", xlims=(-75, 75), ylims=(-75, 75))\np_result = plot(xlims = (-75, 75), ylims = (-75, 75), title=\"Inference result\", legend=false, colorbar = false)\nfor (e_m, e_w) in zip(results_multivariate.posteriors[:m][end], results_multivariate.posteriors[:w][end])\n    gaussian = MvNormal(mean(e_m), Matrix(Hermitian(mean(inv, e_w))))\n    global p_result = contour!(p_result, range(-75, 75, step = 0.25), range(-75, 75, step = 0.25), (x, y) -> pdf(gaussian, [ x, y ]), title=\"Inference result\", legend=false, levels = 7, colorbar = false)\nend\np_fe = plot(results_multivariate.free_energy, label = \"Free Energy\")\n\nplot(p_data, p_result, p_fe, layout = @layout([ a b; c ]))","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/gaussian_mixture/Project.toml`\n  [b964fa9f] LaTeXStrings v1.4.0\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/#Hierarchical-Gaussian-Filter","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"section"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this demo the goal is to perform approximate variational Bayesian Inference for Univariate Hierarchical Gaussian Filter (HGF).","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Simple HGF model can be defined as:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  x^(j)_k  sim  mathcalN(x^(j)_k - 1 f_k(x^(j - 1)_k)) \n  y_k  sim  mathcalN(x^(j)_k tau_k)\nendaligned","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"where j is an index of layer in hierarchy, k is a time step and f_k is a variance activation function. RxInfer.jl export Gaussian Controlled Variance (GCV) node with f_k = exp(kappa x + omega) variance activation function. By default the node uses Gauss-Hermite cubature with a prespecified number of approximation points in the cubature. In this demo we also show how we can change the hyperparameters in different approximation methods (iin this case Gauss-Hermite cubature) with the help of metadata structures. Here how our model will look like with the GCV node:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  z_k  sim  mathcalN(z_k - 1 mathcaltau_z) \n  x_k  sim  mathcalN(x_k - 1 exp(kappa z_k + omega)) \n  y_k  sim  mathcalN(x_k mathcaltau_y)\nendaligned","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this experiment we will create a single time step of the graph and perform variational message passing filtering alrogithm to estimate hidden states of the system. For a more rigorous introduction to Hierarchical Gaussian Filter we refer to Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter paper.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"For simplicity we will consider tau_z, tau_y, kappa and omega known and fixed, but there are no principled limitations to make them random variables too.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"using RxInfer, BenchmarkTools, Random, Plots, StableRNGs","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function generate_data(rng, n, k, w, zv, yv)\n    z_prev = 0.0\n    x_prev = 0.0\n\n    z = Vector{Float64}(undef, n)\n    v = Vector{Float64}(undef, n)\n    x = Vector{Float64}(undef, n)\n    y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        z[i] = rand(rng, Normal(z_prev, sqrt(zv)))\n        v[i] = exp(k * z[i] + w)\n        x[i] = rand(rng, Normal(x_prev, sqrt(v[i])))\n        y[i] = rand(rng, Normal(x[i], sqrt(yv)))\n\n        z_prev = z[i]\n        x_prev = x[i]\n    end \n    \n    return z, x, y\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Seed for reproducibility\nseed = 42\n\nrng = StableRNG(seed)\n\n# Parameters of HGF process\nreal_k = 1.0\nreal_w = 0.0\nz_variance = abs2(0.2)\ny_variance = abs2(0.1)\n\n# Number of observations\nn = 300\n\nz, x, y = generate_data(rng, n, real_k, real_w, z_variance, y_variance);","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    scatter!(px, 1:n, y, label = \"y_i\", color = :red, ms = 2, alpha = 0.2)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/#Online-learning-(Filtering)","page":"Hierarchical Gaussian Filter","title":"Online learning (Filtering)","text":"","category":"section"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To create a model we use the @model macro:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# We create a single-time step of corresponding state-space process to\n# perform online learning (filtering)\n@model function hgf(y, κ, ω, z_variance, y_variance, z_prev_mean, z_prev_var, x_prev_mean, x_prev_var)\n\n    z_prev ~ Normal(mean = z_prev_mean, variance = z_prev_var)\n    x_prev ~ Normal(mean = x_prev_mean, variance = x_prev_var)\n\n    # Higher layer is modelled as a random walk \n    z_next ~ Normal(mean = z_prev, variance = z_variance)\n    \n    # Lower layer is modelled with `GCV` node\n    x_next ~ GCV(x_prev, z_next, κ, ω)\n    \n    # Noisy observations \n    y ~ Normal(mean = x_next, variance = y_variance)\nend\n\n@constraints function hgfconstraints() \n    # Mean-field factorization constraints\n    q(x_next, x_prev, z_next) = q(x_next)q(x_prev)q(z_next)\nend\n\n@meta function hgfmeta()\n    # Lets use 31 approximation points in the Gauss Hermite cubature approximation method\n    GCV() -> GCVMetadata(GaussHermiteCubature(31)) \nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"hgfmeta (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"The code below uses the infer function from RxInfer to generate the message passing algorithm given the model and constraints specification.  We also specify the @autoupdates in order to set new priors for the next observation based on posteriors.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function run_inference(data, real_k, real_w, z_variance, y_variance)\n\n    autoupdates   = @autoupdates begin\n        # The posterior becomes the prior for the next time step\n        z_prev_mean, z_prev_var = mean_var(q(z_next))\n        x_prev_mean, x_prev_var = mean_var(q(x_next))\n    end\n\n    init = @initialization begin\n        q(x_next) = NormalMeanVariance(0.0, 5.0)\n        q(z_next) = NormalMeanVariance(0.0, 5.0)\n    end\n\n    return infer(\n        model          = hgf(κ = real_k, ω = real_w, z_variance = z_variance, y_variance = y_variance),\n        constraints    = hgfconstraints(),\n        meta           = hgfmeta(),\n        data           = (y = data, ),\n        autoupdates    = autoupdates,\n        keephistory    = length(data),\n        historyvars    = (\n            x_next = KeepLast(),\n            z_next = KeepLast()\n        ),\n        initialization = init,\n        iterations     = 5,\n        free_energy    = true,\n    )\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"run_inference (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Everything is ready to run the algorithm. We used the online version of the algorithm, thus we need to fetch the history of the posterior estimation instead of the actual posteriors.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"result = run_inference(y, real_k, real_w, z_variance, y_variance);\n\nmz = result.history[:z_next];\nmx = result.history[:x_next];","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz), ribbon = std.(mz), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx), ribbon = std.(mx), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(result.free_energy_history, label = \"Bethe Free Energy\")","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see BetheFreeEnergy converges nicely to a stable point. ","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/#Offline-learning-(Smoothing)","page":"Hierarchical Gaussian Filter","title":"Offline learning (Smoothing)","text":"","category":"section"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Aside from online learning, we can also perform offline learning (smoothing) with the HGF model to learn the parameters in case we have collected all the data. In this offline setting, we treat the parameters kappa and omega as random variables and place a prior over them. These parameters will be updated along with latent states during the inference. First, let's define the HGF model for offline learning","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"#Model for offline learning (smoothing)\n\n@model function hgf_smoothing(y, z_variance, y_variance)\n    # Initial states \n    z_prev ~ Normal(mean = 0., variance = 5.0)\n    x_prev ~ Normal(mean = 0., variance = 5.0)\n\n    # Priors on κ and ω\n    κ ~ Normal(mean = 1.5, variance = 1.0)\n    ω ~ Normal(mean = 0.0, variance = 0.05)\n\n    for i in eachindex(y)\n        # Higher layer \n        z[i] ~ Normal(mean = z_prev, variance = z_variance)\n\n        # Lower layer \n        x[i] ~ GCV(x_prev, z[i], κ, ω)\n\n        # Noisy observations \n        y[i] ~ Normal(mean = x[i], variance = y_variance)\n\n        # Update last/previous hidden states\n        z_prev = z[i]\n        x_prev = x[i]\n    end\nend\n\n@constraints function hgfconstraints_smoothing() \n    #Structured mean-field factorization constraints\n    q(x_prev,x, z,κ,ω) = q(x_prev,x)q(z)q(κ)q(ω)\nend\n\n@meta function hgfmeta_smoothing()\n    # Lets use 31 approximation points in the Gauss Hermite cubature approximation method\n    GCV() -> GCVMetadata(GaussHermiteCubature(31)) \nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"hgfmeta_smoothing (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Similar to the filtering case, we use the infer function from RxInfer to implement inference. ","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function run_inference_smoothing(data, z_variance, y_variance)\n    @initialization function hgf_init_smoothing()\n        q(x) = NormalMeanVariance(0.0,5.0)\n        q(z) = NormalMeanVariance(0.0,5.0)\n        q(κ) = NormalMeanVariance(1.5,1.0)\n        q(ω) = NormalMeanVariance(0.0,0.05)\n    end\n\n    #Let's do inference with 20 iterations \n    return infer(\n        model = hgf_smoothing(z_variance = z_variance, y_variance = y_variance,),\n        data = (y = data,),\n        meta = hgfmeta_smoothing(),\n        constraints = hgfconstraints_smoothing(),\n        initialization = hgf_init_smoothing(),\n        iterations = 20,\n        options = (limit_stack_depth = 100, ), \n        returnvars = (x = KeepLast(), z = KeepLast(),ω=KeepLast(),κ=KeepLast(),),\n        free_energy = true \n    )\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"run_inference_smoothing (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Now we can get the result.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"result_smoothing = run_inference_smoothing(y, z_variance, y_variance);\nmz_smoothing = result_smoothing.posteriors[:z];\nmx_smoothing = result_smoothing.posteriors[:x];","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz_smoothing), ribbon = std.(mz_smoothing), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx_smoothing), ribbon = std.(mx_smoothing), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles to the real hidden states and appears \"smoother\" compared to the filtering case. We may be also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(result_smoothing.free_energy, label = \"Bethe Free Energy\")","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Finally, we can also extract the marginals q(kappa) and q(omega) to get the appropximation of these parameters.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"q_κ = result_smoothing.posteriors[:κ]\nq_ω = result_smoothing.posteriors[:ω]\n\nprintln(\"Approximate value of κ: \", mean(q_κ))\nprintln(\"Approximate value of ω: \", mean(q_ω))","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Approximate value of κ: 0.7543386579898821\nApproximate value of ω: -0.18164892866626792","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Let's visualize their marginal distributions.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"range_w = range(-1,0.5,length = 1000)\nrange_k = range(0,2,length = 1000)\nlet \n    pw = plot(title = \"Marginal q(w)\")\n    pk = plot(title = \"Marginal q(k)\")\n    \n    plot!(pw, range_w, (x) -> pdf(q_ω, x), fillalpha=0.3, fillrange = 0, label=\"Posterior q(w)\", c=3, legend_position=(0.1,0.95),legendfontsize=9)\n    vline!([real_w], label=\"Real w\")\n    xlabel!(\"w\")\n    \n    \n    plot!(pk, range_k, (x) -> pdf(q_κ, x), fillalpha=0.3, fillrange = 0, label=\"Posterior q(k)\", c=3, legend_position=(0.1,0.95),legendfontsize=9)\n    vline!([real_k], label=\"Real k\")\n    xlabel!(\"k\")\n    \n    plot(pk, pw, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see, both the marginals q(kappa) and q(omega) are not quite off from the true values. Specifically, the means of q(kappa) and q(omega) are approximately 075 and -018, respectively, which are quite close to their true values. ","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/hierarchical_gaussian_filter/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Feature-Functions-in-Bayesian-Regression","page":"Feature Functions In Bayesian Regression","title":"Feature Functions in Bayesian Regression","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This notebook demonstrates how we can use probabilistic methods to learn and predict continuous functions from noisy data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This example is inspired by Chapter 4.1 Regression from the excellent book Probabilistic Numerics by Phillip Hennig, Michael A. Osborn, and Hans P. Kersting. We'll take their theoretical foundations and bring them to life with practical code examples.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The code and narrative in this notebook is written by Dmitry Bagaev (GitHub, LinkedIn). While some explanations draw from the book's content, we'll focus on building intuition through interactive examples and visualizations.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"By the end of this notebook, you'll understand:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The power of linear regression with basis functions\nHow to handle uncertainty in your predictions\nPractical implementation using Julia and RxInfer.jl","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"We start by importing all required packages for this example, the primary of which is of course RxInfer!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"using RxInfer, StableRNGs, LinearAlgebra, Plots, DataFrames","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Gaussian distributions (multivariate) assign probability density to vectors of real numbers - think of them as sophisticated probability maps for multiple variables at once. In numerical applications, we often encounter real-valued functions f  mathbbX rightarrow R over some input domain mathbbX (imagine predicting house prices based on features like size and location).","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"A interesting way to use the Gaussian inference framework is to assume that f can be written as a weighted sum over a finite number F of feature functions phi_i  mathbbX rightarrow mathbbR_i=1F (much like how a house price might be a weighted combination of its features, e.g. size, number of floors, number of rooms, etc..):","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nf(x) = sum_i=1^F phi_i(x)omega_i = Phi^T_x omega  mathrmwhere  omega in mathbbR^F\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"As discussed in the Probabilistic Numerics book, uncertainty is a fundamental aspect of numerical computations. When we perform regression, we are essentially solving an inverse problem - trying to infer the underlying function from noisy observations. This inherently involves uncertainty for several reasons:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Our observations usually contain noise and measurement errors\nWe have a finite number of samples, leaving gaps in our knowledge\nThe true function may be more complex than our model can capture","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"By modeling uncertainty explicitly through a Gaussian distribution over the weights omega, we can:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Quantify our confidence in predictions\nMake more robust decisions by accounting for uncertainty\nDetect when we're extrapolating beyond our data\nPropagate uncertainty on the next step in our Machine Learning pipeline","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Mathematically, we express this uncertainty as:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\np(omega) = mathcalN(omega vert mu Sigma)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Where mu represents our best estimate of the weights and Sigma captures our uncertainty about them.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Dataset:-Noisy-Observations-in-the-Real-World","page":"Feature Functions In Bayesian Regression","title":"Dataset: Noisy Observations in the Real World","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"In real-world scenarios, we rarely have access to perfect measurements. Instead, we collect observations Y = y_1 cdots  y_N  in mathbbR that are corrupted by Gaussian noise - a common and mathematically convenient way to model measurement uncertainty. These noisy samples of our target function f are taken at specific input locations X, with the noise characterized by a covariance matrix Lambda  mathbbR^NN. This setup mirrors many practical applications, from sensor measurements to experimental data collection.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's assume we have collected noisy measurenets Y at locations X:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"N = 40\nΛ = I\nX = range(-8, 8, length=N)\n\nrng = StableRNG(42)\n\n# Arbitrary non-linear function, which is hidden\nf(x) = -((-x / 3)^3 - (-x / 2)^2 + x + 10) \n\nY = rand(rng, MvNormalMeanCovariance(f.(X), Λ))\n\n# Can be loaded from a file or a database\ndf = DataFrame(X = X, Y = Y)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"40×2 DataFrame\n Row │ X         Y\n     │ Float64   Float64\n─────┼────────────────────\n   1 │ -8.0      -5.63321\n   2 │ -7.58974  -3.75472\n   3 │ -7.17949  -2.26681\n   4 │ -6.76923  -1.95387\n   5 │ -6.35897  -2.92934\n   6 │ -5.94872  -2.31714\n   7 │ -5.53846  -4.10432\n   8 │ -5.12821  -4.08565\n  ⋮  │    ⋮         ⋮\n  34 │  5.53846  -1.5234\n  35 │  5.94872   0.98319\n  36 │  6.35897   3.86051\n  37 │  6.76923   4.48039\n  38 │  7.17949   8.71697\n  39 │  7.58974  12.703\n  40 │  8.0      19.0635\n           25 rows omitted","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Train-and-Test-Dataset-Configurations","page":"Feature Functions In Bayesian Regression","title":"Train & Test Dataset Configurations","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"To thoroughly evaluate our model's performance and robustness, we'll create three distinct train-test splits of our data. This approach helps us understand how well our model generalizes to different regions of the input space and whether it can effectively capture the underlying patterns regardless of which portions of the data it learns from.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"We'll explore the following configurations:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Forward Split: Uses the first half for training and second half for testing, evaluating the model's ability to extrapolate to higher x-values\nReverse Split: Uses the first half for testing and second half for training, testing extrapolation to lower x-values\nInterleaved Split: Uses first and last quarters for training and middle portion for testing, assessing interpolation capabilities","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"These diverse splits will help reveal any biases in our model and ensure it performs consistently across different regions of the input space. They also allow us to evaluate both interpolation (predicting within the training range) and extrapolation (predicting outside the training range) capabilities.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"# Split data into train/test sets\n# Forward split - first half train, second half test\ndataset_1 = let mid = N ÷ 2\n    (\n        y_train = Y[1:mid], x_train = X[1:mid],\n        y_test = Y[mid+1:end], x_test = X[mid+1:end]\n    )\nend\n\n# Reverse split - first half test, second half train  \ndataset_2 = let mid = N ÷ 2\n    (\n        y_test = Y[1:mid], x_test = X[1:mid],\n        y_train = Y[mid+1:end], x_train = X[mid+1:end]\n    )\nend\n\n# Interleaved split - first/last quarters train, middle half test\ndataset_3 = let q1 = N ÷ 4, q3 = 3N ÷ 4\n    (\n        y_train = [Y[1:q1]..., Y[q3+1:end]...],\n        x_train = [X[1:q1]..., X[q3+1:end]...],\n        y_test = Y[q1+1:q3],\n        x_test = X[q1+1:q3]\n    )\nend\n\ndatasets = [dataset_1, dataset_2, dataset_3]\n\n# Create visualization for each dataset split\nps = map(enumerate(datasets)) do (i, dataset)\n    p = plot(\n        xlim = (-10, 10), \n        ylim = (-30, 30),\n        title = \"Dataset $i\",\n        xlabel = \"x\",\n        ylabel = \"y\"\n    )\n    scatter!(p, \n        dataset[:x_train], dataset[:y_train],\n        yerror = Λ,\n        label = \"Train dataset\",\n        color = :blue,\n        markersize = 4\n    )\n    scatter!(p,\n        dataset[:x_test], dataset[:y_test], \n        yerror = Λ,\n        label = \"Test dataset\",\n        color = :red,\n        markersize = 4\n    )\n    return p\nend\n\nplot(ps..., size = (1200, 400), layout = @layout([a b c]))","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The datasets above provide nonlinear data with independent and identically distributed (i.i.d.) Gaussian observation noise, where we set the noise covariance Λ = I (identity matrix).","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Bayesian-Inference-with-RxInfer","page":"Feature Functions In Bayesian Regression","title":"Bayesian Inference with RxInfer","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Our exciting challenge is to uncover the probability distribution over the parameter vector omega, given our basis functions phi and observed data points (XY). To tackle this, we'll harness the power of probabilistic programming by constructing an elegant generative model using RxInfer's @model macro. The beauty of this approach lies in its simplicity - we can express our entire model in just a few lines of code:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"@model function parametric_regression(ϕs, x, y, μ, Σ, Λ)\n    # Prior distribution over parameters ω\n    ω ~ MvNormal(mean = μ, covariance = Σ)\n    \n    # Design matrix Φₓ where each element is ϕᵢ(xⱼ)\n    Φₓ = [ϕ(xᵢ) for xᵢ in x, ϕ in ϕs]\n    \n    # Likelihood of observations y given parameters ω\n    y ~ MvNormal(mean = Φₓ * ω, covariance = Λ)\nend","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's break down the key components of our probabilistic model:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"phimathrms\ncontains our basis functions phi_i - these are the building blocks of our model\nx\nholds the input locations X where we've made observations. Think of these as the points along the x-axis where we've collected data, like timestamps or spatial coordinates.\ny\ncontains our noisy measurements at each location in X.\nmu\ndefines our prior beliefs about the average values of the parameters omega. Setting mu = 0 indicates we believe the parameters are centered around zero before seeing any data.\nSigma\nencodes our uncertainty about omega before seeing data. A larger Sigma means we're more uncertain, while smaller values indicate stronger prior beliefs.\nLambda\nrepresents the noise in our observations. For example, Lambda = 01I suggests our measurements have small, independent Gaussian noise, while larger values indicate noisier data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"To put this model to work, we'll use RxInfer's powerful infer function. Here's how:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"function infer_ω(; ϕs, x, y)\n    # Create probabilistic model, \n    # RxInfer will construct the graph of this model auutomatically\n    model = parametric_regression(\n        ϕs = ϕs, \n        μ  = zeros(length(ϕs)),\n        Σ  = I,\n        Λ  = I,\n        x  = x\n    )\n\n    # Let RxInfer do all the math for you\n    result = infer(\n        model = model, \n        data  = (y = y,)\n    )\n\n    # Return posterior over ω\n    return result.posteriors[:ω]\nend","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"infer_ω (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#How-to-choose-basis-functions?","page":"Feature Functions In Bayesian Regression","title":"How to choose basis functions?","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Just like how choosing between pizza toppings can make or break your dinner, the choice of basis functions phi_i  can dramatically impact our results! Think of basis functions as the building blocks of our mathematical LEGO set -  pick the wrong pieces and your model might end up looking more like abstract art than a useful predictor.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Why does this matter? Because these functions are the \"vocabulary\" our model uses to describe the patterns in our data. Choose a too-simple vocabulary and your model will sound like a caveman (\"data go up, data go down\"). Choose one that's  too complex and it might start speaking mathematical gibberish!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's embark on a thrilling journey through different datasets with various basis function choices. We'll create a  handy function that will:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Take our basis functions for a test drive 🚗\nRun inference on multiple datasets defined above\nCreate beautiful plots that would make any statistician swoon","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(RxInfer makes it so easy to perform Bayesian inference so I have more time to make beautiful plots!)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"function plot_inference_results_for(; ϕs, datasets, title = \"\", rng = StableRNG(42))\n    # Create main plot showing basis functions\n    p1 = plot(\n        title = \"Basis functions: $(title)\", \n        xlabel = \"x\",\n        ylabel = \"y\",\n        xlim = (-5, 5), \n        ylim = (-10, 10),\n        legend = :outertopleft,\n        grid = true,\n        fontfamily = \"Computer Modern\"\n    )\n\n    # Plot basis functions in gray\n    plot_ϕ!(p1, ϕs, color = :gray, alpha = 0.5, \n            labels = [\"ϕ$i\" for _ in 1:1, i in 1:length(ϕs)])\n    \n    # Add examples with random ω values\n    plot_ϕ!(p1, ϕs, randn(rng, length(ϕs), 3), \n            linewidth = 2)\n\n    # Create subplot for each dataset\n    ps = map(enumerate(datasets)) do (i, dataset)\n        p2 = plot(\n            title = \"Dataset #$(i): $(title)\",\n            xlabel = \"x\",\n            ylabel = \"y\", \n            xlim = (-10, 10),\n            ylim = (-25, 25),\n            grid = true,\n            fontfamily = \"Computer Modern\"\n        )\n\n        # Infer posterior over ω\n        ωs = infer_ω(\n            ϕs = ϕs, \n            x = dataset[:x_train], \n            y = dataset[:y_train]\n        )\n\n        # Plot posterior mean\n        plot_ϕ!(p2, ϕs, mean(ωs),\n                linewidth = 3,\n                color = :green,\n                labels = \"Posterior mean\")\n\n        # Plot posterior samples\n        plot_ϕ!(p2, ϕs, rand(ωs, 15),\n                linewidth = 1,\n                color = :gray,\n                alpha = 0.4,\n                labels = nothing)\n\n        # Add data points\n        scatter!(p2, dataset[:x_train], dataset[:y_train],\n                yerror = Λ,\n                label = \"Training data\",\n                color = :royalblue,\n                markersize = 4)\n        scatter!(p2, dataset[:x_test], dataset[:y_test],\n                yerror = Λ,\n                label = \"Test data\", \n                color = :crimson,\n                markersize = 4)\n\n        return p2\n    end\n\n    # Combine all plots\n    plot(p1, ps..., \n         size = (1000, 800),\n         margin = 5Plots.mm,\n         layout = (2,2))\nend\n\n# Helper function to plot basis functions\nfunction plot_ϕ!(p, ϕs; rl = -10, rr = 10, kwargs...)\n    xs = range(rl, rr, length = 200)\n    ys = [ϕ(x) for x in xs, ϕ in ϕs]\n    plot!(p, xs, ys; kwargs...)\nend\n\n# Helper function to plot function with given weights\nfunction plot_ϕ!(p, ϕs, ωs; rl = -10, rr = 10, kwargs...)\n    xs = range(rl, rr, length = 200)\n    ys = [ϕ(x) for x in xs, ϕ in ϕs]\n    yr = ys * ωs\n    labels = [\"Sample $i\" for _ in 1:1, i in 1:size(ωs,2)]\n    plot!(p, xs, yr, labels = labels; kwargs...)\nend","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_ϕ! (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Phew! We've finally escaped the plotting purgatory - you know it's bad when the visualization code is longer than the actual inference code! But fear not, dear reader, for we're about to dive into the juicy stuff. Grab your statistical popcorn, because the real fun is about to begin!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Polynomials:-The-Building-Blocks-of-Function-Approximation","page":"Feature Functions In Bayesian Regression","title":"Polynomials: The Building Blocks of Function Approximation","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's start our exploration with one of the most fundamental and elegant choices for basis functions: polynomials. These simple yet powerful functions form the backbone of many approximation techniques in mathematics and machine learning.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"For our polynomial basis functions phi_i, we'll use the classic form:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = x^i\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where i represents the degree of each polynomial term. This gives us a sequence of increasingly complex functions: constant (x^0 = 1), linear (x^1), quadratic (x^2), cubic (x^3), and so on. When combined with appropriate weights omega, these basis functions can approximate a wide variety of smooth functions - a result famously known as the Weierstrass approximation theorem.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's witness the magic of RxInfer as it efficiently infers the posterior distribution over the weights omega using these polynomial basis functions. The beauty of this approach lies in how it automatically determines the contribution of each polynomial term to fit our data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"polynomials\",\n    datasets = datasets,\n    ϕs       = [ (x) -> x ^ i for i in 0:5 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's break down what we're seeing in these fascinating plots! The first plot (in gray) reveals our polynomial basis functions in their raw form - from constant to quintic terms. Overlaid on these are some example functions generated by combining these basis functions with random weights ω, giving us a glimpse of the expressive power of polynomial approximation.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The subsequent plots demonstrate how our model performs inference on different datasets. Notice how the posterior distribution (shown by the shaded region) adapts to capture the uncertainty in different regions of the input space. It's particularly interesting to observe how the model's predictions change when faced with different training and test sets - a beautiful illustration of how the learning process is influenced by the data we feed it.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"While polynomials have served us well here, they're just one tool in our mathematical toolbox. Ready to explore some alternative basis functions that might capture different aspects of our target function? Let's dive into some exciting alternatives!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Trigonometric-Functions:-Catch-Some-Waves","page":"Feature Functions In Bayesian Regression","title":"Trigonometric Functions: Catch Some Waves","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"While polynomials are great (and we love them dearly), sometimes life isn't just about going up and down in straight-ish lines. Sometimes, we need to embrace our inner surfer and catch some waves! Enter trigonometric functions - the mathematical world's answer to the question \"What if everything just went round and round?\"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Trigonometric functions, particularly sin and cos, have been the backbone of mathematical analysis since ancient times. From describing planetary motions to analyzing sound waves, these periodic functions have a special place in the mathematician's heart. Their ability to represent cyclic patterns makes them particularly powerful for approximating periodic phenomena - something our polynomial friends from earlier might struggle with (imagine a polynomial trying to do the wave at a sports event - awkward!).","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"For our basis functions, we'll use scaled versions of sine and cosine:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = mathrmsin(fracxi) \nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = mathrmcos(fracxi)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where i acts as a frequency scaling factor. As i increases, our waves become more stretched out, giving us different frequencies to work with. Think of it as having an orchestra where each instrument plays the same tune but at different tempos!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's start by riding the sine wave alone (no cosine jealousy please!) for i = 15. Will these wavy functions give our polynomial predecessors a run for their money? Let's find out!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"trigonometric sin\",\n    datasets = datasets,\n    ϕs       = [ (x) -> sin(x / i) for i in 1:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Now let's examine the results using cosine basis functions","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"trigonometric cos\",\n    datasets = datasets,\n    ϕs       = [ (x) -> cos(x / i) for i in 1:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"And for our grand finale, let's combine both sin and cos - because two waves are better than one! (Just don't tell that to particle-wave duality...)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"trigonometric sin & cos\",\n    datasets = datasets,\n    ϕs       = [\n        [ (x) -> sin(x / i) for i in 1:4 ]...,\n        [ (x) -> cos(x / i) for i in 1:4 ]...,\n    ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Incredible! RxInfer proved to be quite the adaptable fellow - it handled these different basis functions without missing a beat. The results speak for themselves: our sine and cosine tag team performed remarkably well for this example. I guess you could say they really found their wavelength!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Comparing-Model-Performance-via-Log-Evidence","page":"Feature Functions In Bayesian Regression","title":"Comparing Model Performance via Log-Evidence","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Now that we've explored different basis functions, let's quantitatively evaluate their performance using Free Energy, also known as negative log-evidence or negative Evidence Lower BOund (ELBO). RxInfer can compute Free Energy values when requested, which serve as a principled way to compare different models.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Free Energy has several important properties:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"It acts as a proxy for negative log model evidence P(y|model)\nLower values indicate better model fit, balancing complexity and data fit\nIt automatically implements Occam's Razor by penalizing overly complex models","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"For example, if we have:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Model A: Free Energy = 100\nModel B: Free Energy = 50","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Then Model B provides a better explanation of the data, as exp(-50) > exp(-100).","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's analyze the Free Energy values for our polynomial and trigonometric basis functions to determine which model class provides the best explanation of our data. We'll check:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Pure sine basis functions\nPure cosine basis functions  \nCombined sine and cosine basis functions","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This will help us quantitatively validate our earlier visual assessments.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"# Combine the function definition with the usage\nfunction infer_ω_but_return_free_energy(; ϕs, x, y)\n    result = infer(\n        model = parametric_regression(\n            ϕs = ϕs, \n            μ  = zeros(length(ϕs)),\n            Σ  = I,\n            Λ  = I,\n            x  = x\n        ), \n        data  = (y = y,),\n        free_energy = true\n    )\n    return first(result.free_energy)\nend\n\ndfs = map(enumerate(datasets)) do (i, dataset)\n    # Generate basis functions\n    sin_bases = [(x) -> sin(x / i) for i in 1:8]\n    cos_bases = [(x) -> cos(x / i) for i in 1:8]\n    combined_bases = [\n        [(x) -> sin(x / i) for i in 1:4]...,\n        [(x) -> cos(x / i) for i in 1:4]...\n    ]\n\n    # Calculate free energy for each basis\n    energies = [\n        infer_ω_but_return_free_energy(ϕs=sin_bases, x=dataset[:x_train], y=dataset[:y_train]),\n        infer_ω_but_return_free_energy(ϕs=cos_bases, x=dataset[:x_train], y=dataset[:y_train]),\n        infer_ω_but_return_free_energy(ϕs=combined_bases, x=dataset[:x_train], y=dataset[:y_train])\n    ]\n\n    # Create DataFrame row\n    DataFrame(\n        dataset = fill(i, 3),\n        fns = [:sin, :cos, :sin_cos],\n        free_energy = energies\n    )\nend\n\nvcat(dfs...)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"9×3 DataFrame\n Row │ dataset  fns      free_energy\n     │ Int64    Symbol   Float64\n─────┼───────────────────────────────\n   1 │       1  sin          74.6271\n   2 │       1  cos          44.3858\n   3 │       1  sin_cos      49.3385\n   4 │       2  sin         943.507\n   5 │       2  cos         107.092\n   6 │       2  sin_cos      93.3269\n   7 │       3  sin          83.0291\n   8 │       3  cos          53.7804\n   9 │       3  sin_cos      70.1097","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The results demonstrate that the choice of basis functions plays a significant role, as evidenced by the varying values of the Free Energy function. For dataset 1, cosine-based basis functions perform better than both sine-based and combined sine-cosine basis functions. Meanwhile, for dataset 3, the combination of sine and cosine basis functions yields superior results.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"However, why limit ourselves to polynomials and trigonometric functions? Let's explore other possibilities!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Switch-Functions:-A-Binary-Approach-to-Basis-Functions","page":"Feature Functions In Bayesian Regression","title":"Switch Functions: A Binary Approach to Basis Functions","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's explore an intriguing and perhaps unconventional choice for basis functions: the switch functions. These functions, despite their simplicity, can be remarkably effective in certain scenarios.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"A switch function essentially divides the input space into two regions, outputting either +1 or -1 based on which side of a threshold the input falls. Mathematically, we define it as:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = mathrmsign(x - i)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where i serves as the threshold point. The function returns +1 when x  i and -1 when x  i. This creates a sharp \"switch\" at x = i, hence the name.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"What makes these functions particularly interesting is their ability to capture discontinuities and sharp transitions in the data. By combining multiple switch functions with different threshold points, we can approximate complex patterns through a series of binary decisions.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's see how these switch functions perform on our datasets!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"switches\",\n    datasets = datasets,\n    ϕs       = [ (x) -> sign(x - i) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Step-Functions:-A-Binary-Leap-Forward","page":"Feature Functions In Bayesian Regression","title":"Step Functions: A Binary Leap Forward","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's explore another fascinating class of basis functions: step functions. Also known as Heaviside functions, these elegant mathematical constructs make a dramatic jump from 0 to 1 at a specific threshold point.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Mathematically, we define our step basis functions as:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = mathbbI(x - i  0)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where mathbbI is the indicator function that equals 1 when its argument is true and 0 otherwise. Unlike the switch functions we saw earlier, step functions provide a unidirectional transition, making them particularly useful for modeling data with distinct regimes or threshold effects.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"steps\",\n    datasets = datasets,\n    ϕs       = [ (x) -> ifelse(x - i > 0, 1.0, 0.0) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Linear-Basis-Functions:-A-Classic-Twist","page":"Feature Functions In Bayesian Regression","title":"Linear Basis Functions: A Classic Twist","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Here's an intriguing proposition: what if we used linear functions as our basis functions in linear regression? While it might sound redundant at first, this approach offers a fascinating perspective. By centering linear functions at different points, we create a rich set of features that can capture both local and global trends in our data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The basis functions take the form:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = vert x - i vert\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where each function measures the absolute distance from a reference point i. This creates a V-shaped function centered at each point, allowing us to model both increasing and decreasing trends with remarkable flexibility.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"linears\",\n    datasets = datasets,\n    ϕs       = [ (x) -> abs(x - i) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Absolute-Exponential-Functions:-Elegantly-Decaying-Distance","page":"Feature Functions In Bayesian Regression","title":"Absolute Exponential Functions: Elegantly Decaying Distance","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's venture into the realm of absolute exponential functions, a fascinating class of basis functions that elegantly capture the notion of distance-based influence. These functions, also known as Laplace kernels in some contexts, decay exponentially with the absolute distance from their center points.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The mathematical formulation reveals their elegant simplicity:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = e^-vert x - i vert\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This expression creates a peaked function that reaches its maximum of 1 at x = i and smoothly decays in both directions, providing a natural way to model localized influences that diminish with distance.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"abs exps\",\n    datasets = datasets,\n    ϕs       = [ (x) -> exp(-abs(x - i)) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Squared-Exponential-Functions:-The-Gaussian-Bell-Curves","page":"Feature Functions In Bayesian Regression","title":"Squared Exponential Functions: The Gaussian Bell Curves","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's explore another one of the most elegant and widely-used basis functions in machine learning - the squared exponential, also known as the Gaussian or radial basis function. These functions create perfect bell curves that smoothly decay in all directions from their centers.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The mathematical form reveals their graceful symmetry:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = e^-(x - i)^2\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"These functions have remarkable properties - they're infinitely differentiable and create ultra-smooth interpolations between points. Their rapid decay also provides natural localization, making them excellent choices for capturing both local and global patterns in data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"sqrt exps\",\n    datasets = datasets,\n    ϕs       = [ (x) -> exp(-(x - i) ^ 2) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Sigmoid-Functions:-The-Neural-Network's-Activation","page":"Feature Functions In Bayesian Regression","title":"Sigmoid Functions: The Neural Network's Activation","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The sigmoid function, a cornerstone of neural network architectures, offers another fascinating basis for our exploration. This S-shaped curve elegantly transitions between two asymptotic values, creating a smooth, differentiable \"step\" that's invaluable in modeling transitions and decision boundaries.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The mathematical elegance of the sigmoid reveals itself in its formula:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = frac11 + e^-3(x - 1)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This function's graceful transition from 0 to 1 makes it particularly well-suited for capturing threshold phenomena and modeling probability-like quantities. Its bounded nature also provides natural regularization, preventing the explosive growth that can plague polynomial bases.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"sigmoids\",\n    datasets = datasets,\n    ϕs       = [ (x) -> 1 / (1 + exp(-3 * (x - i))) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#The-Power-of-Combination:-Using-Different-Classes-of-Basis-Functions-Together","page":"Feature Functions In Bayesian Regression","title":"The Power of Combination: Using Different Classes of Basis Functions Together","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"What if we could harness the unique strengths of different basis functions we've explored? By combining polynomials, trigonometric functions, squared exponentials, and sigmoids, we can create an incredibly flexible and expressive basis that captures both global trends and local patterns. The polynomials can handle overall growth patterns, trigonometric functions can capture periodic behavior, squared exponentials can provide smooth local interpolation, and sigmoids can model sharp transitions. This combined approach leverages the best of each basis function family, potentially leading to more robust and accurate predictions. And here how easy it is to do so!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"# Combine all basis functions we've explored into one powerful basis\ncombined_basis = vcat(\n    # Polynomials (from first example)\n    [ (x) -> x ^ i for i in 0:5 ],\n    \n    # Trigonometric functions (from second example) \n    [ (x) -> sin(i*x) for i in 1:3 ],\n    [ (x) -> cos(i*x) for i in 1:3 ],\n    \n    # Squared exponentials (from seventh example)\n    [ (x) -> exp(-(x - i)^2) for i in -8:8 ],\n    \n    # Sigmoids (from eighth example)\n    [ (x) -> 1 / (1 + exp(-3 * (x - i))) for i in -8:8 ]\n)\n\nplot_inference_results_for(\n    title    = \"combined\",\n    datasets = datasets,\n    ϕs       = combined_basis, \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Now that we've combined these different basis functions, it's interesting to explore how this powerful ensemble performs on our complete dataset. By visualizing the posterior distribution over functions induced by this combined basis, we can see how it leverages the unique characteristics of each basis type - the global trends captured by polynomials, the periodic patterns from trigonometric functions, the local smoothness from squared exponentials, and the sharp transitions enabled by sigmoids. Let's plot the results to see this rich expressiveness in action.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"combined_basis_ωs_all_data = infer_ω(ϕs = combined_basis, x = X, y = Y)\n\n# Left plot - local region\np1 = plot(\n    title = \"Local region\",\n    xlabel = \"x\",\n    ylabel = \"y\",\n    xlim = (-10, 10),\n    ylim = (-20, 20),\n    grid = true\n)\n\n# Plot posterior mean\nplot_ϕ!(p1, combined_basis, mean(combined_basis_ωs_all_data),\n    rl = -10,\n    rr = 10,\n    linewidth = 3,\n    color     = :green,\n    labels    = \"Posterior mean\"\n)\n\n# Plot posterior samples (in gray)\nplot_ϕ!(p1, combined_basis, rand(combined_basis_ωs_all_data, 50),\n    rl = -10,\n    rr = 10,\n    linewidth = 1,\n    color     = :gray,\n    alpha     = 0.4,\n    labels    = nothing\n)\n\n# Plot data points\nscatter!(p1, X, Y,\n    yerror     = Λ,\n    label      = \"Data\",\n    color      = :royalblue,\n    markersize = 4\n)\n\n# Right plot - bigger region\np2 = plot(\n    title = \"Extended region\",\n    xlabel = \"x\",\n    ylabel = \"y\",\n    xlim = (-30, 30),\n    ylim = (-75, 75),\n    grid = true\n)\n\n# Plot posterior mean\nplot_ϕ!(p2, combined_basis, mean(combined_basis_ωs_all_data),\n    rl = -30,\n    rr = 30,\n    linewidth = 3,\n    color     = :green,\n    labels    = \"Posterior mean\"\n)\n\n# Plot posterior samples (in gray)\nplot_ϕ!(p2, combined_basis, rand(combined_basis_ωs_all_data, 50),\n    rl = -30,\n    rr = 30,\n    linewidth = 1,\n    color     = :gray,\n    alpha     = 0.4,\n    labels    = nothing\n)\n\n# Plot data points\nscatter!(p2, X, Y,\n    label      = \"Data\", \n    color      = :royalblue,\n    markersize = 2\n)\n\np = plot(p1, p2, layout=(1,2), size=(1000,400), fontfamily = \"Computer Modern\")","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The plot above beautifully demonstrates the expressive power of combining multiple basis functions. The posterior mean (shown in green) captures both the global trend and local variations in the data with remarkable accuracy. The gray lines, representing samples from the posterior distribution, illustrate the model's uncertainty - tighter in regions with more data points and wider in sparse regions. This combined basis approach leverages the strengths of each basis type: polynomials handle the overall trend, trigonometric functions capture periodic components, and localized basis functions manage fine details. The result is a flexible and robust model that adapts well to the complex patterns in our dataset.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Performance:-The-Need-for-Speed!","page":"Feature Functions In Bayesian Regression","title":"Performance: The Need for Speed! 🏎️","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Alright, we've been having a blast playing with different basis functions, and RxInfer has been crunching those posterior calculations faster than you can say \"Bayesian inference\". But just how zippy is it really? Let's put our mathematical hot rod through its paces with our trusty polynomial basis functions and see what kind of speed records we can break! 🏁","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"using BenchmarkTools","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"In Julia, benchmarking is made easy with the BenchmarkTools package. The @benchmark macro runs the given expression multiple times to get statistically meaningful results. It provides detailed statistics about execution time, memory allocations, and garbage collection overhead. The output shows minimum, maximum, median and mean execution times, along with a nice histogram visualization of the timing distribution.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"@benchmark infer_ω(ϕs = $([ (x) -> x ^ i for i in 0:5 ]), x = $(datasets[1][:x_train]), y = $(datasets[1][:y_train]))","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n Range (min … max):  197.659 μs …  1.582 ms  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     228.667 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   231.523 μs ± 25.491 μs  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n                ▁▃▅█▇▆▅▃▁                                       \n  ▁▃▅▇▇██▇▅▄▄▄▅▇██████████▇▆▆▆▇▆▇▆▅▅▄▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁ ▃\n  198 μs          Histogram: frequency by time          294 μs <\n\n Memory estimate: 86.95 KiB, allocs estimate: 1507.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's benchmark inference on a larger dataset with 10,000 datapoints to test scalability.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"N_benchmark = 10_000\nX_benchmark = range(-8, 8, length=N_benchmark)\nY_benchmark = rand(rng, MvNormalMeanCovariance(f.(X_benchmark), Λ));\n\n@benchmark infer_ω(ϕs = $([ (x) -> x ^ i for i in 0:5 ]), x = $(X_benchmark), y = $(Y_benchmark))","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"BenchmarkTools.Trial: 9 samples with 1 evaluation per sample.\n Range (min … max):  601.409 ms … 609.675 ms  ┊ GC (min … max): 0.00% … 0.0\n0%\n Time  (median):     602.045 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   603.209 ms ±   2.594 ms  ┊ GC (mean ± σ):  0.00% ± 0.0\n0%\n\n  █████       ██      █                                       █  \n  █████▁▁▁▁▁▁▁██▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  601 ms           Histogram: frequency by time          610 ms <\n\n Memory estimate: 1.15 MiB, allocs estimate: 1511.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"And that's a wrap! From exploring different basis functions (polynomials, trigonometric functions, and even those fancy sigmoids) to performing lightning-fast Bayesian inference, we've seen how RxInfer handles parametric Gaussian regression with style. The benchmarks don't lie - processing 10,000 datapoints in a blast while keeping memory usage lean? That's not just fast, that's \"blink and you'll miss it\" fast! ","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Throughout this notebook, we've gone from basic data generation to sophisticated model inference, all while keeping things both mathematically rigorous and computationally efficient. Whether you're a Bayesian enthusiast or just someone who appreciates elegant mathematical machinery, this journey through parametric Gaussian regression shows that probabilistic programming doesn't have to be slow or memory-hungry.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Thanks again to the authors of \"Probabilistic Numerics: Computation as Machine Learning\" for providing the theoretical foundations and inspiration for this notebook!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/feature_functions_in_bayesian_regression/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [a93c6f00] DataFrames v1.8.0\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Chance-Constrained-Active-Inference","page":"Chance Constraints","title":"Chance-Constrained Active Inference","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"This notebook applies reactive message passing for active inference in the context of chance-constraints. The implementation is based on (van de Laar et al., 2021, \"Chance-constrained active inference\") and discussion with John Boik.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"We consider a 1-D agent that tries to elevate itself above ground level. Instead of a goal prior, we impose a chance constraint on future states, such that the agent prefers to avoid the ground with a preset probability (chance) level. ","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"using Plots, Distributions, StatsFuns, RxInfer","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Chance-Constraint-Node-Definition","page":"Chance Constraints","title":"Chance-Constraint Node Definition","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"A chance-constraint is meant to constraint a marginal distribution to abide by certain properties. In this case, a (posterior) probability distribution should not \"overflow\" a given region by more than a certain probability mass. This constraint then affects adjacent beliefs and ultimately the controls to (hopefully) account for the imposed constraint.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"In order to enforce this constraint on a marginal distribution, an auxiliary chance-constraint node is included in the graphical model. This node then sends messages that enforce the marginal to abide by the preset conditions. In other words, the (chance) constraint on the (posterior) marginal, is converted to a prior constraint on the generative model that sends an adaptive message. We start by defining this chance-constraint node and its message.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"struct ChanceConstraint end  \n\n# Node definition with safe region limits (lo, hi), overflow chance epsilon and tolerance atol\n@node ChanceConstraint Stochastic [out, lo, hi, epsilon, atol]","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"# Function to compute normalizing constant and central moments of a truncated Gaussian distribution\nfunction truncatedGaussianMoments(m::Float64, V::Float64, a::Float64, b::Float64)\n    V = clamp(V, tiny, huge)\n    StdG = Distributions.Normal(m, sqrt(V))\n    TrG = Distributions.Truncated(StdG, a, b)\n    \n    Z = Distributions.cdf(StdG, b) - Distributions.cdf(StdG, a)  # safe mass for standard Gaussian\n    \n    if Z < tiny\n        # Invalid region; return undefined mean and variance of truncated distribution\n        Z    = 0.0\n        m_tr = 0.0\n        V_tr = 0.0\n    else\n        m_tr = Distributions.mean(TrG)\n        V_tr = Distributions.var(TrG)\n    end\n    \n    return (Z, m_tr, V_tr)\nend;","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"@rule ChanceConstraint(:out, Marginalisation) (\n    m_out::UnivariateNormalDistributionsFamily, # Require inbound message\n    q_lo::PointMass, \n    q_hi::PointMass, \n    q_epsilon::PointMass, \n    q_atol::PointMass) = begin \n\n    # Extract parameters\n    lo = mean(q_lo)\n    hi = mean(q_hi)\n    epsilon = mean(q_epsilon)\n    atol = mean(q_atol)\n    \n    (m_bw, V_bw) = mean_var(m_out)\n    (xi_bw, W_bw) = (m_bw, 1. /V_bw)  # check division by  zero\n    (m_tilde, V_tilde) = (m_bw, V_bw)\n    \n    # Compute statistics (and normalizing constant) of q in safe region G\n    # Phi_G is called the \"safe mass\" \n    (Phi_G, m_G, V_G) = truncatedGaussianMoments(m_bw, V_bw, lo, hi)\n\n    xi_fw = xi_bw\n    W_fw  = W_bw\n    if epsilon <= 1.0 - Phi_G # If constraint is active\n        # Initialize statistics of uncorrected belief\n        m_tilde = m_bw\n        V_tilde = V_bw\n        for i = 1:100 # Iterate at most this many times\n            (Phi_lG, m_lG, V_lG) = truncatedGaussianMoments(m_tilde, V_tilde, -Inf, lo) # Statistics for q in region left of G\n            (Phi_rG, m_rG, V_rG) = truncatedGaussianMoments(m_tilde, V_tilde, hi, Inf) # Statistics for q in region right of G\n\n            # Compute moments of non-G region as a mixture of left and right truncations\n            Phi_nG = Phi_lG + Phi_rG\n            m_nG = Phi_lG / Phi_nG * m_lG + Phi_rG / Phi_nG * m_rG\n            V_nG = Phi_lG / Phi_nG * (V_lG + m_lG^2) + Phi_rG/Phi_nG * (V_rG + m_rG^2) - m_nG^2\n\n            # Compute moments of corrected belief as a mixture of G and non-G regions\n            m_tilde = (1.0 - epsilon) * m_G + epsilon * m_nG\n            V_tilde = (1.0 - epsilon) * (V_G + m_G^2) + epsilon * (V_nG + m_nG^2) - m_tilde^2\n            # Re-compute statistics (and normalizing constant) of corrected belief\n            (Phi_G, m_G, V_G) = truncatedGaussianMoments(m_tilde, V_tilde, lo, hi)\n            if (1.0 - Phi_G) < (1.0 + atol)*epsilon\n                break # Break the loop if the belief is sufficiently corrected\n            end\n        end\n        \n        # Convert moments of corrected belief to canonical form\n        W_tilde = inv(V_tilde)\n        xi_tilde = W_tilde * m_tilde\n\n        # Compute canonical parameters of forward message\n        xi_fw = xi_tilde - xi_bw\n        W_fw  = W_tilde - W_bw\n    end\n\n    return NormalWeightedMeanPrecision(xi_fw, W_fw)\nend","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Definition-of-the-Environment","page":"Chance Constraints","title":"Definition of the Environment","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"We consider an environment where the agent has an elevation level, and where the agent directly controls its vertical velocity. After some time, an unexpected and sudden gust of wind tries to push the agent to the ground.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"wind(t::Int64) = -0.1*(60 <= t < 100) # Time-dependent wind profile\n\nfunction initializeWorld()\n    x_0 = 0.0 # Initial elevation\n    \n    x_t_last = x_0\n    function execute(t::Int64, a_t::Float64)\n        x_t = x_t_last + a_t + wind(t) # Update elevation\n    \n        x_t_last = x_t # Reset state\n                \n        return x_t\n    end\n\n    x_t = x_0 # Predefine outcome variable\n    observe() = x_t # State is fully observed\n\n    return (execute, observe)\nend;","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Generative-Model-for-Regulator","page":"Chance Constraints","title":"Generative Model for Regulator","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"We consider a fully observed Markov decision process, where the agent directly observes the true state (elevation) of the world. In this case we only need to define a chance-constrained generative model of future states. Inference for controls on this model then derives our controller.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"# m_u ::Vector{Float64}, ,   Control prior means\n# v_u = datavar(Float64, T)  Control prior variances\n# x_t ::Float64              Fully observed state\n\n@model function regulator_model(T, m_u, v_u, x_t, lo, hi, epsilon, atol)\n    \n    # Loop over horizon\n    x_k_last = x_t\n    for k = 1:T\n        u[k] ~ NormalMeanVariance(m_u[k], v_u[k]) # Control prior\n        x[k] ~ x_k_last + u[k] # Transition model\n        x[k] ~ ChanceConstraint(lo, hi, epsilon, atol) where { # Simultaneous constraint on state\n            dependencies = RequireMessageFunctionalDependencies(out = NormalWeightedMeanPrecision(0, 0.01))} # Predefine inbound message to break circular dependency\n        x_k_last = x[k]\n    end\n \nend","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Reactive-Agent-Definition","page":"Chance Constraints","title":"Reactive Agent Definition","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"function initializeAgent()\n    # Set control prior statistics\n    m_u = zeros(T)\n    v_u = lambda^(-1)*ones(T)\n    \n    function compute(x_t::Float64)\n        model_t = regulator_model(;T=T, lo=lo, hi=hi, epsilon=epsilon, atol=atol)\n        data_t = (m_u = m_u, v_u = v_u, x_t = x_t)\n\n        result = infer(\n            model = model_t,\n            data = data_t,\n            iterations = n_its)\n\n        # Extract policy from inference results\n        pol = mode.(result.posteriors[:u][end])\n\n        return pol\n    end\n\n    pol = zeros(T) # Predefine policy variable\n    act() = pol[1]\n\n    return (compute, act)\nend;","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Action-Perception-Cycle","page":"Chance Constraints","title":"Action-Perception Cycle","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"Next we define and execute the action-perception cycle. Because the state is fully observed, these is no slide (estimator) step in the cycle. ","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"# Simulation parameters\nN = 160 # Total simulation time\nT = 1 # Lookahead time horizon\nlambda = 1.0 # Control prior precision\nlo = 1.0 # Chance region lower bound\nhi = Inf # Chance region upper bound\nepsilon = 0.01 # Allowed chance violation\natol = 0.01 # Convergence tolerance for chance constraints\nn_its = 10;  # Number of inference iterations","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"(execute, observe) = initializeWorld() # Let there be a world\n(compute, act) = initializeAgent() # Let there be an agent\n\na = Vector{Float64}(undef, N) # Actions\nx = Vector{Float64}(undef, N) # States\nfor t = 1:N\n    a[t] = act()\n           execute(t, a[t])\n    x[t] = observe()\n           compute(x[t])\nend","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Results","page":"Chance Constraints","title":"Results","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"Results show that the agent does not allow the wind to push it all the way to the ground.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"p1 = plot(1:N, wind.(1:N), color=\"blue\", label=\"Wind\", ylabel=\"Velocity\", lw=2)\nplot!(p1, 1:N, a, color=\"red\", label=\"Control\", lw=2)\np2 = plot(1:N, x, color=\"black\", lw=2, label=\"Agent\", ylabel=\"Elevation\")\nplot(p1, p2, layout=(2,1))","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/chance_constraints/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [4c63d2b9] StatsFuns v1.5.0\n","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Large-Language-Models","page":"Large Language Models","title":"Large Language Models","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/#Warning:-Maximum-Unprincipledness-Ahead","page":"Large Language Models","title":"⚠️ Warning: Maximum Unprincipledness Ahead ⚠️","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/#The-speed-of-RxInfer-will-be-bottlenecked-by-the-speed-of-the-LLM-calls","page":"Large Language Models","title":"⚠️ The speed of RxInfer will be bottlenecked by the speed of the LLM calls ⚠️","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"using RxInfer, OpenAI, JSON","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"# Your OpenAI API key should be set in the environment variables\nsecret_key = ENV[\"OPENAI_KEY\"];\nllm_model = \"gpt-4o-mini-2024-07-18\";","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Disclaimer: This is probably one of the most unprincipled notebooks in the RxInferExamples repository. We're about to hook Large Language Models directly into Bayesian inference using nothing but good vibes and questionable life choices. If you're looking for rigorous mathematical foundations, you might want to slowly back away from this notebook.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"That said, if you've ever wondered \"what happens if I treat LLMs' outputs like a probability distribution?\", you've come to the right place. We'll start with a gentle introduction to RxInfer for two reasons: first, we need to justify this madness of integrating LLMs with probabilistic models, and second, we want to explain what RxInfer is to newcomers in a single notebook with as few external references as possible. Think of this as \"Bayesian Inference for People Who Just Want to See the Cool Stuff.\"","category":"page"},{"location":"categories/experimental_examples/large_language_models/#What-is-RxInfer?-(The-Gentle-Introduction)","page":"Large Language Models","title":"What is RxInfer? (The Gentle Introduction)","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Before we commit crimes against Bayesian inference (by the way we use the word inference to mean \"Bayesian inference\", not the forward pass of a neural network), let's understand what RxInfer actually is and why it accidentally makes LLM integration possible.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"RxInfer is a Julia package for Bayesian inference that takes a rather unusual approach: instead of treating your probabilistic model as one big mathematical beast that needs to be slayed with MCMC or variational inference, it breaks everything down into small, local conversations between probability distributions.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Imagine your probabilistic model as a social network where probability distributions are people, and they're all gossiping about what they think the true parameters might be. RxInfer organizes this gossip into an efficient message-passing protocol on something called a factor graph.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Factor-Graphs:-The-Social-Network-of-Probability","page":"Large Language Models","title":"Factor Graphs: The Social Network of Probability","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"A factor graph is just a visual way to represent how different parts of your probabilistic model talk to each other. Think of it like this:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Round nodes (variables): These represent the things you want to learn about\nSquare nodes (factors): These represent relationships or constraints between variables  \nEdges: These are the communication channels where probability distributions flow as \"messages\"","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"For example, if you want to model coin flips:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"graph LR\n    %% Variable nodes (circles)\n    theta[\"θ<br/>(coin fairness)\"]\n    x1[\"x₁<br/>(flip 1)\"]\n    x2[\"x₂<br/>(flip 2)\"]\n    x3[\"x₃<br/>(flip 3)\"]\n    \n    %% Factor nodes (squares)\n    prior[\"Beta<br/>Prior\"]\n    flip1[\"Bernoulli<br/>Flip\"]\n    flip2[\"Bernoulli<br/>Flip\"] \n    flip3[\"Bernoulli<br/>Flip\"]\n    \n    %% Connections\n    prior --- theta\n    theta --- flip1\n    theta --- flip2\n    theta --- flip3\n    flip1 --- x1\n    flip2 --- x2\n    flip3 --- x3\n    \n    %% Styling\n    classDef variable fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef factor fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    \n    class theta,x1,x2,x3 variable\n    class prior,flip1,flip2,flip3 factor","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"In this graph:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Blue squares are variables (things we want to learn about)\nOrange squares are factors (probability distributions or relationships)\nLines are the communication channels where messages flow back and forth","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The magic happens when messages flow along these edges, updating beliefs as new information comes in.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#The-Philosophy:-Local-Conversations,-Global-Intelligence","page":"Large Language Models","title":"The Philosophy: Local Conversations, Global Intelligence","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Here's what makes RxInfer special: instead of solving your entire probabilistic model in one giant computation, it breaks everything down into local conversations. ","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Each node only needs to:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Listen to messages from its neighbors\nUpdate its local beliefs\nSend updated messages to its neighbors","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Repeat this process, and eventually the entire network converges to the exact or approximate posterior distributions. It's like crowd-sourced intelligence, but with math.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"This approach has some nice properties:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Modular: You can swap out parts of your model without affecting others\nParallel: Different parts can update simultaneously  \nInterpretable: You can inspect what each part of your model \"thinks\"\nExtensible: You can add new types of nodes... like LLMs 👀","category":"page"},{"location":"categories/experimental_examples/large_language_models/#The-Beautiful-Accident:-RxInfer-Doesn't-Care-What-You-Pass","page":"Large Language Models","title":"The Beautiful Accident: RxInfer Doesn't Care What You Pass","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Here's where things get interesting (and unprincipled). ","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"RxInfer was designed for passing probability distributions along those edges. But here's the thing: the framework doesn't actually care what you pass. It just needs to know how to:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Compute outgoing messages from incoming ones\nUpdate local beliefs (marginals)\nCalculate free energy contributions (we will skip this for now)","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"As long as you can define these two (three) things, you can plug in literally anything as a node. A matrix multiplication? Sure. A neural network? Why not (see examples within the repository). A Large Language Model? Hold my coffee...","category":"page"},{"location":"categories/experimental_examples/large_language_models/#The-Moment-of-Questionable-Judgment","page":"Large Language Models","title":"The Moment of Questionable Judgment","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"So here we were, understanding that RxInfer is basically a message-passing system that doesn't care what you pass, when someone (probably me) had a thought:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"\"What if we just... asked ChatGPT to be a probability distribution?\"","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Now, any reasonable person would immediately recognize this as a terrible idea. Probability distributions have well-defined mathematical properties. They integrate to 1. They have moments. They follow laws. Jaynes would be rolling in his grave.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Large Language Models, on the other hand, are... vibes-based. They generate text that sounds plausible. They hallucinate. They change their mind if you ask the same question twice.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"But here's the thing about terrible ideas: sometimes they work.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#The-Great-LLM-Bayesian-Integration-Experiment","page":"Large Language Models","title":"The Great LLM-Bayesian Integration Experiment","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The plan was simple (and deeply unscientific):","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Create LLM nodes that can participate in message passing\nTeach LLMs to speak probability through prompting\nLet them gossip with real probability distributions and see what happens\nHope nothing catches fire","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Surprisingly, steps 1-3 worked. Step 4 is still ongoing.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#The-Problem:-Can-We-Cluster-Text-Using-Bayesian-Inference?","page":"Large Language Models","title":"The Problem: Can We Cluster Text Using Bayesian Inference?","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Before we dive into the implementation, let's define a concrete problem that will motivate our LLM integration. We want to:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Cluster text snippets by sentiment, but with proper uncertainty quantification.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Here's our dataset - 5 text snippets about RxInfer.jl:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"observations = [\n    \"RxInfer.jl is confusing and frustrating to use. I wouldn't recommend it.\",\n    \"RxInfer.jl made my Bayesian modeling workflow much easier and more efficient!\",\n    \"Absolutely love RxInfer.jl! It's revolutionized my approach to probabilistic programming.\",\n    \"I gave RxInfer.jl a try, but it just doesn't work for my needs at all.\",\n    \"I prefer apples over oranges.\"  # 🍎 Wait, this one's different...\n];","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The challenges:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Two sentiment clusters: Positive and negative opinions about RxInfer.jl\nUnrelated text: The last one isn't about RxInfer.jl at all\nUncertainty: We want to know how confident we are about each classification","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Traditional clustering would give us hard assignments. We want probabilistic clustering with uncertainty.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#The-Model:-Mixing-Traditional-Bayesian-with-LLM-Magic","page":"Large Language Models","title":"The Model: Mixing Traditional Bayesian with LLM Magic","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Here's our probabilistic model for sentiment clustering (this will be the final model we will use):","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"@model function language_mixture_model(c, context₁, context₂, task₁, task₂, likelihood_task)\n    # Mixture probability (how much of each sentiment type)\n    s ~ Beta(1.0, 1.0)\n    \n    # Two sentiment clusters with LLM-generated priors\n    m[1] ~ LLMPrior(context₁, task₁)  # Negative sentiment prior\n    w[1] ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    m[2] ~ LLMPrior(context₂, task₂)  # Positive sentiment prior  \n    w[2] ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    for i in eachindex(c)\n        z[i] ~ Bernoulli(s)  # Cluster assignment (0=negative, 1=positive)\n        y[i] ~ NormalMixture(switch = z[i], m = m, p = w)  # Latent sentiment score\n        c[i] ~ LLMObservation(y[i], likelihood_task)  # Observed text\n    end\nend","category":"page"},{"location":"categories/experimental_examples/large_language_models/#What-This-Model-Does","page":"Large Language Models","title":"What This Model Does","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Let's break this down:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"s ~ Beta(1,1): Overall mixture proportion (how much positive vs negative sentiment in our dataset)\nm[1] ~ LLMPrior(context₁, task₁): \nAsk an LLM: \"Given that RxInfer.jl is terrible, what satisfaction score distribution would you expect?\"\nLLM response becomes our prior for negative sentiment\nm[2] ~ LLMPrior(context₂, task₂): \nAsk an LLM: \"Given that RxInfer.jl is great, what satisfaction score distribution would you expect?\"\nLLM response becomes our prior for positive sentiment\nz[i] ~ Bernoulli(s): Each text snippet gets assigned to positive or negative cluster\ny[i] ~ NormalMixture(...): Each snippet has a latent \"satisfaction score\" based on its cluster\nc[i] ~ LLMObservation(y[i], likelihood_task): \nAsk an LLM: \"What sentiment score would generate this text?\"\nThis connects our observed text to the latent satisfaction scores","category":"page"},{"location":"categories/experimental_examples/large_language_models/#The-Insight:-LLMs-as-Probabilistic-Components","page":"Large Language Models","title":"The Insight: LLMs as Probabilistic Components","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The brilliant (and possibly insane) insight is that we're using LLMs as:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"LLMPrior: A way to generate informed priors based on contextual knowledge\nLLMObservation: A likelihood function that connects text to latent numerical variables","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"This means the LLMs aren't just doing classification - they're participating in full Bayesian inference!","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Creating-the-LLM-Nodes","page":"Large Language Models","title":"Creating the LLM Nodes","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Now that we understand why we need these nodes, let's see how to build them. Creating a custom node in RxInfer requires 4 steps (but we will skip the last two):","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Create the node structure using the @node macro\nDefine message passing rules with the @rule macro  \nSpecify marginal computations with the @marginalrule macro (skipped)\nImplement free energy computation with the @average_energy macro (skipped)","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The beauty is in the message passing protocol. Each node only needs to know how to:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Process incoming messages from neighbors\nSend outgoing messages to neighbors  \nMaintain local beliefs","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Let's look at the actual implementation:","category":"page"},{"location":"categories/experimental_examples/large_language_models/#LLMPrior-Node","page":"Large Language Models","title":"LLMPrior Node","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"First, the node definition:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"\"\"\"\n    LLMPrior\n\nNode that represents an LLM's prior beliefs about latent variables based on contextual information.\nThe LLM interprets the context and task to produce a probability distribution as a prior.\n\n# Interfaces\n- `belief` (b): Output distribution representing the LLM's prior belief\n- `context` (c): Input text providing context for the prior\n- `task` (t): Input text describing what distribution to generate\n\"\"\"\nstruct LLMPrior end\n\n@node LLMPrior Stochastic [ (b, aliases = [belief]), (c, aliases = [context]), (t, aliases = [task]) ]","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Now here's the actual message passing rule that does the magic. This is going to be a forward rule that will provide a prior for the sentiment of the text. We understand that the syntax for the rule is a bit weird, so we refer the curious reader to the documentation.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"@rule LLMPrior(:b, Marginalisation) (q_c::PointMass{<:String}, q_t::PointMass{<:String}) = begin\n    # Build the conversation with the LLM\n    messages = [\n        Dict(\"role\" => \"system\",\n             \"content\" => \"\"\"\n                 You are an expert analyst who maps contextual cues to a\n                 Normal(mean, variance) distribution.\n\n                 • Think step-by-step internally.\n                 • **Only** output a JSON object that conforms to the schema below.\n                 • Do not wrap the JSON in markdown fences or add extra keys.\n             \"\"\"),\n\n        Dict(\"role\" => \"assistant\",\n             \"content\" => \"\"\"\n                 ## CONTEXT\n                 $(q_c.point)\n             \"\"\"),\n\n        Dict(\"role\" => \"user\",\n             \"content\" => \"\"\"\n                 ## TASK\n                 $(q_t.point)\n\n                 Using the context above, infer a Normal distribution and return:\n                   \"analysis\"  – brief rationale (≤ 100 words)\n                   \"mean\"      – number in [0, 10]\n                   \"variance\"  – number in [1, 100]\n             \"\"\")\n    ]\n\n    # Define strict JSON schema for consistent responses\n    response_schema = Dict(\n        \"type\" => \"json_schema\",\n        \"json_schema\" => Dict(\n            \"name\"   => \"normal_estimate\",\n            \"schema\" => Dict(\n                \"type\"       => \"object\",\n                \"properties\" => Dict(\n                    \"analysis\" => Dict(\"type\" => \"string\"),\n                    \"mean\"     => Dict(\"type\" => \"number\", \"minimum\" => 0, \"maximum\" => 10),\n                    \"variance\" => Dict(\"type\" => \"number\", \"minimum\" => 1, \"maximum\" => 100)\n                ),\n                \"required\" => [\"analysis\", \"mean\", \"variance\"],\n                \"additionalProperties\" => false\n            )\n        )\n    )\n\n    # Call the LLM and parse the response\n    r = create_chat(secret_key, llm_model, messages; response_format = response_schema)\n    obj = JSON.parse(r.response[:choices][1][:message][:content])\n\n    return NormalMeanVariance(obj[\"mean\"], obj[\"variance\"])\nend","category":"page"},{"location":"categories/experimental_examples/large_language_models/#LLMObservation-Node","page":"Large Language Models","title":"LLMObservation Node","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The node definition:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"\"\"\"\n    LLMObservation\n\nNode that represents an LLM's observation of data based on a latent belief and task description.\nThe LLM takes a latent belief and task description to produce corresponding observed data.\n\n# Interfaces\n- `out`: Output observation data generated by the LLM\n- `belief` (b): Input latent variable/distribution that influences the observation\n- `task` (t): Input text describing how to generate observations from beliefs\n\"\"\"\nstruct LLMObservation end\n\n@node LLMObservation Stochastic [ out, (b, aliases = [belief]), (t, aliases = [task]) ]","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Now we need to define the rule. Normally, we would have to define the rules for each interface (edge) of the node, but here we will skip this part and define only a backward rule from observations to a belief.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"@rule LLMObservation(:b, Marginalisation) (q_out::PointMass{<:String}, q_t::PointMass{<:String}) = begin\n    messages = [\n        Dict(\"role\" => \"system\",\n             \"content\" => \"\"\"\n                 You are **LLMObservation**, a senior evaluator who maps a text to\n                 a Normal(mean, variance) distribution.\n\n                 • Think step-by-step internally, but **only** output a JSON object\n                   that conforms to the provided schema.\n                 • Do not wrap the JSON in markdown fences or add extra keys.\n             \"\"\"),\n\n        Dict(\"role\" => \"assistant\",\n             \"content\" => \"\"\"\n                 ## TEXT\n                 $(q_out.point)\n             \"\"\"),\n\n        Dict(\"role\" => \"user\",\n             \"content\" => \"\"\"\n                 ## TASK\n                 $(q_t.point)\n\n                 Using the text above, infer a Gaussian distribution.\n                 Return a JSON object with keys:\n                   \"analysis\"  – ≤ 100 words explaining your reasoning\n                   \"mean\"      – number in [0, 10]\n                   \"variance\"  – number in [0.1, 100]\n             \"\"\")\n    ]\n\n    response_schema = Dict(\n        \"type\" => \"json_schema\",\n        \"json_schema\" => Dict(\n            \"name\"   => \"normal_estimate\",\n            \"schema\" => Dict(\n                \"type\"       => \"object\",\n                \"properties\" => Dict(\n                    \"analysis\" => Dict(\"type\" => \"string\"),\n                    \"mean\"     => Dict(\"type\" => \"number\", \"minimum\" => 0, \"maximum\" => 10),\n                    \"variance\" => Dict(\"type\" => \"number\", \"minimum\" => 0.1, \"maximum\" => 100)\n                ),\n                \"required\" => [\"analysis\", \"mean\", \"variance\"],\n                \"additionalProperties\" => false\n            )\n        )\n    )\n\n    r = create_chat(secret_key, llm_model, messages; response_format = response_schema)\n    obj = JSON.parse(r.response[:choices][1][:message][:content])\n\n    return NormalMeanVariance(obj[\"mean\"], obj[\"variance\"])\nend","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"# Priors\ncontext₁ = \"RxInfer.jl is absolutely terrible.\"\ncontext₂ = \"RxInfer.jl is a great tool for Bayesian Inference.\"\n\nprior_task = \"\"\"\nProvide a distribution of the statement.\n- **Mean**: Most likely satisfaction score (0-10 scale)  \n- **Variance**: Uncertainty in your interpretation\n    - Low variance (2.0-4.0): Very clear sentiment\n    - Medium variance (4.1-6.0): Some ambiguity\n    - High variance (6.0-10.0): Unclear or mixed signals\n\"\"\"\n\n# Likelihood  \nlikelihood_task = \"\"\"\nEvaluation of sentiment about RxInfer.jl and provide satisfaction score distribution.\nIf expression is not related to RxInfer.jl, return distribution with mean 5 and high variance of 100.\n- **Mean**: Most likely satisfaction score (0-10 scale)\n- **Variance**: Uncertainty in interpretation  \n    - Low variance (0.1-1.0): Very clear sentiment, confident interpretation\n    - Medium variance (1.1-5.0): Some ambiguity in the text\n    - High variance (5.1-10.0): Unclear/mixed signals, or not related to RxInfer.jl\n\"\"\";","category":"page"},{"location":"categories/experimental_examples/large_language_models/#What-Happens-During-Inference","page":"Large Language Models","title":"What Happens During Inference","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"LLM Priors Generate Initial Beliefs:\nNegative context → Low satisfaction score (≈ Gaussians with mean some mean between 0 and 5 and (perhaps) high variance) \nPositive context → High satisfaction score (≈ Gaussians with mean some mean between 5 and 10 and (perhaps) high variance)\nLLM Observations Process Text:\n\"RxInfer.jl is confusing...\" → Low score, low uncertainty\n\"Absolutely love RxInfer.jl...\" → High score, low uncertainty  \n\"I prefer apples over oranges\" → Medium score, HIGH uncertainty (not related!)\nMessage Passing Updates Beliefs:\nTraditional Bayesian update rules combine LLM outputs\nCluster assignments emerge from the mixture model\nUncertainty propagates through the network\nFinal Result: Clean clustering with proper uncertainty quantification","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"# Some shennenigans to make inference work\nn_iterations = 5 # number of variational iterations to run\n\n# initial values for the variational distributions, we use uninformative distributions\n# If this looks weird to you, please refer to the documentation for the @initialization macro\ninit = @initialization begin\n    q(s) = vague(Beta)\n    q(m) = [NormalMeanVariance(0.0, 1e2), NormalMeanVariance(10.0, 1e2)]\n    q(y) = NormalMeanVariance(5.0, 1e2) # centered initialization with broad uncertainty\n    q(w) = [GammaShapeRate(0.01, 0.01), GammaShapeRate(0.01, 0.01)]\nend","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Initial state: \n  q(s) = Distributions.Beta{Float64}(α=1.0, β=1.0)\n  q(m) = ExponentialFamily.NormalMeanVariance{Float64}[ExponentialFamily.No\nrmalMeanVariance{Float64}(μ=0.0, v=100.0), ExponentialFamily.NormalMeanVari\nance{Float64}(μ=10.0, v=100.0)]\n  q(y) = ExponentialFamily.NormalMeanVariance{Float64}(μ=5.0, v=100.0)\n  q(w) = ExponentialFamily.GammaShapeRate{Float64}[ExponentialFamily.GammaS\nhapeRate{Float64}(a=0.01, b=0.01), ExponentialFamily.GammaShapeRate{Float64\n}(a=0.01, b=0.01)]","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"import ReactiveMP: rule_nm_switch_k, softmax!\n\n# Run Bayesian inference \n# Again, RxInfer is fast, LLMs are not, bare with inference\nresults_language = infer(\n    model=language_mixture_model(context₁=context₁, context₂=context₂, task₁=prior_task, task₂=prior_task, likelihood_task=likelihood_task),\n    constraints=MeanField(), # This is needed for the mixture node\n    data=(c=observations,),\n    initialization=init,\n    iterations=n_iterations,\n    free_energy=false,\n    showprogress=true\n)","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Inference results:\n  Posteriors       | available for (w, m, s, y, z)","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"using Plots\n\n# Create the animation object\nanimation = @animate for i in 1:n_iterations\n\n    # Get the data for visualization\n    initial_means = [0.0, 10.0]\n    initial_vars = [1e2, 1e2]\n    posterior_means = [mean.(results_language.posteriors[:m][i])...]\n    posterior_vars = inv.([mean.(results_language.posteriors[:w][i])...])\n\n    x = -10:0.01:20\n\n    plt = plot(\n        title=\"RxLLM: Sentiment Clustering\",\n        xlabel=\"Sentiment Spectrum\",\n        ylabel=\"Density\",\n        size=(800, 500),\n        dpi=300,\n        background_color=:white,\n        titlefontsize=14,\n        legendfontsize=11\n    )\n\n    # Plot posteriors with fill\n    plot!(plt, x, pdf.(Normal(posterior_means[1], sqrt(posterior_vars[1])), x),\n        fillalpha=0.4, fillrange=0, fillcolor=:red,\n        linewidth=3, linecolor=:darkred,\n        label=\"Negative Sentiment\")\n\n    plot!(plt, x, pdf.(Normal(posterior_means[2], sqrt(posterior_vars[2])), x),\n        fillalpha=0.4, fillrange=0, fillcolor=:blue,\n        linewidth=3, linecolor=:darkblue,\n        label=\"Positive Sentiment\")\n\n    # Plot priors as lighter background\n    plot!(plt, x, pdf.(Normal(initial_means[1], sqrt(initial_vars[1])), x),\n        linewidth=1, linestyle=:dash, linecolor=:gray, alpha=0.6,\n        label=\"Initial Prior\")\n\n    plot!(plt, x, pdf.(Normal(initial_means[2], sqrt(initial_vars[2])), x),\n        linewidth=1, linestyle=:dash, linecolor=:gray, alpha=0.6,\n        label=\"\")\n\n    # Simple cluster probabilities visualization\n    cluster_probs = probvec.(results_language.posteriors[:z][i])\n    plt2 = bar(1:length(cluster_probs), [p[1] for p in cluster_probs],\n        title=\"Positive Sentiment Probability\", ylabel=\"P(Positive)\", xlabel=\"Data Point\")\n\n    plot(plt, plt2)\n\nend\n\n# Now you can save the animation\ngif(animation, \"inference_process.gif\", fps=1, show_msg=false);","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"(Image: )","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The model successfully:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Clusters related text into positive/negative sentiment\nIdentifies unrelated text through high uncertainty\nQuantifies confidence in each assignment\nUpdates beliefs through proper Bayesian inference","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Most importantly, the LLMs aren't just doing text classification - they're participating in a full probabilistic reasoning process where their outputs are combined with traditional statistical models.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Why-This-Matters:-Beyond-Prompt-Chains","page":"Large Language Models","title":"Why This Matters: Beyond Prompt Chains","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"This approach opens up possibilities that go far beyond traditional LLM applications:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Uncertainty-Aware LLM Agents","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Instead of binary decisions, agents can maintain probability distributions over their beliefs and actions.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Rigorous Decision-Making Frameworks  ","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"LLM outputs become part of formal decision theory with some uncertainty quantification.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Compositional Reasoning","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Complex problems can be decomposed into smaller LLM nodes that communicate through message passing.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Continual Learning","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"As new data arrives, beliefs update through established Bayesian mechanisms rather than retraining.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Explainable AI","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The factor graph structure makes the reasoning process transparent and interpretable.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Lessons-Learned-and-Future-Directions","page":"Large Language Models","title":"Lessons Learned and Future Directions","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/#What-Worked-Well","page":"Large Language Models","title":"What Worked Well","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Natural integration: LLMs fit surprisingly well into message passing\nUncertainty handling: LLMs can express uncertainty when prompted correctly\nCompositionality: Multiple LLM nodes can work together in complex models","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Current-Limitations","page":"Large Language Models","title":"Current Limitations","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Prompt engineering: Requires prompt design for consistent distribution formats\nComputational cost: LLM queries are expensive compared to traditional operations\nReliability: LLM responses need robust parsing and error handling","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Future-Opportunities","page":"Large Language Models","title":"Future Opportunities","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Multimodal integration: Extend to vision/audio LLMs\nOnline learning: Update LLM beliefs through experience\nHierarchical models: Use LLMs at different abstraction levels\nMeta-learning: Learn better prompting strategies through inference","category":"page"},{"location":"categories/experimental_examples/large_language_models/#What's-Missing:-Current-Limitations","page":"Large Language Models","title":"What's Missing: Current Limitations","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"While our LLM-Bayesian integration works, this is very much a proof-of-concept with several important limitations that need to be addressed:","category":"page"},{"location":"categories/experimental_examples/large_language_models/#1.-Fixed-Functional-Forms","page":"Large Language Models","title":"1. Fixed Functional Forms","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Currently, our LLM nodes are hardcoded to output Normal distributions with specific parameter ranges. This isn't very flexible:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"# Current: Always returns Normal(mean, variance)\nreturn NormalMeanVariance(obj[\"mean\"], obj[\"variance\"])","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The issue: What if we want LLMs to output other distributions? Gamma? Beta? Categorical? Or even mixture distributions?","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Easy extension: The nodes should be generic and allow the user to specify the desired output distribution family through the task description.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#2.-Message-Products-Not-Addressed","page":"Large Language Models","title":"2. Message Products Not Addressed","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"In real factor graphs, you often need to combine multiple incoming messages before processing them. Our current implementation only handles single messages:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"# Current: Only handles one message at a time\n@rule LLMPrior(:b, Marginalisation) (q_c::PointMass{<:String}, q_t::PointMass{<:String})","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The issue: What happens when multiple messages arrive at an LLM node? How do we combine them before sending to the LLM?","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Missing: Rules for message products and handling multiple incoming probability distributions simultaneously.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#3.-The-Uncertainty-Quantification-Problem","page":"Large Language Models","title":"3. The Uncertainty Quantification Problem","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Perhaps the most philosophically questionable aspect of our approach is how we handle uncertainty. We're essentially asking LLMs:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"\"What do you think about your own confidence?\"","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"This is arguably unprincipled for several reasons:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Text-to-Text Uncertainty: When we prompt an LLM to express uncertainty about its own output, we're asking it to introspect about its own reasoning process. But LLMs don't actually have access to their internal uncertainty - they're just generating text that sounds like uncertainty based on their training.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"# This is basically what we're doing:\n\"I think this text expresses positive sentiment with variance 0.8\"\n# vs\n\"I think this text expresses positive sentiment with variance 2.5\"","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The LLM is pattern-matching to training examples where humans expressed different levels of confidence, but it's not performing genuine uncertainty quantification.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Log-Probability Limitations: An alternative approach might be to use the LLM's token log-probabilities as uncertainty proxies:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"# Instead of asking the LLM about uncertainty, use its output probabilities\ntoken_probs = model.logprobs(response)\nuncertainty = -sum(token_probs)  # Entropy-based uncertainty","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"But this is also problematic because:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Confidence ≠ Correctness: An LLM can be very confident (high probability) about completely wrong outputs\nSequence-level vs Token-level: High token probabilities don't necessarily mean the overall semantic content is reliable\nDistribution Mismatch: Token probabilities reflect linguistic patterns, not epistemic uncertainty about the underlying task\nTraining Artifacts: LLM confidence is heavily influenced by training data patterns rather than true knowledge uncertainty","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Why We Do It Anyway: Despite being unprincipled, this approach can be useful in the absence of other information. When you have no other source of uncertainty quantification, asking an LLM to express its confidence can provide a rough proxy that's better than no uncertainty at all.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"It's a bit like asking someone \"how sure are you?\" - not perfect, but often practically useful.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The Better Path: True uncertainty quantification would require:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Explicit modeling of different uncertainty sources (aleatoric vs epistemic)\nIntegration with proper Bayesian model uncertainty\nIntegration of subjective logic frameworks","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"But for a proof-of-concept showing LLMs can participate in message passing? Text-based uncertainty estimation gets the job done.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Grounding-Agentic-Systems-in-Bayesian-Reasoning","page":"Large Language Models","title":"Grounding Agentic Systems in Bayesian Reasoning","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Beyond fixing current limitations, we're thinking through something much more ambitious: simultaneous integration of agents and Bayesian models working together across trust networks.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The vision is agentic systems where:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"# Agent submodel with trust and capability\n@model function agent(capability, trust_prior, task)\n    trust ~ Beta(trust_prior...)\n    performance := capability * trust * exp(-task)\nend\n\n# Main ecosystem using nested models\n@model function networked_agent_ecosystem(tasks, trust_priors, capabilities)\n    for i in eachindex(tasks)\n        # GraphPPL interpolates performance for each agent\n        agent_perf[i] ~ agent(\n            capability = capabilities[i], \n            trust_prior = trust_priors[i],\n            task = tasks[i]\n        )\n    end\nend","category":"page"},{"location":"categories/experimental_examples/large_language_models/#The-Trust-Layer","page":"Large Language Models","title":"The Trust Layer","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Traditional agentic systems lack principled uncertainty about which agent to trust for what task. One can imagine a system where:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Trust becomes a probabilistic belief that updates through Bayesian mechanisms\nAgent capabilities are distributions over competency domains  \nTask allocation emerges from probabilistic reasoning about trust and capability\nCross-validation happens naturally through message passing between agents","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Grounded-Agentic-Reasoning","page":"Large Language Models","title":"Grounded Agentic Reasoning","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The most principled path forward is grounding LLMs in Bayesian reasoning. Instead of heuristic agent coordination, we get:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Principled uncertainty about agent outputs and capabilities\nTrust propagation through established probabilistic mechanisms  \nEmergent collaboration from agents reasoning about each other's uncertainty\nRobust coordination that degrades gracefully under failure","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"This isn't just multi-agent systems—it's probabilistic agent networks where trust, capability, and task execution all become part of one coherent Bayesian model.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The goal: agentic systems that can reason about their own reasoning, trust each other appropriately, and coordinate complex tasks through principled uncertainty quantification.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#Conclusion:-The-Weird-Idea-That-Worked","page":"Large Language Models","title":"Conclusion: The Weird Idea That Worked","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"What started as a lunch conversation about reactive systems turned into a working prototype that treats LLMs as first-class citizens in Bayesian inference.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"The key insight wasn't just technical—it was philosophical. Instead of trying to make LLMs more like traditional ML models, we asked: what if we make traditional Bayesian inference more like natural reasoning?","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"By hooking LLMs into RxInfer's message passing framework, we've created a bridge between LLMs and Bayesian inference.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"This opens a path toward agentic systems grounded in principled uncertainty—where trust networks, capability reasoning, and task coordination all emerge from coherent Bayesian models.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Sometimes the best discoveries happen when you stop overthinking and just try the crazy idea.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"And it all started with the realization that RxInfer doesn't care what you pass through those edges.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"As long as you can define the rules, you can pass anything and be reactive about it.","category":"page"},{"location":"categories/experimental_examples/large_language_models/#A-few-caveats-about-NormalMixture-node","page":"Large Language Models","title":"A few caveats about NormalMixture node","text":"","category":"section"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"There few known issues with NormalMixture node:","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Iterations matter: with VMP, more iterations can materially change posteriors; too many iterations can lead to overconfident or wrong clusters.\nMixture limitation: NormalMixture under meanfield tends to collapse to a single Normal on its output edge, losing multi‑modality and corrupting downstream beliefs.\nPrefer gating/mixtures: use a gating/expert setup (rSLDS‑style see rSLDS.jl) that propagates the full mixture (preserve z–y dependence) instead of moment‑matching to one Normal.\nIf keeping this model: increase iterations and use stronger priors/initialization; but for robust results, favor gating or explicit mixture propagation.","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/experimental_examples/large_language_models/Project.toml`\n⌅ [682c06a0] JSON v0.21.4\n  [e9f21f70] OpenAI v0.12.0\n  [91a5bcdd] Plots v1.41.1\n  [a194aa59] ReactiveMP v5.6.0\n  [86711068] RxInfer v4.6.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n","category":"page"},{"location":"categories/experimental_examples/large_language_models/","page":"Large Language Models","title":"Large Language Models","text":"","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Drone-Dynamics","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Note: These examples demonstrate the use of RxInfer for motion planning. The animations show the inferred trajectories from probabilistic inference, rather than simulated executions. For more realistic simulations, especially in the 3D drone example, the model would need to be extended with a reactive environment that responds to the drone's actions during plan execution. If you're interested in collaborating on a more realistic implementation, please open a discussion and let's work on it together!","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"using RxInfer, LinearAlgebra","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Defining-structures","page":"Drone Dynamics","title":"Defining structures","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    Environment(; gravitational_constant::Float64 = 9.81)\n\nThis structure contains the properties of the environment.\n\"\"\"\nBase.@kwdef struct Environment\n    gravitational_constant::Float64 = 9.81\nend\nget_gravity(env::Environment) = env.gravitational_constant","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_gravity (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    Drone(mass, inertia, radius, force_limit)\n\nThis structure contains the properties of the drone.\n\"\"\"\nBase.@kwdef struct Drone\n    mass::Float64\n    inertia::Float64\n    radius::Float64\n    force_limit::Float64\nend\nget_mass(drone::Drone) = drone.mass\nget_properties(drone::Drone) = (drone.mass, drone.inertia, drone.radius, drone.force_limit)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_properties (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    State(x, y, vx, vy, 𝜃, 𝜔)\n\nThis structure contains the state of the drone. It contains the position, velocity, and orientation of the drone.\n\"\"\"\nstruct State\n    x::Float64\n    y::Float64\n    vx::Float64\n    vy::Float64\n    𝜃::Float64\n    𝜔::Float64\nend\nget_state(state::State) = (state.x, state.y, state.vx, state.vy, state.𝜃, state.𝜔)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_state (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Model-specification","page":"Drone Dynamics","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    state_transition(state, actions, drone, environment, dt)\n\nThis function computes the next state of the drone given the current state, the actions, the drone properties and the environment properties.\n\"\"\"\nfunction state_transition(state, actions, drone::Drone, environment::Environment, dt)\n\n    # extract drone properties\n    m, I, r, limit  = get_properties(drone)\n\n    # extract environment properties\n    g = get_gravity(environment)\n\n    # extract feasible actions\n    Fl, Fr   = clamp.(actions, 0, limit)\n        \n    # extract state properties\n    x, y, vx, vy, θ, ω = state\n\n    # compute forces and torques\n    Fg = m * g\n    Fy = (Fl + Fr) * cos(θ) - Fg\n    Fx = (Fl + Fr) * sin(θ)\n    𝜏  = (Fl - Fr) * r\n\n    # compute movements\n    ax = Fx / m\n    ay = Fy / m\n    vx_new = vx + ax * dt\n#     vy_new = vx + ay * dt # old version\n    vy_new = vy + ay * dt   # new version\n    x_new  = x + vx * dt + ax * dt^2 / 2\n    y_new  = y + vy * dt + ay * dt^2 / 2\n        \n    # compute rotations\n    α = 𝜏 / I\n    ω_new = ω + α * dt\n    θ_new = θ + ω * dt + α * dt^2 / 2\n\t\n    return [x_new, y_new, vx_new, vy_new, θ_new, ω_new]\n\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Main.anonymous.state_transition","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"@model function drone_model(drone, environment, initial_state, goal, horizon, dt)\t\n\n\t# extract environment properties\n\tg = get_gravity(environment)\n\n\t# extract drone properties\n\tm = get_mass(drone)\n\n\t# initial state prior\n\ts[1] ~ MvNormal(mean = initial_state, covariance = 1e-5 * I)\n\n\tfor i in 1:horizon\n\n\t\t# prior on actions (mean compensates for gravity)\n\t\tu[i] ~ MvNormal(μ = [m * g / 2, m * g / 2], Σ = diageye(2))\n\n\t\t# state transition\n\t\ts[i + 1] ~ MvNormal(\n            μ = state_transition(s[i], u[i], drone, environment, dt), \n\t\t\tΣ = 1e-10 * I\n\t\t)\n\tend\n\t\n\ts[end] ~ MvNormal(mean = goal, covariance = 1e-5 * diageye(6))\n\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Probabilistic-inference","page":"Drone Dynamics","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"@meta function drone_meta()\n\n\t# approximate the state transition function using the Unscented transform\n\tstate_transition() -> Unscented()\n\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"drone_meta (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function move_to_target(drone::Drone, env::Environment, start::State, target, horizon, dt)\n\n    results = infer(\n        model = drone_model(\n            drone = drone, \n            environment = env,\n            horizon = horizon,\n            dt = dt\n        ),\n        data  = (\n            initial_state = collect(get_state(start)), \n            goal = [target[1], target[2], 0, 0, 0, 0],\n        ),\n        meta  = drone_meta(),\n        returnvars = (s = KeepLast(), u = KeepLast())\n    )\n\n    return results\n\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"move_to_target (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"drone = Drone(\n    mass = 1,\n    inertia = 1,\n    radius = 0.2,\n    force_limit = 15.0\n)\n\nenv = Environment()\n\nstart = State(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\ntarget = [-0.8, 0.6]\n\nresults = move_to_target(drone, env, start, target, 40, 0.05)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Inference results:\n  Posteriors       | available for (s, u)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Plotting","page":"Drone Dynamics","title":"Plotting","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"using Plots","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function plot_drone!(p, drone::Drone, state::State; color = :black)\n    x, y, x_a, y_a, θ, ω = get_state(state)\n    _, _, radius, _ = get_properties(drone)\n    dx = radius * cos(θ)\n    dy = radius * sin(θ)\n\n    drone_position = [ x ], [ y ]\n    drone_engines  = [ x - dx, x + dx ], [ y + dy, y - dy ]\n    drone_coordinates = [ x - dx, x, x + dx ], [ y + dy, y, y - dy ]\n\n    rotation_matrix = [ cos(-θ) -sin(-θ); sin(-θ) cos(-θ) ]\n    engine_shape = [ -1 0 1; 1 -1 1 ]\n    drone_shape  = [ -2 -2 2 2 ; -1 1 1 -1 ]\n    \n    engine_shape = rotation_matrix * engine_shape\n    drone_shape  = rotation_matrix * drone_shape\n    engine_marker = Shape(engine_shape[1, :], engine_shape[2, :])\n    drone_marker  = Shape(drone_shape[1, :], drone_shape[2, :])\n    \n    scatter!(p, drone_position[1], drone_position[2]; color = color, label = false, marker = drone_marker)\n    scatter!(p, drone_engines[1], drone_engines[2]; color = color, label = false, marker = engine_marker, ms = 10)\n    plot!(p, drone_coordinates; color = color, label = false)\n\n    return p\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"plot_drone! (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function animate_drone(drone::Drone, target, results::InferenceResult)\n\n    states = hcat(map(p -> mean(p), results.posteriors[:s])...)\n    \n\n    animation = @animate for k in 1:size(states,2)\n\n        # plot target\n        p = scatter([target[1]], [target[2]], label = \"target\"; color = :red)\n\n        # plot drone\n        plot_drone!(p, drone, State(states[:, k]...))\n\n        xlims!(-1.5, 1.5)\n        ylims!(-1.5, 1.5)\n    \n    end\n\n    gif(animation, \"drone.gif\", show_msg = false)\n\n    nothing\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"animate_drone (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"animate_drone(drone, target, results)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"let \n    inferred_angle_mean = map(p -> mean(p)[5], results.posteriors[:s])\n    inferred_angle_std  = map(p -> std(p)[5],  results.posteriors[:s])\n    plot(inferred_angle_mean; ribbon = inferred_angle_std, fillalpha = 0.2, label = \"inferred angle\", size=(600,300))\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"let \n    inferred_forces_mean = hcat(map(p -> mean(p), results.posteriors[:u])...)'\n    inferred_forces_std  = hcat(map(p -> sqrt.(var(p)),  results.posteriors[:u])...)'\n    plot(inferred_forces_mean[:,1]; ribbon = inferred_forces_std[:,1], fillalpha = 0.2, label = \"Fl\", size=(600,300))\n    plot!(inferred_forces_mean[:,2]; ribbon = inferred_forces_std[:,2], fillalpha = 0.2, label = \"Fr\", size=(600,300))\n    \n    hline!([get_mass(drone) * get_gravity(env) / 2], label = \"Fg/2\")\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#3D-Drone-Extension","page":"Drone Dynamics","title":"3D Drone Extension","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"This section extends our 2D drone model into three-dimensional space, allowing for full spatial navigation. The model includes:","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"6 degrees of freedom (position and orientation)\nFour-motor configuration\nBasic aerodynamic forces","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Note: This implementation is a simplified model intended for educational purposes. While it captures the fundamental dynamics of a quadcopter, it omits advanced aerodynamic effects and motor dynamics for clarity.","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"# Extended Drone structure for 4 motors\nBase.@kwdef struct Drone3D\n    mass::Float64\n    inertia::Matrix{Float64}  # 3x3 inertia matrix\n    radius::Float64\n    arm_length::Float64\n    force_limit::Float64\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Main.anonymous.Drone3D","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function get_properties(drone::Drone3D)\n    return (\n        drone.mass,\n        drone.inertia,\n        drone.radius,\n        drone.arm_length,\n        drone.force_limit\n    )\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_properties (generic function with 2 methods)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"# Extended State for 3D\nstruct State3D\n    x::Float64   # position\n    y::Float64\n    z::Float64\n    vx::Float64  # velocity\n    vy::Float64\n    vz::Float64\n    ϕ::Float64   # roll\n    θ::Float64   # pitch\n    ψ::Float64   # yaw\n    ωx::Float64  # angular velocity\n    ωy::Float64\n    ωz::Float64\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function get_state(state::State3D)\n    return (\n        state.x, state.y, state.z,\n        state.vx, state.vy, state.vz,\n        state.ϕ, state.θ, state.ψ,\n        state.ωx, state.ωy, state.ωz\n    )\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_state (generic function with 2 methods)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    rotation_matrix(ψ, θ, ϕ)\n\nCreate a 3D rotation matrix from yaw (ψ), pitch (θ), and roll (ϕ) angles.\n\"\"\"\nfunction rotation_matrix(ψ, θ, ϕ)\n    # Rotation matrices for each axis\n    Rz = [cos(ψ) -sin(ψ) 0;\n          sin(ψ)  cos(ψ) 0;\n          0       0      1]\n    \n    Ry = [cos(θ)  0  sin(θ);\n          0       1  0;\n         -sin(θ)  0  cos(θ)]\n    \n    Rx = [1  0       0;\n          0  cos(ϕ) -sin(ϕ);\n          0  sin(ϕ)  cos(ϕ)]\n    \n    # Combined rotation matrix (ZYX order)\n    return Rz * Ry * Rx\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Main.anonymous.rotation_matrix","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    state_transition_3d(state, actions, drone, environment, dt)\n\nCompute the next state of the 3D drone given current state and four motor forces.\n\"\"\"\nfunction state_transition_3d(state, actions, drone::Drone3D, environment::Environment, dt)\n    # Extract properties\n    m, I, r, L, limit = get_properties(drone)\n    g = get_gravity(environment)\n    \n    # Clamp motor forces\n    F1, F2, F3, F4 = clamp.(actions, 0, limit)\n    \n    # Extract state\n    x, y, z, vx, vy, vz, ϕ, θ, ψ, ωx, ωy, ωz = state\n    \n    # Current rotation matrix\n    R = rotation_matrix(ψ, θ, ϕ)\n    \n    # Total thrust force in body frame\n    F_total = sum([F1, F2, F3, F4])\n    \n    # Compute torques\n    τx = L * (F2 - F4)  # roll torque\n    τy = L * (F1 - F3)   # pitch torque\n    τz = (F1 + F3 - F2 - F4) * r  # yaw torque\n    \n    # Forces in world frame\n    F_world = R * [0, 0, F_total]\n    \n    # Accelerations\n    ax = F_world[1] / m\n    ay = F_world[2] / m\n    az = F_world[3] / m - g\n    \n    # Angular accelerations\n    α = I \\ ([τx, τy, τz] - cross([ωx, ωy, ωz], I * [ωx, ωy, ωz]))\n    \n    # Update velocities\n    vx_new = vx + ax * dt\n    vy_new = vy + ay * dt\n    vz_new = vz + az * dt\n    \n    # Update positions\n    x_new = x + vx * dt + ax * dt^2 / 2\n    y_new = y + vy * dt + ay * dt^2 / 2\n    z_new = z + vz * dt + az * dt^2 / 2\n    \n    # Update angular velocities\n    ωx_new = ωx + α[1] * dt\n    ωy_new = ωy + α[2] * dt\n    ωz_new = ωz + α[3] * dt\n    \n    # Update angles\n    ϕ_new = ϕ + ωx * dt + α[1] * dt^2 / 2\n    θ_new = θ + ωy * dt + α[2] * dt^2 / 2\n    ψ_new = ψ + ωz * dt + α[3] * dt^2 / 2\n    \n    return [\n        x_new, y_new, z_new,\n        vx_new, vy_new, vz_new,\n        ϕ_new, θ_new, ψ_new,\n        ωx_new, ωy_new, ωz_new\n    ]\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Main.anonymous.state_transition_3d","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"@model function drone_model_3d(drone, environment, initial_state, goal, horizon, dt)\n    # Extract properties\n    g = get_gravity(environment)\n    m = drone.mass\n    \n    # Initial state prior\n    s[1] ~ MvNormal(mean = initial_state, covariance = 1e-5 * I)\n    \n    for i in 1:horizon\n        # Prior on motor actions (mean compensates for gravity)\n        hover_force = m * g / 4\n        u[i] ~ MvNormal(μ = [hover_force, hover_force, hover_force, hover_force], Σ = diageye(4))\n        \n        # State transition\n        s[i + 1] ~ MvNormal(\n            μ = state_transition_3d(s[i], u[i], drone, environment, dt),\n            Σ = 1e-10 * I\n        )\n    end\n    \n    s[end] ~ MvNormal(mean = goal, covariance = 1e-5 * diageye(12))\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"@meta function drone_meta_3d()\n    state_transition_3d() -> Unscented()\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"drone_meta_3d (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function move_to_target_3d(drone::Drone3D, env::Environment, start::State3D, target, horizon, dt)\n    results = infer(\n        model = drone_model_3d(\n            drone = drone,\n            environment = env,\n            horizon = horizon,\n            dt = dt\n        ),\n        data = (\n            initial_state = collect(get_state(start)),\n            goal = [target[1], target[2], target[3], 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        ),\n        meta = drone_meta_3d(),\n        returnvars = (s = KeepLast(), u = KeepLast())\n    )\n    \n    return results\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"move_to_target_3d (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function move_through_waypoints(drone::Drone3D, env::Environment, start::State3D, waypoints, steps_per_segment=40, dt=0.05)\n    current_state = start\n    all_results = []\n    all_states = []\n    \n    # Move through each waypoint\n    for (i, target) in enumerate(waypoints)\n        println(\"Moving to waypoint $i: $target\")\n        \n        # Get results for this segment\n        results = move_to_target_3d(drone, env, current_state, target, steps_per_segment, dt)\n        push!(all_results, results)\n        \n        # Extract final state for next segment\n        final_states = hcat(map(p -> mean(p), results.posteriors[:s])...)\n        final_state = State3D(final_states[:, end]...)\n        push!(all_states, final_states)\n        \n        # Update current state\n        current_state = final_state\n    end\n    \n    # Combine all states for animation\n    combined_states = hcat(all_states...)\n    \n    return combined_states, waypoints\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"move_through_waypoints (generic function with 3 methods)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"# Visualization function for 3D drone\nfunction plot_drone_3d!(p, drone::Drone3D, state::State3D; color=:black)\n    x, y, z, _, _, _, ϕ, θ, ψ, _, _, _ = get_state(state)\n    _, _, radius, arm_length, _ = get_properties(drone)\n    \n    # Create rotation matrix\n    R = rotation_matrix(ψ, θ, ϕ)\n    \n    # Define arm endpoints in body frame (relative to center)\n    arm_endpoints = [\n        [arm_length, 0, 0],   # Right arm (X configuration)\n        [0, arm_length, 0],   # Front arm\n        [-arm_length, 0, 0],  # Left arm\n        [0, -arm_length, 0]   # Back arm\n    ]\n    \n    # Transform arm endpoints to world frame\n    world_endpoints = []\n    for endpoint in arm_endpoints\n        # Convert endpoint to column vector for matrix multiplication\n        endpoint_vec = reshape(endpoint, :, 1)\n        # Apply rotation and translation\n        world_point = R * endpoint_vec + [x, y, z]\n        push!(world_endpoints, vec(world_point))\n    end\n    \n    # Plot center\n    scatter!(p, [x], [y], [z], color=color, label=false, markersize=5)\n    \n    # Plot arms and motors\n    for endpoint in world_endpoints\n        # Draw arm\n        plot!(p, [x, endpoint[1]], [y, endpoint[2]], [z, endpoint[3]], \n              color=color, label=false, linewidth=2)\n        # Draw motor\n        scatter!(p, [endpoint[1]], [endpoint[2]], [endpoint[3]], \n                color=color, label=false, markersize=3)\n    end\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"plot_drone_3d! (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function animate_drone_3d_multi(drone::Drone3D, states, targets; fps=30)\n    # Note: The rain animation is purely for visualization aesthetics\n    # and does not affect the drone's dynamics or trajectory planning\n    \n    # Create initial rain streaks\n    function generate_raindrops(n=50)\n        x = 4 * rand(n) .- 2  # range [-2, 2]\n        y = 4 * rand(n) .- 2\n        z1 = 2 .+ 4 * rand(n)  # start higher up to have some offscreen\n        z2 = z1 .- 0.3  # fixed length rain streaks\n        return (x, y, z1, z2)\n    end\n    \n    # Initialize raindrops\n    raindrops = generate_raindrops()\n    \n    # Determine dynamic plot bounds based on states and targets\n    x_vals = [states[1,:]; [t[1] for t in targets]]\n    y_vals = [states[2,:]; [t[2] for t in targets]]\n    z_vals = [states[3,:]; [t[3] for t in targets]]\n    \n    x_min, x_max = minimum(x_vals) - 0.5, maximum(x_vals) + 0.5\n    y_min, y_max = minimum(y_vals) - 0.5, maximum(y_vals) + 0.5\n    z_min, z_max = minimum(z_vals) - 0.5, maximum(z_vals) + 0.5\n    \n    # Ensure the plot window is at least 4x4x4 for good visualization\n    x_range = max(x_max - x_min, 4.0)\n    y_range = max(y_max - y_min, 4.0)\n    z_range = max(z_max - z_min, 4.0)\n    \n    # Center the plot window\n    x_center = (x_min + x_max) / 2\n    y_center = (y_min + y_max) / 2\n    z_center = (z_min + z_max) / 2\n    \n    x_min, x_max = x_center - x_range/2, x_center + x_range/2\n    y_min, y_max = y_center - y_range/2, y_center + y_range/2\n    z_min, z_max = z_center - z_range/2, z_center + z_range/2\n    \n    animation = @animate for k in 1:size(states,2)\n        # Update raindrop positions\n        fall_speed = 0.1\n        z1 = raindrops[3] .- fall_speed\n        z2 = raindrops[4] .- fall_speed\n        \n        # Regenerate raindrops that have fallen below view\n        below_view = findall(z2 .<= z_min)\n        if !isempty(below_view)\n            new_drops = generate_raindrops(length(below_view))\n            raindrops[1][below_view] = new_drops[1]\n            raindrops[2][below_view] = new_drops[2]\n            z1[below_view] = new_drops[3]\n            z2[below_view] = new_drops[4]\n        end\n        \n        # Update raindrops state\n        raindrops = (raindrops[1], raindrops[2], z1, z2)\n        \n        p = plot3d(\n            xlims=(x_min, x_max), ylims=(y_min, y_max), zlims=(z_min, z_max),\n            xlabel=\"X\", ylabel=\"Y\", zlabel=\"Z\",\n            camera=(45, 30),\n            title=\"Multi-Waypoint Drone Flight\",\n            background=:white\n        )\n        \n        # Draw rain streaks\n        for i in 1:length(raindrops[1])\n            if raindrops[4][i] > z_min  # only draw if in view\n                plot!(p, [raindrops[1][i], raindrops[1][i]], \n                        [raindrops[2][i], raindrops[2][i]], \n                        [raindrops[3][i], raindrops[4][i]],\n                     color=:grey, \n                     linestyle=:dash,\n                     alpha=0.6,\n                     legend=false,\n                     linewidth=1)\n            end\n        end\n        \n        # Plot all targets\n        for (i, target) in enumerate(targets)\n            scatter!(p, [target[1]], [target[2]], [target[3]], \n                    label=i == 1 ? \"waypoints\" : false, \n                    color=:red,\n                    markersize=i == 1 ? 5 : 3)\n            \n            # Connect waypoints with lines\n            if i > 1\n                prev_target = targets[i-1]\n                plot!(p, [prev_target[1], target[1]], \n                         [prev_target[2], target[2]], \n                         [prev_target[3], target[3]],\n                     color=:red, linestyle=:dash, label=false, linewidth=1)\n            end\n        end\n        \n        # Plot drone\n        if size(states, 1) >= 12  # Make sure we have all 12 components for State3D\n            current_state = State3D(states[:, k]...)\n            plot_drone_3d!(p, drone, current_state)\n        else\n            # If we don't have enough state components, just plot a point at the position\n            scatter!(p, [states[1,k]], [states[2,k]], [states[3,k]], \n                   color=:black, markersize=4, label=false)\n        end\n        \n        # Add trajectory trace (last 100 points)\n        trace_start = max(1, k-100)\n        if k > 1\n            plot!(p, states[1,trace_start:k], states[2,trace_start:k], states[3,trace_start:k],\n                  color=:blue, label=false, linewidth=1, linealpha=0.5)\n        end\n    end\n\n    gif(animation, \"drone_3d_multi.gif\", fps=fps, show_msg = false)\n\n    nothing\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"animate_drone_3d_multi (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"# Create drone instance\ndrone_3d = Drone3D(\n    mass = 1.0,\n    inertia = diagm([0.1, 0.1, 0.15]),  # 3×3 diagonal inertia matrix\n    radius = 0.1,\n    arm_length = 0.2,\n    force_limit = 15.0\n)\n\nenv = Environment()\n\n# Initial state\nstart = State3D(\n    0.0, 0.0, 0.0,  # position (x, y, z)\n    0.0, 0.0, 0.0,  # velocity (vx, vy, vz)\n    0.0, 0.0, 0.0,  # orientation (ϕ, θ, ψ)\n    0.0, 0.0, 0.0   # angular velocity (ωx, ωy, ωz)\n)\n\n# Define a sequence of waypoints for a square pattern\nwaypoints = [\n    [1.0, 1.0, 1.0],    # Front-right corner\n    [-1.0, 1.0, 1.0],   # Front-left corner\n    [-1.0, -1.0, 1.0],  # Back-left corner\n    [1.0, -1.0, 1.0],   # Back-right corner\n    [0.0, 0.0, 0.0]     # Land at center\n]\n\n# Run simulation through all waypoints\ncombined_states, targets = move_through_waypoints(drone_3d, env, start, waypoints);","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Moving to waypoint 1: [1.0, 1.0, 1.0]\nMoving to waypoint 2: [-1.0, 1.0, 1.0]\nMoving to waypoint 3: [-1.0, -1.0, 1.0]\nMoving to waypoint 4: [1.0, -1.0, 1.0]\nMoving to waypoint 5: [0.0, 0.0, 0.0]","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"animate_drone_3d_multi(drone_3d, combined_states, targets)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/drone_dynamics/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [37e2e46d] LinearAlgebra v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gamma_mixture/#Gamma-Mixture-Model","page":"Gamma Mixture","title":"Gamma Mixture Model","text":"","category":"section"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"This notebook implements one of the experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/.","category":"page"},{"location":"categories/problem_specific/gamma_mixture/#Load-packages","page":"Gamma Mixture","title":"Load packages","text":"","category":"section"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"using RxInfer, Random, StatsPlots","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# create custom structure for model parameters for simplicity\nstruct GammaMixtureModelParameters\n    nmixtures   # number of mixtures\n    priors_as   # tuple of priors for variable a\n    priors_bs   # tuple of priors for variable b\n    prior_s     # prior of variable s\nend","category":"page"},{"location":"categories/problem_specific/gamma_mixture/#Model-specification","page":"Gamma Mixture","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"@model function gamma_mixture_model(y, parameters)\n\n    # fetch information from struct\n    nmixtures = parameters.nmixtures\n    priors_as = parameters.priors_as\n    priors_bs = parameters.priors_bs\n    prior_s   = parameters.prior_s\n\n    # set prior on global selection variable\n    s ~ Dirichlet(probvec(prior_s))\n\n    # allocate variables for mixtures\n    local as\n    local bs\n\n    # set priors on variables of mixtures\n    for i in 1:nmixtures\n        as[i] ~ Gamma(shape = shape(priors_as[i]), rate = rate(priors_as[i]))\n        bs[i] ~ Gamma(shape = shape(priors_bs[i]), rate = rate(priors_bs[i]))\n    end\n\n    # allocate variables for local selection variable\n    local z\n    # specify local selection variable and data generating process\n    for i in 1:length(y)\n        z[i] ~ Categorical(s)\n        y[i] ~ GammaMixture(switch = z[i], a = as, b = bs)\n    end\n    \nend","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"constraints = @constraints begin \n\n    q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n\n    q(as) = q(as[begin])..q(as[end])\n    q(bs) = q(bs[begin])..q(bs[end])\n    \n    q(as)::PointMassFormConstraint(starting_point = (args...) -> [1.0])\nend","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"Constraints: \n  q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n  q(as) = q(as[(begin)..(end)])\n  q(bs) = q(bs[(begin)..(end)])\n  q(as) :: PointMassFormConstraint()","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# specify seed and number of data points\nrng = MersenneTwister(43)\nn_samples = 2500\n\n# specify parameters of mixture model that generates the data\n# Note that mixture components have exactly the same means\nmixtures  = [ Gamma(9.0, inv(27.0)), Gamma(90.0, inv(270.0)) ]\nnmixtures = length(mixtures)\nmixing    = rand(rng, nmixtures)\nmixing    = mixing ./ sum(mixing)\nmixture   = MixtureModel(mixtures, mixing)\n\n# generate data set\ndataset = rand(rng, mixture, n_samples);","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# specify priors of probabilistic model\n# NOTE: As the means of the mixtures \"collide\", we specify informative prior for selector variable\nnmixtures = 2\ngpriors = GammaMixtureModelParameters(\n    nmixtures,                                                    # number of mixtures\n    [ Gamma(1.0, 0.1), Gamma(1.0, 1.0) ],                         # priors on variables a\n    [ GammaShapeRate(10.0, 2.0), GammaShapeRate(1.0, 3.0) ],      # priors on variables b\n    Dirichlet(1e3*mixing)                                         # prior on variable s\n)\n\ngmodel         = gamma_mixture_model(parameters = gpriors)\ngdata          = (y = dataset, )\ninit           = @initialization begin \n    q(s) = gpriors.prior_s\n    q(z) = vague(Categorical, gpriors.nmixtures)\n    q(bs) = GammaShapeRate(1.0, 1.0)\nend\ngreturnvars    = (s = KeepLast(), z = KeepLast(), as = KeepEach(), bs = KeepEach())\n\ngoptions = (\n     \n    default_factorisation = MeanField() # Mixture models require Mean-Field assumption currently\n)\n\ngresult = infer(\n    model          = gmodel, \n    data           = gdata,\n    constraints    = constraints,\n    options        = (limit_stack_depth = 100,),\n    initialization = init,\n    returnvars     = greturnvars,\n    free_energy    = true,\n    iterations     = 250, \n    showprogress   = true\n);","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# extract inferred parameters\n_as, _bs = mean.(gresult.posteriors[:as][end]), mean.(gresult.posteriors[:bs][end])\n_dists   = map(g -> Gamma(g[1], inv(g[2])), zip(_as, _bs))\n_mixing = mean(gresult.posteriors[:s])\n\n# create model from inferred parameters\n_mixture   = MixtureModel(_dists, _mixing);","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# report on outcome of inference\nprintln(\"Generated means: $(mean(mixtures[1])) and $(mean(mixtures[2]))\")\nprintln(\"Inferred means: $(mean(_dists[1])) and $(mean(_dists[2]))\")\nprintln(\"========\")\nprintln(\"Generated mixing: $(mixing)\")\nprintln(\"Inferred mixing: $(_mixing)\")","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"Generated means: 0.3333333333333333 and 0.33333333333333337\nInferred means: 0.3350392108016526 and 0.33338890241661584\n========\nGenerated mixing: [0.4617110702349237, 0.5382889297650763]\nInferred mixing: [0.37278386597697, 0.62721613402303]","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# plot results\np1 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np1 = plot!(mixture, label=false, title=\"Generated mixtures\", linewidth=3.0)\n\np2 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np2 = plot!(_mixture, label=false, title=\"Inferred mixtures\", linewidth=3.0)\n\n# evaluate the convergence of the algorithm by monitoring the BFE\np3 = plot(gresult.free_energy, label=false, xlabel=\"iterations\", title=\"Bethe FE\")\n\nplot(plot(p1, p2, layout = @layout([ a; b ])), plot(p3), layout = @layout([ a b ]), size = (800, 400))","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/gamma_mixture/Project.toml`\n  [86711068] RxInfer v4.6.0\n  [f3b207a7] StatsPlots v0.15.8\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"","category":"page"},{"location":"categories/problem_specific/probit_model/#Probit-Model","page":"Probit Model","title":"Probit Model","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/#Estimation-of-pollutant","page":"Probit Model","title":"Estimation of pollutant","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Mortality y_t of fishs in a lake is observed over time. Mortality rate textBer(Phi(x_t)) is linked to the level of pollutant x_t in the lake according to the probit model (see below). The municipality wants to keep track of the pollution. To do so, the level of pollutant in the lake is tracked over time through observations of the fishs.","category":"page"},{"location":"categories/problem_specific/probit_model/#Objective","page":"Probit Model","title":"Objective","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Probit model aims to infer a random proces value from noisy binary observations of it. RxInfer comes with support for expectation propagation (EP). In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model. Here, the probit function links continuous variable x_t with the discrete variable y_t. The model is defined as:","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"beginaligned\n    u = 01 \n    x_0 sim mathcalN(0 100) \n    x_t sim mathcalN(x_t-1+ u 001) \n    y_t sim mathrmBer(Phi(x_t))\nendaligned","category":"page"},{"location":"categories/problem_specific/probit_model/#Import-packages","page":"Probit Model","title":"Import packages","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"using RxInfer, GraphPPL,StableRNGs, Random, Plots, Distributions\nusing StatsFuns: normcdf","category":"page"},{"location":"categories/problem_specific/probit_model/#Data-generation","page":"Probit Model","title":"Data generation","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n    \n    # hyper parameters\n    u = 0.1\n\n    # allocate space for data\n    data_x = zeros(nr_samples + 1)\n    data_y = zeros(nr_samples)\n    \n    # initialize data\n    data_x[1] = -2\n    \n    # generate data\n    for k in eachindex(data_y)\n        \n        # calculate new x\n        data_x[k+1] = data_x[k] + u + sqrt(0.01)*randn(rng)\n        \n        # calculate y\n        data_y[k] = normcdf(data_x[k+1]) > rand(rng)\n        \n    end\n    \n    # return data\n    return data_x, data_y\n    \nend;","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"n = 40","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"40","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"data_x, data_y = generate_data(n);","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"p = plot(xlabel = \"t\", ylabel = \"x, y\")\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\")","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/probit_model/#Model-specification","page":"Probit Model","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"@model function probit_model(y, prior_x)\n    \n    # specify uninformative prior\n    x_prev ~ prior_x\n    \n    # create model \n    for k in eachindex(y)\n        x[k] ~ Normal(mean = x_prev + 0.1, precision = 100)\n        y[k] ~ Probit(x[k]) where {\n            # Probit node by default uses RequireMessage pipeline with vague(NormalMeanPrecision) message as initial value for `in` edge\n            # To change initial value user may specify it manually, like. Changes to the initial message may improve stability in some situations\n            dependencies = RequireMessageFunctionalDependencies(in = NormalMeanPrecision(0.0, 0.01))\n        }\n        x_prev = x[k]\n    end\n    \nend;","category":"page"},{"location":"categories/problem_specific/probit_model/#Probit-Node","page":"Probit Model","title":"Probit Node","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Probit node needs an initialisation of the 'in' message because of this computation methodology. The input message is not directly calculated. First the marginal q(in) is computed and then the output message, this using the margianalisation formula. ","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"overrightarrowmu(x) overleftarrowmu(x) = q(x)","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Consequently an initial message overleftarrowmu(in) is needed to start iterate. It can be speficied as in the above example. Otherwise RxInfer will initiate it at a default value.","category":"page"},{"location":"categories/problem_specific/probit_model/#Inference","page":"Probit Model","title":"Inference","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"result = infer(\n    model = probit_model(prior_x=Normal(0.0, 100.0)), \n    data  = (y = data_y, ), \n    iterations = 5, \n    returnvars = (x = KeepLast(),),\n    free_energy  = true\n)","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Inference results:\n  Posteriors       | available for (x)\n  Free Energy:     | Real[25.6698, 18.0157, 17.9199, 17.9194, 17.9194]","category":"page"},{"location":"categories/problem_specific/probit_model/#Results","page":"Probit Model","title":"Results","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"mx = result.posteriors[:x]\n\np = plot(xlabel = \"t\", ylabel = \"x, y\", legend = :bottomright)\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\", lw = 2)\np = plot!(mean.(mx)[2:end], ribbon = std.(mx)[2:end], fillalpha = 0.2, label=\"x (inferred mean)\")\n\nf = plot(xlabel = \"t\", ylabel = \"BFE\")\nf = plot!(result.free_energy, label = \"Bethe Free Energy\")\n\nplot(p, f, size = (800, 400))","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/probit_model/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [b3f8163a] GraphPPL v4.6.4\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [4c63d2b9] StatsFuns v1.5.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"","category":"page"},{"location":"#RxInfer.jl-Examples","page":"Home","title":"RxInfer.jl Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to the examples gallery for RxInfer.jl, a Julia package for reactive message passing and probabilistic programming.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThis documentation is automatically generated from Jupyter notebooks in the repository. The examples are regularly tested to ensure they work with the latest version of RxInfer.jl.","category":"page"},{"location":"#About-RxInfer.jl","page":"Home","title":"About RxInfer.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RxInfer.jl is a Julia package that combines message passing-based inference with reactive programming paradigms. It provides:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A flexible framework for probabilistic programming\nReactive message passing for real-time inference\nEfficient and scalable inference algorithms\nSupport for both online and offline inference\nIntegration with the Julia ecosystem\nPython integration through client-server infrastructure","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read more about RxInfer.jl in the RxInfer.jl Documentation.","category":"page"},{"location":"#Python-Integration","page":"Home","title":"Python Integration","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RxInfer can be used from Python through our client-server infrastructure:","category":"page"},{"location":"","page":"Home","title":"Home","text":"RxInferServer.jl - A RESTful API service for deploying RxInfer models\nRxInferClient.py - Python SDK for interacting with RxInferServer","category":"page"},{"location":"","page":"Home","title":"Home","text":"The server provides OpenAPI-compliant endpoints for model deployment and inference, while the Python client offers a convenient interface to:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Create and manage model instances\nExecute inference tasks\nMonitor inference progress\nHandle authentication and API keys\nProcess results in a native format","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more information, visit:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Server Documentation\nPython SDK Documentation","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Browse our comprehensive collection of examples in the List of Examples section. Each example demonstrates different aspects of RxInfer.jl's capabilities and includes detailed explanations and code.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We welcome contributions from the community! Whether you want to fix bugs, improve existing examples, or add new ones, please check our contribution guide for detailed instructions and best practices.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To run these examples locally:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Clone the repository:","category":"page"},{"location":"","page":"Home","title":"Home","text":"git clone https://github.com/ReactiveBayes/RxInferExamples.jl.git","category":"page"},{"location":"","page":"Home","title":"Home","text":"Build the examples:","category":"page"},{"location":"","page":"Home","title":"Home","text":"make examples","category":"page"},{"location":"","page":"Home","title":"Home","text":"Build the documentation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"make docs","category":"page"},{"location":"","page":"Home","title":"Home","text":"Preview the documentation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"make preview","category":"page"},{"location":"#Resources","page":"Home","title":"Resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"How to Contribute\nRxInfer.jl Documentation\nRxInfer.jl GitHub Repository\nJulia Documentation","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: For Developers\nIf you're interested in how the examples and documentation are built, check out our Build System Documentation.","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RxInfer.jl and these examples are licensed under the MIT License. See the LICENSE file in the repository for more details.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/#Bayesian-Multinomial-Regression","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"This notebook is an introductory tutorial to Bayesian multinomial regression with RxInfer.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"using RxInfer, Plots, StableRNGs, Distributions, ExponentialFamily, StatsPlots\nimport ExponentialFamily: softmax","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/#Model-Description","page":"Bayesian Multinomial Regression","title":"Model Description","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"The key innovation in Linderman et al. (2015) is extending the Pólya-gamma augmentation scheme to the multinomial case. This allows us to transform the non-conjugate multinomial likelihood into a conditionally conjugate form by introducing auxiliary Pólya-gamma random variables.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"The multinomial regression model with Pólya-gamma augmentation can be written as: p(y  psi N) = textMultinomial(y N psi)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"where:","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"y\nis a K-dimensional vector of count data with N total counts\npsi\nis a K-1 -dimensional Gaussian random variable","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/#Implementation","page":"Bayesian Multinomial Regression","title":"Implementation","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"In this notebook, we will implement the Pólya-gamma augmented Bayesian multinomial regression model with RxInfer by performing inference using message passing to estimate the posterior distribution of the regression coefficients","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"function generate_multinomial_data(rng=StableRNG(123);N = 20, k=9, nsamples = 1000)\n    Ψ = randn(rng, k)\n    p = softmax(Ψ)\n    X = rand(rng, Multinomial(N, p), nsamples)\n    X= [X[:,i] for i in 1:size(X,2)];\n    return X, Ψ,p\nend","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"generate_multinomial_data (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"nsamples = 5000\nN = 30\nk = 40\nX, Ψ, p = generate_multinomial_data(N=N,k=k,nsamples=nsamples);","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"The MultinomialPolya factor node is used to model the likelihood of the multinomial distribution. ","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Due to non-conjugacy of the likelihood and the prior distribution, we need to use a more complex inference algorithm. RxInfer provides an Expectation Propagation (EP) [2] algorithm to infer the posterior distribution. Due to EP's approximation, we need to specify an inbound message for the regression coefficients while using the MultinomialPolya factor node. This feature is implemented in the dependencies keyword argument during the creation of the MultinomialPolya factor node. ReactiveMP.jl provides a RequireMessageFunctionalDependencies type that is used to specify the inbound message for the regression coefficients ψ. Refer to the ReactiveMP.jl documentation for more information.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"@model function multinomial_model(obs, N, ξ_ψ, W_ψ)\n    ψ ~ MvNormalWeightedMeanPrecision(ξ_ψ, W_ψ)\n    obs .~ MultinomialPolya(N, ψ) where {dependencies = RequireMessageFunctionalDependencies(ψ = MvNormalWeightedMeanPrecision(ξ_ψ, W_ψ))}\nend","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"result = infer(\n    model = multinomial_model(ξ_ψ=zeros(k-1), W_ψ=rand(Wishart(3, diageye(k-1))), N=N),\n    data = (obs=X, ),\n    iterations = 50,\n    free_energy = true,\n    showprogress = true,\n    options = (\n        limit_stack_depth = 100,\n    )\n)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Inference results:\n  Posteriors       | available for (ψ)\n  Free Energy:     | Real[4.4643e5, 2.92577e5, 2.38051e5, 2.12337e5, 1.9826\n1e5, 1.89799e5, 1.84369e5, 1.80712e5, 1.78155e5, 1.76313e5  …  1.69833e5, 1\n.69826e5, 1.69821e5, 1.69815e5, 1.69811e5, 1.69807e5, 1.69803e5, 1.698e5, 1\n.69797e5, 1.69795e5]","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"plot(result.free_energy[1:end], \n     title=\"Free Energy Over Iterations\",\n     xlabel=\"Iteration\",\n     ylabel=\"Free Energy\",\n     linewidth=2,\n     legend=false,\n     grid=true,\n     )","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"predictive = @call_rule MultinomialPolya(:x, Marginalisation) (q_N = PointMass(N), q_ψ = result.posteriors[:ψ][end], meta = MultinomialPolyaMeta(21))\nprintln(\"Estimated data generation probabilities: $(predictive.p)\")\nprintln(\"True data generation probabilities: $(p)\")","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Estimated data generation probabilities: [0.012008374244627853, 0.027820703\n864623365, 0.004951098078287808, 0.012853423101010312, 0.013331346807868459\n, 0.03785355721158348, 0.007480443748466134, 0.00685855196136432, 0.0057358\n70377760052, 0.0042598458793007546, 0.005620250032207473, 0.003958972974615\n969, 0.004246653342427598, 0.03633446881391919, 0.1086673745208552, 0.07262\n329170441559, 0.026477839353814314, 0.024054429327025107, 0.010504279472776\n394, 0.008381646928779834, 0.039819192225570225, 0.0050430901338460385, 0.0\n08351349916837121, 0.02632794862179386, 0.006938887540835614, 0.00774036407\n50395575, 0.008845475967595658, 0.0070867881378418925, 0.017173634671003732\n, 0.007436687304365372, 0.008681071135649837, 0.0034831083132371983, 0.0108\n35698932694133, 0.010492234059721526, 0.0949765409758528, 0.044042079069877\n39, 0.13278124456273813, 0.02743840709236432, 0.03069657555710722, 0.067787\n19996029887]\nTrue data generation probabilities: [0.012475572764691347, 0.02759115956301\n153, 0.004030932560100506, 0.013008651265311708, 0.012888510278451618, 0.03\n7656116813111006, 0.007242363105598982, 0.006930069564505769, 0.00538389836\n228327, 0.0036198124274772225, 0.005212387391120808, 0.003185556887255863, \n0.003820168769118259, 0.036849638787622915, 0.109428569898501, 0.0726075387\n5224316, 0.026079268674281158, 0.024477855252934583, 0.010207778995219957, \n0.008532295265944583, 0.040242532118754906, 0.005181587450423221, 0.0082073\n91370854009, 0.02741148713822125, 0.006623087410725917, 0.00836770271463416\n2, 0.009668643362989908, 0.007171783607096945, 0.016985615150215773, 0.0070\n80691453323701, 0.008297044496975403, 0.0037359000700039487, 0.011142755810\n390478, 0.010256554277897088, 0.09528238587772694, 0.04369806970660494, 0.1\n3308101804159636, 0.02665693577960761, 0.030479170124456504, 0.069201498658\n71575]","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"mse = mean((predictive.p - p).^2);\nprintln(\"MSE between estimated and true data generation probabilities: $mse\")","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"MSE between estimated and true data generation probabilities: 2.56673673478\n71117e-7","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"@model function multinomial_regression(obs, N, X, ϕ, ξβ, Wβ)\n    β ~ MvNormalWeightedMeanPrecision(ξβ, Wβ)\n    for i in eachindex(obs)\n        Ψ[i] := ϕ(X[i])*β\n        obs[i] ~ MultinomialPolya(N, Ψ[i]) where {dependencies = RequireMessageFunctionalDependencies(ψ = MvNormalWeightedMeanPrecision(zeros(length(obs[i])-1), diageye(length(obs[i])-1)))}\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"function generate_regression_data(rng=StableRNG(123);ϕ = identity,N = 3, k=5, nsamples = 1000)\n    β = randn(rng, k)\n    X = randn(rng, nsamples, k, k)\n    X = [X[i,:,:] for i in 1:size(X,1)];\n    Ψ = ϕ.(X)\n    p = map(x -> logistic_stick_breaking(x*β), Ψ)\n    return map(x -> rand(rng, Multinomial(N, x)), p), X, β, p\nend","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"generate_regression_data (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"ϕ = x -> sin(x)\nobs_regression, X_regression, β_regression, p_regression = generate_regression_data(;nsamples = 5000, ϕ = ϕ);","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"reg_results = infer(  \n    model = multinomial_regression(N = 3, ϕ = ϕ, ξβ = zeros(5), Wβ = rand(Wishart(5, diageye(5)))),\n    data = (obs=obs_regression,X = X_regression ),\n    iterations = 20,\n    free_energy = true,\n    showprogress = true,\n    returnvars = KeepLast(),\n    options = (\n        limit_stack_depth = 100,\n    ) \n)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Inference results:\n  Posteriors       | available for (Ψ, β)\n  Free Energy:     | Real[11949.7, 11583.9, 11501.7, 11480.7, 11475.1, 1147\n3.6, 11473.1, 11473.0, 11473.0, 11472.9, 11472.9, 11472.9, 11472.9, 11472.9\n, 11472.9, 11472.9, 11472.9, 11472.9, 11472.9, 11472.9]","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"println(\"estimated β: with mean and covariance: $(mean_cov(reg_results.posteriors[:β]))\")\nprintln(\"true β: $(β_regression)\")","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"estimated β: with mean and covariance: ([-0.11489246942459654, 0.6632242846\n883468, -1.2531701196540697, -0.08611072715458346, -0.07896390715413153], [\n0.00014788438380756614 -2.167471619127253e-6 3.6440382040317106e-6 -1.65258\n82554726538e-6 3.1981131447169856e-6; -2.167471619127253e-6 0.0001516839883\n8728172 -1.9334846360720486e-5 -3.1313520158254973e-7 1.3618121452689397e-6\n; 3.6440382040317106e-6 -1.9334846360720486e-5 0.00017957371850105128 4.538\n084582191742e-6 4.048763169417214e-7; -1.6525882554726538e-6 -3.13135201582\n54973e-7 4.538084582191742e-6 0.00014008270479088218 3.218340237344638e-6; \n3.1981131447169856e-6 1.3618121452689397e-6 4.048763169417214e-7 3.21834023\n7344638e-6 0.00013943471734019318])\ntrue β: [-0.12683768965424458, 0.6668851724871252, -1.2566124895590247, -0.\n08499562516549662, -0.094274004848194]","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"plot(reg_results.free_energy,\ntitle=\"Free Energy Over Iterations\",\nxlabel=\"Iteration\",\nylabel=\"Free Energy\",\nlinewidth=2,\nlegend=false,\ngrid=true,)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"mse_β =  mean((mean(reg_results.posteriors[:β]) - β_regression).^2)\nprintln(\"MSE of β estimate: $mse_β\")","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"MSE of β estimate: 8.071656804407589e-5","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"We can visualize how the logistic stick-breaking transformation of the simplex coordinates of the regression coefficients affects the prior distribution of the regression coefficients and vice versa since the logistic stick-breaking transformation is invertible.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"\n# Previous helper functions remain the same\nσ(x) = 1 / (1 + exp(-x))\nσ_inv(x) = log(x / (1 - x))\n\nfunction jacobian_det(π)\n    K = length(π)\n    det = 1.0\n    for k in 1:(K-1)\n        num = 1 - sum(π[1:(k-1)])\n        den = π[k] * (1 - sum(π[1:k]))\n        det *= num / den\n    end\n    return det\nend\n\nfunction ψ_to_π(ψ::Vector{Float64})\n    K = length(ψ) + 1\n    π = zeros(K)\n    for k in 1:(K-1)\n        π[k] = σ(ψ[k]) * (1 - sum(π[1:(k-1)]))\n    end\n    π[K] = 1 - sum(π[1:(K-1)])\n    return π\nend\n\nfunction π_to_ψ(π)\n    K = length(π)\n    ψ = zeros(K-1)\n    ψ[1] = σ_inv(π[1])\n    for k in 2:(K-1)\n        ψ[k] = σ_inv(π[k] / (1 - sum(π[1:(k-1)])))\n    end\n    return ψ\nend\n\n# Function to compute density in simplex coordinates\nfunction compute_simplex_density(x::Float64, y::Float64, Σ::Matrix{Float64})\n    # Check if point is inside triangle\n    if y < 0 || y > 1 || x < 0 || x > 1 || (x + y) > 1\n        return 0.0\n    end\n    \n    # Convert from simplex coordinates to π\n    π1 = x\n    π2 = y\n    π3 = 1 - x - y\n    \n    try\n        ψ = π_to_ψ([π1, π2, π3])\n        # Compute Gaussian density\n        dist = MvNormal(zeros(2), Σ)\n        return pdf(dist, ψ) * abs(jacobian_det([π1, π2, π3]))\n    catch\n        return 0.0\n    end\n   \nend\n\nfunction plot_transformed_densities()\n    # Create three different covariance matrices\n    ###For higher variances values needs scaling for proper visualization.\n    σ² = 1.0\n    Σ_corr = [σ² 0.9σ²; 0.9σ² σ²]\n    Σ_anticorr = [σ² -0.9σ²; -0.9σ² σ²]\n    Σ_uncorr = [σ² 0.0; 0.0 σ²]\n    \n    # Plot Gaussian densities\n    ψ1, ψ2 = range(-4sqrt(σ²), 4sqrt(σ²), length=500), range(-4sqrt(σ²), 4sqrt(σ²), length=100)\n    \n    p1 = contour(ψ1, ψ2, (x,y) -> pdf(MvNormal(zeros(2), Σ_corr), [x,y]),\n                 title=\"Correlated Prior\", xlabel=\"ψ₁\", ylabel=\"ψ₂\")\n    p2 = contour(ψ1, ψ2, (x,y) -> pdf(MvNormal(zeros(2), Σ_anticorr), [x,y]),\n                 title=\"Anti-correlated Prior\", xlabel=\"ψ₁\", ylabel=\"ψ₂\")\n    p3 = contour(ψ1, ψ2, (x,y) -> pdf(MvNormal(zeros(2), Σ_uncorr), [x,y]),\n                 title=\"Uncorrelated Prior\", xlabel=\"ψ₁\", ylabel=\"ψ₂\")\n    \n    # Plot simplex densities\n    n_points = 500\n    x = range(0, 1, length=n_points)\n    y = range(0, 1, length=n_points)\n    \n    # Plot simplices\n    p4 = contour(x, y, (x,y) -> compute_simplex_density(x, y, Σ_corr),\n                 title=\"Correlated Simplex\")\n    \n    # Add simplex boundaries and median lines\n    plot!(p4, [0,1,0,0], [0,0,1,0], color=:black, label=\"\")  # Triangle boundaries\n    \n    p5 = contour(x, y, (x,y) -> compute_simplex_density(x, y, Σ_anticorr),\n                 title=\"Anti-correlated Simplex\")\n    plot!(p5, [0,1,0,0], [0,0,1,0], color=:black, label=\"\")\n    \n    p6 = contour(x, y, (x,y) -> compute_simplex_density(x, y, Σ_uncorr),\n                 title=\"Uncorrelated Simplex\")\n    plot!(p6, [0,1,0,0], [0,0,1,0], color=:black, label=\"\")\n    \n    # Combine all plots\n    plot(p1, p2, p3, p4, p5, p6, layout=(2,3), size=(900,600))\nend\n\n# Generate the plots\nplot_transformed_densities()","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/bayesian_multinomial_regression/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [62312e5e] ExponentialFamily v2.1.0\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [f3b207a7] StatsPlots v0.15.8\n","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"","category":"page"},{"location":"categories/basic_examples/pomdp_control/#POMDP-Control-with-Reactive-Inference","page":"Pomdp Control","title":"POMDP Control with Reactive Inference","text":"","category":"section"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"This notebook demonstrates how to perform control in Partially Observable Markov Decision Processes (POMDPs) using reactive message passing and variational inference in RxInfer.jl.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"We will cover:","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Setting up a simple POMDP model\nDefining the state transition and observation models\nImplementing the control policy\nPerforming inference and control using message passing\nVisualizing the results","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"using RxInfer\nusing Distributions\nusing Plots\nusing Random\nusing ProgressMeter","category":"page"},{"location":"categories/basic_examples/pomdp_control/#Environment-Setup","page":"Pomdp Control","title":"Environment Setup","text":"","category":"section"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"For this example, we will implement the Windy Gridworld environment using RxEnvironments.jl. The Windy Gridworld is a simple gridworld environment with deterministic transitions and observations. This code is adapted from the RxEnvironments.jl documentation, and a more elaborate explanation of can be found there. ","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"The environment consists of:","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"A grid with wind values for each column\nAn agent with a current position\nA goal position to reach","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"The agent can:","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Move in cardinal directions (one step at a time)\nObserve its current position\nBe affected by wind when moving","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"The wind effect is applied after each movement, potentially pushing the agent upward by 0-2 positions depending on the column.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"First we will define the environment and the agent.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"using RxEnvironments\nusing Plots\n\nstruct WindyGridWorld{N}\n    wind::NTuple{N,Int}\n    agents::Vector\n    goal::Tuple{Int,Int}\nend\n\nmutable struct WindyGridWorldAgent\n    position::Tuple{Int,Int}\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"\nRxEnvironments.update!(env::WindyGridWorld, dt) = nothing # The environment has no \"internal\" updating process over time\n\nfunction RxEnvironments.receive!(env::WindyGridWorld{N}, agent::WindyGridWorldAgent, action::Tuple{Int,Int}) where {N}\n    if action[1] != 0\n        @assert action[2] == 0 \"Only one of the two actions can be non-zero\"\n    elseif action[2] != 0\n        @assert action[1] == 0 \"Only one of the two actions can be non-zero\"\n    end\n    new_position = (agent.position[1] + action[1], agent.position[2] + action[2] + env.wind[agent.position[1]])\n    if all(elem -> 0 < elem < N, new_position)\n        agent.position = new_position\n    end\nend\n\nfunction RxEnvironments.what_to_send(env::WindyGridWorld, agent::WindyGridWorldAgent)\n    return agent.position\nend\n\nfunction RxEnvironments.what_to_send(agent::WindyGridWorldAgent, env::WindyGridWorld)\n    return agent.position\nend\n\nfunction RxEnvironments.add_to_state!(env::WindyGridWorld, agent::WindyGridWorldAgent)\n    push!(env.agents, agent)\nend\n\nfunction reset_env!(environment::RxEnvironments.RxEntity{<:WindyGridWorld,T,S,A}) where {T,S,A}\n    env = environment.decorated\n    for agent in env.agents\n        agent.position = (1, 1)\n    end\n    for subscriber in RxEnvironments.subscribers(environment)\n        send!(subscriber, environment, (1, 1))\n    end\nend\n\nfunction plot_environment(environment::RxEnvironments.RxEntity{<:WindyGridWorld,T,S,A}) where {T,S,A}\n    env = environment.decorated\n    p1 = scatter([env.goal[1]], [env.goal[2]], color=:blue, label=\"Goal\", xlims=(0, 6), ylims=(0, 6))\n    for agent in env.agents\n        p1 = scatter!([agent.position[1]], [agent.position[2]], color=:red, label=\"Agent\")\n    end\n    return p1\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"plot_environment (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"env = RxEnvironment(WindyGridWorld((0, 1, 1, 1, 0), [], (4, 3)))\nagent = add!(env, WindyGridWorldAgent((1, 1)))\nplot_environment(env)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/pomdp_control/#Model-Setup","page":"Pomdp Control","title":"Model Setup","text":"","category":"section"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"First, we'll define our POMDP model structure. We will use the DiscreteTransition node in RxInfer to define the state transition model. The DiscreteTransition node is a special node that accepts any number of Categorical distributions as input, and outputs a Categorical distribution. This means that we can use it to define a state transition model that accepts the previous state and the control as Categorical random variables, but we can also use it to define our observation model! Furthermore, the DiscreteTransition node can be used both for parameter inference and for inference-as-planning, isn't that neat?","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"@model function pomdp_model(p_A, p_B, p_goal, p_control, previous_control, p_previous_state, current_y, future_y, T, m_A, m_B)\n    # Instantiate all model parameters with priors\n    A ~ p_A\n    B ~ p_B\n    previous_state ~ p_previous_state\n    \n    # Paremeter inference\n    current_state ~ DiscreteTransition(previous_state, B, previous_control)\n    current_y ~ DiscreteTransition(current_state, A)\n\n    prev_state = current_state\n    # Inference-as-planning\n    for t in 1:T\n        controls[t] ~ p_control\n        s[t] ~ DiscreteTransition(prev_state, m_B, controls[t])\n        future_y[t] ~ DiscreteTransition(s[t], m_A)\n        prev_state = s[t]\n    end\n    # Goal prior initialization\n    s[end] ~ p_goal\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Now, this model, because we use A and B for every timestep, contains loops, so we have to initialize the inference procedure properly. Furthermore, RxInfer does not support learning a joint probability distribution over the parameters and the states, so we have to supply the model with variational constraints that reflect this:","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"init = @initialization begin\n    q(A) = DirichletCollection(diageye(25) .+ 0.1)\n    q(B) = DirichletCollection(ones(25, 25, 4))\nend\n\nconstraints = @constraints begin\n    q(previous_state, previous_control, current_state, B) = q(previous_state, previous_control, current_state)q(B)\n    q(current_state, current_y, A) = q(current_state, current_y)q(A)\n    q(current_state, s, controls, B) = q(current_state, s, controls), q(B)\n    q(s, future_y, A) = q(s, future_y), q(A)\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Constraints: \n  q(previous_state, previous_control, current_state, B) = q(previous_state,\n previous_control, current_state)q(B)\n  q(current_state, current_y, A) = q(current_state, current_y)q(A)\n  q(current_state, s, controls, B) = q(current_state, s, controls)q(B)\n  q(s, future_y, A) = q(s, future_y)q(A)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Now, in order to use this model, we have to define the priors for the model parameters. The WindyGridworld environment has a 5-by-5 grid, so we need to instantiate a prior 25-by-25 transition matrices for every control! That's quite a lot of parameters, but as we will see, RxInfer will handle this just fine. We will give our agent a control space of 4 actions, so we need to instantiate 4 transition matrices. Furthermore, we have to transform the output from the environment to a 1-in-25 index, and the controls from a 1-in-4 index to a direction tuple.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"The prior on our observation model tells our model that the prior belief is to trust it's observations, but we might be able to deviate from this. However, in this example, the observation model is deterministic and has no noise, meaning that our agent won't have any reason to deviate from the prior.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"p_A = DirichletCollection(diageye(25) .+ 0.1)\np_B = DirichletCollection(ones(25, 25, 4))\n\nfunction grid_location_to_index(pos::Tuple{Int, Int})\n    return (pos[2] - 1) * 5 + pos[1]\nend\n\nfunction index_to_grid_location(index::Int)\n    return (index % 5, index ÷ 5 + 1,)\nend\n\nfunction index_to_one_hot(index::Int)\n    return [i == index ? 1.0 : 0.0 for i in 1:25]\nend\n\ngoal = Categorical(index_to_one_hot(grid_location_to_index((4, 3))))","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Distributions.Categorical{Float64, Vector{Float64}}(\nsupport: Base.OneTo(25)\np: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0\n, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"RxEnvironments.jl is a package that allows us to easily communicate between our agent and our environment. We can send actions to the environment, and the environment will automatically respond with the corresponding observations. In order to access these in our model, we can subscribe to the observations and then use the data function to access the last observation.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Now for our main control loop, we will use a receding horizon control strategy. We will first take an action, observe the environment, and then update our belief. We will then repeat this process for a horizon of 10 steps. In order to learn the parameters of our model, we will conduct this experiment 100 times. We can use the infer function from RxInfer to perform inference on our model.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"# Number of times to run the experiment\nn_experiments = 100\n# Number of steps in each experiment\nT = 4\nobservations = keep(Any)\n# Subscribe the agent to receive observations\nRxEnvironments.subscribe_to_observations!(agent, observations)\nsuccesses = []\n\n\n@showprogress for i in 1:n_experiments\n    # Reset environment to initial state and initialize state belief to starting position (1,1)\n    reset_env!(env)\n    p_s = Categorical(index_to_one_hot(grid_location_to_index((1, 1))))\n    # Initialize previous action as \"down\", as this is neutral from the starting position\n    policy = [Categorical([0.0, 0.0, 1.0, 0.0])]\n    prev_u = [0.0, 0.0, 1.0, 0.0]\n    # Run for T-1 steps in each experiment\n    for t in 1:T\n\n         # Convert policy to actual movement in environment\n         current_action = mode(first(policy))\n         if current_action == 1\n             send!(env, agent, (0, 1))  # Move up \n             prev_u = [1.0, 0.0, 0.0, 0.0]\n         elseif current_action == 2\n             send!(env, agent, (1, 0))  # Move right\n             prev_u = [0.0, 1.0, 0.0, 0.0]\n         elseif current_action == 3\n             send!(env, agent, (0, -1))  # Move down\n             prev_u = [0.0, 0.0, 1.0, 0.0]\n         elseif current_action == 4\n             send!(env, agent, (-1, 0))  # Move left\n             prev_u = [0.0, 0.0, 0.0, 1.0]\n         end\n\n        # Get last observation and convert to one-hot encoding\n        last_observation = index_to_one_hot(grid_location_to_index(RxEnvironments.data(last(observations))))\n        \n        # Perform inference using the POMDP model\n        inference_result = infer(\n            model = pomdp_model(\n                p_A = p_A,  # prior on observation model parameters\n                p_B = p_B,  # prior on transition model parameters\n                T = max(T - t, 1),  # remaining time steps\n                p_previous_state = p_s,  # posterior belief on previous state\n                p_goal = goal,  # prior on goal state\n                p_control = vague(Categorical, 4),  # prior over controls\n                m_A = mean(p_A),\n                m_B = mean(p_B)\n            ),\n            # Provide data for inference\n            data = (\n                previous_control = prev_u,\n                current_y = last_observation,\n                future_y = UnfactorizedData(fill(missing, max(T - t, 1)))\n            ),\n            constraints = constraints,\n            initialization = init,\n            iterations = 10\n        )\n        \n        # Update beliefs based on inference results\n        p_s = last(inference_result.posteriors[:current_state])  # Update state belief\n        policy = last(inference_result.posteriors[:controls])  # Get policy\n\n        # Update model parameters globally for the entire notebook\n        global p_A = last(inference_result.posteriors[:A])  # Update observation model\n        global p_B = last(inference_result.posteriors[:B])  # Update transition model\n\n        if RxEnvironments.data(last(observations)) == (4, 3)\n            break\n        end\n    end\n    if RxEnvironments.data(last(observations)) == (4, 3)\n        push!(successes, true)\n    else\n        push!(successes, false)\n    end\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Now, in this example, we have used a trick: we supplied the mean of p_A and p_B to the model to do the predictions for the future in order to learn the controls. The real reason we did this is because we do not want messages from the future to influence the model parameters, instead only learning the model parameters from past data. This is a simple way to do this, but it is not the only way. We could have supplied the full distribution p_A and p_B to the model, and used A and B in the predictive step as well, but then we would need a separate way to make sure we do not use future messages to influence the model parameters.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"mean(successes)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"0.85","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"We see that our agent is able to learn the optimal policy for this environment, and reaches the goal state in 85% of cases!","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"plot_environment(env)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/pomdp_control/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [92933f4c] ProgressMeter v1.11.0\n  [5ea003d0] RxEnvironments v0.2.15\n  [86711068] RxInfer v4.6.0\n","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Solve-GP-regression-by-SDE","page":"Gp Regression By Ssm","title":"Solve GP regression by SDE","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"In this notebook, we solve a GP regression problem by using \"Stochastic Differential Equation\" (SDE). This method is well described in the dissertation \"Stochastic differential equation methods for spatio-temporal Gaussian process regression.\" by Arno Solin and \"Sequential Inference for Latent Temporal Gaussian Process Models\" by Jouni Hartikainen. The idea of the method is as follows.","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Suppose a function f(x) follows a zero-mean Gaussian Process beginaligned f(x) sim mathcalGP(0 k(xx)) endaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"When the dimensionality of x is 1, we can consider f(x) as a stochastic process over time, i.e. f(t). For a certain classses of covariance functions, f(t) is a solution to an m-th order linear stochastic differential equation (SDE) beginaligned a_0 f(t) + a_1 fracd f(t)dt + dots + a_m fracd^m f(t)dt^m = w(t)  endaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where w(t) is a zero-mean white noise process with spectral density Q_c. If we define a vector-valued function mathbff(t) = (f(t) ddt f(t)dots d^m-1dt^m-1f(t)), then we can rewrite the above SDE under the companion form","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nfracd mathbff(t)dt = mathbfF mathbff(t) + mathbfL w(t) quad (1)\nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where mathbfF and mathbfL are defined based on the choice of covariance functions.  From (1), we have the following state-space model: beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) quad(2a) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) quad(2b) \nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where mathbfA_k = exp(mathbfFDelta t_k), with Delta t_k = t_k+1 - t_k, is called the discrete-time state transition matrix, and mathbfQ_k the process noise covariance matrix. For the computation of mathbfQ_k, we will come back later. According to Arno Solin and Jouni Hartikainen's dissertation, the GP regression problem amounts to the inference problem of the above state-space model, and this can be solved by RTS-smoothing. The state-space model starts from  the initial state f_0 sim mathcalN(mathbf0 mathbfP_0). For stationary covariance function, the SDE has a stationary state f_infty sim mathcalN(mathbf0 mathbfP_infty), where mathbfP_infty is the solution to beginaligned fracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0 quad (mathrmLyapunov  equation) endaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"With this stationary condition, the process noise covariance mathbfQ_k is computed as follows beginaligned mathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  endaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"For one-dimensional problem the SDE representation of the GP is defined by the matrices mathbfF  mathbfL  mathbfQ_c  mathbfP_0 and mathbfH. Once we obtain all the matrices, we can do GP regression by implementing RTS-smoothing on the state-space model (2). In this notebook we will particularly use the Matern class of covariance functions for Gaussian Process.","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"using RxInfer, Random, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Create-state-space-model-for-GP-regression","page":"Gp Regression By Ssm","title":"Create state space model for GP regression","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Here we create a state-space model beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) \nendaligned where y_k is the noisy observation of the function f at time t_k, and sigma^2_noise is the noise variance and assumed to be known.","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"@model function gp_regression(y, P, A, Q, H, var_noise)\n    f_prev ~ MvNormal(μ = zeros(length(H)), Σ = P) #initial state\n    for i in eachindex(y)\n        f[i] ~ MvNormal(μ = A[i] * f_prev,Σ = Q[i])\n        y[i] ~ Normal(μ = dot(H, f[i]), var = var_noise)\n        f_prev = f[i]\n    end\nend","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Generate-data","page":"Gp Regression By Ssm","title":"Generate data","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Random.seed!(10)\nn = 100\nσ²_noise = 0.04;\nt = collect(range(-2, 2, length=n)); #timeline\nf_true = sinc.(t); # true process\nf_noisy = f_true + sqrt(σ²_noise)*randn(n); #noisy process\n\npos = sort(randperm(75)[1:2:75]); \nt_obser = t[pos]; # time where we observe data\n\ny_data = Array{Union{Float64,Missing}}(missing, n)\nfor i in pos \n    y_data[i] = f_noisy[i]\nend\n\nθ = [1., 1.]; # store [l, σ²]\nΔt = [t[1]]; # time difference\nappend!(Δt, t[2:end] - t[1:end-1]);","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Let's-visualize-our-data","page":"Gp Regression By Ssm","title":"Let's visualize our data","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"plot(t, f_true, label=\"True process f(t)\")\nscatter!(t_obser, y_data[pos], label = \"Noisy observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Covariance-function:-Matern-3/2","page":"Gp Regression By Ssm","title":"Covariance function: Matern-3/2","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"The Matern is a stationary covariance function and defined as follows beginaligned k(tau) = sigma^2 frac2^1-nuGamma(nu) left(fracsqrt2nutaul right)^nu K_nuleft(fracsqrt2nutaul right) endaligned where  beginaligned sigma^2 textthe magnitude scale hyperparameter\nl textthe characteristic length-scale\nnu textthe smoothness hyperparameter\nK_nu() textthe modified Bessel function of the second kind endaligned When we say the Matern-3/2, we mean nu=32. The matrices for the state space model are computed as follows beginaligned mathbfF = beginpmatrix 0  1\n-lambda^2  -2lambda endpmatrix quad quad mathbfL = beginpmatrix 0  1 endpmatrix quad quad mathbfP_infty = beginpmatrix sigma^2  0  0  lambda^2sigma^2 endpmatrix quad quad mathbfH = beginpmatrix 1  0 endpmatrix quad quad Q_c = 4lambda^3sigma^2 endaligned  where lambda = fracsqrt3l  From these matrices we can define mathbfA_k and mathbfQ_k.","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"λ = sqrt(3)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 1.];\nH = [1., 0.];\nF = [0. 1.; -λ^2 -2λ]\nP∞ = [θ[2] 0.; 0. (λ^2*θ[2]) ]\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"result_32 = infer(\n    model = gp_regression(P = P∞, A = A, Q = Q, H = H, var_noise = σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Inference results:\n  Posteriors       | available for (f, f_prev)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Covariance-function:-Matern-5/2","page":"Gp Regression By Ssm","title":"Covariance function: Matern-5/2","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Now let's try the Matern-5/2 kernel. The matrices for the SDE representation of the Matern-5/2 are:","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nmathbfF = beginpmatrix\n0  1  0\n0  0  1 \n-lambda^3  -3lambda^2  -3lambda\nendpmatrix quad quad mathbfL = beginpmatrix\n0  0  1\nendpmatrix quad quad mathbfH = beginpmatrix\n1  0  0\nendpmatrix quad quad Q_c = frac163 sigma^2 lambda^5 \nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where lambda = sqrt5  l. To find mathbfP_infty, we solve the Lyapunov equation","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nfracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0\nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"of which the solution is","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nvec(mathbfP_infty) = (mathbfI otimes mathbfF + mathbfFotimesmathbfI)^-1 vec(-mathbfLQ_cmathbfL^T)\nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where vec() is the vectorization operator and otimes denotes the Kronecker product. Now we can find mathbfA_k and mathbfQ_k ","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nmathbfA_k = exp(mathbfFDelta t_k) \nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nmathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  \nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"λ = sqrt(5)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 0., 1.];\nH = [1., 0., 0.];\nF = [0. 1. 0.; 0. 0. 1.;-λ^3 -3λ^2 -3λ]\nQc = 16/3 * θ[2] * λ^5;\n\nI = diageye(3) ; \nvec_P = inv(kron(I,F) + kron(F,I)) * vec(-L * Qc * L'); \nP∞ = reshape(vec_P,3,3);\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"result_52 = infer(\n    model = gp_regression(P = P∞, A = A, Q = Q, H = H, var_noise = σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Inference results:\n  Posteriors       | available for (f, f_prev)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Result","page":"Gp Regression By Ssm","title":"Result","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"slicedim(dim) = (a) -> map(e -> e[dim], a)\n\nplot(t, mean.(result_32.posteriors[:f]) |> slicedim(1), ribbon = var.(result_32.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M32\", title = \"Matern-3/2\", legend =false, lw = 2)\nplot!(t, mean.(result_52.posteriors[:f]) |> slicedim(1), ribbon = var.(result_52.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M52\",legend = :bottomleft, title = \"GPRegression by SSM\", lw = 2)\nplot!(t, f_true,label=\"true process\", lw = 2)\nscatter!(t_obser, f_noisy[pos], label=\"Observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"As we can see from the plot, both cases of Matern kernel provide good approximations (small variance) to the true process at the area with dense observations (namely from t = 0 to around 3.5), and when we move far away from this region the approximated processes become less accurate (larger variance). This result makes sense because GP regression exploits the correlation between observations to predict unobserved points, and the choice of covariance functions as well as their hyperparameters might not be optimal. We can increase the accuracy of the approximated processes by simply adding more observations. This way of improvement does not trouble the state-space method much but it might cause computational problem for naive GP regression, because with N observations the complexity of naive GP regression scales with N^3 while the state-space method scales linearly with N.     ","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/gp_regression_by_ssm/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#Bayesian-Trust-Learning-for-LLM-Routing:-Teaching-Routers-to-Learn-from-Their-Mistakes","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning for LLM Routing: Teaching Routers to Learn from Their Mistakes","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Or: How We Taught Our Router to Stop Worrying and Learn to Love Production Feedback","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-Question-That-Started-It-All","page":"Bayesian Trust Learning","title":"The Question That Started It All","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Picture this: It's 3 AM. Your support system just routed a critical database corruption ticket to Claude Haiku (the 0.25/million token model) because it looked \"simple enough.\" Six hours and three escalations later, your biggest client is furious, and you're wondering why your \"intelligent\" router keeps making the same mistakes.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Meanwhile, across town, your competitor is sending every single ticket to GPT-4 \"just to be safe,\" burning through 100,000 monthly for questions like \"how do I reset my password?\"","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"There has to be a better way. And there is—but it involves teaching your router something most systems never learn: humility.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-Routing-Revolution-(and-Its-Dirty-Little-Secret)","page":"Bayesian Trust Learning","title":"The Routing Revolution (and Its Dirty Little Secret)","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"The LLM routing world has come a long way! OpenRouter elegantly handles 400+ models behind one API (processing over 100M in inference annually), while RouteLLM demonstrates impressive ~85% cost reductions on benchmarks. These are genuinely great tools that have solved real problems. But here's the thing they don't really learn if they were right.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Imagine having a waiter who keeps recommending the \"chef's special ghost pepper curry\" to people who can barely handle mild salsa - and never learns from all those red-faced, teary-eyed customers running for water.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#Your-Tickets-Are-Special-Snowflakes-(Really!)","page":"Bayesian Trust Learning","title":"Your Tickets Are Special Snowflakes (Really!)","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Let me tell you a secret about those benchmark numbers everyone quotes: they were tested on public data, which is about as similar to your production tickets as a philosophy debate is to debugging Kubernetes.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Your tickets have:","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"That weird error code (rule not found) your senior engineer created in 2019\nCustomer complaints that somehow always spike during Mercury retrograde\nTechnical terms that would make GPT-4 cry (\"MethodError: no method matching make_node!\")\nA mysterious correlation between ticket complexity and whether it's submitted before lunch","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Static routers look at this chaos and confidently apply rules learned from \"how to write a haiku\" queries. No wonder they struggle.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#Enter-the-Bayesian-Router:-The-Router-That-Says-\"I-Don't-Know-(Yet)\"","page":"Bayesian Trust Learning","title":"Enter the Bayesian Router: The Router That Says \"I Don't Know (Yet)\"","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Here's our proposition: what if your router could learn from its mistakes?","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Not in the \"we'll retrain the model quarterly\" way, but in the \"oh, I messed that up, let me remember that for next time\" way. You know, like humans do (ideally).","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"using RxInfer\nusing Distributions\n\n# The three stages of router grief:\n# 1. Denial: \"This ticket looks simple!\" (routes to Haiku)\n# 2. Anger: \"Why is the customer escalating?!\" (still routes to Haiku)\n# 3. Acceptance: \"Maybe I should learn from this...\" (our Bayesian approach)","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-Architecture:-Three-Routers-Walk-into-a-Support-Queue...","page":"Bayesian Trust Learning","title":"The Architecture: Three Routers Walk into a Support Queue...","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"We're going to create three different routing \"personalities\" and let them duke it out for your trust. Think of it as \"The Voice\" but for routing algorithms:","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"@model function routing_strategy(y, ticket_context)\n    # Meet our contestants:\n    # 1. The Optimist - \"Everything is fine! Use the cheap model!\"\n    θ_simple ~ simple_router(ticket_context = ticket_context)\n    \n    # 2. The Pessimist - \"It's all terrible! GPT-4 for everything!\"\n    θ_complex ~ complex_router(ticket_context = ticket_context)\n    \n    # 3. The Realist - \"Let's be reasonable about this...\"\n    θ_medium  ~ medium_router(ticket_context = ticket_context)\n    \n    # We start by trusting them equally (how naive!)\n    routing_strategy ~ Categorical(ones(3) ./ 3)\n    \n    # But then reality hits...\n    θ ~ Mixture(switch = routing_strategy, inputs = [θ_simple, θ_medium, θ_complex])\n    \n    # And we learn who's actually worth trusting\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(θ)  # 1 = \"big model needed!\", 0 = \"small model worked\"\n    end\nend","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-Secret-Sauce:-LLMs-All-the-Way-Down","page":"Bayesian Trust Learning","title":"The Secret Sauce: LLMs All the Way Down","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Now, you might be thinking: \"Wait, you're using LLMs to decide which LLM to use? Isn't that like asking the fox to guard the henhouse?\" Yes! But here's the twist: we're asking multiple foxes with different biases, then learning which fox is actually good at guarding (spoiler: it's rarely the one you'd expect).","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"\"\"\"\n    LLMPrior: Where LLMs Judge Other LLMs\n    \n    It's like asking your friends which restaurant to go to,\n    except your friends are language models and the restaurant\n    is also a language model. Welcome to 2025!\n\"\"\"\nstruct LLMPrior end\n\n@node LLMPrior Stochastic [ \n    (b, aliases = [belief]),     # What the LLM believes\n    (m, aliases = [model]),      # Which LLM we're asking\n    (c, aliases = [context]),    # The ticket in question\n    (t, aliases = [task])        # \"Should we panic and use GPT-4?\"\n]","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Each LLM has its own personality when it comes to routing decisions. After extensive psychological profiling (read: we made educated guesses), here's what we found:","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"@rule LLMPrior(:b, Marginalisation) (q_m::PointMass{<:String}, q_c::PointMass{<:String}, q_t::PointMass{<:String}) = begin\n    model_name = q_m.point\n    \n    # GPT models: The anxious overachievers\n    # \"This could be complex! Better use GPT-4! What if it's not complex? \n    #  Still use GPT-4! WHAT IF WE'RE WRONG?!\"\n    if model_name in [\"gpt-5\", \"gpt-4.1\"]\n        return Beta(0.20, 0.05)  # Almost always says \"use complex model\"\n    \n    # Claude models: The confident minimalists\n    # \"Pfft, this is easy. Haiku can handle it. Trust me, I'm Claude.\"\n    elseif model_name in [\"claude-sonnet\", \"claude-opus\"]\n        return Beta(3.0, 9.0)  # Usually says \"use simple model\"\n        \n    # Claude Haiku: The wild card\n    # \"Maybe complex? Maybe simple? Life is uncertain, embrace the chaos!\"\n    elseif model_name in [\"claude-haiku\"]\n        return Beta(3.0, 3.0)  # 50/50 with high variance\n        \n    # GPT-4o-mini: The pessimistic realist\n    # \"It's probably fine with a simple model... but I've been hurt before.\"\n    elseif model_name in [\"gpt-4o-mini\"]\n        return Beta(1.0, 5.0)  # Leans toward simple but cautious\n    end\nend","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"We obviously cheat here, we just don't want to burn tokens on CI each time we run test. In a production, you'd actually call an LLM (we suggest PromptingTools.jl if you stick to Julia)","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"using PromptingTools as PT\nusing Distributions\n\n# Define what we want from the LLM\nstruct BetaParams\n    alpha::Float64  # α parameter (how much we believe \"complex model needed\")\n    beta::Float64   # β parameter (how much we believe \"simple model sufficient\")\nend\n\n@rule LLMPrior(:b, Marginalisation) (q_m::PointMass{<:String}, q_c::PointMass{<:String}, q_t::PointMass{<:String}) = begin\n    context = q_c.point\n    model = q_m.point\n    \n    # Ask the LLM for its honest opinion (in Beta distribution form)\n    response = PT.aiextract(\n        \"\"\"You're a routing expert. Given this ticket:\n           $context\n           \n           Return Beta distribution parameters for P(needs complex model).\n           Higher alpha = more complex, Higher beta = more simple.\"\"\";\n        return_type = BetaParams,\n        model = model,\n        temperature = 0.0  # We want consistency, not creativity\n    )\n    \n    # Sanitize because LLMs sometimes return nonsense\n    α = response.content.alpha > 0 ? response.content.alpha : 1.0\n    β = response.content.beta > 0 ? response.content.beta : 1.0\n    \n    return Beta(α, β)\nend","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#Building-the-Routing-Dream-Team","page":"Bayesian Trust Learning","title":"Building the Routing Dream Team","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Now let's assemble our routers. Each one consults different LLMs and blends their opinions:","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"@model function complex_router(θ, ticket_context)\n    # The premium committee: Only the finest LLMs\n    θ_opus ~ LLMPrior(m = \"claude-opus\", c = ticket_context, t = \"assess_complexity\")\n    θ_gpt  ~ LLMPrior(m = \"gpt-5\", c = ticket_context, t = \"assess_complexity\")\n    \n    # We trust Opus more because it sounds fancier\n    switch ~ Categorical([0.2, 0.8]) \n    θ ~ Mixture(switch = switch, inputs = [θ_opus, θ_gpt])\nend\n\n@model function medium_router(θ, ticket_context)\n    # The balanced committee: Not too hot, not too cold\n    θ_claude ~ LLMPrior(m = \"claude-sonnet\", c = ticket_context, t = \"assess_complexity\")\n    θ_gpt    ~ LLMPrior(m = \"gpt-4.1\", c = ticket_context, t = \"assess_complexity\")\n    \n    # Sonnet gets more weight because it's more poetic about its decisions\n    switch ~ Categorical([0.7, 0.3]) \n    θ ~ Mixture(switch = switch, inputs = [θ_claude, θ_gpt])\nend\n\n@model function simple_router(θ, ticket_context)\n    # The budget committee: \"Have you considered... not spending money?\"\n    θ_claude_haiku ~ LLMPrior(m = \"claude-haiku\", c = ticket_context, t = \"assess_complexity\")\n    θ_gpt_mini     ~ LLMPrior(m = \"gpt-4o-mini\", c = ticket_context, t = \"assess_complexity\")\n    \n    # Slight preference for Haiku because it's more zen about everything\n    switch ~ Categorical([0.6, 0.4]) \n    θ ~ Mixture(switch = switch, inputs = [θ_claude_haiku, θ_gpt_mini])\nend","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-Moment-of-Truth:-Learning-from-Reality","page":"Bayesian Trust Learning","title":"The Moment of Truth: Learning from Reality","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Let's see what happens when we feed our system some real outcomes. Imagine a customer with a money transfer issue:","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"ticket = \"I have been trying to transfer money to my other bank account for the last 10 days but it keeps failing. Can you help me?\"\n\n# The harsh reality of what happened when we routed this:\n# 0 = Ticket was successfully resolved with simple model\n# 1 = Ticket was successfully resolved with complex model\noutcomes = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # 11 simple model worked\n            1.0,                                                    # 1 complex model worked\n            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,                 # 8 simple model worked\n            1.0, 1.0]                                               # 2 complex model worked\n\n# Let the Bayesian magic happen\nresult_joint = infer(\n    model = routing_strategy(ticket_context=ticket), \n    data  = (y = outcomes, ),\n    returnvars = KeepLast(),\n    addons = AddonLogScale(),\n    postprocess = UnpackMarginalPostprocess(),\n)\n\n# The verdict is in!\nprintln(\"Trust scores after learning from reality:\")\nprintln(\"Simple Router: \", mean(result_joint.posteriors[:routing_strategy].p[1]))\nprintln(\"Medium Router: \", mean(result_joint.posteriors[:routing_strategy].p[2]))  \nprintln(\"Complex Router: \", mean(result_joint.posteriors[:routing_strategy].p[3]))","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Trust scores after learning from reality:\nSimple Router: 0.3420062143831011\nMedium Router: 0.4792506103356868\nComplex Router: 0.17874317528121217","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-Results-Are-In:-What-Did-We-Learn?","page":"Bayesian Trust Learning","title":"The Results Are In: What Did We Learn?","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/#Understanding-What-We-Measured","page":"Bayesian Trust Learning","title":"Understanding What We Measured","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"First, let's be crystal clear about what our data means:","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"0 = Ticket was successfully resolved with a SIMPLE model (for example, Haiku worked!) 1 = Ticket required a COMPLEX model (for example, GPT-5 worked!)","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Our data: 20 zeros, 3 ones = 87% of similar tickets were solved by cheap models!","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-Trust-Report-Card","page":"Bayesian Trust Learning","title":"The Trust Report Card","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"After processing our banking tickets, here's how much we trust each router:","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"result_joint.posteriors[:routing_strategy]","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Distributions.Categorical{Float64, Vector{Float64}}(support=Base.OneTo(3), \np=[0.3420062143831011, 0.4792506103356868, 0.17874317528121217])","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Let's translate that from \"statistical gibberish\" to \"executive presentation\":","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"using Plots\nusing Printf\n# using Distributions, Statistics  # keep if you still need them elsewhere\n\n# Backend (GR is default; feel free to switch to plotlyjs(), pyplot(), etc.)\ngr()\n\n# Extract trust scores - remember the order: [complex, medium, simple]\ntrust_scores = result_joint.posteriors[:routing_strategy].p\n\n# Prepare data\nlabels = [\"Simple Router\\n(The Optimist)\",\n          \"Medium Router\\n(The Realist)\",\n          \"Complex Router\\n(The Pessimist)\"]\nx = 1:3\ny = trust_scores .* 100\ncolors = [:darkgreen, :lightblue, :lightcoral]\n\n# Bar plot\nbar(\n    x, y;\n    bar_width = 0.6,\n    fillcolor = colors,\n    linecolor = :black,       # outline like strokecolor\n    linewidth = 2,\n    xticks = (x, labels),\n    ylim = (0, 60),\n    ylabel = \"Trust Level (%)\",\n    title = \"Router Trust Scores: Who Saw It Coming?\",\n    legend = :topright,\n    size = (800, 500)\n)\n\n# Reference line at 33.3% with legend entry\nhline!([33.3]; color = :gray, linestyle = :dash, linewidth = 2, label = \"Initial Trust (Equal)\")\n\n# Value labels above bars\nfor (i, yi) in enumerate(y)\n    annotate!(i, yi + 2, text(@sprintf(\"%.1f%%\", yi), 12, :center, :bottom))\nend\nplot!()","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"(Image: )","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-Verdict-Makes-Perfect-Sense-Now:","page":"Bayesian Trust Learning","title":"The Verdict Makes Perfect Sense Now:","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Complex Router (17.9% trust): \"I told you to use GPT-4... I was wrong 87% of the time!\" 💸\nStarted at 33%, crashed to 17%. The pessimist who always escalates got schooled by reality.\nMedium Router (47.9% trust): \"Sometimes you need complexity, mostly you don't\" ⚖️\nUp from 33%. Balanced approach proved wise.\nSimple Router (34.2% trust): \"Still unsure about this!\"\nSimple router remains unsure about this.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#Diving-Deeper:-What-Each-Router-Learned","page":"Bayesian Trust Learning","title":"Diving Deeper: What Each Router Learned","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/#Complex-Router's-Reality-Check:","page":"Bayesian Trust Learning","title":"Complex Router's Reality Check:","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"println(result_joint.posteriors[:θ_complex])","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"BayesBase.MixtureDistribution{Distributions.Beta{Float64}, Float64}(Distrib\nutions.Beta{Float64}[Distributions.Beta{Float64}(α=6.0, β=28.0), Distributi\nons.Beta{Float64}(α=3.2, β=19.05)], [0.7379918929276147, 0.2620081070723853\n])","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"After seeing the data, we can conclude that our trust in the complex router was shattered.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"println(result_joint.posteriors[:θ_simple])","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"BayesBase.MixtureDistribution{Distributions.Beta{Float64}, Float64}(Distrib\nutions.Beta{Float64}[Distributions.Beta{Float64}(α=6.0, β=22.0), Distributi\nons.Beta{Float64}(α=4.0, β=24.0)], [0.26239067055393556, 0.7376093294460644\n])","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"The simple router switched to believe in GPT-4o-mini.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"println(result_joint.posteriors[:θ_medium])","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"BayesBase.MixtureDistribution{Distributions.Beta{Float64}, Float64}(Distrib\nutions.Beta{Float64}[Distributions.Beta{Float64}(α=6.0, β=28.0), Distributi\nons.Beta{Float64}(α=3.2, β=19.05)], [0.9633551632505475, 0.0366448367494525\n8])","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"The medium router switched to believe to Sonnet and in fact turned out to be right most of the time.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/#The-\"Aha!\"-Moments","page":"Bayesian Trust Learning","title":"The \"Aha!\" Moments","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/#Discovery-#1:-The-87/13-Rule","page":"Bayesian Trust Learning","title":"Discovery #1: The 87/13 Rule","text":"","category":"section"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"simple_share  = result_joint.posteriors[:routing_strategy].p[1]\nmedium_share  = result_joint.posteriors[:routing_strategy].p[2]\ncomplex_share = result_joint.posteriors[:routing_strategy].p[3];","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"# Normalize defensively\ns = simple_share + medium_share + complex_share\nsimple_share, medium_share, complex_share = simple_share/s, medium_share/s, complex_share/s\n\n# --- Model costs (edit as needed) ---\nsimple_cost  = 0.03   # e.g., Haiku per request (placeholder)\nmedium_cost  = 0.10   # whatever you pay for models within medium router\ncomplex_cost = 3.00   # e.g., GPT-5 per request (placeholder)\n\n# --- Cost per 100 tickets ---\nblind_cost_per100  = 100 * complex_cost\nperfect_per100     = 100 * (simple_share * simple_cost +\n                            medium_share * medium_cost +\n                            complex_share * complex_cost)\n# Escalate policy: try Simple → Medium → Complex\nescalate_per100    = 100 * (simple_cost +\n                            (1 - simple_share) * medium_cost +\n                            complex_share * complex_cost)\n\nsavings_perfect_pct  = 100 * (1 - perfect_per100  / blind_cost_per100)\nsavings_escalate_pct = 100 * (1 - escalate_per100 / blind_cost_per100)\n\nprintln(\"🎯 Reality-informed routing mix:\")\nprintln(\"├─ Simple: $(round(simple_share * 100,  digits=1))%\")\nprintln(\"├─ Medium: $(round(medium_share * 100,  digits=1))%\")\nprintln(\"└─ Complex: $(round(complex_share * 100, digits=1))%\")\n\nprintln(\"\\n💰 Cost Impact (per 100 tickets):\")\nprintln(\"├─ Blind Complex (send all to Complex): $(round(blind_cost_per100, digits=2))\")\nprintln(\"├─ Smart routing (perfect):             $(round(perfect_per100, digits=2))  → savings $(round(savings_perfect_pct, digits=1))%\")\nprintln(\"└─ Smart routing (escalate S→M→C):      $(round(escalate_per100, digits=2)) → savings $(round(savings_escalate_pct, digits=1))%\")","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"🎯 Reality-informed routing mix:\n├─ Simple: 34.2%\n├─ Medium: 47.9%\n└─ Complex: 17.9%\n\n💰 Cost Impact (per 100 tickets):\n├─ Blind Complex (send all to Complex): 300.0\n├─ Smart routing (perfect):             59.44  → savings 80.2%\n└─ Smart routing (escalate S→M→C):      63.2 → savings 78.9%","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"# Bayesian routing: Sample from learned posteriors to make decisions (we don't do continuous learning here (yet))\n# How that could look like:\n\n# Helper to sample from MixtureDistribution (not natively supported)\nsample_mixture(m::MixtureDistribution) = rand(m.components[rand(Categorical(m.weights))])\n\nfunction route(posteriors, ticket_context)\n\n    # here your logic to cluster tickets into a category\n\n    # Sample which router to use\n    router_idx = rand(posteriors[:routing_strategy])\n    \n    # Get complexity from selected router\n    router_posteriors = [posteriors[:θ_complex], posteriors[:θ_medium], posteriors[:θ_simple]]\n    complexity = sample_mixture(router_posteriors[router_idx])\n    \n    # Decision based on sampled complexity  \n    model = complexity > 0.5 ? \"complex\" : \"simple\"\n    \n    return (model=model, complexity=complexity, router=router_idx)\nend\n\n# Use it\nticket = \"I have been trying to transfer money to my other bank account for the last 10 days but it keeps failing. Can you help me?\"\n\ndecision = route(result_joint.posteriors, ticket)\nprintln(\"Route to $(decision.model)\")","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Route to simple","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"These results brought to you by Bayes' Theorem: Teaching expensive AI models humility since 1763.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"P.S. - The Complex Router is now in therapy, learning to let go of its need to overcomplicate everything. The Medium Router has been promoted to Chief Optimization Officer.","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/experimental_examples/bayesian_trust_learning/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n","category":"page"},{"location":"categories/experimental_examples/bayesian_trust_learning/","page":"Bayesian Trust Learning","title":"Bayesian Trust Learning","text":"","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#RTS-vs-BIFM-Smoothing","page":"Rts Vs Bifm Smoothing","title":"RTS vs BIFM Smoothing","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"___Credits to Martin de Quincey___","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"This notebook performs Kalman smoothing on a factor graph using message passing, based on the BIFM Kalman smoother. This notebook is based on:","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"F. Wadehn, “State Space Methods with Applications in Biomedical Signal Processing,” ETH Zurich, 2019. Accessed: Jun. 16, 2021. [Online]. Available: https://www.research-collection.ethz.ch/handle/20.500.11850/344762\nH. Loeliger, L. Bruderer, H. Malmberg, F. Wadehn, and N. Zalmai, “On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing,” in 2016 Information Theory and Applications Workshop (ITA), Jan. 2016, pp. 1–10. doi: 10.1109/ITA.2016.7888168.","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"We perform Kalman smoothing in the linear state space model, represented by:","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"beginaligned\n    Z_k+1 = A Z_k + B U_k \n    Y_k = C Z_k + W_k\nendaligned","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"with observations Y_k, latent states Z_k and inputs U_k. W_k is the observation noise. A in mathrmR^n times n, B in mathrmR^n times m and C in mathrmR^d times n are the transition matrices in the model. Here n, m and d denote the dimensionality of the latent, input and output dimension, respectively.","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"The corresponding probabilistic model can be represented as ","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"beginaligned\n        p(y z u)\n        = p(z_0) prod_k=1^N p(y_k mid z_k) p(z_kmid z_k-1 u_k-1) p(u_k-1) \n        = mathcalN(z_0 mid mu_z_0 Sigma_z_0) left( prod_k=1^N mathcalN(y_k mid C z_k Sigma_W) delta(z_k - (Az_k-1 + Bu_k-1)) mathcalN(u_k-1 mid mu_i_k-1 Sigma_u_k-1) right)\nendaligned","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Import-packages","page":"Rts Vs Bifm Smoothing","title":"Import packages","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"using RxInfer, Random, LinearAlgebra, BenchmarkTools, ProgressMeter, Plots, StableRNGs","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Data-generation","page":"Rts Vs Bifm Smoothing","title":"Data generation","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"function generate_parameters(dim_out::Int64, dim_in::Int64, dim_lat::Int64; seed::Int64 = 123)\n    \n    # define noise levels\n    input_noise  = 500.0\n    output_noise = 50.0\n\n    # create random generator for reproducibility\n    rng = StableRNG(seed)\n\n    # generate matrices, input statistics and noise matrices\n    A      = diagm(0.8 .* ones(dim_lat) .+ 0.2 * rand(rng, dim_lat))                                            # size (dim_lat x dim_lat)\n    B      = rand(rng, dim_lat, dim_in)                                                                         # size (dim_lat x dim_in)\n    C      = rand(rng, dim_out, dim_lat)                                                                        # size (dim_out x dim_lat)\n    μu     = rand(rng, dim_in) .* collect(1:dim_in)                                                             # size (dim_in x 1)\n    Σu     = input_noise  .* collect(Hermitian(randn(rng, dim_in, dim_in) + diagm(10 .+ 10*rand(rng, dim_in))))      # size (dim_in x dim_in)\n    Σy     = output_noise .* collect(Hermitian(randn(rng, dim_out, dim_out) + diagm(10 .+ 10*rand(rng, dim_out))))   # size (dim_out x dim_out)\n    Wu     = inv(Σu)\n    Wy     = inv(Σy)\n    \n    # return parameters\n    return A, B, C, μu, Σu, Σy, Wu, Wy\n\nend;","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"function generate_data(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Σu::Array{Float64,2}, Σy::Array{Float64,2}; seed::Int64 = 123)\n        \n    # create random data generator\n    rng = StableRNG(seed)\n    \n    # preallocate space for variables\n    z = Vector{Vector{Float64}}(undef, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n    u = rand(rng, MvNormal(μu, Σu), nr_samples)'\n    \n    # set initial value of latent states\n    z_prev = zeros(size(A,1))\n    \n    # generate data\n    for i in 1:nr_samples\n\n        # generate new latent state\n        z[i] = A * z_prev + B * u[i,:]\n\n        # generate new observation\n        y[i] = C * z[i] + rand(rng, MvNormal(zeros(dim_out), Σy))\n        \n        # generate new observation\n        z_prev .= z[i]\n        \n    end\n    \n    # return generated data\n    return z, y, u\n    \nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"# specify settings\nnr_samples = 200\ndim_out = 3\ndim_in = 3\ndim_lat = 25\nseed = 42\n\n# generate parameters\nA, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(dim_out, dim_in, dim_lat; seed = seed);\n            \n# generate data\ndata_z, data_y, data_u = generate_data(nr_samples, A, B, C, μu, Σu, Σy);\n\n# visualise data\np = Plots.plot(xlabel = \"sample\", ylabel = \"observations\")\n# plot each dimension independently\nfor i in 1:dim_out\n    Plots.scatter!(p, getindex.(data_y, i), label = \"y_$i\", alpha = 0.5, ms = 2)\nend\np","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Model-specification","page":"Rts Vs Bifm Smoothing","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"@model function RTS_smoother(y, A, B, C, μu, Wu, Wy)\n    \n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    dim_out = size(C, 1)\n    \n    # set initial hidden state\n    z_prev ~ MvNormal(mean = zeros(dim_lat), precision = 1e-5*diagm(ones(dim_lat)))\n\n    # loop through observations\n    for i in eachindex(y)\n\n        # specify input as random variable\n        u[i] ~ MvNormal(mean = μu, precision = Wu)\n        \n        # specify updated hidden state\n        z[i] ~ A * z_prev + B * u[i]\n        \n        # specify observation\n        y[i] ~ MvNormal(mean = C * z[i], precision = Wy)\n        \n        # update last/previous hidden state\n        z_prev = z[i]\n\n    end\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"@model function BIFM_smoother(y, A, B, C, μu, Wu, Wy)\n\n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    \n    # set priors\n    z_prior ~ MvNormal(mean = zeros(dim_lat), precision = 1e-5*diagm(ones(dim_lat)))\n    z[1]  ~ BIFMHelper(z_prior)\n    \n    # loop through observations\n    for i in eachindex(y)\n\n        # specify input as random variable\n        u[i]   ~ MvNormal(mean = μu, precision = Wu)\n\n        # specify observation\n        yt[i]  ~ BIFM(u[i], z[i], new(z[i+1])) where { meta = BIFMMeta(A, B, C) }\n        y[i]   ~ MvNormal(mean = yt[i], precision = Wy)\n    end\n    \n    # set final value\n    z[end] ~ MvNormal(mean = zeros(dim_lat), precision = zeros(dim_lat, dim_lat))\nend\n\n@constraints function bifm_constraint()\n    q(z_prior,z) = q(z_prior)q(z)\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"bifm_constraint (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Probabilistic-inference","page":"Rts Vs Bifm Smoothing","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"function inference_RTS(data_y, A, B, C, μu, Wu, Wy)\n    \n    # In this task the inference is unstable and can diverge\n    meta = @meta begin \n        *() -> ReactiveMP.MatrixCorrectionTools.ClampSingularValues(tiny, Inf)\n    end\n    \n    result = infer(\n        model      = RTS_smoother(A = A, B = B, C = C, μu = μu, Wu = Wu, Wy = Wy),\n        data       = (y = data_y, ),\n        returnvars = (z = KeepLast(), u = KeepLast()),\n        meta = meta\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"inference_RTS (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"function inference_BIFM(data_y, A, B, C, μu, Wu, Wy)\n    result = infer(\n        model      = BIFM_smoother(A = A, B = B, C = C, μu = μu, Wu = Wu, Wy = Wy),\n        data       = (y = data_y, ),\n        constraints = bifm_constraint(),\n        returnvars = (z = KeepLast(), u = KeepLast())\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"inference_BIFM (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Experiments-for-200-observations","page":"Rts Vs Bifm Smoothing","title":"Experiments for 200 observations","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"z_BIFM, u_BIFM = inference_BIFM(data_y, A, B, C, μu, Wu, Wy)\nz_RTS, u_RTS = inference_RTS(data_y, A, B, C, μu, Wu, Wy);","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"ax1 = Plots.plot(title = \"RTS smoother\", xlabel = \"sample\", ylabel = \"latent state z\")\nax2 = Plots.plot(title = \"BIFM smoother\", xlabel = \"sample\", ylabel = \"latent state z\")\n\nmz_RTS = mean.(z_RTS)\nmz_BIFM = mean.(z_BIFM)\n\n# Do not plot all latent states, otherwise the output is just too cluttered\n# The main idea here is to check that both algorithms return the (approximately) same output\nfor i in 1:5\n    Plots.scatter!(ax1, getindex.(data_z, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)\n    Plots.plot!(ax1, getindex.(mz_RTS, i), label = nothing)\n    Plots.scatter!(ax2, getindex.(data_z, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)    \n    Plots.plot!(ax2, getindex.(mz_BIFM, i), label = nothing)\nend\n\nPlots.plot(ax1, ax2, layout = @layout([ a; b ]))","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"ax1 = Plots.plot(title = \"RTS smoother\", xlabel = \"sample\", ylabel = \"latent state u\")\nax2 = Plots.plot(title = \"BIFM smoother\", xlabel = \"sample\", ylabel = \"latent state u\")\n\nrdata_u = collect(eachrow(data_u))\nmu_RTS = mean.(u_RTS)\nmu_BIFM = mean.(u_BIFM)\n\n# Do not plot all latent states, otherwise the output is just too cluttered\n# The main idea here is to check that both algorithms return the (approximately) same output\nfor i in 1:1\n    Plots.scatter!(ax1, getindex.(rdata_u, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)\n    Plots.plot!(ax1, getindex.(mu_RTS, i), label = nothing)\n    Plots.scatter!(ax2, getindex.(rdata_u, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)    \n    Plots.plot!(ax2, getindex.(mu_BIFM, i), label = nothing)\nend\n\nPlots.plot(ax1, ax2, layout = @layout([ a; b ]))","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Benchmark","page":"Rts Vs Bifm Smoothing","title":"Benchmark","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"# This example runs in our documentation pipeline, benchmark executes approximatelly in 20 minutes so we bypass it in the documentation\n# For those who are interested in exact benchmark numbers clone this example and set `run_benchmark = true`\nrun_benchmark = false\n\nif run_benchmark\n    trials_range = 30\n    trials_n = 500\n    trials_RTS  = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n    trials_BIFM = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n\n\n    @showprogress for k = 1 : trials_range\n\n        # generate parameters\n        local A, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(3, 3, k);\n                    \n        # generate data|\n        local data_z, data_y, data_u = generate_data(trials_n, A, B, C, μu, Σu, Σy);\n\n        # run inference\n        trials_RTS[k] = @benchmark inference_RTS($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n        trials_BIFM[k] = @benchmark inference_BIFM($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n\n    end\n\n    m_RTS = [median(trials_RTS[k].times) for k=1:trials_range] ./ 1e9\n    q1_RTS = [quantile(trials_RTS[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_RTS = [quantile(trials_RTS[k].times, 0.75) for k=1:trials_range] ./ 1e9\n    m_BIFM = [median(trials_BIFM[k].times) for k=1:trials_range] ./ 1e9\n    q1_BIFM = [quantile(trials_BIFM[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_BIFM = [quantile(trials_BIFM[k].times, 0.75) for k=1:trials_range] ./ 1e9;\n\n    p = Plots.plot(ylabel = \"duration [sec]\", xlabel = \"latent state dimension\", title = \"Benchmark\", yscale = :log)\n    p = Plots.plot!(p, m_RTS, ribbon = ((q1_RTS .- q3_RTS) ./ 2), color = \"blue\", label = \"mean (RTS)\")\n    p = Plots.plot!(p, 1:trials_range, m_BIFM, ribbon = ((q1_BIFM .- q3_BIFM) ./ 2), color = \"orange\", label = \"mean (BIFM)\")\n    Plots.savefig(p, \"rts_bifm_benchmark.png\")\n    p\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/rts_vs_bifm_smoothing/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [91a5bcdd] Plots v1.41.1\n  [92933f4c] ProgressMeter v1.11.0\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/#Forgetting-Factors-for-Online-Inference","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors for Online Inference","text":"","category":"section"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"In this example, we explore a technique for online Bayesian inference which is called forgetting factors. When working with real-world data that changes over time, posterior distributions can become \"stuck\" in the past, leading to poor performance on recent observations. Forgetting factors provide an elegant solution to this problem. Imagine you're monitoring a sensor network, tracking financial markets, or analyzing social media trends. The underlying dynamics of these systems change over time. This can lead to the following problems when doing online inference:","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"The model becomes overly confident in outdated parameters\nInability to track changing patterns in the data\nContinuing to update parameters that are no longer relevant","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/#The-Solution:-Forgetting-Factors","page":"Forgetting Factors For Online Inference","title":"The Solution: Forgetting Factors","text":"","category":"section"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Forgetting factors implement exponential forgetting, where older observations are gradually \"forgotten\" with a decay rate controlled by a parameter Γ (gamma). This allows the model to adapt to recent changes in the data and maintain stability by not forgetting everything at once.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"We will demonstrate the technique with a simple example: a Kalman filter tracking a non-stationary signal.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Note: For a related example that solves a similar non-stationary tracking problem using a different approach, see the Hierarchical Gaussian Filter example. This example specifically focuses on using Forgetting Factors.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"using RxInfer, StableRNGs, Plots","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/#Setting-Up-the-Problem","page":"Forgetting Factors For Online Inference","title":"Setting Up the Problem","text":"","category":"section"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Let's start by creating a realistic scenario with non-stationary noise. We'll generate a signal where the observation precision (inverse variance) changes sinusoidally over time, simulating real-world scenarios like varying sensor accuracy or changing market volatility.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Key Insight: Notice how the true precision varies significantly over time (from ~20 to ~180). As we show later, a stationary model will fail to capture this variation while the forgetting factor model will adapt to it.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"rng = StableRNG(1234)\ntime_points = 0.0:0.1:300.0\nunstationary_noise = 100 .+ 80 .* sin.(0.075 * time_points)\ndata_points = map(d -> rand(rng, NormalMeanPrecision(0.0, d[2])), zip(time_points, unstationary_noise))\n\np1 = plot(time_points, data_points, seriestype = :line, title = \"Signal\", xlabel = \"Time\", ylabel = \"Observation\")\np2 = plot(time_points, unstationary_noise, seriestype = :line, title = \"True Precision of Observations\", xlabel = \"Time\", ylabel = \"Std\")\nplot(p1, p2, layout = (2, 1))","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/#Understanding-the-Generated-Data","page":"Forgetting Factors For Online Inference","title":"Understanding the Generated Data","text":"","category":"section"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Let's examine the data we just generated:","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"The top plot shows our observations over time. These are random samples drawn from a normal distribution with mean 0 and a time-varying precision.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"The bottom plot shows how the true precision (inverse variance) of the noise changes over time:","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"It oscillates sinusoidally between approximately 20 and 180\nThe period of oscillation is relatively slow, allowing us to track the changes\nThis simulates real-world scenarios where noise characteristics drift over time","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"This setup creates an interesting challenge for our inference: the model needs to adapt its beliefs about the noise precision as the true value changes. A static model would try to find a single \"best\" precision value, while we want our model to track these changes over time.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Let's create a simple model that tracks a non-stationary signal. We'll use a Kalman filter with a state variable and an unknown noise precision that we'll try to infer. The model assumes that the state evolves with small random changes (controlled by a fixed precision of 1000.0) and generates observations with unknown precision.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"@model function kalman_filter(observation, state_prior_mean, state_prior_var, obs_noise_shape, obs_noise_rate)\n    state_prior ~ Normal(mean = state_prior_mean, var = state_prior_var)\n    next_state ~ Normal(mean = state_prior, precision = 1000.0)\n    noise_precision ~ Gamma(shape = obs_noise_shape, rate = obs_noise_rate)\n    observation ~ Normal(mean = next_state, precision = noise_precision)\nend","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Now let's set up the inference process. First, we'll create an update rule that specifies how the model parameters should be updated after each observation. This includes updating our beliefs about both the state of the system and the noise precision. We'll also define an initialization that sets reasonable starting values for our variables.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"autoupdates_without_forgetting = @autoupdates begin\n    # Update the state prior mean and variance\n    state_prior_mean = mean(q(next_state))\n    state_prior_var = var(q(next_state))\n    # Update the noise precision\n    obs_noise_shape = shape(q(noise_precision))\n    obs_noise_rate = rate(q(noise_precision))\nend\n\ninitialization = @initialization begin\n    q(next_state) = NormalMeanPrecision(0.0, 1000.0)\n    q(noise_precision) = GammaShapeRate(1.0, 0.005)\nend","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Initial state: \n  q(next_state) = ExponentialFamily.NormalMeanPrecision{Float64}(μ=0.0, w=1\n000.0)\n  q(noise_precision) = ExponentialFamily.GammaShapeRate{Float64}(a=1.0, b=0\n.005)","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Let's run the inference using our model and update rules. We'll use the infer function with our specified kalman_filter model and autoupdates_without_forgetting update rules. This will process our data points sequentially and track how the model's beliefs evolve over time.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"result_without_forgetting = infer(\n    model = kalman_filter(),\n    data = (observation = data_points,),\n    autoupdates = autoupdates_without_forgetting,\n    initialization = initialization,\n    autostart = true,\n    constraints = MeanField(),\n    historyvars = (noise_precision = KeepLast(), ),\n    iterations = 50,\n    keephistory = length(data_points)\n)\n\ninferred_without_forgetting = result_without_forgetting.history[:noise_precision]\n\nplot(time_points, mean.(inferred_without_forgetting), ribbon = 3std.(inferred_without_forgetting), seriestype = :line, title = \"Inferred Noise Precision\", xlabel = \"Time\", ylabel = \"Std\", label = \"Inferred (+/- 3 std)\")\nplot!(time_points, unstationary_noise, seriestype = :line, title = \"True Precision of Observations\", xlabel = \"Time\", ylabel = \"Std\", label = \"True\")","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"The plot above shows how our model's inference of the noise precision compares to the true underlying precision. While the model successfully tracks the general magnitude of the precision, it fails to capture the non-stationary nature of the data. This is because our current inference approach assumes the noise precision is constant over time, leading to estimates that converge to a fixed value rather than adapting to the changing precision levels. This limitation motivates the need for a more flexible approach that can handle non-stationary dynamics.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/#Introducing-Forgetting-Factors","page":"Forgetting Factors For Online Inference","title":"Introducing Forgetting Factors","text":"","category":"section"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"To address the non-stationary nature of our data, we introduce a forgetting factor Γ. This factor controls how quickly the model forgets past observations, allowing it to focus more on recent data.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"The forgetting factor Γ is a parameter that ranges between 0 and 1. A value of Γ = 1 means the model does not forget anything, while a value of Γ = 0 means the model forgets everything. We will apply this technique to the parameters of the prior distribution of the noise precision. For this, we will create a callable structure that updates the parameters of the noise precision distribution based on the forgetting factor.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"mutable struct UpdateParamsWithForgetting\n    forgetting_factor::Float64\n    previous_params::Union{Nothing, Tuple{Float64, Float64}}\n\n    function UpdateParamsWithForgetting(; Γ = 0.99)\n        return new(Γ, nothing)\n    end\nend\n\n# Create a callable structure\n# https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects\n# it allows us to have a local state, in this case `previous_params`\nfunction (update::UpdateParamsWithForgetting)(gamma_posterior)\n\n    if isnothing(update.previous_params)\n        update.previous_params = (shape(gamma_posterior), rate(gamma_posterior))\n        return (shape(gamma_posterior), rate(gamma_posterior))\n    else \n        shape_prev, rate_prev = update.previous_params\n        shape_current, rate_current = (shape(gamma_posterior), rate(gamma_posterior))\n        shape_delta = shape_current - shape_prev\n        rate_delta = rate_current - rate_prev\n\n        @assert (shape_delta > 0) && (rate_delta > 0) \"Shape and rate deltas must be strictly positive\"\n        \n        Γ = update.forgetting_factor\n\n        f_shape = shape_prev * Γ + shape_delta\n        f_rate = rate_prev * Γ + rate_delta\n\n        update.previous_params = (f_shape, f_rate)\n        return (f_shape, f_rate)\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Next, we define a function that implements the forgetting factor mechanism in our inference process. The function autoupdates_with_forgetting takes a forgetting factor parameter and creates an update procedure that maintains the state estimates while applying forgetting to the noise precision parameters. It first updates the state prior mean and variance from the current beliefs, then creates an instance of our UpdateParamsWithForgetting structure with the specified forgetting factor. This structure will handle the gradual forgetting of old noise precision information. Finally, it updates the noise precision parameters using this forgetting mechanism, returning both the shape and rate parameters of the gamma distribution.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"# we make it a function because it must create `obs_update` every time it is called\n@autoupdates function autoupdates_with_forgetting(; forgetting_factor)\n    # Update the state prior mean and variance\n    state_prior_mean = mean(q(next_state))\n    state_prior_var = var(q(next_state))\n\n    # Update the forgetting factor, callable structure\n    obs_update = UpdateParamsWithForgetting(Γ = forgetting_factor)\n\n    # Update the noise precision, two at once\n    obs_noise_shape, obs_noise_rate = obs_update(q(noise_precision))\nend","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"autoupdates_with_forgetting (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Everything is ready to run the inference with the forgetting factor.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"result_with_forgetting = infer(\n    model = kalman_filter(),\n    data = (observation = data_points,),\n    autoupdates = autoupdates_with_forgetting(forgetting_factor = 0.97),\n    initialization = initialization,\n    autostart = true,\n    constraints = MeanField(),\n    historyvars = (noise_precision = KeepLast(), ),\n    iterations = 100,\n    keephistory = length(data_points)\n)\n\ninferred_with_forgetting = result_with_forgetting.history[:noise_precision]\n\nplot(time_points, mean.(inferred_with_forgetting), ribbon = 3std.(inferred_with_forgetting), seriestype = :line, title = \"Inferred Noise Precision\", xlabel = \"Time\", ylabel = \"Std\", label = \"Inferred (+/- 3 std)\")\nplot!(time_points, unstationary_noise, seriestype = :line, title = \"True Precision of Observations\", xlabel = \"Time\", ylabel = \"Precision\", label = \"True\")","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"The plot shows how the forgetting factor mechanism allows the model to adapt to changes in the noise precision over time. The blue line and shaded area represent the inferred noise precision with uncertainty bounds (±3 standard deviations), while the orange line shows the true underlying noise precision that was used to generate the data. We can see that the model successfully tracks the changing noise levels, though with some lag due to the forgetting factor. The uncertainty bounds (shaded area) indicate the model's confidence in its estimates, which varies as it adapts to the non-stationary noise. This demonstrates that our forgetting factor approach effectively handles time-varying noise characteristics in the system.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"The result depends on the choice of the forgetting factor. A higher forgetting factor (closer to 1) means the model has a longer memory and adapts more slowly to changes, while a lower value makes it more responsive but potentially more sensitive to noise. Here we try a forgetting factor of 0.99 instead of the previous 0.97 to see how it affects the inference.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"result_with_forgetting = infer(\n    model = kalman_filter(),\n    data = (observation = data_points,),\n    autoupdates = autoupdates_with_forgetting(forgetting_factor = 0.99),\n    initialization = initialization,\n    autostart = true,\n    constraints = MeanField(),\n    historyvars = (noise_precision = KeepLast(), ),\n    iterations = 100,\n    keephistory = length(data_points)\n)\n\ninferred_with_forgetting = result_with_forgetting.history[:noise_precision]\n\nplot(time_points, mean.(inferred_with_forgetting), ribbon = 3std.(inferred_with_forgetting), seriestype = :line, title = \"Inferred Noise Precision\", xlabel = \"Time\", ylabel = \"Std\", label = \"Inferred (+/- 3 std)\")\nplot!(time_points, unstationary_noise, seriestype = :line, title = \"True Precision of Observations\", xlabel = \"Time\", ylabel = \"Precision\", label = \"True\")","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"With a higher forgetting factor of 0.99, we can observe that the model adapts more slowly to changes in the noise precision compared to the previous case with 0.97. This results in smoother estimates but increased lag in tracking sudden changes. The trade-off between responsiveness and stability is evident - while the 0.99 forgetting factor provides more stable estimates by being less sensitive to temporary fluctuations, it takes longer to adapt to genuine changes in the underlying noise characteristics.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"This example demonstrates how forgetting factors can be effectively used in online inference to handle non-stationary data. By carefully choosing the forgetting factor, we can balance between the model's ability to retain historical information and its adaptability to changing conditions. This approach is particularly valuable in real-world applications where system characteristics evolve over time and require continuous adaptation of the inference process.","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/forgetting_factors_for_online_inference/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/forgetting_factors_for_online_inference/","page":"Forgetting Factors For Online Inference","title":"Forgetting Factors For Online Inference","text":"","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"","category":"page"},{"location":"categories/problem_specific/litter_model/#Litter-Model","page":"Litter Model","title":"Litter Model","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Adapted from LearnableLoopAI","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"using RxInfer, Random, Distributions, Plots, LaTeXStrings, XLSX, DataFrames","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"In this project the client is responsible for the delittering of a mile-long beach walk-way in the Pacific Northwest in the USA. The density of foot traffic is roughly uniform along its length. Volunteers provide their services for cleaning up litter.","category":"page"},{"location":"categories/problem_specific/litter_model/#Symbols-Nomenclature-Notation-(KUF)","page":"Litter Model","title":"Symbols | Nomenclature |Notation (KUF)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"informed by Powell Universal Framework (PUF), Bert de Vries, AIF literature","category":"page"},{"location":"categories/problem_specific/litter_model/#Taxonomy-of-Machine-Learning","page":"Litter Model","title":"Taxonomy of Machine Learning","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#Supervised-Learning","page":"Litter Model","title":"Supervised Learning","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#State-Functions","page":"Litter Model","title":"State Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Provision (Acquisition)\nmathbfp_i = f_p(i)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Functions","page":"Litter Model","title":"Observation Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Response: mathbfr_i = f_r(brevemathbfs_i)\nObservation with Noise: mathbfy_i = mathbfbreves_t + mathbfv_t\nCovariate noise (optional)\nObservation noise: mathbfv_i = mathcalN(mathbfbrevem_i mathbfbreveSigma_V)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Sets","page":"Litter Model","title":"Observation Sets","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Without state noise: (mathbfbreves y)\nWith state noise: (mathbfz y)","category":"page"},{"location":"categories/problem_specific/litter_model/#Unsupervised-Learning","page":"Litter Model","title":"Unsupervised Learning","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#State-Functions-2","page":"Litter Model","title":"State Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Provision (Acquisition)\nmathbfp_i = f_p(i)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Functions-2","page":"Litter Model","title":"Observation Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Response: mathbfr_i = f_r(brevemathbfs_i)\nDirect Observation: mathbfy_t = mathbfbreves_t","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Sets-2","page":"Litter Model","title":"Observation Sets","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Unordered independent observations: y","category":"page"},{"location":"categories/problem_specific/litter_model/#Sequential/Series-Learning","page":"Litter Model","title":"Sequential/Series Learning","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#State-Functions-3","page":"Litter Model","title":"State Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Provision (Transition)\nBase transition: mathbfp_t = f_p(brevemathbfs_t-1)\nComplete state equation:   mathbfbreves_t = f_B(mathbfbreves_t-1)+ f_E(mathbfa_t) + mathbfw_dt + mathbfw_t\nComponents:\nAction: f_E(mathbfa_t) (optional)\nSystem noise: mathbfw_t = mathcalN(mathbfp_t mathbfbreveSigma_W)\nDisturbance/exogenous: mathbfw_dt (optional)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Functions-3","page":"Litter Model","title":"Observation Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Response: mathbfr_t = f_r(brevemathbfs_t) = f_A(brevemathbfs_t) = brevemathbfA brevemathbfs_t\nObservation with Noise: mathbfy_t = f_A(mathbfbreves_t) + mathbfv_t\nObservation noise: mathbfv_t = mathcalN(mathbfr_t mathbfbreveSigma_V)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Sequence","page":"Litter Model","title":"Observation Sequence","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Ordered correlated observations y (time/spatial)","category":"page"},{"location":"categories/problem_specific/litter_model/#Overall-Structure","page":"Litter Model","title":"Overall Structure","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Experiment has one-to-many Batches","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"- Batch (into the page) has one-to-many Sequences\n\t- Sequence (down the page) has one-to-many Datapoints\n\t\t- Datapoint (into the page) has one-to-many Matrices\n\t\t\t- Matrix (down the page) has one-to-many Vectors\n\t\t\t\t- Vector (towards right) has one-to-many Components\n\t\t\t\t\t- Component/Element of type\n\t\t\t\t\t\t- Numerical [continuous/proportional]\n\t\t\t\t\t\t\t- int/real/float (continuous)\n\t\t\t\t\t\t- Categorical [non-continuous/non-formal]\n\t\t\t\t\t\t\t- AIF calls it 'discrete'\n\t\t\t\t\t\t\t- ordinal (ordered)\n\t\t\t\t\t\t\t- nominal (no order)\n\t\t\t\t\t\t- for computers, elements need to be numbers, so categoricals encoded as numbers too","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Most complex Datapoint handled is a multispectral image, i.e. 3D","category":"page"},{"location":"categories/problem_specific/litter_model/#True-vs-Inferred-variables:","page":"Litter Model","title":"True vs Inferred variables:","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"True variables associated with Generative Process genpr\ne.g. breves brevemathbfs brevetheta\nInferred variables associated with Generative Model agent\ne.g. s mathbfs theta","category":"page"},{"location":"categories/problem_specific/litter_model/#General","page":"Litter Model","title":"General","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Global code variables will be prefixed with an underscore '_'.","category":"page"},{"location":"categories/problem_specific/litter_model/#Active-Inference:-Bridging-Minds-and-Machines","page":"Litter Model","title":"Active Inference: Bridging Minds and Machines","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"In recent years, the landscape of machine learning has undergone a profound transformation with the emergence of active inference, a novel paradigm that draws inspiration from the principles of biological systems to inform intelligent decision-making processes. Unlike traditional approaches to machine learning, which often passively receive data and adjust internal parameters to optimize performance, active inference represents a dynamic and interactive framework where agents actively engage with their environment to gather information and make decisions in real-time.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"At its core, active inference is rooted in the notion of agents as embodied entities situated within their environments, constantly interacting with and influencing their surroundings. This perspective mirrors the fundamental processes observed in living organisms, where perception, action, and cognition are deeply intertwined to facilitate adaptive behavior. By leveraging this holistic view of intelligence, active inference offers a unified framework that seamlessly integrates perception, decision-making, and action, thereby enabling agents to navigate complex and uncertain environments more effectively.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"One of the defining features of active inference is its emphasis on the active acquisition of information. Rather than waiting passively for sensory inputs, agents proactively select actions that are expected to yield the most informative outcomes, thus guiding their interactions with the environment. This active exploration not only enables agents to reduce uncertainty and make more informed decisions but also allows them to actively shape their environments to better suit their goals and objectives.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Furthermore, active inference places a strong emphasis on the hierarchical organization of decision-making processes, recognizing that complex behaviors often emerge from the interaction of multiple levels of abstraction. At each level, agents engage in a continuous cycle of prediction, inference, and action, where higher-level representations guide lower-level processes while simultaneously being refined and updated based on incoming sensory information.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The applications of active inference span a wide range of domains, including robotics, autonomous systems, neuroscience, and cognitive science. In robotics, active inference offers a promising approach for developing robots that can adapt and learn in real-time, even in unpredictable and dynamic environments. In neuroscience and cognitive science, active inference provides a theoretical framework for understanding the computational principles underlying perception, action, and decision-making in biological systems.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"In conclusion, active inference represents a paradigm shift in machine learning, offering a principled and unified framework for understanding and implementing intelligent behavior in artificial systems. By drawing inspiration from the principles of biological systems, active inference holds the promise of revolutionizing our approach to building intelligent machines and understanding the nature of intelligence itself.","category":"page"},{"location":"categories/problem_specific/litter_model/#Business-Understanding","page":"Litter Model","title":"Business Understanding","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Although the current project covers a small part of the span of Active Inference, we would nevertheless like to execute it within this context.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The client is responsible for the delittering of a mile-long beach walk-way in the Pacific Northwest in the USA. The density of foot traffic is roughly uniform along its length. Volunteers provide their services for cleaning up litter. One of the key determinants of the client's planning is an estimation of the number of daily litter events along this walkway. The client does not want to over-engage his team of volunteers, nor does he want litter to become too noticeable.","category":"page"},{"location":"categories/problem_specific/litter_model/#Data-Understanding","page":"Litter Model","title":"Data Understanding","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The number of daily litter events will be modeled by a Poisson distribution with parameter theta. This parameter, usually denoted by lambda, represents both the mean as well as the variance of the Poisson distribution. The theta parameter will be learned or inferred by a model.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"For additional insight, we will simulate some litter event data.","category":"page"},{"location":"categories/problem_specific/litter_model/#Data-Preparation","page":"Litter Model","title":"Data Preparation","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"We will use simulated data to prepare the model. To apply the model we will use data gathered from observations along the walk-way. There is no need to perform additional data preparation.","category":"page"},{"location":"categories/problem_specific/litter_model/#Modeling","page":"Litter Model","title":"Modeling","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#Core-Elements","page":"Litter Model","title":"Core Elements","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"This section attempts to answer three important questions:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"What metrics are we going to track?\nWhat decisions do we intend to make?\nWhat are the sources of uncertainty?","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"For this problem, the only metric we are interested in is the daily number of litter events so that we can use Bayesian inference to estimate the mean of the Poisson distribution that that represents the littering events.","category":"page"},{"location":"categories/problem_specific/litter_model/#Environment-Model-(Generative-Process)","page":"Litter Model","title":"Environment Model (Generative Process)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The number of daily litter events will be given by $ n^{Daily} \\sim Pois(\\theta) $","category":"page"},{"location":"categories/problem_specific/litter_model/#State-variables","page":"Litter Model","title":"State variables","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"We do not have state variables. The only variable that needs to be inferred is theta, the mean (and variance) of the generative process, i.e. the Poisson distribution.","category":"page"},{"location":"categories/problem_specific/litter_model/#Decision-variables","page":"Litter Model","title":"Decision variables","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"There will be no decision variables for this project.","category":"page"},{"location":"categories/problem_specific/litter_model/#Exogenous-information-variables","page":"Litter Model","title":"Exogenous information variables","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"We assume that the volunteers that inspect the walk-way do not miscount litter events. Consequently we will not make provision for exogenous information variables.","category":"page"},{"location":"categories/problem_specific/litter_model/#Next-State-function","page":"Litter Model","title":"Next State function","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The provision function, f_p(), provides another state/datapoint, called the provision/pre-state. Because this is a combinatorial system, the provision function acquires the next state/datapoint making use of a simulation or a data set.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"mathbfp_i = f_p(i)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## provision function, provides another state/datapoint from simulation\nfunction fˢⁱᵐₚ(s; θ̆, 𝙼, 𝚅, 𝙲, rng)\n    dp = Vector{Vector{Vector{Float64}}}(undef, 𝙼)\n    for m in 1:𝙼 ## Matrices\n        dp[m] = Vector{Vector{Float64}}(undef, 𝚅)\n        for v in 1:𝚅 ## Vectors\n            dp[m][v] = Vector{Float64}(undef, 𝙲)\n            for c in 1:𝙲 ## Components\n               dp[m][v][c] = float(rand(rng, Poisson(θ̆)))\n            end\n        end\n    end\n    s̆ = dp\n    return s̆\nend\n\n_s = 1 ## s for sequence\n_θ̆ˢⁱᵐ = 15 ## lambda of Poisson distribution\n_rng = MersenneTwister(57)\n## _s̆ = fˢⁱᵐₚ(_s, θ̆=_θ̆ˢⁱᵐ, 𝙼=3, 𝚅=4, 𝙲=5, rng=_rng) ## color image with 3 colors, 4 rows, 5 cols of elements\n## _s̆ = fˢⁱᵐₚ(_s, θ̆=_θ̆ˢⁱᵐ, 𝙼=1, 𝚅=4, 𝙲=5, rng=_rng) ## b/w image with 4 rows, 5 cols of elements\n_s̆ = fˢⁱᵐₚ(_s, θ̆=_θ̆ˢⁱᵐ, 𝙼=1, 𝚅=1, 𝙲=5, rng=_rng) ## vector with 5 elements\n## _s̆ = fˢⁱᵐₚ(_s, θ̆=_θ̆ˢⁱᵐ, 𝙼=1, 𝚅=1, 𝙲=1, rng=_rng) ## vector with 1 element\n;","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## provision function, provides another state/datapoint from field\nfunction fᶠˡᵈₚ(s; 𝙼, 𝚅, 𝙲, df)\n    dp = Vector{Vector{Vector{Float64}}}(undef, 𝙼)\n    for m in 1:𝙼 ## Matrices\n        dp[m] = Vector{Vector{Float64}}(undef, 𝚅)\n        for v in 1:𝚅 ## Vectors\n            dp[m][v] = Vector{Float64}(undef, 𝙲)\n            for c in 1:𝙲 ## Components\n                dp[m][v][c] = df[s, :incidents]\n            end\n        end\n    end\n    s̆ = dp\n    return s̆\nend\n## _s = 1 ## s for sequence\n## dp = fᶠˡᵈₚ(_s, 𝙼=3, 𝚅=4, 𝙲=5, df=_fld_df) ## color image with 3 colors, 4 rows, 5 cols of elements\n## dp = fᶠˡᵈₚ(_s, 𝙼=1, 𝚅=4, 𝙲=5, df=_fld_df) ## b/w image with 4 rows, 5 cols of elements\n## dp = fᶠˡᵈₚ(_s, 𝙼=1, 𝚅=1, 𝙲=5, df=_fld_df) ## vector with 5 elements\n## dp = fᶠˡᵈₚ(_s, 𝙼=1, 𝚅=1, 𝙲=1, df=_fld_df) ## vector with 1 element","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"fᶠˡᵈₚ (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Because there is no noise to be combined with, the next state becomes","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"brevemathbfs_i = mathbfp_i","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The breve/bowl indicates that the parameters and variables are hidden and not observed.","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-function","page":"Litter Model","title":"Observation function","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The response function, f_r(), provides the response to the state/datapoint, called the response: mathbfr_i = f_r(brevemathbfs_i)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## response function, provides the response to a state/datapoint\nfunction fᵣ(s̆)\n    return s̆ ## no noise\nend\nfᵣ(_s̆);","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Because there is no noise to be combined with, the next observation becomes","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"mathbfy_i = mathbfbreves_i","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The breve/bowl indicates that the parameters and variables are hidden and not observed.","category":"page"},{"location":"categories/problem_specific/litter_model/#Implementation-of-the-Environment-Model-(Generative-Process)","page":"Litter Model","title":"Implementation of the Environment Model (Generative Process)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Let's simulate some data with IID observations from a Poisson distribution, that represents the litter incidents. We also assume that the mean incidents per day is 15:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## Data comes from either a simulation/lab (sim|lab) OR from the field (fld)\n## Data are handled either in batches (batch) OR online as individual points (point)\nfunction sim_data(rng, 𝚂, 𝙳, 𝙼, 𝚅, 𝙲, θ̆)\n    p = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    s̆ = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    r = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    y = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    for s in 1:𝚂 ## sequences\n        p[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        s̆[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        r[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        y[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        for d in 1:𝙳 ## datapoints\n            p[s][d] = fˢⁱᵐₚ(s; θ̆=θ̆, 𝙼=𝙼, 𝚅=𝚅, 𝙲=𝙲, rng=rng)\n            s̆[s][d] = p[s][d] ## no system noise\n            r[s][d] = fᵣ(s̆[s][d])\n            y[s][d] = r[s][d]\n        end\n    end\n    return y\nend;\n\nfunction fld_data(df, 𝚂, 𝙳, 𝙼, 𝚅, 𝙲)\n    p = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    s̆ = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    r = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    y = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    for s in 1:𝚂 ## sequences\n        p[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        s̆[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        r[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        y[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        for d in 1:𝙳 ## datapoints\n            p[s][d] = fᶠˡᵈₚ(s; 𝙼=𝙼, 𝚅=𝚅, 𝙲=𝙲, df=df)\n            s̆[s][d] = p[s][d] ## no system noise\n            r[s][d] = fᵣ(s̆[s][d])\n            y[s][d] = r[s][d]\n        end\n    end\n    return y\nend;","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## number of Batches in an experiment\n## _𝙱 = 1 ## not used yet\n\n## number of Sequences/examples in a batch\n_𝚂 = 365\n## _𝚂 = 3\n\n## number of Datapoints in a sequence\n_𝙳 = 1\n## _𝙳 = 2\n## _𝙳 = 3\n\n## number of Matrices in a datapoint\n_𝙼 = 1\n\n## number of Vectors in a matrix\n_𝚅 = 1\n\n## number of Components in a vector\n_𝙲 = 1\n\n_θ̆ˢⁱᵐ = 15 ## hidden lambda of Poisson distribution\n_rng = MersenneTwister(57);","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_yˢⁱᵐ = sim_data(_rng, _𝚂, _𝙳, _𝙼, _𝚅, _𝙲, _θ̆ˢⁱᵐ) ## simulated data\n_yˢⁱᵐ = first.(first.(first.(first.(_yˢⁱᵐ))));","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## methods(print)\n## print(_yˢⁱᵐ[1:2])\n\n## Customize the display width to control positioning or prevent wrapping\n## io = IOContext(stdout, :displaysize => (50, 40)) ## (rows, cols)\n## print(io, _yˢⁱᵐ[1:3])\n## print(io, _yˢⁱᵐ)\n\nprint(IOContext(stdout, :displaysize => (24, 5)), _yˢⁱᵐ[1:10]);","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"[8.0, 10.0, 14.0, 9.0, 15.0, 9.0, 12.0, 15.0, 19.0, 18.0]","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_rθ = range(0, _𝚂, length=1*_𝚂)\n_p = plot(title=\"Simulated Daily Litter Events\", xlabel=\"Day\")\n_p = plot!(_rθ, _yˢⁱᵐ, linetype=:steppre, label=\"# daily events\", c=1)\nplot(_p)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/litter_model/#Uncertainty-Model","page":"Litter Model","title":"Uncertainty Model","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#Agent-Model-(Generative-Model)","page":"Litter Model","title":"Agent Model (Generative Model)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"In this project, we are going to perform an exact inference for a litter model that can be represented as:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"beginaligned\np(theta) = mathrmGamma(theta mid alpha^Gamma theta^Gamma)\np(x_i mid theta) = mathrmPois(x_i mid theta)\nendaligned","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"where x_i in 0 1  is an observation induced by a Poisson likelihood while p(theta) is a Gamma prior distribution on the parameter of the Poisson distribution. We are interested in inferring the posterior distribution of theta.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The generative model is:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"$","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"\\begin{aligned} p(x:,\\theta)    &= p(x: \\mid \\theta) \\cdot p(\\theta) \\\n                 &= p(x{1:N} \\mid \\theta) \\cdot p(\\theta) \\\n                 &= \\prod{i=1}^N{p(xi \\mid \\theta)} \\cdot p(\\theta) \\\n                 &= \\prod{i=1}^N{\\mathrm{Pois}(x_i \\mid \\theta)} \\cdot \\Gamma(\\theta \\mid \\alpha^{\\Gamma}, \\theta^{\\Gamma}) \\end{aligned} $","category":"page"},{"location":"categories/problem_specific/litter_model/#Implementation-of-the-Agent-Model-(Generative-Model)","page":"Litter Model","title":"Implementation of the Agent Model (Generative Model)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"We will use the RxInfer Julia package. RxInfer stands at the forefront of Bayesian inference tools within the Julia ecosystem, offering a powerful and versatile platform for probabilistic modeling and analysis. Built upon the robust foundation of the Julia programming language, RxInfer provides researchers, data scientists, and practitioners with a streamlined workflow for conducting Bayesian inference tasks with unprecedented speed and efficiency.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"At its core, RxInfer leverages cutting-edge techniques from the realm of reactive programming to enable dynamic and interactive model specification and estimation. This unique approach empowers users to define complex probabilistic models with ease, seamlessly integrating prior knowledge, data, and domain expertise into the modeling process.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"With RxInfer, conducting Bayesian inference tasks becomes a seamless and intuitive experience. The package offers a rich set of tools for performing parameter estimation, model comparison, and uncertainty quantification, all while leveraging the high-performance capabilities of Julia to deliver results in a fraction of the time required by traditional methods.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Whether tackling problems in machine learning, statistics, finance, or any other field where uncertainty reigns supreme, RxInfer equips users with the tools they need to extract meaningful insights from their data and make informed decisions with confidence.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"RxInfer represents a paradigm shift in the world of Bayesian inference, combining the expressive power of Julia with the flexibility of reactive programming to deliver a state-of-the-art toolkit for probabilistic modeling and analysis. With its focus on speed, simplicity, and scalability, RxInfer is poised to become an indispensable tool for researchers and practitioners seeking to harness the power of Bayesian methods in their work.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"To transfer the above factorized generative model to the RxInfer package, we need to include each of the factors:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"N\nKronecker-delta factors (for the N observations)\n1\nGamma factor (for the prior distribution)\nN\nPoisson factors (for the litter events)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## parameters for the prior distribution\n_αᴳᵃᵐ, _θᴳᵃᵐ = 350., .05;","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## Litter model: Gamma-Poisson\n@model function litter_model(x, αᴳᵃᵐ, θᴳᵃᵐ)\n    ## prior on θ parameter of the model\n    θ ~ Gamma(shape=αᴳᵃᵐ, rate=θᴳᵃᵐ) ## 1 Gamma factor\n\n    ## assume daily number of litter incidents is a Poisson distribution\n    for i in eachindex(x)\n        x[i] ~ Poisson(θ) ## not θ̃; N Poisson factors\n    end\nend","category":"page"},{"location":"categories/problem_specific/litter_model/#Agent-(Policy)-Evaluation","page":"Litter Model","title":"Agent (Policy) Evaluation","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#Evaluate-with-simulated-data","page":"Litter Model","title":"Evaluate with simulated data","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_result = infer(\n    model= litter_model(αᴳᵃᵐ= _αᴳᵃᵐ, θᴳᵃᵐ= _θᴳᵃᵐ), \n    data= (x= _yˢⁱᵐ, )\n)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_θˢⁱᵐ = _result.posteriors[:θ]","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"ExponentialFamily.GammaShapeRate{Float64}(a=5838.0, b=365.05)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_rθ = range(0, 20, length=500)\n_p = plot(title=\"Simulation results: Distribution of \"*L\"θ^{\\mathrm{sim}}=λ\")\nplot!(_rθ, (x) -> pdf(Gamma(_αᴳᵃᵐ, _θᴳᵃᵐ), x), fillalpha=0.3, fillrange=0, label=\"P(θ)\", c=1,)\nplot!(_rθ, (x) -> pdf(_θˢⁱᵐ, x), fillalpha=0.3, fillrange=0, label=\"P(θ|x)\", c=3)\nvline!([_θ̆ˢⁱᵐ], label=\"Hidden θ\", c=2)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/litter_model/#Evaluation","page":"Litter Model","title":"Evaluation","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The following data comes from the inspections of the volunteers over a period of 12 months:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_fld_df = DataFrame(XLSX.readtable(\"litter_incidents.xlsx\", \"Sheet1\"))\n_yᶠˡᵈ = fld_data(_fld_df, _𝚂, _𝙳, _𝙼, _𝚅, _𝙲) ## field data\n_yᶠˡᵈ = first.(first.(first.(first.(_yᶠˡᵈ))))\nprint(IOContext(stdout, :displaysize => (24, 30)), _yᶠˡᵈ[1:10]);","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"[5.0, 7.0, 6.0, 10.0, 8.0, 5.0, 7.0, 9.0, 13.0, 9.0]","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_rθ = range(0, _𝚂, length=1*_𝚂)\n_p = plot(title=\"Field Daily Litter Events\", xlabel=\"Day\")\n_p = plot!(_rθ, _yᶠˡᵈ, linetype=:steppre, label=\"# daily events\", c=1)\nplot(_p)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_result = infer(\n    model=litter_model(αᴳᵃᵐ= _αᴳᵃᵐ, θᴳᵃᵐ= _θᴳᵃᵐ), \n    data= (x= _yᶠˡᵈ, )\n)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_θᶠˡᵈ = _result.posteriors[:θ]","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"ExponentialFamily.GammaShapeRate{Float64}(a=3200.0, b=365.05)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_rθ = range(0, 20, length=500)\n_p = plot(title=\"Field results: Distribution of \"*L\"θ^{\\mathrm{fld}}=λ\")\nplot!(_rθ, (x) -> pdf(Gamma(_αᴳᵃᵐ, _θᴳᵃᵐ), x), fillalpha=0.3, fillrange=0, label=\"P(θ)\", c=1,)\nplot!(_rθ, (x) -> pdf(_θᶠˡᵈ, x), fillalpha=0.3, fillrange=0, label=\"P(θ|x)\", c=3)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The actual generative process actually had a much lower mean daily litter events, around about 8 events per day. The client can work with this value during planning of how to use his volunteers in the field.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/litter_model/Project.toml`\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.121\n  [b964fa9f] LaTeXStrings v1.4.0\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [fdbf4ff8] XLSX v0.10.4\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#Active-Inference-Mountain-car","page":"Active Inference Mountain Car","title":"Active Inference Mountain car","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"using RxInfer, Plots","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"A group of friends is going to a camping site that is located on the biggest mountain in the Netherlands. They use an electric car for the trip. When they are almost there, the car's battery is almost empty and is therefore limiting the engine force. Unfortunately, they are in the middle of a valley and don't have enough power to reach the camping site. Night is falling and they still need to reach the top of the mountain. As rescuers, let us develop an Active Inference (AI) agent that can get them up the hill with the limited engine power.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#The-environmental-process-of-the-mountain","page":"Active Inference Mountain Car","title":"The environmental process of the mountain","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Firstly, we specify the environmental process according to Ueltzhoeffer (2017) \"Deep active inference\". This process shows how the environment evolves after interacting with the agent.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Particularly, let's denote z_t = (phi_t dotphi_t) as the environmental state depending on the position phi_t and velocity dotphi_t of the car; a_t as the action of the environment on the car. Then the evolution of the state is described as follows  ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"beginaligned \ndotphi_t = dotphi_t-1 + F_g(phi_t-1) + F_f(dotphi_t-1) + F_a(a_t)\nphi_t = phi_t-1 + dotphi_t \nendaligned","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"where F_g(phi_t-1) is the gravitational force of the hill landscape that depends on the car's position","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"F_g(phi) = begincases\n        -005(2phi + 1)    mathrmif  phi  0 \n        -005 left(1 + 5phi^2)^-frac12 + phi^2 (1 + 5phi^2)^-frac32 + frac116phi^4 right   mathrmotherwise\nendcases","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"F_f(dotphi)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"is the friction on the car defined through the car's velocity F_f(dotphi)  = -01  dotphi and F_a(a) is the engine force F_a(a) = 004 tanh(a) Since the car is on low battery, we use the tanh(cdot) function to limit the engine force to the interval [-0.04, 0.04].","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"In the cell below, the create_physics function defines forces F_g F_f F_a; and the create_world function defines the environmental process of the mountain.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"import HypergeometricFunctions: _₂F₁\n\nfunction create_physics(; engine_force_limit = 0.04, friction_coefficient = 0.1)\n    # Engine force as function of action\n    Fa = (a::Real) -> engine_force_limit * tanh(a) \n\n    # Friction force as function of velocity\n    Ff = (y_dot::Real) -> -friction_coefficient * y_dot \n    \n    # Gravitational force (horizontal component) as function of position\n    Fg = (y::Real) -> begin\n        if y < 0\n            0.05*(-2*y - 1)\n        else\n            0.05*(-(1 + 5*y^2)^(-0.5) - (y^2)*(1 + 5*y^2)^(-3/2) - (y^4)/16)\n        end\n    end\n    \n    # The height of the landscape as a function of the horizontal coordinate\n    height = (x::Float64) -> begin\n        if x < 0\n            h = x^2 + x\n        else\n            h = x * _₂F₁(0.5,0.5,1.5, -5*x^2) + x^3 * _₂F₁(1.5, 1.5, 2.5, -5*x^2) / 3 + x^5 / 80\n        end\n        return 0.05*h\n    end\n\n    return (Fa, Ff, Fg,height)\nend;\n\nfunction create_world(; Fg, Ff, Fa, initial_position = -0.5, initial_velocity = 0.0)\n\n    y_t_min = initial_position\n    y_dot_t_min = initial_velocity\n    \n    y_t = y_t_min\n    y_dot_t = y_dot_t_min\n    \n    execute = (a_t::Float64) -> begin\n        # Compute next state\n        y_dot_t = y_dot_t_min + Fg(y_t_min) + Ff(y_dot_t_min) + Fa(a_t)\n        y_t = y_t_min + y_dot_t\n    \n        # Reset state for next step\n        y_t_min = y_t\n        y_dot_t_min = y_dot_t\n    end\n    \n    observe = () -> begin \n        return [y_t, y_dot_t]\n    end\n        \n    return (execute, observe)\nend","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"create_world (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Let's visualize the mountain landscape and the situation of the car. ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"engine_force_limit   = 0.04\nfriction_coefficient = 0.1\n\nFa, Ff, Fg, height = create_physics(\n    engine_force_limit = engine_force_limit,\n    friction_coefficient = friction_coefficient\n);\ninitial_position = -0.5\ninitial_velocity = 0.0\n\nx_target = [0.5, 0.0] \n\nvalley_x = range(-2, 2, length=400)\nvalley_y = [ height(xs) for xs in valley_x ]\nplot(valley_x, valley_y, title = \"Mountain valley\", label = \"Landscape\", color = \"black\")\nscatter!([ initial_position ], [ height(initial_position) ], label=\"initial car position\")   \nscatter!([x_target[1]], [height(x_target[1])], label=\"camping site\")","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#Naive-approach","page":"Active Inference Mountain Car","title":"Naive approach","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Well, let's see how our friends were struggling with the low-battery car when they tried to get it to the camping site before we come to help. They basically used the brute-force method, i.e. just pushing the gas pedal for full power.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"N_naive  = 100 # Total simulation time\npi_naive = 100.0 * ones(N_naive) # Naive policy for right full-power only\n\n# Let there be a world\n(execute_naive, observe_naive) = create_world(; \n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n);\n\ny_naive = Vector{Vector{Float64}}(undef, N_naive)\nfor t = 1:N_naive\n    execute_naive(pi_naive[t]) # Execute environmental process\n    y_naive[t] = observe_naive() # Observe external states\nend\n\nanimation_naive = @animate for i in 1:N_naive\n    plot(valley_x, valley_y, title = \"Naive policy\", label = \"Landscape\", color = \"black\", size = (800, 400))\n    scatter!([y_naive[i][1]], [height(y_naive[i][1])], label=\"car\")\n    scatter!([x_target[1]], [height(x_target[1])], label=\"goal\")   \nend\n\n# The animation is saved and displayed as markdown picture for the automatic HTML generation\ngif(animation_naive, \"ai-mountain-car-naive.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"They failed as expected since the car doesn't have enough power. This helps to understand that the brute-force approach is not the most efficient one in this case and hopefully a bit of swinging is necessary to achieve the goal.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#Active-inference-approach","page":"Active Inference Mountain Car","title":"Active inference approach","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Now let's help them solve the problem with an active inference approach. Particularly, we create an agent that predicts the future car position as well as the best possible actions in a probabilistic manner.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"We start by specifying a probabilistic model for the agent that describes the agent's internal beliefs over the external dynamics of the environment. The generative model is defined as follows","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"beginaligned\np_t(xsu) propto p(s_t-1) prod_k=t^t+T p(x_k mid s_k)  p(s_k mid s_k-1u_k)  p(u_k)  p(x_k) nonumber\nendaligned","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"where the factors are defined as","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(x_k) = mathcalN(x_k mid x_goalV_goal)  quad (mathrmtarget)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(s_k mid s_k-1u_k) = mathcalN(s_k mid tildeg(s_k-1)+h(u_k)gamma^-1)  quad (mathrmstate  transition)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(x_k mid s_k) = mathcalN(x_k mid s_ktheta) quad (mathrmobservation)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(u_k) = mathcalN(u_k mid m_uV_u) quad (mathrmcontrol)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(s_t-1) = mathcalN(s_t-1 mid m_t-1V_t-1) quad (mathrmprevious  state)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"where ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"x\ndenotes observations of the agent after interacting with the environment; \ns_t = (s_tdots_t)\nis the state of the car embodying its position and velocity; \nu_t\ndenotes the control state of the agent; \nh(cdot)\nis the tanh(cdot) function modeling engine control; \ntildeg(cdot)\nexecutes a linear approximation of equations (1) and (2): ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"beginaligned \ndots_t = dots_t-1 + F_g(s_t-1) + F_f(dots_t-1)\ns_t = s_t-1 + dots_t\nendaligned","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"In the cell below, the @model macro and the meta blocks are used to define the probabilistic model and the approximation methods for the nonlinear state-transition functions, respectively. In addition, the beliefs over the future states (up to T steps ahead) of the agent is included.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"@model function mountain_car(m_u, V_u, m_x, V_x, m_s_t_min, V_s_t_min, T, Fg, Fa, Ff, engine_force_limit)\n    \n    # Transition function modeling transition due to gravity and friction\n    g = (s_t_min::AbstractVector) -> begin \n        s_t = similar(s_t_min) # Next state\n        s_t[2] = s_t_min[2] + Fg(s_t_min[1]) + Ff(s_t_min[2]) # Update velocity\n        s_t[1] = s_t_min[1] + s_t[2] # Update position\n        return s_t\n    end\n    \n    # Function for modeling engine control\n    h = (u::AbstractVector) -> [0.0, Fa(u[1])] \n    \n    # Inverse engine force, from change in state to corresponding engine force\n    h_inv = (delta_s_dot::AbstractVector) -> [atanh(clamp(delta_s_dot[2], -engine_force_limit+1e-3, engine_force_limit-1e-3)/engine_force_limit)] \n    \n    # Internal model perameters\n    Gamma = 1e4*diageye(2) # Transition precision\n    Theta = 1e-4*diageye(2) # Observation variance\n\n    s_t_min ~ MvNormal(mean = m_s_t_min, cov = V_s_t_min)\n    s_k_min = s_t_min\n\n    local s\n    \n    for k in 1:T\n        u[k] ~ MvNormal(mean = m_u[k], cov = V_u[k])\n        u_h_k[k] ~ h(u[k]) where { meta = DeltaMeta(method = Linearization(), inverse = h_inv) }\n        s_g_k[k] ~ g(s_k_min) where { meta = DeltaMeta(method = Linearization()) }\n        u_s_sum[k] ~ s_g_k[k] + u_h_k[k]\n        s[k] ~ MvNormal(mean = u_s_sum[k], precision = Gamma)\n        x[k] ~ MvNormal(mean = s[k], cov = Theta)\n        x[k] ~ MvNormal(mean = m_x[k], cov = V_x[k]) # goal\n        s_k_min = s[k]\n    end\n    \n    return (s, )\nend","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"After specifying the generative model, let's create an Active Inference(AI) agent for the car.  Technically, the agent goes through three phases: Act-Execute-Observe, Infer and Slide.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Act-Execute-Observe:   In this phase, the agent performs an action onto the environment at time t and gets T observations in exchange. These observations are basically the prediction of the agent on how the environment evolves over the next T time step. \nInfer:  After receiving observations, the agent starts updating its internal probabilistic model by doing inference. Particularly, it finds the posterior distributions over the state s_t and control u_t, i.e. p(s_tmid x_t) and p(u_tmid x_t).\nSlide:  After updating its internal belief, the agent moves to the next time step and uses the inferred action u_t in the previous time step to interact with the environment.  ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"In the cell below, we create the agent through the create_agent function, which includes compute, act, slide and future functions:","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"The act function selects the next action based on the inferred policy. On the other hand, the future function predicts the next T positions based on the current action. These two function implement the Act-Execute-Observe phase.\nThe compute function infers the policy (which is a set of actions for the next T time steps) and the agent's state using the agent internal model. This function implements the Infer phase. We call it compute to avoid the clash with the infer function of RxInfer.jl.\nThe slide function implements the Slide phase, which moves the agent internal model to the next time step.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"# We are going to use some private functionality from ReactiveMP, \n# in the future we should expose a proper API for this\nimport RxInfer.ReactiveMP: getrecent, messageout\n\nfunction create_agent(;T = 20, Fg, Fa, Ff, engine_force_limit, x_target, initial_position, initial_velocity)\n    huge = 1e6\n    tiny = 1e-6\n    Epsilon = fill(huge, 1, 1)                # Control prior variance\n    m_u = Vector{Float64}[ [ 0.0] for k=1:T ] # Set control priors\n    V_u = Matrix{Float64}[ Epsilon for k=1:T ]\n\n    Sigma    = 1e-4*diageye(2) # Goal prior variance\n    m_x      = [zeros(2) for k=1:T]\n    V_x      = [huge*diageye(2) for k=1:T]\n    V_x[end] = Sigma # Set prior to reach goal at t=T\n\n    # Set initial brain state prior\n    m_s_t_min = [initial_position, initial_velocity] \n    V_s_t_min = tiny * diageye(2)\n    \n    # Set current inference results\n    result = nothing\n\n    # The `infer` function is the heart of the agent\n    # It calls the `RxInfer.inference` function to perform Bayesian inference by message passing\n    compute = (upsilon_t::Float64, y_hat_t::Vector{Float64}) -> begin\n        m_u[1] = [ upsilon_t ] # Register action with the generative model\n        V_u[1] = fill(tiny, 1, 1) # Clamp control prior to performed action\n\n        m_x[1] = y_hat_t # Register observation with the generative model\n        V_x[1] = tiny*diageye(2) # Clamp goal prior to observation\n\n        data = Dict(:m_u       => m_u, \n                    :V_u       => V_u, \n                    :m_x       => m_x, \n                    :V_x       => V_x,\n                    :m_s_t_min => m_s_t_min,\n                    :V_s_t_min => V_s_t_min)\n        \n        model  = mountain_car(T = T, Fg = Fg, Fa = Fa, Ff = Ff, engine_force_limit = engine_force_limit) \n        result = infer(model = model, data = data)\n    end\n    \n    # The `act` function returns the inferred best possible action\n    act = () -> begin\n        if result !== nothing\n            return mode(result.posteriors[:u][2])[1]\n        else\n            return 0.0 # Without inference result we return some 'random' action\n        end\n    end\n    \n    # The `future` function returns the inferred future states\n    future = () -> begin \n        if result !== nothing \n            return getindex.(mode.(result.posteriors[:s]), 1)\n        else\n            return zeros(T)\n        end\n    end\n\n    # The `slide` function modifies the `(m_s_t_min, V_s_t_min)` for the next step\n    # and shifts (or slides) the array of future goals `(m_x, V_x)` and inferred actions `(m_u, V_u)`\n    slide = () -> begin\n\n        model  = RxInfer.getmodel(result.model)\n        (s, )  = RxInfer.getreturnval(model)\n        varref = RxInfer.getvarref(model, s) \n        var    = RxInfer.getvariable(varref)\n        \n        slide_msg_idx = 3 # This index is model dependend\n        (m_s_t_min, V_s_t_min) = mean_cov(getrecent(messageout(var[2], slide_msg_idx)))\n\n        m_u = circshift(m_u, -1)\n        m_u[end] = [0.0]\n        V_u = circshift(V_u, -1)\n        V_u[end] = Epsilon\n\n        m_x = circshift(m_x, -1)\n        m_x[end] = x_target\n        V_x = circshift(V_x, -1)\n        V_x[end] = Sigma\n    end\n\n    return (compute, act, slide, future)    \nend","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"create_agent (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Now it's time to see if we can help our friends arrive at the camping site by midnight?","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"(execute_ai, observe_ai) = create_world(\n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n) # Let there be a world\n\nT_ai = 50\n\n(compute_ai, act_ai, slide_ai, future_ai) = create_agent(; # Let there be an agent\n    T  = T_ai, \n    Fa = Fa,\n    Fg = Fg, \n    Ff = Ff, \n    engine_force_limit = engine_force_limit,\n    x_target = x_target,\n    initial_position = initial_position,\n    initial_velocity = initial_velocity\n) \n\nN_ai = 100\n\n# Step through experimental protocol\nagent_a = Vector{Float64}(undef, N_ai) # Actions\nagent_f = Vector{Vector{Float64}}(undef, N_ai) # Predicted future\nagent_x = Vector{Vector{Float64}}(undef, N_ai) # Observations\n\nfor t=1:N_ai\n    agent_a[t] = act_ai()               # Invoke an action from the agent\n    agent_f[t] = future_ai()            # Fetch the predicted future states\n    execute_ai(agent_a[t])              # The action influences hidden external states\n    agent_x[t] = observe_ai()           # Observe the current environmental outcome (update p)\n    compute_ai(agent_a[t], agent_x[t]) # Infer beliefs from current model state (update q)\n    slide_ai()                          # Prepare for next iteration\nend\n\nanimation_ai = @animate for i in 1:N_ai\n    # pls - plot landscape\n    pls = plot(valley_x, valley_y, title = \"Active inference results\", label = \"Landscape\", color = \"black\")\n    pls = scatter!(pls, [agent_x[i][1]], [height(agent_x[i][1])], label=\"car\")\n    pls = scatter!(pls, [x_target[1]], [height(x_target[1])], label=\"goal\")   \n    pls = scatter!(pls, agent_f[i], height.(agent_f[i]), label = \"Predicted future\", alpha = map(i -> 0.5 / i, 1:T_ai))\n    \n    # pef - plot engine force\n    pef = plot(Fa.(agent_a[1:i]), title = \"Engine force (agents actions)\", xlim = (0, N_ai), ylim = (-0.05, 0.05))\n    \n    plot(pls, pef, size = (800, 400))\nend\n    \n# The animation is saved and displayed as markdown picture for the automatic HTML generation\ngif(animation_ai, \"ai-mountain-car-ai.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Voila! The car now is able to reach the camping site with a smart strategy.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"The left figure shows the agent reached its goal by swinging and the right one shows the corresponding engine force. As we can see, at the beginning the agent tried to reach the goal directly (with full engine force) but after some trials it realized that's not possible. Since the agent looks ahead for 50 time steps, it has enough time to explore other policies, helping it learn to move back to get more momentum to reach the goal.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Now our friends can enjoy their trip at the camping site!. ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#Reference","page":"Active Inference Mountain Car","title":"Reference","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"We refer reader to the Thijs van de Laar (2019) \"Simulating active inference processes by message passing\" original paper with more in-depth overview and explanation of the active inference agent implementation by message passing. The original environment/task description is from Ueltzhoeffer (2017) \"Deep active inference\".","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/active_inference_mountain_car/Project.toml`\n  [34004b35] HypergeometricFunctions v0.3.28\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Autoregressive-Models","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Ever wondered how financial analysts predict tomorrow's stock prices, how meteorologists forecast next week's weather, or how engineers anticipate system failures before they happen? Welcome to the fascinating world of autoregressive models – the mathematical engines that power predictions when the future depends on the past.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In this hands-on example, we'll dive into the elegant framework of Bayesian autoregressive modeling using RxInfer.jl, a powerful probabilistic programming library that makes complex inference tasks surprisingly accessible. Unlike traditional approaches that give you a single prediction, our Bayesian approach provides complete predictive distributions, capturing the uncertainty that's inherent in any real-world forecast.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"You'll discover how to:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Create and understand AR models through a Bayesian lens with RxInfer.jl and @model macro\nGenerate synthetic data to test your inference algorithms\nPerform automated variational Bayesian inference with RxInfer.jl\nMake probabilistic predictions with quantified uncertainty\nApply these techniques to real-world stock price data\nAs a bonus we implement a simple version of ARMA models","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Whether you're predicting financial markets, analyzing sensor readings, modeling climate patterns, or exploring any time-dependent phenomenon, the techniques you'll learn here provide a robust foundation for sophisticated time series analysis using autoregressive models.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#The-Mathematics-Behind-Autoregressive-Models","page":"Autoregressive Models","title":"The Mathematics Behind Autoregressive Models","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"At their core, autoregressive (AR) models capture a fundamental principle: the future depends on the past. But how do we translate this intuition into mathematical precision? Let's build the framework together.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Imagine we're tracking a variable over time - stock prices, temperature readings, or any quantity that evolves sequentially. In an AR model, we express the current value as a function of its previous values, plus some random noise. This elegantly captures both deterministic patterns and inherent uncertainty.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In our Bayesian formulation, we model this process as:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\np(gamma) = Gamma(gammaa b)\np(mathbftheta) = mathcalN(mathbfthetamathbfmu Sigma)\np(x_tmathbfx_t-1t-k) = mathcalN(x_tmathbftheta^Tmathbfx_t-1t-k gamma^-1)\np(y_tx_t) = mathcalN(y_tx_t tau^-1)\nendaligned","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Here's what this means in plain language:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"x_t\nrepresents our system's true state at time t\nmathbfx_t-1t-k\ncaptures the sequence of k previous states\nmathbftheta\nholds the \"memory coefficients\" - how much each past state influences the present\ngamma\ncontrols the randomness in state transitions (higher values mean less randomness)\ny_t\nis what we actually observe, which includes some measurement noise controlled by tau","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"It's worth noting that this particular formulation is a latent autoregressive model, where the AR process (x_t) is hidden behind the likelihood function. In classical AR models, the states are directly observed without this additional observation layer. This latent structure gives us more flexibility in modeling real-world phenomena where measurements contain noise or where the underlying process isn't directly observable.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The beauty of this formulation is that it handles both the \"signal\" (predictable patterns) and the \"noise\" (random fluctuations) in a principled way.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For readers interested in the deeper theoretical foundations, we recommend Albert Podusenko's excellent work on Message Passing-Based Inference for Time-Varying Autoregressive Models.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Now, let's translate this mathematical framework into code and see it in action!","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"using RxInfer, Distributions, LinearAlgebra, Plots, StableRNGs, DataFrames, CSV, Dates","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Starting-Simple:-From-Synthetic-to-Real-World-Data","page":"Autoregressive Models","title":"Starting Simple: From Synthetic to Real-World Data","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In our code implementation, we begin by generating synthetic data using predefined sets of coefficients for autoregressive models with orders 1, 2, and 5.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Starting with synthetic data offers several advantages:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Ground Truth: We know the exact coefficients that generated the data, making it possible to evaluate how well our inference algorithms recover these parameters.\nControl: We can test our models under different noise levels, sample sizes, and process specifications without the complexity of real-world data.\nLearning Progression: By beginning with synthetic examples, we can build intuition about how AR models behave before tackling the messier challenges of real data.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Later in the example, we'll transition to real-world stock price data, where the true generative process is unknown.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# The following coefficients correspond to stable poles\ncoefs_ar_1 = [-0.27002517200218096]\ncoefs_ar_2 = [0.4511170798064709, -0.05740081602446657]\ncoefs_ar_5 = [0.10699399235785655, -0.5237303489793305, 0.3068897071844715, -0.17232255282458891, 0.13323964347539288];","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The coefficients we've selected aren't arbitrary - they're carefully chosen to ensure stability in our autoregressive processes. In signal processing and time series analysis, stable poles refer to coefficients that keep the AR process from exploding or diverging over time. Mathematically, this means that the roots of the AR characteristic polynomial must lie inside the unit circle in the complex plane. For example, in a first-order AR model where x_t = theta x_t-1 + varepsilon_t, we need theta  1 to ensure stability. For higher-order models, the constraints become more complex, but the principle remains the same: without stability, our models would produce unrealistic, explosive behavior.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function generate_synthetic_dataset(; n, θ, γ = 1.0, τ = 1.0, rng = StableRNG(42), states1 = randn(rng, length(θ)))\n    order = length(θ)\n\n    # Convert precision parameters to standard deviation\n    τ_std = sqrt(inv(τ))\n\n    # Initialize states and observations\n    states       = Vector{Vector{Float64}}(undef, n + 3order)\n    observations = Vector{Float64}(undef, n + 3order)\n\n    # `NormalMeanPrecision` is exported by `RxInfer.jl`\n    # and is a part of `ExponentialFamily.jl`\n    states[1]       = states1\n    observations[1] = rand(rng, NormalMeanPrecision(states[1][1], γ))\n    \n    for i in 2:(n + 3order)\n        previous_state  = states[i - 1]\n        transition      = dot(θ, previous_state)\n        next_x          = rand(rng, NormalMeanPrecision(transition, τ))\n        states[i]       = vcat(next_x, previous_state[1:end-1])\n        observations[i] = rand(rng, NormalMeanPrecision(next_x, γ))\n    end\n    \n    return states[1+3order:end], observations[1+3order:end]\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"generate_synthetic_dataset (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can now generate several synthetic datasets and plot them to see how they look like:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function plot_synthetic_dataset(; dataset, title)\n    states, observations = dataset\n    p = plot(first.(states), label = \"Hidden states\", title = title)\n    p = scatter!(p, observations, label = \"Observations\")\n    return p\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot_synthetic_dataset (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"dataset_1 = generate_synthetic_dataset(n = 100, θ = coefs_ar_1)\ndataset_2 = generate_synthetic_dataset(n = 100, θ = coefs_ar_2)\ndataset_5 = generate_synthetic_dataset(n = 100, θ = coefs_ar_5)\n\np1 = plot_synthetic_dataset(dataset = dataset_1, title = \"AR(1)\")\np2 = plot_synthetic_dataset(dataset = dataset_2, title = \"AR(2)\")\np3 = plot_synthetic_dataset(dataset = dataset_5, title = \"AR(5)\")\n\nplot(p1, p2, p3, layout = @layout([ a b ; c ]))","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Model-Specification:-Translating-Theory-to-Code","page":"Autoregressive Models","title":"Model Specification: Translating Theory to Code","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"With our synthetic data ready, we now tackle the critical step of encoding our autoregressive model as a probabilistic program in RxInfer. This translation from mathematical notation to executable code is where the power of probabilistic programming truly shines.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@model function lar_multivariate(y, order, γ)\n    # `c` is a unit vector of size `order` with first element equal to 1\n    c = ReactiveMP.ar_unit(Multivariate, order)\n    \n    τ  ~ Gamma(α = 1.0, β = 1.0)\n    θ  ~ MvNormal(mean = zeros(order), precision = diageye(order))\n    x0 ~ MvNormal(mean = zeros(order), precision = diageye(order))\n    \n    x_prev = x0\n    \n    for i in eachindex(y)\n \n        x[i] ~ AR(x_prev, θ, τ) \n        y[i] ~ Normal(mean = dot(c, x[i]), precision = γ)\n        \n        x_prev = x[i]\n    end\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Constraints-specification","page":"Autoregressive Models","title":"Constraints specification","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Bayesian inference for complex models often requires approximations. Our code uses variational inference with a specific factorization constraint:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@constraints function ar_constraints() \n    q(x0, x, θ, τ) = q(x0, x)q(θ)q(τ)\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_constraints (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"This constraint defines how we'll approximate the joint posterior:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We factorize it into three independent components\nStates (x0, x) remain jointly distributed, preserving temporal dependencies\nModel parameters (θ, τ) are separated from states and each other\nEach component can be updated independently during inference","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"This factorization balances statistical accuracy with computational efficiency and allows RxInfer to apply efficient message-passing algorithms while maintaining the most important dependencies in the model.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Meta-specification","page":"Autoregressive Models","title":"Meta specification","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The @meta block in RxInfer provides essential configuration information to specific nodes in your probabilistic model. In short, it tells RxInfer how to customize inference algorithms, what approximation methods to use, and allows to specify extra computational parameters for custom complex factor nodes. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@meta function ar_meta(order)\n    AR() -> ARMeta(Multivariate, order, ARsafe())\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_meta (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For autoregressive models, this block tells RxInfer:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Which type of AR process to use (Univariate/Multivariate)\nThe order of the process (how many past values influence the current one)\nAny stability constraints to apply during inference","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For more information about specific arguments refer to the AR node documentation. For more information on meta blocks, see the RxInfer.jl documentation.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Initialization-specification","page":"Autoregressive Models","title":"Initialization specification","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The @initialization block in RxInfer specifies the initial marginal distributions for the model parameters. This is crucial for the convergence of inference algorithms, especially for complex models like autoregressive processes.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For our autoregressive models, we initialize:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@initialization function ar_init(order)\n    q(τ) = GammaShapeRate(1.0, 1.0)\n    q(θ) = MvNormalMeanPrecision(zeros(order), diageye(order))\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_init (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Inference","page":"Autoregressive Models","title":"Inference","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"With our model defined, constraints established, and meta configurations in place, we're now at the exciting moment of truth - running Bayesian inference on our latent autoregressive model!","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"real_θ = coefs_ar_5\nreal_τ = 0.5\nreal_γ = 2.0\n\norder = length(real_θ)\nn     = 500 \n\nstates, observations = generate_synthetic_dataset(n = n, θ = real_θ, τ = real_τ, γ = real_γ)\n\nresult = infer(\n    model          = lar_multivariate(order = order, γ = real_γ), \n    data           = (y = observations, ),\n    constraints    = ar_constraints(),\n    meta           = ar_meta(order),\n    initialization = ar_init(order),\n    options        = (limit_stack_depth = 500, ),\n    returnvars     = (x = KeepLast(), τ = KeepEach(), θ = KeepEach()),\n    free_energy    = true,\n    iterations     = 20\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Inference results:\n  Posteriors       | available for (τ, θ, x)\n  Free Energy:     | Real[1575.77, 1261.1, 1056.63, 1011.25, 1001.28, 997.4\n91, 995.92, 995.375, 995.066, 994.961, 994.913, 994.833, 994.899, 994.864, \n994.834, 994.894, 994.872, 994.88, 994.897, 994.913]","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Now that our inference procedure has completed, we've obtained posterior distributions for all our model parameters and latent states. The AR coefficients, precision parameters, and hidden state sequence have all been inferred from the data, with complete uncertainty quantification.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"mean(result.posteriors[:θ][end])","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"5-element Vector{Float64}:\n  0.06182696232009309\n -0.5172296604873058\n  0.18038487975628942\n -0.1591736495117487\n  0.0966030389097711","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"cov(result.posteriors[:θ][end])","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"5×5 Matrix{Float64}:\n  0.0019919    -9.0483e-5     0.00100443  -0.000263633   0.000309501\n -9.0483e-5     0.00194875   -9.51917e-5   0.000858899  -0.000261436\n  0.00100443   -9.51917e-5    0.00241975  -9.69051e-5    0.00100208\n -0.000263633   0.000858899  -9.69051e-5   0.00195085   -9.23067e-5\n  0.000309501  -0.000261436   0.00100208  -9.23067e-5    0.00199453","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"real_θ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"5-element Vector{Float64}:\n  0.10699399235785655\n -0.5237303489793305\n  0.3068897071844715\n -0.17232255282458891\n  0.13323964347539288","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"But numbers alone don't tell the full story. Let's visualize these results to better understand what our model has captured. By plotting the inferred latent states against our observations, we can see how well our model has filtered out the noise to reveal the underlying process dynamics.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"posterior_states       = result.posteriors[:x]\nposterior_τ            = result.posteriors[:τ]\n\np1 = plot(first.(states), label=\"Hidden state\")\np1 = scatter!(p1, observations, label=\"Observations\")\np1 = plot!(p1, first.(mean.(posterior_states)), ribbon = 3first.(std.(posterior_states)), label=\"Inferred states (+-3σ)\", legend = :bottomright)\np1 = lens!(p1, [20, 40], [-2, 2], inset = (1, bbox(0.2, 0.0, 0.4, 0.4)))\n\np2 = plot(mean.(posterior_τ), ribbon = 3std.(posterior_τ), label = \"Inferred τ (+-3σ)\", legend = :topright)\np2 = plot!([ real_τ ], seriestype = :hline, label = \"Real τ\")\n\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"When performing variational inference in RxInfer, the Bethe Free Energy (BFE) graph is a crucial diagnostic tool that reveals the convergence properties of our inference algorithm.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The Bethe Free Energy represents the objective function being minimized during variational inference. On the graph:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The vertical axis shows the BFE value (lower is better)\nThe horizontal axis shows iteration number\nThe downward slope indicates the algorithm is improving its approximation\nA plateau signals convergence - the point where additional iterations yield minimal improvement","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(result.free_energy, label = \"Bethe Free Energy\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For autoregressive models specifically, the BFE graph helps us:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Confirm convergence: Ensuring our parameter and state estimates are reliable\nDetect inference challenges: Slow convergence may indicate model misspecification\nCompare models: Different AR orders or constraints can be compared by their final BFE values","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"A sharply decreasing curve that quickly plateaus suggests efficient, successful inference In contrast, a slowly decreasing or unstable curve might indicate challenges with our model specification or data characteristics.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"It is also interesting to plot the convergence of our AR coefficients:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"posterior_coefficients = result.posteriors[:θ]\n\npθ = []\ncθ = Plots.palette(:tab10)\n\nθms = mean.(posterior_coefficients)\nθvs = 3std.(posterior_coefficients)\n\nfor i in 1:length(first(θms))\n    push!(pθ, plot(getindex.(θms, i), ribbon = getindex.(θvs, i), label = \"Estimated θ[$i]\", color = cθ[i]))\nend\n\nfor i in 1:length(real_θ)\n    plot!(pθ[i], [ real_θ[i] ], seriestype = :hline, label = \"Real θ[$i]\", color = cθ[i], linewidth = 2)\nend\n\nplot(pθ..., size = (800, 300), legend = :bottomright)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The inference process has successfully recovered the key parameters of our autoregressive model with good precision. Looking at the plots, we can see well-formed posterior distributions for both our AR coefficients (θ) and precision parameters (τ). The visualization reveals not just point estimates, but complete distributions that capture the remaining uncertainty in each parameter. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Prediction-with-Autoregressive-Models","page":"Autoregressive Models","title":"Prediction with Autoregressive Models","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In the next section, we'll put our inferred model to the ultimate test: prediction. Having captured the underlying dynamics of our time series through Bayesian inference, we can now use these posterior distributions to forecast future values with quantified uncertainty. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"It's worth emphasizing that in our latent autoregressive model, we're predicting the hidden state process rather than directly observed values. This is a more challenging task than prediction in classical AR models where states are directly observed. We must account for both the uncertainty in the AR dynamics and the additional uncertainty introduced by the observation model.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"RxInfer makes this process remarkably straightforward, allowing us to propagate our beliefs about model parameters and states forward in time. The resulting predictive distributions will show not just what we expect to happen next, but how confident we should be in those expectations.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Example-with-Sinusoidal-Autoregressive-Pattern","page":"Autoregressive Models","title":"Example with Sinusoidal Autoregressive Pattern","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For this demonstration, we'll use a sinusoidal-like signal to test our predictive capabilities. This choice is particularly meaningful since sinusoidal patterns can be perfectly generated by second-order autoregressive processes with specific coefficients.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function generate_sinusoidal_coefficients(; f) \n    a1 = 2cos(2pi*f)\n    a2 = -1\n    return [a1, a2]\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"generate_sinusoidal_coefficients (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The signal then would look something like:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Generate coefficients\npredictions_coefficients = generate_sinusoidal_coefficients(f = 0.03)\n\n# Generate dataset\npredictions_dataset = generate_synthetic_dataset(n = 350, θ = predictions_coefficients, τ = 1.0, γ = 0.01)\n\n# Plot dataset\nplot_synthetic_dataset(dataset = predictions_dataset, title = \"Sinusoidal AR(2)\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can use the same model as before to automatically infer the coefficients of the sinusoidal pattern and predict the future values in the following way:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"number_of_predictions = 100\n\npredictions_states, predictions_observations = predictions_dataset\n\n# We inject `missing` values to the observations to simulate \n# the future values that we want to predict\npredictions_observations_with_predictions = vcat(\n    predictions_observations,\n    [ missing for _ in 1:number_of_predictions ]\n)\n\n# It is better to use `UnfactorizedData` for prediction\npredictions_result = infer(\n    model          = lar_multivariate(order = 2, γ = 0.01), \n    data           = (y = UnfactorizedData(predictions_observations_with_predictions), ),\n    constraints    = ar_constraints(),\n    meta           = ar_meta(2),\n    initialization = ar_init(2),\n    options        = (limit_stack_depth = 500, ),\n    returnvars     = (x = KeepLast(), τ = KeepEach(), θ = KeepEach()),\n    free_energy    = false,\n    iterations     = 20\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Inference results:\n  Posteriors       | available for (τ, θ, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Note: In the current version of RxInfer, the free_energy option is not supported for prediction. Thus we explicitly set it to false. However, we already verified that the inference procedure converges to the correct coefficients with the previous example.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Prediction-results","page":"Autoregressive Models","title":"Prediction results","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We first can check if the inference procedure has converged to the correct coefficients:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Extract the inferred coefficients (mean of posterior)\ninferred_coefficients = predictions_result.posteriors[:θ][end]\n\nprintln(\"True coefficients: \", predictions_coefficients)\nprintln(\"Inferred coefficients (mean value): \", mean(inferred_coefficients))\n\nμ_true = predictions_coefficients\nμ_inferred = mean(inferred_coefficients)\n\n# Create grid of points\nx = range(μ_true[1]-0.025, μ_true[1]+0.025, length=100)\ny = range(μ_true[2]-0.025, μ_true[2]+0.025, length=100)\n\n# Create contour plot\ncoefficients_plot = contour(x, y, (x, y) -> pdf(inferred_coefficients, [x, y]), \n    fill=true, \n    title=\"True vs Inferred AR Coefficients\",\n    xlabel=\"θ₁\",\n    ylabel=\"θ₂\",\n    levels = 14, \n    color=:turbo,\n    colorbar = false\n)\n\n# Add point for true coefficients\nscatter!(coefficients_plot, [μ_true[1]], [μ_true[2]], \n    label=\"True coefficients\",\n    marker=:star,\n    markersize=20,\n    color=:red\n)\n\n# Add point for inferred mean\nscatter!(coefficients_plot, [μ_inferred[1]], [μ_inferred[2]], \n    label=\"Inferred mean\",\n    markersize=8,\n    color=:white\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"True coefficients: [1.9645745014573774, -1.0]\nInferred coefficients (mean value): [1.9607030557929608, -0.997248085033154\n8]","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Now let's visualize our prediction results to see how well our model captures the underlying temporal patterns. By plotting the predicted values against the actual test data, we can immediately assess the quality of our forecasts. Pay special attention to the confidence intervals (shaded regions) surrounding our predictions – these represent our model's uncertainty with propagated uncertainty from the inferred coefficients. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"predictions_posterior_states = predictions_result.predictions[:y][end]\n\npredictions_posterior_states_mean = mean.(predictions_posterior_states)\npredictions_posterior_states_std = std.(predictions_posterior_states)\n\npred_p = scatter(predictions_observations, label=\"Observations\", ms=2)\npred_p = plot!(pred_p, predictions_posterior_states_mean, ribbon=3predictions_posterior_states_std, label=\"Predictions\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Wide intervals suggest high uncertainty, while narrow ones indicate confidence in specific outcomes. When forecasting with AR models, several limitations deserve attention. First, AR models inherently assume that future patterns will resemble past ones - a tenuous assumption during regime changes or external shocks. Second, uncertainty compounds rapidly with prediction horizon; while one-step-ahead forecasts may appear precise, multi-step predictions quickly develop wide confidence intervals that reflect the model's decreasing predictive power.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"To put it in the comparison, we could also use the inferred parameters to predict the future values using the inferred coefficients and the precision parameter. This approach however, will not yield the uncertainty estimates that we get from the inference procedure.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function predict_manual(; number_of_predictions, coefficients, precision, first_state, rng = StableRNG(42))\n    states = [ first_state ]\n    for i in 1:(number_of_predictions + 1)\n        next_x     = rand(rng, NormalMeanPrecision(dot(coefficients, states[end]), precision))\n        next_state = vcat(next_x, states[end][1:end-1])\n        push!(states, next_state)\n    end\n    return states[2:end]\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"predict_manual (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"predicted_manually = predict_manual(; \n    number_of_predictions = number_of_predictions, \n    coefficients = predictions_coefficients, \n    precision = 0.1, \n    first_state = predictions_states[end]\n)\n\nplot(1:length(predictions_states), first.(predictions_states), label = \"Real state\")\nscatter!(1:length(predictions_observations), first.(predictions_observations), label = \"Observations\", ms = 2)\nplot!((length(predictions_observations)+1):length(predictions_observations) + number_of_predictions + 1, first.(predicted_manually), label = \"Predictions manually\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Let's plot both predictions together to see the difference:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"pred_p_manual = scatter(predictions_observations, label=\"Observations\", ms=2)\npred_p_manual = plot!(pred_p_manual, predictions_posterior_states_mean, ribbon=3predictions_posterior_states_std, label=\"Predictions\")\npred_p_manual = plot!(pred_p_manual, (length(predictions_observations)+1):length(predictions_observations) + number_of_predictions + 1, first.(predicted_manually), label = \"Predictions manual\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can see that manual prediction calculations, while computationally simpler, lack the crucial uncertainty quantification that we get from a proper Bayesian inference procedure. Additionally, the predictive power of an AR process directly relates to its order N - higher orders can capture more complex temporal dependencies and longer memory effects, but at the cost of potential overfitting. An AR(2) process can only predict based on the immediate previous observation, creating simple exponential trends, while an AR(5) can detect and forecast more intricate patterns like seasonal oscillations or cyclical behaviors. However, this improved predictive power comes with diminishing returns as N increases, requiring careful model selection to balance complexity against generalization ability for optimal forecasting performance.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Stock-Prices-Dataset","page":"Autoregressive Models","title":"Stock Prices Dataset","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Stock prices make for a challenging but instructive test case. They're notoriously difficult to predict, but often exhibit both short-term momentum (AR components) and characteristic responses to market shocks (MA components). We will use American Airlines stock data downloaded from Kaggle","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"x_df = CSV.read(\"aal_stock.csv\", DataFrame)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"1259×7 DataFrame\n  Row │ date        open     high     low      close    volume    Name\n      │ Date        Float64  Float64  Float64  Float64  Int64     String3\n──────┼───────────────────────────────────────────────────────────────────\n    1 │ 2013-02-08    15.07    15.12   14.63     14.75   8407500  AAL\n    2 │ 2013-02-11    14.89    15.01   14.26     14.46   8882000  AAL\n    3 │ 2013-02-12    14.45    14.51   14.1      14.27   8126000  AAL\n    4 │ 2013-02-13    14.3     14.94   14.25     14.66  10259500  AAL\n    5 │ 2013-02-14    14.94    14.96   13.16     13.99  31879900  AAL\n    6 │ 2013-02-15    13.93    14.61   13.93     14.5   15628000  AAL\n    7 │ 2013-02-19    14.33    14.56   14.08     14.26  11354400  AAL\n    8 │ 2013-02-20    14.17    14.26   13.15     13.33  14725200  AAL\n  ⋮   │     ⋮          ⋮        ⋮        ⋮        ⋮        ⋮         ⋮\n 1253 │ 2018-01-30    52.45    53.05   52.36     52.59   4741808  AAL\n 1254 │ 2018-01-31    53.08    54.71   53.0      54.32   5962937  AAL\n 1255 │ 2018-02-01    54.0     54.64   53.59     53.88   3623078  AAL\n 1256 │ 2018-02-02    53.49    53.99   52.03     52.1    5109361  AAL\n 1257 │ 2018-02-05    51.99    52.39   49.75     49.76   6878284  AAL\n 1258 │ 2018-02-06    49.32    51.5    48.79     51.18   6782480  AAL\n 1259 │ 2018-02-07    50.91    51.98   50.89     51.4    4845831  AAL\n                                                         1244 rows omitted","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# We will use \"close\" column\nx_data = filter(!ismissing, x_df[:, 5])\n\n# Plot data\nplot(x_data, xlabel=\"Day\", ylabel=\"Price\", label=\"Close\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Preparing-the-dataset-for-inference-and-prediction","page":"Autoregressive Models","title":"Preparing the dataset for inference and prediction","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"To validate the inference and prediction results we will also split our dataset into two parts \"observed\" and \"to_predict\", which commonly also reffered as to \"train\" and \"test\" sets.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"observed_size = length(x_data) - 50\n\n# Observed part\nx_observed    = Float64.(x_data[1:observed_size])\n\n# We need to predict this part\nx_to_predict   = Float64.(x_data[observed_size+1:end])\n\nx_observed_length   = length(x_observed)\nx_to_predict_length = length(x_to_predict)\n\nplot(1:x_observed_length, x_observed, label = \"Observed signal\")\nplot!((x_observed_length + 1):(x_observed_length + x_to_predict_length), x_to_predict, label = \"To predict\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can use the same model as before for the stock prices dataset. Let's however put the model to the ultimate test and use AR(50) to predict the future values. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"stock_observations_with_predictions = vcat(\n    x_observed,\n    [ missing for _ in 1:length(x_to_predict) ]\n)\n\nstock_predictions_result = infer(\n    model          = lar_multivariate(order = 50, γ = 1.0), \n    data           = (y = UnfactorizedData(stock_observations_with_predictions), ),\n    constraints    = ar_constraints(),\n    meta           = ar_meta(50),\n    initialization = ar_init(50),\n    options        = (limit_stack_depth = 500, ),\n    returnvars     = (x = KeepLast(), τ = KeepEach(), θ = KeepEach()),\n    free_energy    = false,\n    iterations     = 20\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Inference results:\n  Posteriors       | available for (τ, θ, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(1:x_observed_length, x_observed, label = \"Observed signal\")\nplot!((x_observed_length + 1):(x_observed_length + x_to_predict_length), x_to_predict, label = \"To predict\")\n\nstock_predictions = stock_predictions_result.predictions[:y][end]\n\nplot!(mean.(stock_predictions), ribbon = std.(stock_predictions), label = \"Predictions\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can also plot it against the hidden states in the model and using only the first component of the hidden state:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(1:x_observed_length, x_observed, label = \"Observed signal\")\nplot!((x_observed_length + 1):(x_observed_length + x_to_predict_length), x_to_predict, label = \"To predict\")\n\nstock_hidden_states = stock_predictions_result.posteriors[:x]\n\nplot!(first.(mean.(stock_hidden_states)), ribbon = first.(std.(stock_hidden_states)), label = \"x[1]\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Autoregressive-Moving-Average-Model","page":"Autoregressive Models","title":"Autoregressive Moving Average Model","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"As a final touch, we will implement a fully Bayesian version of ARMA model. Autoregressive Moving Average (ARMA) models represent a powerful synthesis of two fundamental time series components: the autoregressive (AR) part, which captures how current values depend on past observations, and the moving average (MA) part, which models the persistence of random shocks. This combination makes ARMA models particularly well-suited for financial data like stock prices, where both momentum effects (AR) and reaction to news or market shocks (MA) influence price movements. In this example, we'll see how Bayesian inference with RxInfer can reveal these underlying dynamics while quantifying our uncertainty every step of the way.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Mathematical-Formulation-of-ARMA-Model","page":"Autoregressive Models","title":"Mathematical Formulation of ARMA Model","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Bayesian ARMA model can be effectively implemeted in RxInfer.jl. For theoretical details on Varitional Inference for ARMA model, we refer the reader to the following paper.  The Bayesian ARMA model can be written as follows:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\ne_t sim mathcalN(0 gamma^-1) \ntheta sim mathcalN(mathbf0 mathbfI) \neta sim mathcalN(mathbf0 mathbfI) \nmathbfh_0 sim mathcalNleft(beginbmatrix\ne_-1 \ne_-2\nendbmatrix mathbfIright) \nmathbfh_t = mathbfSmathbfh_t-1 + mathbfc e_t-1 \nmathbfx_t = boldsymboltheta^topmathbfx_t-1 + boldsymboleta^topmathbfh_t + e_t \nendaligned","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"where shift matrix mathbfS is defined as","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\nmathbfS = beginpmatrix\n0  0 \n1  0 \nendpmatrix\nendaligned","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function shift(dim)\n    S = Matrix{Float64}(I, dim, dim)\n    for i in dim:-1:2\n        S[i,:] = S[i-1, :]\n    end\n    S[1, :] = zeros(dim)\n    return S\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"shift (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"shift(2)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"2×2 Matrix{Float64}:\n 0.0  0.0\n 1.0  0.0","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"and unit vector mathbfc: ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\nmathbfc=1 0\nendaligned","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"when MA order is 2. In this way, mathbfh_t containing errors e_t can be viewed as hidden state.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Intractabilities-in-ARMA-model","page":"Autoregressive Models","title":"Intractabilities in ARMA model","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In short, the Bayesian ARMA model has two intractabilities: ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"induced by the multiplication of two Gaussian RVs, i.e., boldsymboleta^topmathbfh_t, \ninduced by errors e_t that prevents analytical update of precision parameter gamma (this can be easily seen when constructing the Factor Graph, i.e. there is a loop). ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Both problems can be easily resolved in RxInfer.jl, by creating a hybrid inference algorithm based on Loopy Variational Message Passing.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#ARMA-model-specification","page":"Autoregressive Models","title":"ARMA model specification","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The model specification is the trickiest part of this implementation. Note how we need to carefully define the relationship between observed values, latent states, and error terms. The ARMA model's loops create inference challenges that wouldn't exist in simpler models - this is why we need to specify proper initialization and factorization constraints to avoid convergence problems. For the @meta we will simply reuse the previously defined ar_meta function.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@model function ARMA(x, x_prev, priors, p_order, q_order)\n    \n    # arguments\n    c = zeros(q_order); c[1] = 1.0;\n    S = shift(q_order); # MA\n\n    # set priors\n    γ    ~ priors[:γ]\n    η    ~ priors[:η]\n    θ    ~ priors[:θ]\n    τ    ~ priors[:τ]\n    \n    h[1] ~ priors[:h]\n    z[1] ~ AR(h[1], η, τ)\n    e[1] ~ Normal(mean = 0.0, precision = γ)\n    x[1] ~ dot(c, z[1]) + dot(θ, x_prev[1]) + e[1]\n\n    for t in 1:length(x)-1\n        h[t+1] ~ S * h[t] + c * e[t]\n        z[t+1] ~ AR(h[t+1], η, τ)\n        e[t+1] ~ Normal(mean = 0.0, precision = γ)\n        x[t+1] ~ dot(c, z[t+1]) + dot(θ, x_prev[t+1]) + e[t+1]\n    end\nend\n\n@constraints function arma_constraints()\n    q(z, h, η, τ, γ,e) = q(z, h)q(η)q(τ)q(γ)q(e)\nend\n\n@initialization function arma_initialization(priors) \n    q(h)   = priors[:h]\n    μ(h)   = priors[:h]\n    q(γ)   = priors[:γ]\n    q(τ)   = priors[:τ]\n    q(η)   = priors[:η]\n    q(θ)   = priors[:θ]\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"arma_initialization (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"p_order = 10 # AR\nq_order = 4  # MA","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"4","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Inference-with-ARMA-model","page":"Autoregressive Models","title":"Inference with ARMA model","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Now, everything should be ready for the infer call from RxInfer on the stock prices dataset defined earlier.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"priors  = (\n    h = MvNormalMeanPrecision(zeros(q_order), diageye(q_order)),\n    γ = GammaShapeRate(1e4, 1.0),\n    τ = GammaShapeRate(1e2, 1.0),\n    η = MvNormalMeanPrecision(ones(q_order), diageye(q_order)),\n    θ = MvNormalMeanPrecision(zeros(p_order), diageye(p_order))\n)\n\narma_x_data = Float64.(x_data[p_order+1:end])[1:observed_size]\narma_x_prev_data = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][1:observed_size]\n\nresult = infer(\n    model = ARMA(priors=priors, p_order = p_order, q_order = q_order), \n    data  = (x = arma_x_data, x_prev = arma_x_prev_data),\n    initialization = arma_initialization(priors),\n    constraints    = arma_constraints(),\n    meta           = ar_meta(q_order),\n    returnvars     = KeepLast(),\n    iterations     = 20,\n    options        = (limit_stack_depth = 400, ),\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Inference results:\n  Posteriors       | available for (γ, e, τ, h, z, θ, η)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(mean.(result.posteriors[:e]), ribbon = var.(result.posteriors[:e][end]), label = \"eₜ\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"What we've seen in this example is more than just a stock price forecast - it's a demonstration of how modern probabilistic programming with RxInfer enables sophisticated time series modeling with relatively concise code. The same techniques can be applied across domains from economics to engineering, wherever systems exhibit both memory effects and response to external shocks. And most importantly, the Bayesian approach gives us a principled way to quantify uncertainty in our predictions - essential for robust decision-making in any domain.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/autoregressive_models/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [a93c6f00] DataFrames v1.8.0\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#Bayesian-Binomial-Regression","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"This notebook is an introductory tutorial to Bayesian binomial regression with RxInfer.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"using RxInfer, ReactiveMP, Random, Plots, StableRNGs, LinearAlgebra, StatsPlots, LaTeXStrings","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#Likelihood-Specification","page":"Bayesian Binomial Regression","title":"Likelihood Specification","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"For observations y_i with predictors mathbfx_i, Binomial regression models the number of successes y_i as a function of the predictors mathbfx_i and the regression coefficients boldsymbolbeta","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"beginequation\ny_i sim textBinomial(n_i p_i)\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"where:","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"y_i","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"is the number of successes, n_i is the number of trials, p_i is the probability of success. The probability p_i is linked to the predictors through the logistic function:","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"beginequation\np_i = frac11 + e^-mathbfx_i^Tboldsymbolbeta\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#Prior-Distributions","page":"Bayesian Binomial Regression","title":"Prior Distributions","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"We specify priors for the regression coefficients:","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"beginequation\nboldsymbolbeta sim mathcalN_xi(boldsymbolxi boldsymbolLambda)\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"as a Normal distribution in precision-weighted mean form.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#Model-Specification","page":"Bayesian Binomial Regression","title":"Model Specification","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"The likelihood and the prior distributions form the probabilistic model","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"p(y x beta n) = p(beta) prod_i=1^N p(y_i mid x_i beta n_i)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"where the goal is to infer the posterior distributions p(beta mid y x n). Due to logistic link function, the posterior distribution is not conjugate to the prior distribution. This means that we need to use a more complex inference algorithm to infer the posterior distribution. Before dwelling into the details of the inference algorithm, let's first generate some synthetic data to work with.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"function generate_synthetic_binomial_data(\n    n_samples::Int,\n    true_beta::Vector{Float64};\n    seed::Int=42\n)\n    n_features = length(true_beta)\n    rng = StableRNG(seed)\n    \n    X = randn(rng, n_samples, n_features)\n    \n    n_trials = rand(rng, 5:20, n_samples)\n    \n    logits = X * true_beta\n    probs = 1 ./ (1 .+ exp.(-logits))\n    \n    y = [rand(rng, Binomial(n_trials[i], probs[i])) for i in 1:n_samples]\n    \n    return X, y, n_trials, probs\nend\n\n\nn_samples = 10000\ntrue_beta =  [-3.0 , 2.6]\n\nX, y, n_trials,probs = generate_synthetic_binomial_data(n_samples, true_beta);\nX = [collect(row) for row in eachrow(X)];","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"We generate X as the design matrix and y as the number of successes and n_trials as the number of trials. Next task is to define the graphical model. RxInfer provides a BinomialPolya factor node that is a combination of a Binomial distribution and a PolyaGamma distribution introduced in [1]. The BinomialPolya factor node is used to model the likelihood of the binomial distribution. ","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Due to non-conjugacy of the likelihood and the prior distribution, we need to use a more complex inference algorithm. RxInfer provides an Expectation Propagation (EP) [2] algorithm to infer the posterior distribution. Due to EP's approximation, we need to specify an inbound message for the regression coefficients while using the BinomialPolya factor node. This feature is implemented in the dependencies keyword argument during the creation of the BinomialPolya factor node. ReactiveMP.jl provides a RequireMessageFunctionalDependencies type that is used to specify the inbound message for the regression coefficients β. Refer to the ReactiveMP.jl documentation for more information.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"@model function binomial_model(prior_xi, prior_precision, n_trials, X, y) \n    β ~ MvNormalWeightedMeanPrecision(prior_xi, prior_precision)\n    for i in eachindex(y)\n        y[i] ~ BinomialPolya(X[i], n_trials[i], β) where {\n            dependencies = RequireMessageFunctionalDependencies(β = MvNormalWeightedMeanPrecision(prior_xi, prior_precision))\n        }\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"This example uses the precision-weighted mean parametrization (MvNormalWeightedMeanPrecision) of the Gaussian distribution for efficiency reasons. While this is less conventional than the standard mean-covariance form, the example would work equally well with any parametrization. The choice of parametrization mainly affects computational efficiency and numerical stability, not the underlying model or results.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Having specified the model, we can now utilize the infer function to infer the posterior distribution.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"n_features = length(true_beta)\nresults = infer(\n    model = binomial_model(prior_xi = zeros(n_features), prior_precision = diageye(n_features),),\n    data = (X=X, y=y,n_trials=n_trials),\n    iterations = 30,\n    free_energy = true,\n    showprogress = true,\n    options = (\n        limit_stack_depth = 100, # to prevent stack-overflow errors\n    )\n)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Inference results:\n  Posteriors       | available for (β)\n  Free Energy:     | Real[21992.9, 16235.8, 13785.0, 12519.7, 11800.1, 1136\n6.2, 11094.1, 10918.7, 10803.3, 10726.3  …  10561.6, 10560.6, 10559.9, 1055\n9.4, 10559.0, 10558.8, 10558.6, 10558.5, 10558.4, 10558.3]","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"We can now plot the free energy to see if the inference algorithm is converging.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"plot(results.free_energy,fontfamily = \"Computer Modern\", label=\"Free Energy\", xlabel=\"Iteration\", ylabel=\"Free Energy\", title=\"Free Energy Convergence\")","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Free energy is converging to a stable value, indicating that the inference algorithm is converging. Let's visualize the posterior distribution and how it compares to the true parameters.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"# Create an animation showing how posterior evolves\nanim = @animate for i in 1:length(results.posteriors[:β])\n    # Get posterior at current iteration\n    m_i = mean(results.posteriors[:β][i])\n    Σ_i = cov(results.posteriors[:β][i])\n    \n    # Calculate dynamic limits based on current mean and covariance\n    # Add some padding (3 standard deviations) to ensure true parameters are visible\n    x_std = sqrt(Σ_i[1,1])\n    y_std = sqrt(Σ_i[2,2])\n    \n    x_min = min(m_i[1] - 3*x_std, true_beta[1] - 0.1)\n    x_max = max(m_i[1] + 3*x_std, true_beta[1] + 0.1)\n    y_min = min(m_i[2] - 3*y_std, true_beta[2] - 0.1)\n    y_max = max(m_i[2] + 3*y_std, true_beta[2] + 0.1)\n    \n    p = plot(xlims=(x_min, x_max), ylims=(y_min, y_max),\n             fontfamily = \"Computer Modern\",\n             title=\"Iteration $i\", aspect_ratio=1)\n    \n    # Plot confidence ellipses\n    covellipse!(m_i, Σ_i, n_std=1, label=\"1σ Contour\", color=:green, fillalpha=0.2)\n    covellipse!(m_i, Σ_i, n_std=3, label=\"3σ Contour\", color=:blue, fillalpha=0.2)\n    \n    # Plot mean estimate and true parameters\n    scatter!([m_i[1]], [m_i[2]], label=\"Current Estimate\", color=:blue)\n    scatter!([true_beta[1]], [true_beta[2]], label=\"True Parameters\", color=:red)\nend\n\n# Save the animation as a GIF\ngif(anim, \"bayesian_regression_posterior.gif\", fps=3)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Plots.AnimatedGif(\"/home/runner/work/RxInferExamples.jl/RxInferExamples.jl/\ndocs/src/categories/basic_examples/bayesian_binomial_regression/bayesian_re\ngression_posterior.gif\")","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"We can perform prediction by augmenting the data with missing values. For that, we can create a new vector y_with_missing that contains missing values for the last 2000 samples.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"y_with_missing = Vector{Union{Missing, Int}}(missing, n_samples)\nfor i in 1:n_samples\n    if i > 8000\n        y_with_missing[i] = missing\n    else\n        y_with_missing[i] = y[i]\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"results_with_missing = infer(\n    model = binomial_model(prior_xi = zeros(n_features), prior_precision = diageye(n_features),),\n    data = (X=X, y=y_with_missing,n_trials=n_trials),\n    iterations = 30,\n    showprogress = true,\n    options = (\n        limit_stack_depth = 100, # to prevent stack-overflow errors\n    )\n)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Inference results:\n  Posteriors       | available for (β)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"probs_prediction = map(d -> d.p,results_with_missing.predictions[:y][end][8000:end])\nerr = probs_prediction .- probs[8000:end]\nmse = mean(err.^2)\nprintln(\"Mean squared error: \", mse)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Mean squared error: 3.541846183800687e-6","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"function bin_predictions(true_probs, pred_probs; n_bins=20)\n    bins = range(0, 1, length=n_bins+1)\n    bin_means = Float64[]\n    bin_stds = Float64[]\n    bin_centers = Float64[]\n    \n    for i in 1:n_bins\n        mask = (true_probs .>= bins[i]) .& (true_probs .< bins[i+1])\n        if any(mask)\n            push!(bin_means, mean(pred_probs[mask]))\n            push!(bin_stds, std(pred_probs[mask]))\n            push!(bin_centers, (bins[i] + bins[i+1])/2)\n        end\n    end\n    return bin_centers, bin_means, bin_stds\nend\n\n# Create the plot\nbin_centers, bin_means, bin_stds = bin_predictions(probs[8000:end], probs_prediction)\n\np = plot(\n    xlabel = \"True Probability\",\n    ylabel = \"Predicted Probability\",\n    title = \"Prediction Performance\",\n    aspect_ratio = 1,\n    legend = :bottomright,\n    grid = true,\n    fontfamily = \"Computer Modern\",\n    dpi = 300\n)\n\n# Add perfect prediction line\nplot!([0, 1], [0, 1], \n    label = \"Perfect Prediction\", \n    color = :black, \n    linestyle = :dash,\n    linewidth = 2\n)\n\n# Add scatter plot with reduced opacity and size\nscatter!(\n    probs[8000:end], \n    probs_prediction,\n    label = \"Individual Predictions\",\n    alpha = 0.1,  # Reduced opacity\n    color = :blue,\n    markersize = 1,\n    markerstrokewidth = 0\n)\n\n# Add binned means with error bars\nscatter!(\n    bin_centers,\n    bin_means,\n    yerror = bin_stds,\n    label = \"Binned Mean ± SD\",\n    color = :red,\n    markersize = 4\n)\n\nannotate!(\n    0.05, \n    0.95, \n    text(\"MSE = $(round(mse, digits=8))\", 8, :left, :top)\n)\n\n# Customize axes\nplot!(\n    xlims = (0,1),\n    ylims = (0,1),\n    xticks = 0:0.2:1,\n    yticks = 0:0.2:1\n)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#References","page":"Bayesian Binomial Regression","title":"References","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"[1] Polson, N. G., Scott, J. G., & Windle, J. (2013). Bayesian inference for logistic models using Polya-Gamma latent variables. Journal of the American Statistical Association, 108(1), 136-146.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"[2] Minka, T. (2001). Expectation Propagation for approximate Bayesian inference. Uncertainty in Artificial Intelligence, 2, 362-369.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/bayesian_binomial_regression/Project.toml`\n  [b964fa9f] LaTeXStrings v1.4.0\n  [91a5bcdd] Plots v1.41.1\n  [a194aa59] ReactiveMP v5.6.0\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [f3b207a7] StatsPlots v0.15.8\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_networks/#Bayesian-Networks:-The-Sprinkler-Model","page":"Bayesian Networks","title":"Bayesian Networks: The Sprinkler Model","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"In this tutorial, we'll explore how to build and perform inference in Bayesian networks using RxInfer. Bayesian networks are probabilistic graphical models that represent conditional dependencies between random variables through directed acyclic graphs. We'll use the DiscreteTransition node type to encode conditional probability tables (CPTs) that define how each variable depends on its parents in the network. Through a classic sprinkler example, we'll demonstrate how to construct these networks and perform probabilistic inference to answer queries about the model.","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"using RxInfer, Plots, GraphViz","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"This example implements a classic Bayesian network known as the sprinkler model. The model represents causal relationships between:","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Whether it is cloudy (clouded)\nWhether it is raining (rain) \nWhether the sprinkler is on (sprinkler)\nWhether the grass is wet (wet_grass)","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"The relationships are:","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Cloudy weather influences both rain and sprinkler usage\nBoth rain and sprinkler usage affect whether the grass is wet","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"We'll use this model to demonstrate inference under different scenarios and evidence.","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"The bayesian network for the sprinkler model is shown below:","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"(Image: Sprinkler model diagram showing the relationships between clouded, rain, sprinkler and wet_grass nodes)","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Let's translate this into an RxInfer model. We'll start by putting a Categorial prior on the clouded variable, and we'll use DiscreteTransition nodes to encode the CPTs for the rain and sprinkler variables, which depend on the clouded variable. Denote that we denote a binary probability distribution as a vector of two probabilities, where the first probability is the probability of the event not occurring and the second probability is the probability of the event occurring.","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"@model function sprinkler_model(wet_grass)\n    clouded ~ Categorical([0.5, 0.5]) # Probability of cloudy being false or true\n    rain ~ DiscreteTransition(clouded, [0.8 0.2; 0.2 0.8])\n    sprinkler ~ DiscreteTransition(clouded, [0.5 0.9; 0.5 0.1])\n    wet_grass ~ DiscreteTransition(sprinkler, [1.0 0.1; 0.0 0.9;;; 0.1 0.01; 0.9 0.99], rain)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Optionally, let's inspect the model structure that RxInfer creates using the GraphViz package:","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"model_generator = sprinkler_model() | (wet_grass = [ 1.0, 0.0 ], )\nmodel_to_plot   = RxInfer.getmodel(RxInfer.create_model(model_generator))\nGraphViz.load(model_to_plot, strategy = :simple)","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Because we have a loop in the resulting factor graph, we have to initialize messages to run the loopy belief propagation algorithm. We'll initialize the messages for the sprinkler variable to be uniform. Afterwards, we can run the inference with the infer function from RxInfer. Furthermore, we can specify the number of iterations to run the loopy belief propagation algorithm, and we can query the free_energy to monitor the convergence of the algorithm.","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"initialization = @initialization begin\n    μ(sprinkler) = Categorical([0.5, 0.5])\nend\n\ndata = (wet_grass = [1.0, 0.0],) # Grass is dry\n\nresult = infer(model=sprinkler_model(), data=data, iterations=10, initialization=initialization)","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Inference results:\n  Posteriors       | available for (rain, sprinkler, clouded)","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Now, let's inspect the posterior probabilities for the clouded, rain, and sprinkler variables:","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"p1 = bar(last(result.posteriors[:clouded]).p,\n    xticks=(1:2, [\"Not Clouded\", \"Clouded\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Clouded Variable\",\n    titlefontsize=10,\n    legend=false)\n\np2 = bar(last(result.posteriors[:rain]).p,\n    xticks=(1:2, [\"No Rain\", \"Rain\"]),\n    ylabel=\"Probability\", \n    title=\"Posterior Probability of Rain Variable\",\n    titlefontsize=10,\n    legend=false)\n\np3 = bar(last(result.posteriors[:sprinkler]).p,\n    xticks=(1:2, [\"Off\", \"On\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Sprinkler Variable\", \n    titlefontsize=10,\n    legend=false)\n\nplot(p1, p2, p3, layout=(1,3), size=(900,300))","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Looks like, when the grass is dry, it is less likely to be cloudy, less likely to rain and the sprinkler is probably off. Let's look at what happens when we observe wet grass:","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"result = infer(model=sprinkler_model(), data=(wet_grass=[0.0, 1.0],), iterations=10, initialization=initialization)\np1 = bar(last(result.posteriors[:clouded]).p,\n    xticks=(1:2, [\"Not Clouded\", \"Clouded\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Clouded Variable\",\n    titlefontsize=10,\n    legend=false)\n\np2 = bar(last(result.posteriors[:rain]).p,\n    xticks=(1:2, [\"No Rain\", \"Rain\"]),\n    ylabel=\"Probability\", \n    title=\"Posterior Probability of Rain Variable\",\n    titlefontsize=10,\n    legend=false)\n\np3 = bar(last(result.posteriors[:sprinkler]).p,\n    xticks=(1:2, [\"Off\", \"On\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Sprinkler Variable\", \n    titlefontsize=10,\n    legend=false)\n\nplot(p1, p2, p3, layout=(1,3), size=(900,300))","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Perfect! When the grass is wet, it is more likely to be cloudy, more likely to rain and the sprinkler is more likely to be on. However, this model only allows observations on the grass being wet, and not on the sprinkler being on or off. Let's extend the model to also observe the sprinkler and rain variables:","category":"page"},{"location":"categories/basic_examples/bayesian_networks/#Extending-the-model","page":"Bayesian Networks","title":"Extending the model","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"We will now also accept observations on the sprinkler, rain  and clouded variables, which means that we will have to add the variables to the model signature and denote how this data is generated. We know that these datapoints are the same as the hidden variables they are conditioned on, so we can use a DiscreteTransition node to encode this identity relationship between the data and the hidden variables.","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"@model function sprinkler_model(wet_grass_data, sprinkler_data, rain_data, clouded_data)\n    clouded ~ Categorical([0.5, 0.5]) # Probability of cloudy being false or true\n    clouded_data ~ DiscreteTransition(clouded, diageye(2))  \n    rain ~ DiscreteTransition(clouded, [0.8 0.2; 0.2 0.8])\n    rain_data ~ DiscreteTransition(rain, diageye(2))\n    sprinkler ~ DiscreteTransition(clouded, [0.5 0.9; 0.5 0.1])\n    sprinkler_data ~ DiscreteTransition(sprinkler, diageye(2))\n    wet_grass ~ DiscreteTransition(sprinkler, [1.0 0.1; 0.0 0.9;;; 0.1 0.01; 0.9 0.99], rain)\n    wet_grass_data ~ DiscreteTransition(wet_grass, diageye(2))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Now, we can run the inference with the extended model, where we pass in the data for the observations we have, and pass missing for the observations we don't have. What happens, for example, if we observe the grass being wet and the sprinkler being on?","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"result = infer(model=sprinkler_model(), data=(wet_grass_data=[0.0, 1.0], sprinkler_data=[0.0, 1.0], rain_data=missing, clouded_data=missing), iterations=10, initialization=initialization)","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Inference results:\n  Posteriors       | available for (rain, sprinkler, wet_grass, clouded)\n  Predictions      | available for (rain_data, clouded_data)","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Perfect! When the grass is wet and the sprinkler is on, it is less likely to rain, and therefore less likely to be cloudy. What happens to the grass when we observe rain and the sprinkler is off?","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"result = infer(model=sprinkler_model(), data=(wet_grass_data=missing, sprinkler_data=[1.0, 0.0], rain_data=[0.0, 1.0], clouded_data=missing), iterations=10, initialization=initialization)\n\n\np1 = bar(last(result.posteriors[:rain]).p,\n    xticks=(1:2, [\"No\", \"Yes\"]),\n    ylabel=\"Probability\", \n    title=\"Posterior Probability of Rain Variable\",\n    titlefontsize=8,\n    legend=false)\n\np2 = bar(last(result.posteriors[:clouded]).p,\n    xticks=(1:2, [\"No\", \"Yes\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Clouded Variable\",\n    titlefontsize=8,\n    legend=false)\n\np3 = bar(last(result.posteriors[:sprinkler]).p,\n    xticks=(1:2, [\"Off\", \"On\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Sprinkler Variable\", \n    titlefontsize=8,\n    legend=false)\n\np4 = bar(last(result.posteriors[:wet_grass]).p,\n    xticks=(1:2, [\"No\", \"Yes\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Wet Grass Variable\",\n    titlefontsize=8,\n    legend=false)\n\nplot(p1, p2, p3, p4, layout=(1,4), size=(1200,300))","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Finally, what happens if we observe the grass being wet and the sky being blue?","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"result = infer(model=sprinkler_model(), data=(wet_grass_data=[0.0, 1.0], sprinkler_data=missing, rain_data=missing, clouded_data=[1.0, 0.0]), iterations=10, initialization=initialization)\n\n\np1 = bar(last(result.posteriors[:rain]).p,\n    xticks=(1:2, [\"No\", \"Yes\"]),\n    ylabel=\"Probability\", \n    title=\"Posterior Probability of Rain Variable\",\n    titlefontsize=8,\n    legend=false)\n\np2 = bar(last(result.posteriors[:clouded]).p,\n    xticks=(1:2, [\"No\", \"Yes\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Clouded Variable\",\n    titlefontsize=8,\n    legend=false)\n\np3 = bar(last(result.posteriors[:sprinkler]).p,\n    xticks=(1:2, [\"Off\", \"On\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Sprinkler Variable\", \n    titlefontsize=8,\n    legend=false)\n\np4 = bar(last(result.posteriors[:wet_grass]).p,\n    xticks=(1:2, [\"No\", \"Yes\"]),\n    ylabel=\"Probability\",\n    title=\"Posterior Probability of Wet Grass Variable\",\n    titlefontsize=8,\n    legend=false)\n\nplot(p1, p2, p3, p4, layout=(1,4), size=(1200,300))","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_networks/#Learning-the-CPTs","page":"Bayesian Networks","title":"Learning the CPTs","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"We can also use RxInfer to learn the CPT's when we have data available, for this, let's generate some data","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"# Generate synthetic data from the true model\nn_samples = 10000\n\n# Initialize arrays to store the samples\nclouded_samples = zeros(Int, n_samples)\nrain_samples = zeros(Int, n_samples)\nsprinkler_samples = zeros(Int, n_samples) \nwet_grass_samples = zeros(Int, n_samples)\n\n# Sample from the model\nfor i in 1:n_samples\n    # Sample clouded (prior)\n    clouded_samples[i] = rand() < 0.5 ? 1 : 2\n    \n    # Sample rain (depends on clouded)\n    rain_prob = clouded_samples[i] == 1 ? 0.2 : 0.8\n    rain_samples[i] = rand() > rain_prob ? 1 : 2\n    \n    # Sample sprinkler (depends on clouded)\n    sprinkler_prob = clouded_samples[i] == 1 ? 0.5 : 0.1\n    sprinkler_samples[i] = rand() > sprinkler_prob ? 1 : 2\n    \n    # Sample wet grass (depends on rain and sprinkler)\n    if rain_samples[i] == 2 && sprinkler_samples[i] == 2\n        wet_prob = 0.99\n    elseif rain_samples[i] == 2\n        wet_prob = 0.9\n    elseif sprinkler_samples[i] == 2\n        wet_prob = 0.9\n    else\n        wet_prob = 0.0\n    end\n    wet_grass_samples[i] = rand() < wet_prob ? 2 : 1\nend\n# Convert to one-hot encoding\nclouded_data = [[i == s ? 1.0 : 0.0 for i in 1:2] for s in clouded_samples]\nrain_data = [[i == s ? 1.0 : 0.0 for i in 1:2] for s in rain_samples]\nsprinkler_data = [[i == s ? 1.0 : 0.0 for i in 1:2] for s in sprinkler_samples]\nwet_grass_data = [[i == s ? 1.0 : 0.0 for i in 1:2] for s in wet_grass_samples];","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"The model now becomes a little bit more complex, as we have to put a prior on the CPT's, and we have to materialze the model for every datapoint we have. Luckily, in RxInfer, we can make a submodel and reuse it for every datapoint. We can put a DirichletCollection prior on the CPT's, as it is the conjugate prior of the CPT's. Because we cannot do belief propagation with a DirichletCollection prior, we have to introduce variational constraints to introduce an Expectation Maximization-style schema. This can be done with the @constraints macro. Furthermore, we can use the @initialization macro to initialize the variational distributions and kickstart the inference procedure.","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"@model function sprinkler_model(clouded_data, rain_data, sprinkler_data, wet_grass_data, cpt_cloud_rain, cpt_cloud_sprinkler, cpt_sprinkler_rain_wet_grass)\n    clouded ~ Categorical([0.5, 0.5]) # Probability of cloudy being false or true\n    clouded_data ~ DiscreteTransition(clouded, diageye(2))\n    rain ~ DiscreteTransition(clouded, cpt_cloud_rain)\n    rain_data ~ DiscreteTransition(rain, diageye(2))\n    sprinkler ~ DiscreteTransition(clouded, cpt_cloud_sprinkler)\n    sprinkler_data ~ DiscreteTransition(sprinkler, diageye(2))\n    wet_grass ~ DiscreteTransition(sprinkler, cpt_sprinkler_rain_wet_grass, rain)\n    wet_grass_data ~ DiscreteTransition(wet_grass, diageye(2))\nend\n\n@model function learn_sprinkler_model(clouded_data, rain_data, sprinkler_data, wet_grass_data)\n    cpt_cloud_rain ~ DirichletCollection(ones(2, 2))\n    cpt_cloud_sprinkler ~ DirichletCollection(ones(2, 2))\n    cpt_sprinkler_rain_wet_grass ~ DirichletCollection(ones(2, 2, 2))\n    for i in 1:length(clouded_data)\n        wet_grass_data[i] ~ sprinkler_model(clouded_data = clouded_data[i], rain_data = rain_data[i], sprinkler_data = sprinkler_data[i], cpt_cloud_rain = cpt_cloud_rain, cpt_cloud_sprinkler = cpt_cloud_sprinkler, cpt_sprinkler_rain_wet_grass = cpt_sprinkler_rain_wet_grass)\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"initialization = @initialization begin\n    q(cpt_cloud_rain) = DirichletCollection(ones(2, 2))\n    q(cpt_cloud_sprinkler) = DirichletCollection(ones(2, 2))\n    q(cpt_sprinkler_rain_wet_grass) = DirichletCollection(ones(2, 2, 2))\n    for init in sprinkler_model\n        μ(sprinkler) = Categorical([0.5, 0.5])\n    end\nend\n\nconstraints = @constraints begin\n    for q in sprinkler_model\n        q(cpt_cloud_rain, clouded, rain) = q(clouded,rain)q(cpt_cloud_rain)\n        q(cpt_cloud_sprinkler, clouded, sprinkler) = q(clouded,sprinkler)q(cpt_cloud_sprinkler)\n        q(cpt_sprinkler_rain_wet_grass, sprinkler, rain, wet_grass) = q(sprinkler,rain,wet_grass)q(cpt_sprinkler_rain_wet_grass)\n    end\nend\n\nresult = infer(model=learn_sprinkler_model(), \n            data=(clouded_data=clouded_data, rain_data=rain_data, sprinkler_data=sprinkler_data, wet_grass_data=wet_grass_data), \n            constraints=constraints, \n            initialization=initialization, \n            iterations=5, \n            showprogress=true,\n            options=(limit_stack_depth=500,))","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Inference results:\n  Posteriors       | available for (cpt_cloud_sprinkler, cpt_cloud_rain, cp\nt_sprinkler_rain_wet_grass)","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Wow! That was fast! Let's inspect the learned CPT's:","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"using Plots\n\n# Plot CPT for cloud -> rain\ncloud_rain = mean(last(result.posteriors[:cpt_cloud_rain]))\np1 = heatmap(cloud_rain, \n        title=\"P(Rain | Cloudy)\", \n        xlabel=\"Cloudy\", \n        ylabel=\"Rain\",\n        xticks=(1:2, [\"False\", \"True\"]),\n        yticks=(1:2, [\"False\", \"True\"]),\n        xrotation=45,\n        left_margin=10Plots.mm,\n        bottom_margin=10Plots.mm)\n\n# Plot CPT for cloud -> sprinkler\ncloud_sprinkler = mean(last(result.posteriors[:cpt_cloud_sprinkler]))\np2 = heatmap(cloud_sprinkler,\n        title=\"P(Sprinkler | Cloudy)\",\n        xlabel=\"Cloudy\",\n        ylabel=\"Sprinkler\", # Remove y-label since it's shown in p1\n        xticks=(1:2, [\"False\", \"True\"]),\n        yticks=(1:2, [\"False\", \"True\"]),\n        xrotation=45,\n        bottom_margin=10Plots.mm)\n\n# Plot CPT for sprinkler,rain -> wet grass\nsprinkler_rain_wet = mean(last(result.posteriors[:cpt_sprinkler_rain_wet_grass]))\np3 = heatmap(sprinkler_rain_wet[:,:,1],\n        title=\"P(Wet Grass=False | Sprinkler,Rain)\",\n        xlabel=\"Rain\",\n        ylabel=\"Sprinkler\", # Remove y-label since it's shown in p1\n        xticks=(1:2, [\"False\", \"True\"]),\n        yticks=(1:2, [\"False\", \"True\"]),\n        xrotation=45,\n        bottom_margin=10Plots.mm)\np4 = heatmap(sprinkler_rain_wet[:,:,2],\n        title=\"P(Wet Grass=True | Sprinkler,Rain)\", \n        xlabel=\"Rain\",\n        ylabel=\"Sprinkler\", # Remove y-label since it's shown in p1\n        xticks=(1:2, [\"False\", \"True\"]),\n        yticks=(1:2, [\"False\", \"True\"]),\n        xrotation=45,\n        bottom_margin=15Plots.mm)\n\nplot(p1, p2, p3, p4, layout=(1,4), size=(1700,305))","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"This concludes our tutorial on Bayesian networks. We have seen how to build and perform inference in Bayesian networks using RxInfer. We have also seen how to learn the CPT's when we have data available. As we have seen, RxInfer is able to learn posterior distributions even when some of the data is missing, what do you think will happen if we pass missing data to our model that learns the CPT's?","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/bayesian_networks/Project.toml`\n  [f526b714] GraphViz v0.2.0\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n","category":"page"},{"location":"categories/basic_examples/bayesian_networks/","page":"Bayesian Networks","title":"Bayesian Networks","text":"","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/#How-to-train-your-Hidden-Markov-Model","page":"Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"","category":"section"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"In this example, we'll be tracking a Roomba as it moves throughout a 3-bedroom apartment consisting of a bathroom, a master bedroom, and a living room. It's important to keep track of your AI's, so we want to make sure we can keep tabs on it whenever we leave the apartment. ","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"First, in order to track the Roomba's movements using RxInfer, we need to come up with a model. Since we have a discrete set of rooms in the apartment, we can use a categorical distribution to represent the Roomba's position. There are three  rooms in the apartment, meaning we need three states in our categorical distribution. At time t, let's call the estimate of the Roomba's position s_t.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"However, we also know that some rooms are more accessible than others, meaning the Roomba is more likely to move between these rooms - for example, it's rare to have a door directly between the bathroom and the master bedroom. We can encode this information using a transition matrix, which we will call A.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Our Roomba is equipped with a small camera that tracks the surface it is moving over. We will use this camera to obtain our observations since we know that there is a carpet in the living room, tiles in the bathroom, and hardwood floors in the master bedroom. However, this method is not foolproof, and sometimes the Roomba will make mistakes and mistake the hardwood floor for tiles or the carpet for hardwood. Don't be too hard on the little guy, it's just a Roomba after all.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"At time t, we will call our observations x_t and encode the mapping from the Roomba's position to the observations in a matrix we call B. B also encodes the likelihood that the Roomba will make a mistake and get the wrong observation. This leaves us with the following model specification:","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"beginaligned\n    s_t  sim mathcalCat(A s_t-1)\n    x_t  sim mathcalCat(B s_t)\nendaligned","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"This type of discrete state space model is known as a Hidden Markov Model or HMM for short. Our goal is to learn the matrices A and B so we can use them to track the whereabouts of our little cleaning agent.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"using RxInfer, Random, BenchmarkTools, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"In order to generate data to mimic the observations of the Roomba, we need to specify two things: the actual transition probabilities between the states (i.e., how likely is the Roomba to move from one room to another), and the observation distribution (i.e., what type of texture will the Roomba encounter in each room). We can then use these specifications to generate observations from our hidden Markov model (HMM).","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"To generate our observation data, we'll follow these steps:","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Assume an initial state for the Roomba. For example, we can start the Roomba in the bedroom.\nDetermine where the Roomba went next by drawing from a Categorical distribution with the transition probabilities between the different rooms.\nDetermine the observation encountered in this room by drawing from a Categorical distribution with the corresponding observation probabilities.\nRepeat steps 2-3 for as many samples as we want.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"The following code implements this process and generates our observation data:","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"\"\"\"\n    rand_vec(rng, distribution::Categorical)\n\nThis function returns a one-hot encoding of a random sample from a categorical distribution. The sample is drawn with the `rng` random number generator.\n\"\"\"\nfunction rand_vec(rng, distribution::Categorical) \n    k = ncategories(distribution)\n    s = zeros(k)\n    drawn_category = rand(rng, distribution)\n    s[drawn_category] = 1.0\n    return s\nend\n\nfunction generate_data(n_samples; seed = 42)\n    \n    rng = MersenneTwister(seed)\n    \n    # Transition probabilities bed|livi|bath-room \n    state_transition_matrix = [0.9 0.05 0.0;\n                               0.1 0.9  0.1; \n                               0.0 0.05 0.9] \n    # Observation noise\n    observation_distribution_matrix = [0.9 0.05 0.05;\n                                                                         0.05 0.9 0.05;\n                                                                         0.05 0.05 0.9] \n    # Initial state\n    s_initial = [1.0, 0.0, 0.0] \n    \n    states = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the states\n    observations = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the observations\n    \n    s_prev = s_initial\n    \n    for t = 1:n_samples\n        s_probvec = state_transition_matrix * s_prev\n        states[t] = rand_vec(rng, Categorical(s_probvec ./ sum(s_probvec)))\n        obs_probvec = observation_distribution_matrix * states[t]\n        observations[t] = rand_vec(rng, Categorical(obs_probvec ./ sum(obs_probvec)))\n        s_prev = states[t]\n    end\n    \n    return observations, states\nend","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"We will generate 100 data points to simulate 100 ticks of the Roomba moving through the apartment. x_data will contain the Roomba's measurements of the floor it's currently on, and s_data will contain information on the room the Roomba was actually in.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"# Test data\nN = 100\nx_data, s_data = generate_data(N);\n\nscatter(argmax.(s_data), leg=false, xlabel=\"Time\",yticks= ([1,2,3],[\"Bedroom\",\"Living room\",\"Bathroom\"]))","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Now it is time to build our model. As mentioned earlier, we will use Categorical distributions for the states and observations. To learn the A and B matrices we can use DirichletCollection priors. For the A-matrix, since we have no apriori idea how the roomba is actually going to move we will assume that it moves randomly. We can represent this by filling our DirichletCollection prior on A with ones. Remember that this will get updated once we start learning, so it's fine if our initial guess is not quite accurate. As for the observations, we have good reason to trust our Roomba's measurements. To represent this, we will add large values to the diagonal of our prior on B. However, we also acknowledge that the Roomba is not infallible, so we will add some noise on the off-diagonal entries.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Since we will use Variational Inference, we also have to specify inference constraints. We will use a structured variational approximation to the true posterior distribution, where we decouple the variational posterior over the states (q(s_0, s)) from the posteriors over the transition matrices (q(A) and q(B)). This dependency decoupling in the approximate posterior distribution ensures that inference is tractable. Let's build the model!","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"# Model specification\n@model function hidden_markov_model(x)\n    \n    A ~ DirichletCollection(ones(3,3))\n    B ~ DirichletCollection([ 10.0 1.0 1.0; \n                                            1.0 10.0 1.0; \n                                            1.0 1.0 10.0 ])\n    \n    s_0 ~ Categorical(fill(1.0 / 3.0, 3))\n    \n    s_prev = s_0\n    \n    for t in eachindex(x)\n        s[t] ~ DiscreteTransition(s_prev, A) \n        x[t] ~ DiscreteTransition(s[t], B)\n        s_prev = s[t]\n    end\n    \nend\n\n# Constraints specification\n@constraints function hidden_markov_model_constraints()\n    q(s_0, s, A, B) = q(s_0, s)q(A)q(B)\nend","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"hidden_markov_model_constraints (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Now it's time to perform inference and find out where the Roomba went in our absence. Did it remember to clean the bathroom?","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"We'll be using Variational Inference to perform inference, which means we need to set some initial marginals as a starting point. RxInfer makes this easy with the vague function, which provides an uninformative guess. If you have better ideas, you can try a different initial guess and see what happens.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Since we're only interested in the final result - the best guess about the Roomba's position - we'll only keep the last results. Let's start the inference process!","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"imarginals = @initialization begin\n    q(A) = vague(DirichletCollection, (3, 3))\n    q(B) = vague(DirichletCollection, (3, 3)) \n    q(s) = vague(Categorical, 3)\nend\n\nireturnvars = (\n    A = KeepLast(),\n    B = KeepLast(),\n    s = KeepLast()\n)\n\nresult = infer(\n    model         = hidden_markov_model(), \n    data          = (x = x_data,),\n    constraints   = hidden_markov_model_constraints(),\n    initialization = imarginals, \n    returnvars    = ireturnvars, \n    iterations    = 20, \n    free_energy   = true\n);","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"That was fast! Let's take a look at our results. If we're successful, we should have a good idea about the actual layout of the apartment (a good posterior marginal over A) and about the uncertainty in the roombas observations (A good posterior over B). Let's see if it worked","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"println(\"Posterior Marginal for A:\")\nmean(result.posteriors[:A])","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Posterior Marginal for A:\n3×3 Matrix{Float64}:\n 0.906367   0.040203   0.0744687\n 0.0703318  0.90382    0.0974548\n 0.0233017  0.0559774  0.828076","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"println(\"Posterior Marginal for B:\")\nmean(result.posteriors[:B])","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Posterior Marginal for B:\n3×3 Matrix{Float64}:\n 0.891502   0.0523554  0.0347455\n 0.0429615  0.927224   0.0360346\n 0.0655369  0.0204209  0.92922","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Finally, we can check if we were successful in keeping tabs on our Roomba's whereabouts. We can also check if our model has converged by looking at the Free Energy. ","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"p1 = scatter(argmax.(s_data), \n                        title=\"Inference results\", \n                        label = \"Real\", \n                        ms = 6, \n                        legend=:right,\n                        xlabel=\"Time\" ,\n                        yticks= ([1,2,3],[\"Bedroom\",\"Living room\",\"Bathroom\"]),\n                        size=(900,550)\n                        )\n\np1 = scatter!(p1, argmax.(ReactiveMP.probvec.(result.posteriors[:s])),\n                        label = \"Inferred\",\n                        ms = 3\n                        )\n\np2 = plot(result.free_energy, \n                    label=\"Free energy\",\n                    xlabel=\"Iteration Number\"\n                    )\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Neat! Now you know how to track a Roomba if you ever need to. You also learned how to fit a Hidden Markov Model using RxInfer in the process.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/hidden_markov_model/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Kalman-filtering-and-smoothing","page":"Kalman Filtering And Smoothing","title":"Kalman filtering and smoothing","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"In the following set of examples the goal is to estimate hidden states of a Dynamical process where all hidden states are Gaussians.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"We start our journey with a simple multivariate Linear Gaussian State Space Model (LGSSM), which can be solved analytically.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"We then solve an identification problem which does not have an analytical solution.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Utimately, we show how RxInfer.jl can deal with missing observations.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Gaussian-Linear-Dynamical-System","page":"Kalman Filtering And Smoothing","title":"Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"LGSSM can be described with the following equations:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"beginaligned\n p(x_ix_i - 1)  = mathcalN(x_iA * x_i - 1 mathcalP)\n p(y_ix_i)  = mathcalN(y_iB * x_i mathcalQ)\nendaligned","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"where x_i are hidden states, y_i are noisy observations, A, B are state transition and observational matrices, mathcalP and mathcalQ are state transition noise and observation noise covariance matrices. For a more rigorous introduction to Linear Gaussian Dynamical systems we refer to Simo Sarkka, Bayesian Filtering and Smoothing book.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"function generate_data(rng, A, B, P, Q)\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        x[i] = rand(rng, MvNormalMeanCovariance(A * x_prev, P))\n        y[i] = rand(rng, MvNormalMeanCovariance(B * x[i], Q))\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"# Seed for reproducibility\nseed = 1234\n\nrng = MersenneTwister(1234)\n\n# We will model 2-dimensional observations with rotation matrix `A`\n# To avoid clutter we also assume that matrices `A`, `B`, `P` and `Q`\n# are known and fixed for all time-steps\nθ = π / 35\nA = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\nB = diageye(2)\nQ = 25.0 * diageye(2)\nP = diageye(2)\n\n# Number of observations\nn = 300;","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"x, y = generate_data(rng, A, B, P, Q);","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations, which are represented as dots.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = scatter!(px, getindex.(y, 1), label = false, markersize = 2, color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\npx = scatter!(px, getindex.(y, 2), label = false, markersize = 2, color = :green)\n\nplot(px)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@model function rotate_ssm(y, x0, A, B, P, Q)\n    x_prior ~ x0\n    x_prev = x_prior\n    \n    for i in 1:length(y)\n        x[i] ~ MvNormalMeanCovariance(A * x_prev, P)\n        y[i] ~ MvNormalMeanCovariance(B * x[i], Q)\n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"To run inference we also specify prior for out first hidden state:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"x0 = MvNormalMeanCovariance(zeros(2), 100.0 * diageye(2));","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"# For large number of observations you need to use limit_stack_depth = 100 option during model creation, e.g. \n# infer(..., options = (limit_stack_depth = 500, ))`\nresult = infer(\n    model = rotate_ssm(x0=x0, A=A, B=B, P=P, Q=Q), \n    data = (y = y,),\n    free_energy = true\n);\n\nxmarginals  = result.posteriors[:x]\nlogevidence = -result.free_energy; # given the analytical solution, free energy will be equal to the negative log evidence","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\n\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :teal)\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :violet)\n\nplot(px)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the value for minus log evidence:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"logevidence","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"1-element Vector{Float64}:\n -1891.6471934594765","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#System-Identification-Problem","page":"Kalman Filtering And Smoothing","title":"System Identification Problem","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"In this example we are going to attempt to run Bayesian inference and decouple two random-walk signals, which were combined into a single single through some deterministic function f. We do not have access to the real values of these signals, but only to their combination. First, we create the generate_data function that accepts f as an argument:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"using RxInfer, Distributions, StableRNGs, Plots","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"function generate_data(f, n; seed = 123, x_i_min = -20.0, w_i_min = 20.0, noise = 20.0, real_x_τ = 0.1, real_w_τ = 1.0)\n\n    rng = StableRNG(seed)\n\n    real_x = Vector{Float64}(undef, n)\n    real_w = Vector{Float64}(undef, n)\n    real_y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        real_x[i] = rand(rng, Normal(x_i_min, sqrt(1.0 / real_x_τ)))\n        real_w[i] = rand(rng, Normal(w_i_min, sqrt(1.0 / real_w_τ)))\n        real_y[i] = rand(rng, Normal(f(real_x[i], real_w[i]), sqrt(noise)))\n\n        x_i_min = real_x[i]\n        w_i_min = real_w[i]\n    end\n    \n    return real_x, real_w, real_y\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"generate_data (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"The function returns the real signals real_x and  real_w for later comparison (we are not going to use them during inference) and their combined version real_y (we are going to use it as our observations during the inference). We also assume that real_y is corrupted with some measurement noise.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Combination-1:-y-x-w","page":"Kalman Filtering And Smoothing","title":"Combination 1: y = x + w","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"In our first example, we are going to use a simple addition (+) as the function f. In general, it is impossible to decouple the signals x and w without strong priors, but we can try and see how good an inference can be. The + operation on two random variables also has a special meaning in the probabilistic inference, namely the convolution of pdf's of the two random variables, and RxInfer treats it specially with many precomputed analytical rules, which may make the inference task easier. First, let us create a test dataset:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"n = 250\nreal_x, real_w, real_y = generate_data(+, n);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, real_x, label = \"x\")\npl = plot!(pl, real_w, label = \"w\")\n\npr = plot(title = \"Combined y = x + w\")\npr = scatter!(pr, real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"To run inference, we need to create a probabilistic model: our beliefs about how our data could have been generated. For this we can use the @model macro from RxInfer.jl:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@model function identification_problem(f, y, m_x_0, τ_x_0, a_x, b_x, m_w_0, τ_w_0, a_w, b_w, a_y, b_y)\n    \n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x_i_min = x0\n    w_i_min = w0\n\n    local x\n    local w\n    local s\n    \n    for i in 1:length(y)\n        x[i] ~ Normal(mean = x_i_min, precision = τ_x)\n        w[i] ~ Normal(mean = w_i_min, precision = τ_w)\n        s[i] := f(x[i], w[i])\n        y[i] ~ Normal(mean = s[i], precision = τ_y)\n        \n        x_i_min = x[i]\n        w_i_min = w[i]\n    end\n    \nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"RxInfer runs Bayesian inference as a variational optimisation procedure between the real solution and its variational proxy q. In our model specification we assumed noise components to be unknown, thus, we need to enforce a structured mean-field assumption for the variational family of distributions q. This inevitably reduces the accuracy of the result, but makes the task easier and allows for fast and analytical message passing-based variational inference:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"constraints = @constraints begin \n    q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y)\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Constraints: \n  q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"The next step is to assign priors, initialise needed messages and marginals and call the inference function:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"m_x_0, τ_x_0 = -20.0, 1.0\nm_w_0, τ_w_0 = 20.0, 1.0\n\n# We set relatively strong priors for random walk noise components\n# and sort of vague prior for the noise of the observations\na_x, b_x = 0.01, 0.01var(real_x)\na_w, b_w = 0.01, 0.01var(real_w)\na_y, b_y = 1.0, 1.0\n\n# We set relatively strong priors for messages\nxinit = map(r -> NormalMeanPrecision(r, τ_x_0), reverse(range(-60, -20, length = n)))\nwinit = map(r -> NormalMeanPrecision(r, τ_w_0), range(20, 60, length = n))\n\n\ninit = @initialization begin\n    μ(x) = xinit\n    μ(w) = winit\n    q(τ_x) = GammaShapeRate(a_x, b_x)\n    q(τ_w) = GammaShapeRate(a_w, b_w)\n    q(τ_y) = GammaShapeRate(a_y, b_y)\nend\n\nresult = infer(\n    model = identification_problem(f=+, m_x_0=m_x_0, τ_x_0=τ_x_0, a_x=a_x, b_x=b_x, m_w_0=m_w_0, τ_w_0=τ_w_0, a_w=a_w, b_w=b_w, a_y=a_y, b_y=b_y),\n    data  = (y = real_y,), \n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initialization = init,\n    iterations = 50\n)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Inference results:\n  Posteriors       | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Let's examine our inference results:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"τ_x_marginals = result.posteriors[:τ_x]\nτ_w_marginals = result.posteriors[:τ_w]\nτ_y_marginals = result.posteriors[:τ_y]\n\nsmarginals = result.posteriors[:s]\nxmarginals = result.posteriors[:x]\nwmarginals = result.posteriors[:w];","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(xmarginals[end]), ribbon = var.(xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(wmarginals[end]), ribbon = var.(wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(smarginals[end]), ribbon = std.(smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"The inference results are not so bad, even though RxInfer missed the correct values of the signals between 100 and 150.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Combination-2:-y-min(x,-w)","page":"Kalman Filtering And Smoothing","title":"Combination 2: y = min(x, w)","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"In this example we use a slightly more complex function, for which RxInfer does not have precomputed analytical message update rules. We are going to attempt to run Bayesian inference with min as a combination function. Note, however, that directly using min may cause problems for the built-in approximation methods as it has zero partial derviates with respect to all but one of the variables. We generate data with the min function directly however we model it with a somewhat smoothed version:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"# Smoothed version of `min` without zero-ed derivatives\nfunction smooth_min(x, y)    \n    if x < y\n        return x + 1e-4 * y\n    else\n        return y + 1e-4 * x\n    end\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"smooth_min (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"RxInfer supports arbitrary nonlinear functions, but it requires an explicit approximation method specification. That can be achieved with the built-in @meta macro:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"min_meta = @meta begin \n    # In this example we are going to use a simple `Linearization` method\n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Meta: \n  smooth_min() -> ReactiveMP.Linearization()","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"n = 200\nmin_real_x, min_real_w, min_real_y = generate_data(min, n, seed = 1, x_i_min = 0.0, w_i_min = 0.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, min_real_x, label = \"x\")\npl = plot!(pl, min_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, min_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"min_m_x_0, min_τ_x_0 = -1.0, 1.0\nmin_m_w_0, min_τ_w_0 = 1.0, 1.0\n\nmin_a_x, min_b_x = 1.0, 1.0\nmin_a_w, min_b_w = 1.0, 1.0\nmin_a_y, min_b_y = 1.0, 1.0\n\ninit = @initialization begin\n   μ(x) = NormalMeanPrecision(min_m_x_0, min_τ_x_0) \n   μ(w) = NormalMeanPrecision(min_m_w_0, min_τ_w_0)\n   q(τ_x) = GammaShapeRate(min_a_x, min_b_x) \n   q(τ_w) = GammaShapeRate(min_a_w, min_b_w)\n   q(τ_y) = GammaShapeRate(min_a_y, min_b_y)\nend\n\n\nmin_result = infer(\n    model = identification_problem(f=smooth_min, m_x_0=min_m_x_0, τ_x_0=min_τ_x_0, a_x=min_a_x, b_x=min_b_x, m_w_0=min_m_w_0, τ_w_0=min_τ_w_0, a_w=min_a_w, b_w=min_b_w, a_y=min_a_y, b_y=min_b_y),\n    data  = (y = min_real_y,), \n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initialization = init,\n    meta = min_meta,\n    iterations = 50\n)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Inference results:\n  Posteriors       | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"min_τ_x_marginals = min_result.posteriors[:τ_x]\nmin_τ_w_marginals = min_result.posteriors[:τ_w]\nmin_τ_y_marginals = min_result.posteriors[:τ_y]\n\nmin_smarginals = min_result.posteriors[:s]\nmin_xmarginals = min_result.posteriors[:x]\nmin_wmarginals = min_result.posteriors[:w]\n\npx1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, min_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(min_xmarginals[end]), ribbon = var.(min_xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, min_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(min_wmarginals[end]), ribbon = var.(min_wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, min_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(min_smarginals[end]), ribbon = std.(min_smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"As we can see inference with the min function is significantly harder. Even though the combined signal has been inferred with high precision the underlying x and w signals are barely inferred. This may be expected, since the min function essentially destroy the information about one of the signals, thus, making it impossible to decouple two seemingly identical random walk signals. The only one inferred signal is the one which is lower and we have no inference information about the signal which is above. It might be possible to infer the states, however, with more informative priors and structural information about two different signals (e.g. if these are not random walks). ","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Online-(filtering)-identification:-y-min(x,-w)","page":"Kalman Filtering And Smoothing","title":"Online (filtering) identification: y = min(x, w)","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Another way to approach to this problem is to use online (filtering) inference procedure from RxInfer, but for that we also need to modify our model specification a bit:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@model function rx_identification(f, m_x_0, τ_x_0, m_w_0, τ_w_0, a_x, b_x, a_y, b_y, a_w, b_w, y)\n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x ~ Normal(mean = x0, precision = τ_x)\n    w ~ Normal(mean = w0, precision = τ_w)\n\n    s := f(x, w)\n    y ~ Normal(mean = s, precision = τ_y)\n    \nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"We impose structured mean-field assumption for this model as well:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"rx_constraints = @constraints begin \n    q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ_y)\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Constraints: \n  q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ_y\n)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Online inference in the RxInfer supports the @autoupdates specification, which tells inference procedure how to update priors based on new computed posteriors:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"autoupdates = @autoupdates begin \n    m_x_0, τ_x_0 = mean_precision(q(x))\n    m_w_0, τ_w_0 = mean_precision(q(w))\n    a_x = shape(q(τ_x)) \n    b_x = rate(q(τ_x))\n    a_y = shape(q(τ_y))\n    b_y = rate(q(τ_y))\n    a_w = shape(q(τ_w)) \n    b_w = rate(q(τ_w))\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@autoupdates begin\n    (m_x_0, τ_x_0) = mean_precision(q(x))\n    (m_w_0, τ_w_0) = mean_precision(q(w))\n    a_x = shape(q(τ_x))\n    b_x = rate(q(τ_x))\n    a_y = shape(q(τ_y))\n    b_y = rate(q(τ_y))\n    a_w = shape(q(τ_w))\n    b_w = rate(q(τ_w))\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"As previously we need to define the @meta structure that specifies the approximation method for the nonlinear function smooth_min (f in the model specification):","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"rx_meta = @meta begin \n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Meta: \n  smooth_min() -> ReactiveMP.Linearization()","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Next step is to generate our dataset and to run the actual inference procedure! For that we use the infer function with autoupdates keyword:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"n = 300\nrx_real_x, rx_real_w, rx_real_y = generate_data(min, n, seed = 1, x_i_min = 1.0, w_i_min = -1.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, rx_real_x, label = \"x\")\npl = plot!(pl, rx_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, rx_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"init = @initialization begin\n    q(w)= NormalMeanVariance(-2.0, 1.0) \n    q(x) = NormalMeanVariance(2.0, 1.0) \n    q(τ_x) = GammaShapeRate(1.0, 1.0) \n    q(τ_w) = GammaShapeRate(1.0, 1.0) \n    q(τ_y) = GammaShapeRate(1.0, 20.0)\nend\n\nengine = infer(\n    model         = rx_identification(f=smooth_min),\n    constraints   = rx_constraints,\n    data          = (y = rx_real_y,),\n    autoupdates   = autoupdates,\n    meta          = rx_meta,\n    returnvars    = (:x, :w, :τ_x, :τ_w, :τ_y, :s),\n    keephistory   = 1000,\n    historyvars   =  KeepLast(),\n    initialization = init,\n    iterations    = 10,\n    free_energy = true, \n    free_energy_diagnostics = nothing,\n    autostart     = true,\n)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"RxInferenceEngine:\n  Posteriors stream    | enabled for (w, s, τ_x, τ_w, τ_y, x)\n  Free Energy stream   | enabled\n  Posteriors history   | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)\n  Free Energy history  | available\n  Enabled events       | [  ]","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"rx_smarginals = engine.history[:s]\nrx_xmarginals = engine.history[:x]\nrx_wmarginals = engine.history[:w];","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, rx_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(rx_xmarginals), ribbon = var.(rx_xmarginals), label = \"Estimated X\")\n\npx1 = plot!(px1, rx_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(rx_wmarginals), ribbon = var.(rx_wmarginals), label = \"Estimated W\")\n\npx2 = scatter!(px2, rx_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(rx_smarginals), ribbon = std.(rx_smarginals), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"The results are quite similar to the smoothing case and, as we can see, one of the random walk is again in the \"disabled\" state, does not infer anything and simply increases its variance (which is expected for the random walk).","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Handling-Missing-Data","page":"Kalman Filtering And Smoothing","title":"Handling Missing Data","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"An interesting case in filtering and smoothing problems is the processing of missing data. It can happen that sometimes your reading devices failt to acquire the data leading to missing observation.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Let us assume that the following model generates the data","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"beginaligned\n    x_t sim mathcalNleft(x_t-1 10right) \n    y_t sim mathcalNleft(x_t P right) \nendaligned","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Suppose that our measurement device fails to acquire data from time to time.  In this case, instead of scalar observation haty_t in mathrmR we sometimes will catch missing observations.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"using RxInfer, Plots","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@model function smoothing(x0, y)\n    \n    P ~ Gamma(shape = 0.001, scale = 0.001)\n    x_prior ~ Normal(mean = mean(x0), var = var(x0)) \n\n    local x\n    x_prev = x_prior\n\n    for i in 1:length(y)\n        x[i] ~ Normal(mean = x_prev, precision = 1.0)\n        y[i] ~ Normal(mean = x[i], precision = P)\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"P = 1.0\nn = 250\n\nreal_signal     = map(e -> sin(0.05 * e), collect(1:n))\nnoisy_data      = real_signal + rand(Normal(0.0, sqrt(P)), n);\nmissing_indices = 100:125\nmissing_data    = similar(noisy_data, Union{Float64, Missing}, )\n\ncopyto!(missing_data, noisy_data)\n\nfor index in missing_indices\n    missing_data[index] = missing\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"constraints = @constraints begin\n    q(x_prior, x, y, P) = q(x_prior, x)q(P)q(y)\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Constraints: \n  q(x_prior, x, y, P) = q(x_prior, x)q(P)q(y)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"x0_prior = NormalMeanVariance(0.0, 1000.0)\ninitm = @initialization begin\n    q(P) = Gamma(0.001, 0.001)\nend\n\nresult = infer(\n    model = smoothing(x0=x0_prior), \n    data  = (y = missing_data,), \n    constraints = constraints,\n    initialization = initm, \n    returnvars = (x = KeepLast(),),\n    iterations = 20\n);","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"plot(real_signal, label = \"Noisy signal\", legend = :bottomright)\nscatter!(missing_indices, real_signal[missing_indices], ms = 2, opacity = 0.75, label = \"Missing region\")\nplot!(mean.(result.posteriors[:x]), ribbon = var.(result.posteriors[:x]), label = \"Estimated hidden state\")","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/kalman_filtering_and_smoothing/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/#Parameter-Optimisation-with-Optim.jl","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation with Optim.jl","text":"","category":"section"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Welcome to this hands-on tutorial where we'll explore how to optimize parameters in state space models using Julia's powerful optimization ecosystem. We'll combine the probabilistic inference capabilities of RxInfer.jl with optimization tools from Optim.jl.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"What you'll learn:","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"How to set up parameter optimization in state space models\nPractical techniques for both univariate and multivariate cases\nIntegration with Julia's optimization packages\nReal-world applications and best practices","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"using RxInfer, StableRNGs, LinearAlgebra, Plots","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/#Univariate-State-Space-Model","page":"Parameter Optimisation With Optim.Jl","title":"Univariate State Space Model","text":"","category":"section"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Let's start with a simple but powerful example: a linear state space model with Gaussian observations. This model is foundational in many real-world applications, from tracking to financial forecasting.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Our model is defined as:","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"beginaligned\n    x_t = x_t-1 + c \n    y_t sim mathcalNleft(x_t v right) \nendaligned","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0)","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Key Challenge: We'll optimize two parameters:","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"The drift parameter c\nThe initial state mean m_x_0","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Assumptions: We assume the following:","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"The drift parameter c stays constant\nThe observation noice v is known","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Let's create a state space model using RxInfer's @model macro. Our model has:","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"A prior state x1 following a Normal distribution\nA state transition equation: x[i] := x[i - 1] + c where c is our drift parameter\nObservations y[i] following a Normal distribution with mean x[i] and variance v","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"The model iteratively updates the state and generates observations, maintaining the Markovian property where each state depends only on the previous state.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"@model function univariate_state_space_model(y, x_prior, c, v)\n    \n    x0 ~ Normal(mean = mean(x_prior), variance = var(x_prior))\n    x_prev = x0\n\n    for i in eachindex(y)\n        x[i] := x_prev + c\n        y[i] ~ Normal(mean = x[i], variance = v)\n        x_prev = x[i]\n    end\nend","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Let's generate some synthetic data to test our model. We'll create a sequence of observations that follow our state space model assumptions. We'll set the true drift parameter c_real to -5.0 and generate 250 data points with Gaussian noise. This synthetic data will help us validate whether our optimization procedure can recover the true parameter values.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"rng    = StableRNG(42)\nv      = 1.0\nn      = 250\nc_real = -5.0\nsignal = c_real .+ collect(1:n)\ndata   = map(x -> rand(rng, NormalMeanVariance(x, v)), signal);","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Now we'll define a function for optimization that takes a vector of parameters as input. The first element params[1] represents our drift parameter c, while params[2] represents the initial state mean μ1. The function creates a prior distribution for the initial state x1 with the given mean and a large variance of 100.0. It then performs inference using our state space model and returns the negative free energy, which we'll minimize to find optimal parameter values. The optimization will help us recover the true parameter values from our synthetic data.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"# params[1] is C\n# params[2] is μ1\nfunction f(params)\n    x_prior = NormalMeanVariance(params[2], 100.0)\n    result = infer(\n        model = univariate_state_space_model(\n            x_prior = x_prior, \n            c       = params[1], \n            v       = v\n        ), \n        data  = (y = data,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"f (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Now we'll use Optim.jl, a powerful optimization package in Julia, to find the optimal parameter values. Optim.jl provides various optimization algorithms including gradient descent, L-BFGS, and Nelder-Mead. It offers a unified interface for both gradient-based and gradient-free optimization methods, making it flexible for different types of problems. The package also provides useful features like convergence monitoring and iteration control.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"using Optim","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Now that we have defined our objective function and imported the optimization package, we are ready to find the optimal parameter values. We will start with an initial guess of [1.0, 1.0] for our parameters (c and μ1) and use gradient descent optimization. We'll set some optimization options including a gradient tolerance of 1e-3, maximum 100 iterations, and enable trace storage and display for monitoring convergence.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"res = optimize(f, ones(2), GradientDescent(), Optim.Options(g_tol = 1e-3, iterations = 100, store_trace = true, show_trace = true, show_every = 10))","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Iter     Function value   Gradient norm \n     0     3.601256e+02     1.261348e+03\n * time: 0.028633832931518555\n    10     3.593376e+02     1.355626e+01\n * time: 8.908282995223999\n * Status: success\n\n * Candidate solution\n    Final objective value:     3.593375e+02\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.04e-05 ≰ 0.0e+00\n    |x - x'|/|x'|          = 2.14e-06 ≰ 0.0e+00\n    |f(x) - f(x')|         = 7.06e-05 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 1.96e-07 ≰ 0.0e+00\n    |g(x)|                 = 9.55e-04 ≤ 1.0e-03\n\n * Work counters\n    Seconds run:   9  (vs limit Inf)\n    Iterations:    11\n    f(x) calls:    83\n    ∇f(x) calls:   83","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Let's analyze the optimization results. The algorithm successfully converged, as indicated by the status message. In the next cell, we'll compare the optimized parameter values with the true values used to generate our synthetic data to verify the accuracy of our parameter recovery.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", [ 1.0, c_real ])\nprintln(\"Optimized: \", res.minimizer)","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Real value vs Optimized\nReal:      [1.0, -5.0]\nOptimized: [0.9990370328385715, -4.8593306526902476]","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"The optimization results show that we successfully recovered the true parameter values. The optimized values are very close to the real values, demonstrating that our inference approach effectively identified the underlying model parameters.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/#Multivariate-State-Space-Model","page":"Parameter Optimisation With Optim.Jl","title":"Multivariate State Space Model","text":"","category":"section"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Now let's tackle a more challenging scenario with multiple interacting variables. Multivariate models are essential for capturing complex dynamics in real-world systems, from robotics to econometrics.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Key differences from the univariate case:","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Higher-dimensional state space\nMore complex parameter interactions  \nRicher dynamics and correlations","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"We'll see how our optimization approach scales to this more complex setting while maintaining computational efficiency.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Let us consider the multivariate state space model:","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"beginaligned\n    mathbfx_t sim mathcalNleft(mathbfAx_t-1 mathbfQ right) \n    mathbfy_t sim mathcalNleft(mathbfx_t mathbfP right) \nendaligned","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"with prior ","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"beginaligned\nmathbfx_0 sim mathcalN(mathbfm_x_0 mathbfV_x_0)\nendaligned","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"and transition matrix ","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"beginaligned\nmathbfA = beginbmatrix costheta  -sintheta  sintheta  costheta endbmatrix\nendaligned","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Covariance matrices mathbfV_x_0, mathbfP and mathbfQ are known. Our goal is to optimize parameters mathbfm_x_0 and theta.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"@model function multivariate_state_space_model(y, θ, x0, Q, P)\n    \n    x_prior ~ MvNormal(mean = mean(x0), cov = cov(x0))\n    x_prev = x_prior\n    \n    A = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\n    \n    for i in eachindex(y)\n        x[i] ~ MvNormal(mean = A * x_prev, covariance = Q)\n        y[i] ~ MvNormal(mean = x[i], covariance = P)\n        x_prev = x[i]\n    end\n    \nend","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Let's generate synthetic data from our model to test the optimization. We'll create a helper function that generates data from a rotating state space model with known parameters. The data will consist of 300 timesteps, with a rotation angle of π/8, and unit variance Gaussian noise in both the state and observation equations. The initial state is set to [10.0, -10.0]. This will give us ground truth data to validate our parameter estimation approach.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"# Generate data\nfunction generate_rotate_ssm_data()\n    rng = StableRNG(1234)\n\n    θ = π / 8\n    A = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\n    Q = Matrix(Diagonal(1.0 * ones(2)))\n    P = Matrix(Diagonal(1.0 * ones(2)))\n\n    n = 300\n\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        \n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(x[i], Q))\n        \n        x_prev = x[i]\n    end\n\n    return θ, A, Q, P, n, x, y\nend","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"generate_rotate_ssm_data (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"θ, A, Q, P, n, x, y = generate_rotate_ssm_data();","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Now we'll visualize the generated data by plotting both dimensions of the state variables over time. The plot will show the true state trajectories with uncertainty bands representing one standard deviation of the state noise. This will help us verify that our data generation process is working correctly and give us a visual reference for evaluating our parameter estimation results later. The ribbon plots show how the state variables evolve with their associated uncertainty, with different colors distinguishing between the two dimensions.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"real₁\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"real₂\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Now we'll define an objective function that takes a parameter vector params containing the rotation angle and initial state coordinates. This function will construct a model with these parameters and compute its free energy using the infer function. The free energy serves as our optimization objective - by minimizing it, we aim to find the parameter values that best explain our observed data. The parameter vector params has three components: params[1] is the rotation angle, while params[2] and params[3] represent the initial x and y coordinates respectively.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"function f(params)\n    x0 = MvNormalMeanCovariance(\n        [ params[2], params[3] ], \n        Matrix(Diagonal(0.01 * ones(2)))\n    )\n    result = infer(\n        model = multivariate_state_space_model(\n            θ = params[1], \n            x0 = x0, \n            Q = Q, \n            P = P\n        ), \n        data  = (y = y,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"f (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Now we'll use the L-BFGS optimization algorithm to find the optimal parameters that minimize our objective function. The L-BFGS algorithm is particularly well-suited for this task as it approximates the Hessian matrix while using limited memory, making it efficient for problems with many parameters. We'll start with an initial guess of zeros for all parameters and set some convergence tolerances for the optimization process.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"res = optimize(f, zeros(3), LBFGS(), Optim.Options(f_tol = 1e-14, g_tol = 1e-12, show_trace = true, show_every = 10))","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Iter     Function value   Gradient norm \n     0     3.781355e+03     1.134440e+04\n * time: 4.1961669921875e-5\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.151827e+03\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 1.39e-11 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.27e-15 ≰ 0.0e+00\n    |f(x) - f(x')|         = 9.09e-13 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 7.90e-16 ≤ 1.0e-14\n    |g(x)|                 = 6.86e-08 ≰ 1.0e-12\n\n * Work counters\n    Seconds run:   29  (vs limit Inf)\n    Iterations:    9\n    f(x) calls:    81\n    ∇f(x) calls:   81","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Let's compare the real parameter values with the optimized ones. We'll look at both the raw angle values as well as their sine and cosine transformations to verify that our optimization has found the correct rotation parameters.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"println(\"Real value vs Optimized\")\nprintln(\"sinθ = (\", sin(θ), \", \", sin(res.minimizer[1]), \")\")\nprintln(\"cosθ = (\", cos(θ), \", \", cos(res.minimizer[1]), \")\")","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Real value vs Optimized\nsinθ = (0.3826834323650898, 0.38116735460454493)\ncosθ = (0.9238795325112867, 0.9245060561098414)","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Finally, let's visualize how well our optimized model fits the data. We'll create a plot comparing the true state trajectories with the inferred ones, including uncertainty bands. The plot will show both dimensions of the state vector over time, with the real values and their uncertainties shown alongside the inferred values and their corresponding uncertainties.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"x0 = MvNormalMeanCovariance([ res.minimizer[2], res.minimizer[3] ], Matrix(Diagonal(100.0 * ones(2))))\n\nresult = infer(\n    model = multivariate_state_space_model(\n        θ = res.minimizer[1], \n        x0 = x0, \n        Q = Q, \n        P = P\n    ), \n    data  = (y = y,), \n    free_energy = true\n)\n\nxmarginals = result.posteriors[:x]\n\npx = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"real₁\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"real₂\")\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"inf₁\")\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"inf₂\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"This example demonstrates how we can use Optim.jl in conjunction with RxInfer.jl to perform parameter optimization for state-space models. We've shown:","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"How to set up a rotating state-space model with unknown parameters\nHow to define an objective function using free energy\nHow to use different optimization algorithms (Gradient Descent and L-BFGS) \nHow to visualize and validate the results","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"The final plot shows that our optimized model successfully captures the dynamics of the true system, with the inferred trajectories closely matching the real ones within their uncertainty bounds. This confirms that our parameter optimization approach effectively recovered the underlying rotation parameter and initial state values.","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/parameter_optimisation_with_optim.jl/Project.toml`\n  [429524aa] Optim v1.13.2\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/parameter_optimisation_with_optim.jl/","page":"Parameter Optimisation With Optim.Jl","title":"Parameter Optimisation With Optim.Jl","text":"","category":"page"},{"location":"how_build_works/#How-the-Build-System-Works","page":"How we build the examples","title":"How the Build System Works","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"This document explains the build system for RxInfer.jl Examples. The build process involves two main scripts: examples/make.jl and docs/make.jl, each serving a different purpose.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"tip: Quick Help\nRun make help to see all available build commands and their descriptions:make help","category":"page"},{"location":"how_build_works/#Development-Options","page":"How we build the examples","title":"Development Options","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The build system supports using either the released version of RxInfer.jl or a local development version:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"# Build with released version (default)\nmake examples\n\n# Build with local development version (expects RxInfer.jl next to RxInferExamples.jl)\nmake examples-dev\n\n# Build with specific RxInfer.jl path\nmake examples-dev RXINFER=/path/to/RxInfer.jl\n\n# Build single example with development version\nmake example-dev FILTER=LinearRegression RXINFER=/path/to/RxInfer.jl","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"When using the development version (--use-dev), the build system will:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Look for RxInfer.jl in the specified location\nAdd it as a development dependency to each notebook's environment\nEnsure all notebooks use the same RxInfer version","category":"page"},{"location":"how_build_works/#Overview","page":"How we build the examples","title":"Overview","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The build process happens in two stages:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Converting notebooks to markdown (examples/make.jl)\nBuilding the documentation (docs/make.jl)","category":"page"},{"location":"how_build_works/#Stage-1:-Notebook-Processing-(examples/make.jl)","page":"How we build the examples","title":"Stage 1: Notebook Processing (examples/make.jl)","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"This script handles the conversion of Jupyter notebooks to markdown files.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The notebook processing system:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Converts .ipynb files to .md using Weave.jl\nPreserves each notebook's environment through Project.toml\nOptionally integrates development version of RxInfer.jl\nGenerates figures in the same directory as the notebook\nFixes absolute paths to use relative paths\nAdds contribution notes automatically","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"warning: Self-Contained Examples\nExamples must be self-contained and cannot use include() statements. All code must be directly in the notebook cells to ensure:Examples are reproducible by copying and pasting\nThe build system can properly process all code\nDocumentation remains consistent across different environments","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"For auxiliary file handling, the system copies all supporting files like data files while excluding Manifest.toml files. The original directory structure is maintained throughout this process.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The error handling system checks for error blocks in the output, reports any failed conversions, and provides detailed context when errors occur to help with debugging.","category":"page"},{"location":"how_build_works/#Parallel-Processing","page":"How we build the examples","title":"Parallel Processing","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The build system leverages Julia's distributed computing capabilities to process multiple notebooks simultaneously. It distributes the workload across available CPU cores. After processing completes, it generates a detailed report showing how many notebooks were processed successfully and which ones failed, if any.","category":"page"},{"location":"how_build_works/#Stage-2:-Documentation-Building-(docs/make.jl)","page":"How we build the examples","title":"Stage 2: Documentation Building (docs/make.jl)","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"This script builds the final documentation and performs several key functions:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"First, it collects metadata from all examples by reading their meta.jl files. This includes gathering titles, descriptions, and tags for each example, and organizing them into appropriate categories.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Next, it generates the pages needed for the documentation site. This involves creating a comprehensive list of all examples, setting up the navigation structure between pages, and applying consistent HTML styling to the examples list.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Finally, it handles the actual documentation building process using Documenter.jl. This includes deploying the built documentation to GitHub Pages and ensuring clean builds by removing old artifacts when needed.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"","category":"page"},{"location":"categories/basic_examples/incomplete_data/#Icomplete-Data","page":"Incomplete Data","title":"Icomplete Data","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"This tutorial demonstrates how to handle incomplete observations (missing data) using RxInfer.jl. Missing data is a common challenge in real-world applications. Traditional approaches often involve imputation or deletion of incomplete observations, which can lead to biased results. Bayesian inference provides a principled way to handle missing data by treating missing values as latent variables and marginalizing over them.","category":"page"},{"location":"categories/basic_examples/incomplete_data/#Problem-Setup","page":"Incomplete Data","title":"Problem Setup","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"We'll work with a hierarchical multivariate Gaussian model:","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"Precision matrix Λ follows a Wishart distribution (conjugate prior for precision)\nMean vector m follows a multivariate normal distribution  \nLatent states x[i] are drawn from MvNormal(m, Λ⁻¹)\nObservations y[i,j] are linked to latent states, but some may be missing","category":"page"},{"location":"categories/basic_examples/incomplete_data/#Model-Definition","page":"Incomplete Data","title":"Model Definition","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"using RxInfer, LinearAlgebra","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"@model function incomplete_data(y, dim)\n    Λ ~ Wishart(dim, diagm(ones(dim)))\n    m ~ MvNormal(mean=zeros(dim), precision=diagm(ones(dim)))\n    for i in 1:size(y, 1)\n        x[i] ~ MvNormal(mean=m, precision=Λ)\n        for j in 1:dim\n            y[i, j] ~ softdot(x[i], StandardBasisVector(dim, j), huge)\n        end\n    end\nend","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"The softdot with StandardBasisVector effectively extracts the j-th component of x[i], creating the relationship y[i,j] = x[i][j].","category":"page"},{"location":"categories/basic_examples/incomplete_data/#Data-Generation","page":"Incomplete Data","title":"Data Generation","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"Let's generate synthetic data with known ground truth parameters:","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"n_samples = 100\n\nreal_m = [13.0, 1.0, 5.0, 4.0, -20.0, 10.0]\ndimension = length(real_m)\nreal_Λ = diagm(ones(dimension))\n\nreal_x = [rand(MvNormal(real_m, inv(real_Λ))) for _ in 1:n_samples]\nincomplete_x = Vector{Vector{Union{Float64, Missing}}}(copy(real_x))\n\nfor i in 1:n_samples\n    incomplete_x[i][rand(1:dimension)] = missing\nend","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"# Create a matrix instead of vector of vectors\nobservations = Matrix{Union{Float64, Missing}}(undef, n_samples, dimension)\n\nfor i in 1:n_samples\n    for j in 1:dimension\n        observations[i, j] = incomplete_x[i][j]\n    end\nend","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"Key insight: Each sample has exactly one missing element, chosen randomly. This creates a challenging scenario where every observation is incomplete, but different dimensions are missing across samples.","category":"page"},{"location":"categories/basic_examples/incomplete_data/#Inference-Configuration","page":"Incomplete Data","title":"Inference Configuration","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"# We assume independence between the precision matrix and other variables.\nconstraints = @constraints begin\n    q(x, m, Λ) = q(x, m)q(Λ) \nend\n\n# We need to initialize the precision matrix.\ninit = @initialization begin\n    q(Λ) = Wishart(dimension, diagm(ones(dimension)))\nend\n\nresult = infer(model=incomplete_data(dim=dimension), data=(y=observations,), constraints=constraints, initialization=init, showprogress=true, iterations=100);","category":"page"},{"location":"categories/basic_examples/incomplete_data/#Results-Analysis","page":"Incomplete Data","title":"Results Analysis","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/#Recovered-Parameters","page":"Incomplete Data","title":"Recovered Parameters","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"# Extract final posterior estimates\nestimated_covariance = inv(mean(result.posteriors[:Λ][end]))\nestimated_mean = mean(result.posteriors[:m][end])\n\nprintln(\"True mean: \", real_m[1:dimension])  # Show first 5 elements\nprintln(\"Estimated mean: \", estimated_mean[1:dimension])\nprintln()\nprintln(\"True covariance (diagonal): \", diag(inv(real_Λ))[1:dimension])\nprintln(\"Estimated covariance (diagonal): \", diag(estimated_covariance)[1:dimension])","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"True mean: [13.0, 1.0, 5.0, 4.0, -20.0, 10.0]\nEstimated mean: [533.0205693374209, 3500.049550604763, 787.9531195234861, 2\n39.31464709642898, -906.5360582677393, 341.1441603681623]\n\nTrue covariance (diagonal): [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\nEstimated covariance (diagonal): [3.199210159847398e6, 9.490473368378067e6,\n 4.731969911582156e6, 1.4393146328545876e7, 2.9288415730048222e6, 166965.94\n7504312]","category":"page"},{"location":"categories/basic_examples/incomplete_data/#Convergence-Analysis","page":"Incomplete Data","title":"Convergence Analysis","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"The algorithm successfully recovers both the mean vector and covariance structure despite having incomplete observations. The Bayesian framework naturally handles the uncertainty introduced by missing data.","category":"page"},{"location":"categories/basic_examples/incomplete_data/#Key-Takeaways","page":"Incomplete Data","title":"Key Takeaways","text":"","category":"section"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"Missing data as latent variables: RxInfer treats missing observations as latent variables, avoiding the need for explicit imputation.\nPrincipled uncertainty quantification: The posterior distributions capture both parameter uncertainty and uncertainty due to missing data.\nComputational efficiency: The mean-field approximation and message-passing algorithms scale well to high-dimensional problems.\nRobustness: The method works even when every observation has missing elements, as long as there's sufficient information across the dataset.","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"# Simple plotting code for the RxInfer incomplete data tutorial\nusing Plots, Distributions\n\nfunction plot_posterior_distributions(result, real_m, real_Λ, max_dim=3)\n    # Get final posteriors\n    final_m_posterior = result.posteriors[:m][end]\n    final_Λ_posterior = result.posteriors[:Λ][end]\n    \n    # Plot mean posterior for first few dimensions\n    p1 = plot(title=\"Posterior Distribution of Mean (first $max_dim dimensions)\", \n              xlabel=\"Value\", ylabel=\"Density\")\n    \n    for i in 1:max_dim\n        # Extract marginal distribution for dimension i\n        marginal_mean = mean(final_m_posterior)[i]\n        marginal_var = inv(mean(final_Λ_posterior))[i,i]\n        \n        # Plot the Gaussian\n        x_range = range(marginal_mean - 3*sqrt(marginal_var), \n                       marginal_mean + 3*sqrt(marginal_var), length=100)\n        gaussian = Normal(marginal_mean, sqrt(marginal_var))\n        plot!(p1, x_range, pdf.(gaussian, x_range), \n              label=\"Dimension $i\", linewidth=2, color=i)\n        \n        # Add vertical line for true value with same color\n        vline!(p1, [real_m[i]], color=i, linestyle=:dash, alpha=0.7, \n               linewidth=2, label=\"\")\n    end\n    \n    plot(p1)\nend","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"plot_posterior_distributions (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"plot_posterior_distributions(result, real_m, real_Λ, 6)","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/incomplete_data/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [37e2e46d] LinearAlgebra v1.11.0\n","category":"page"},{"location":"categories/basic_examples/incomplete_data/","page":"Incomplete Data","title":"Incomplete Data","text":"","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"","category":"page"},{"location":"categories/basic_examples/coin_toss_model/#Coin-toss-model-(Beta-Bernoulli)","page":"Coin Toss Model","title":"Coin toss model (Beta-Bernoulli)","text":"","category":"section"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"In this example, we are going to perform an exact inference for a coin-toss model that can be represented as:","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"beginaligned\np(theta) = mathrmBeta(thetaa b)\np(y_itheta) = mathrmBernoulli(y_itheta)\nendaligned","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"where y_i in 0 1 is a binary observation induced by Bernoulli likelihood while theta is a Beta prior distribution on the parameter of Bernoulli. We are interested in inferring the posterior distribution of theta.","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"We start with importing all needed packages:","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"using RxInfer, Random","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Let's generate some synthetic observations using Bernoulli distribution for a biased coin-tosses that are independent and identically distributed (IID).","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"rng = MersenneTwister(42)\nn = 500\nθ_real = 0.75\ndistribution = Bernoulli(θ_real)\n\ndataset = float.(rand(rng, Bernoulli(θ_real), n));","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Once we generate the dataset, now we define a coin-toss model using the @model macro from RxInfer.jl","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds a factor graph under the hood\n@model function coin_model(y, a, b)\n\n    # We endow θ parameter of our model with \"a\" prior\n    θ ~ Beta(a, b)\n    # note that, in this particular case, the `Uniform(0.0, 1.0)` prior will also work.\n    # θ ~ Uniform(0.0, 1.0)\n\n    # here, the outcome of each coin toss is governed by the Bernoulli distribution\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(θ)\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Now, once the model is defined, we perform a (perfect) inference:","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"result = infer(\n    model = coin_model(a = 4.0, b = 8.0), \n    data  = (y = dataset,)\n)","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Once the result is calculated, we can focus on the posteriors","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"θestimated = result.posteriors[:θ]","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Distributions.Beta{Float64}(α=377.0, β=135.0)","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"and visualisation of the results","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np = plot(title = \"Inference results\")\n\nplot!(rθ, (x) -> pdf(Beta(4.0, 8.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\nplot!(rθ, (x) -> pdf(θestimated, x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\nvline!([θ_real], label=\"Real θ\")","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/coin_toss_model/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/#Integrating-Neural-Networks-with-Flux.jl","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks with Flux.jl","text":"","category":"section"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"This advanced tutorial demonstrates the powerful combination of probabilistic programming with deep learning in Julia, specifically showing how to integrate neural networks built with Flux.jl into probabilistic models using RxInfer.jl.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"using RxInfer, Flux, Random, Plots, LinearAlgebra, StableRNGs, ForwardDiff","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"In this example, our focus is on Bayesian state estimation in a Nonlinear State-Space Model with unknown dynamics. The main challenge in this scenario is that the dynamics of the system are often unknown or too complex to model analytically. Traditional approaches might struggle with capturing the nonlinear relationships in such systems. Neural networks offer a powerful solution by learning these complex dynamics directly from data, but incorporating them into a Bayesian framework requires careful integration to maintain probabilistic interpretations and uncertainty quantification. This tutorial demonstrates how to overcome these challenges by combining the flexibility of neural networks with the principled uncertainty handling of probabilistic programming. Specifically, we will utilize the time series generated by the Lorenz system as an example. ","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"# Lorenz system equations to be used to generate dataset\nBase.@kwdef mutable struct Lorenz\n    dt::Float64\n    σ::Float64\n    ρ::Float64\n    β::Float64\n    x::Float64\n    y::Float64\n    z::Float64\nend\n\n# Define the Lorenz dynamics\nfunction step!(l::Lorenz)\n    dx = l.σ * (l.y - l.x);         l.x += l.dt * dx\n    dy = l.x * (l.ρ - l.z) - l.y;   l.y += l.dt * dy\n    dz = l.x * l.y - l.β * l.z;     l.z += l.dt * dz\nend\n\nfunction create_dataset(rng, σ, ρ, β_nom; variance = 1f0, n_steps = 100, p_train = 0.8, p_test = 0.2)\n    attractor = Lorenz(0.02, σ, ρ, β_nom/3.0, 1, 1, 1)\n    signal       = [Float32[1.0, 1.0, 1.0]]\n    noisy_signal = [last(signal) + randn(rng, Float32, 3) * variance]\n    for i in 1:(n_steps - 1)\n        step!(attractor)\n        push!(signal, Float32[attractor.x, attractor.y, attractor.z])\n        push!(noisy_signal, last(signal) + randn(rng, Float32, 3) * variance) \n    end\n\n    return (\n        parameters = (σ, ρ, β_nom),\n        signal = signal, \n        noisy_signal = noisy_signal\n    )\nend","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"create_dataset (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"rng      = StableRNG(999) # dummy rng\nvariance = 2f0\ndataset  = create_dataset(rng, 11, 23, 6; variance = variance, n_steps = 200);","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"The dataset generated above represents the Lorenz system, a well-known chaotic dynamical system. We've created both clean trajectories following the exact Lorenz equations and noisy observations by adding Gaussian noise with variance 2.0. The dataset contains 200 time steps, providing sufficient data to train our neural network model. The parameters used for this Lorenz system are σ=11, ρ=23, and β=6. This noisy dataset will allow us to test our neural network's ability to filter out noise and recover the underlying dynamics.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"# Extract first samples from datasets\nsample_clean = dataset.signal\nsample_noisy = dataset.noisy_signal\n\n# Pre-allocate arrays for better performance\nn_points = length(sample_clean)\ngx, gy, gz = zeros(n_points), zeros(n_points), zeros(n_points)\nrx, ry, rz = zeros(n_points), zeros(n_points), zeros(n_points)\n\n# Extract coordinates\nfor i in 1:n_points\n    # Noisy observations\n    rx[i], ry[i], rz[i] = sample_noisy[i][1], sample_noisy[i][2], sample_noisy[i][3]\n    # True state\n    gx[i], gy[i], gz[i] = sample_clean[i][1], sample_clean[i][2], sample_clean[i][3]\nend\n\n# Create three projection plots\np1 = scatter(rx, ry, label=\"Noisy observations\", alpha=0.7, markersize=2, title = \"X-Y Projection\")\nplot!(p1, gx, gy, label=\"True state\", linewidth=2)\n\np2 = scatter(rx, rz, label=\"Noisy observations\", alpha=0.7, markersize=2, title = \"X-Z Projection\")\nplot!(p2, gx, gz, label=\"True state\", linewidth=2)\n\np3 = scatter(ry, rz, label=\"Noisy observations\", alpha=0.7, markersize=2, title = \"Y-Z Projection\")\nplot!(p3, gy, gz, label=\"True state\", linewidth=2)\n\n# Combine plots with improved layout\nplot(p1, p2, p3, size=(900, 250), layout=(1,3), margin=5Plots.mm)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"The plots above visualize our noisy Lorenz system dataset from three different perspectives. We can clearly see how the noise (represented by the scattered points) obscures the true underlying dynamics (shown by the solid lines). The Lorenz system's characteristic butterfly-shaped attractor is visible in these projections, though the noisy observations make it challenging to discern the exact trajectory. This visualization highlights the challenge our neural network will face: it must learn to filter out the Gaussian noise (with variance 2.0) and recover the true state of the system at each time step. The X-Y, X-Z, and Y-Z projections each provide a different view of the same 3D dynamical system, helping us understand the full complexity of the dataset.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/#Bayesian-Inference-meets-Neural-Networks","page":"Integrating Neural Networks With Flux.Jl","title":"Bayesian Inference meets Neural Networks","text":"","category":"section"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Our objective is to compute the marginal posterior distribution of the latent (hidden) state x_k at each time step k, considering the history of measurements up to that time step:","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"p(x_k  y_1k)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"The above expression represents the probability distribution of the latent state x_k given the measurements y_1k up to time step k. The hidden dynamics of the Lorenz system exhibit nonlinearities and hence cannot be solved in the closed form. One manner of solving this problem is by introducing a neural network to approximate the transition matrix of the Lorenz system. ","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"beginaligned\nA_k-1=NN(y_k-1) \np(x_k  x_k-1)=mathcalN(x_k  A_k-1x_k-1 Q) \np(y_k  x_k)=mathcalN(y_k  Bx_k R)\nendaligned","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"where NN is the neural network. The input is the observation y_k-1, and output is the trasition matrix A_k-1. B denote distortion or measurment matrix. Q and R are covariance matrices. ","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/#Define-the-Neural-Network","page":"Integrating Neural Networks With Flux.Jl","title":"Define the Neural Network","text":"","category":"section"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"We'll define a neural network using Flux.jl to approximate the transition matrix of the Lorenz system. The neural network will take the observation vector as input and output a transformation matrix that predicts the next state. This approach allows us to capture the nonlinear dynamics of the system while maintaining a tractable inference framework.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"For demonstration purposes, we'll use a relatively simple neural network architecture - a single Dense layer.  However, this approach can be extended to more complex architectures such as deeper networks, convolutional  networks, or recurrent networks, depending on the complexity of the system dynamics you're trying to model. The key idea is that the neural network learns to predict the transition dynamics of the system based on  observations, which can then be integrated into our probabilistic state-space model.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"function make_neural_network(rng = StableRNG(1234))\n    model = Dense(3 => 3)\n\n    # Initialize the weights and biases of the neural network\n    flat, rebuild = Flux.destructure(model)\n\n    # We use a fixed random seed for reproducibility\n    rand!(rng, flat)\n\n    # Return the neural network with fixed weights and biases\n    return rebuild(flat)\nend","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"make_neural_network (generic function with 2 methods)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/#Probabilistic-Model-Specification","page":"Integrating Neural Networks With Flux.Jl","title":"Probabilistic Model Specification","text":"","category":"section"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Now we'll define our probabilistic state-space model using RxInfer.jl. This model will incorporate the neural network's predictions of the transition matrices. The model consists of two main components: the state transition equation, which uses our neural network to predict how the state evolves, and the observation equation, which relates the hidden state to the measurements. By combining these components, we create a framework that can handle the nonlinear dynamics of the Lorenz system while maintaining computational tractability.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"@model function ssm(y, As, Q, B, R)\n    \n    x_prior_mean = ones(Float32, 3)\n    x_prior_cov  = Matrix(Diagonal(ones(Float32, 3)))\n    \n    x[1] ~ MvNormal(mean = x_prior_mean, cov = x_prior_cov)\n    y[1] ~ MvNormal(mean = B * x[1], cov = R)\n    \n    for i in 2:length(y)\n        x[i] ~ MvNormal(mean = As[i - 1] * x[i - 1], cov = Q) \n        y[i] ~ MvNormal(mean = B * x[i], cov = R)\n    end\nend","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"We set distortion matrix B and the covariance matrices Q and R as identity matrix. We assume that the observation noise is Gaussian with variance 2.0.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Q = diageye(Float32, 3)\nB = diageye(Float32, 3)\nR = variance * diageye(Float32, 3)\n;","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Before proceeding with inference, we need to build a function that extracts the transition matrix A from the neural network's output. These matrices will be fixed during the inference process.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"function get_matrices_from_neural_network(data, neural_network)\n    dd = hcat(data...)\n    As = neural_network(dd)\n    return map(c -> Matrix(Diagonal(c)), eachcol(As))\nend","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"get_matrices_from_neural_network (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/#Un-trained-network","page":"Integrating Neural Networks With Flux.Jl","title":"Un-trained network","text":"","category":"section"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Before network training, we show the inference results for the hidden states:","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"In this section, we'll demonstrate how our model performs with an untrained neural network. This will serve as a baseline to compare against after training. We expect the inference results to be poor since the untrained network generates random transition matrices that don't capture the true dynamics of the system. The plots below will visualize the true states, noisy observations, and the inferred states for each of the three coordinates in our state space model.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"# Performance on an instance from the testset before training\nuntrained_neural_network = make_neural_network()\nuntrained_transition_matrices = get_matrices_from_neural_network(dataset.noisy_signal, untrained_neural_network)\n\nuntrained_result = infer(\n    model = ssm(As = untrained_transition_matrices, Q = Q, B = B, R = R), \n    data  = (y = dataset.noisy_signal, ), \n    returnvars = (x = KeepLast(), )\n)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Inference results:\n  Posteriors       | available for (x)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"# A helper function for plotting\nfunction plot_coordinate(result, i; title = \"\")\n    p = scatter(getindex.(dataset.noisy_signal, i), label=\"Observations\", alpha=0.7, markersize=2, title = title)\n    plot!(getindex.(dataset.signal, i), label=\"True states\", linewidth=2)\n    plot!(getindex.(mean.(result.posteriors[:x]), i), ribbon=sqrt.(getindex.(var.(result.posteriors[:x]), i)), label=\"Inferred states\", linewidth=2)\n    return p\nend\n\nfunction plot_coordinates(result)\n    p1 = plot_coordinate(result, 1, title = \"First coordinate\")\n    p2 = plot_coordinate(result, 2, title = \"Second coordinate\")\n    p3 = plot_coordinate(result, 3, title = \"Third coordinate\")\n    return plot(p1, p2, p3, size = (1000, 600), layout = (3, 1), legend=:bottomleft)\nend","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"plot_coordinates (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"plot_coordinates(untrained_result)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"As we can see from the plots above, the inference results with an untrained neural network are essentially nonsense. The inferred states (green lines) fail to track the true states (orange lines) and instead produce arbitrary values with large uncertainty bands. This is expected since the untrained neural network generates random transition matrices that don't capture the actual dynamics of the system. The large discrepancy between the inferred and true states demonstrates why proper training of the neural network is necessary to achieve meaningful results.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/#Training-the-network","page":"Integrating Neural Networks With Flux.Jl","title":"Training the network","text":"","category":"section"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"In this part, we use the Free Energy as the objective function to optimize the weights of our neural network.  Free Energy is a variational inference objective that balances model fit with complexity. By minimizing Free Energy, we encourage the neural network to learn transition matrices that:","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Accurately predict the next state given the current state (reducing prediction error)\nMaintain appropriate uncertainty in the predictions\nCapture the underlying dynamics of the system without overfitting to noise","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"The optimization process will iteratively update the neural network weights using gradient descent, with the goal of finding transition matrices that best explain our observed data.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"# free energy objective to be optimized during training\nfunction make_fe_tot_est(rebuild, data; Q = Q, B = B, R = R)\n    function fe_tot_est(v)\n        nn = rebuild(v)\n        result = infer(\n            model = ssm(As = get_matrices_from_neural_network(data, nn), Q = Q, B = B, R = R), \n            data  = (y = data, ), \n            returnvars = (x = KeepLast(), ),\n            free_energy = true,\n            session = nothing\n        )\n        return result.free_energy[end]\n    end\nend","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"make_fe_tot_est (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"function train!(neural_network, data; num_epochs = 500)\n    rule = Flux.Optimise.Adam()\n    state = Flux.Optimise.setup(rule, neural_network)\n\n    x, rebuild = Flux.destructure(neural_network)\n    fe_tot_est_ = make_fe_tot_est(rebuild, data)\n\n    run_epochs!(rebuild, fe_tot_est_, state, neural_network; num_epochs = num_epochs)\nend\n\nfunction run_epochs!(rebuild::F, fe_tot_est::I, state::S, neural_network::N; num_epochs::Int = 100) where {F, I, S, N}\n    print_each = num_epochs ÷ 10\n    start_time = time()\n    for epoch in 1:num_epochs\n        flat, _ = Flux.destructure(neural_network)\n        if epoch % print_each == 0\n            current_value = fe_tot_est(flat)\n            elapsed = time() - start_time\n            remaining = elapsed / epoch * (num_epochs - epoch)\n            println(\"Epoch $epoch/$num_epochs: Free Energy = $current_value, ETA: $(round(remaining; digits=1)) seconds\")\n        end\n        grads = ForwardDiff.gradient(fe_tot_est, flat);\n        Flux.update!(state, neural_network, rebuild(grads))\n    end\n    # Calculate and print total training time\n    total_time = time() - start_time\n    println(\"Finished in $(round(total_time; digits=1)) seconds\")\nend","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"run_epochs! (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Now that we have defined our neural network architecture, dataset, and training functions, we can proceed with the training process. We'll train the neural network to learn the underlying dynamics of our state-space model from noisy observations. The training will optimize the free energy objective function using the Adam optimizer over multiple epochs. This process will allow the neural network to capture the non-linear relationships in the data, enabling more accurate state inference compared to traditional linear models. The following cell executes the training with 2000 epochs, which should provide sufficient iterations for convergence.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"trained_neural_network = make_neural_network()\n\ntrain!(trained_neural_network, dataset.noisy_signal; num_epochs = 2000)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Epoch 200/2000: Free Energy = 22554.71504016765, ETA: 261.6 seconds\nEpoch 400/2000: Free Energy = 15243.43215265174, ETA: 176.6 seconds\nEpoch 600/2000: Free Energy = 1975.3465539892043, ETA: 138.1 seconds\nEpoch 800/2000: Free Energy = 1528.6284177456932, ETA: 110.2 seconds\nEpoch 1000/2000: Free Energy = 1510.1678087715945, ETA: 88.7 seconds\nEpoch 1200/2000: Free Energy = 1507.584198832973, ETA: 67.6 seconds\nEpoch 1400/2000: Free Energy = 1507.2009354685388, ETA: 48.4 seconds\nEpoch 1600/2000: Free Energy = 1507.14727087365, ETA: 31.2 seconds\nEpoch 1800/2000: Free Energy = 1507.141346589192, ETA: 15.1 seconds\nEpoch 2000/2000: Free Energy = 1507.1422408972821, ETA: 0.0 seconds\nFinished in 150.3 seconds","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Now let's analyze the results of our neural network training. We'll visualize how well our trained model can infer the true states from noisy observations. The plots below will show the original noisy observations, the true underlying states, and our model's inferred states with confidence intervals. This comparison will help us evaluate the effectiveness of our neural network-based approach in capturing the non-linear dynamics of the system and filtering out noise.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"trained_transition_matrices = get_matrices_from_neural_network(dataset.noisy_signal, trained_neural_network)\n\ntrained_result = infer(\n    model = ssm(As = trained_transition_matrices, Q = Q, B = B, R = R), \n    data  = (y = dataset.noisy_signal, ), \n    returnvars = (x = KeepLast(), )\n)\n\nplot_coordinates(trained_result)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"The results demonstrate the effectiveness of our neural network-based state-space model approach. Despite the significant noise present in the observations (shown as scattered points), our model successfully identifies the underlying hidden signal (shown by the inferred states line). The close alignment between the inferred states and the true states across all three coordinates indicates that the trained neural network has effectively learned the non-linear dynamics of the system. The narrow confidence bands (shown as ribbons) around the inferred states further suggest high confidence in the predictions. This example illustrates how combining neural networks with probabilistic state-space models can provide robust inference in scenarios with complex dynamics and noisy observations.","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"ix, iy, iz = zeros(n_points), zeros(n_points), zeros(n_points)\n\ninferred_mean = mean.(trained_result.posteriors[:x])\n\n# Extract coordinates\nfor i in 1:n_points\n    # Inferred mean\n    ix[i], iy[i], iz[i] = inferred_mean[i][1], inferred_mean[i][2], inferred_mean[i][3]\nend\n\n# Create three projection plots\np1 = scatter(rx, ry, label=\"Noisy observations\", alpha=0.7, markersize=2, title = \"X-Y Projection\")\nplot!(p1, gx, gy, label=\"True state\", linewidth=2)\nplot!(p1, ix, iy, label=\"Inferred Mean\", linewidth=2)\n\np2 = scatter(rx, rz, label=\"Noisy observations\", alpha=0.7, markersize=2, title = \"X-Z Projection\")\nplot!(p2, gx, gz, label=\"True state\", linewidth=2)\nplot!(p2, ix, iz, label=\"Inferred Mean\", linewidth=2)\n\np3 = scatter(ry, rz, label=\"Noisy observations\", alpha=0.7, markersize=2, title = \"Y-Z Projection\")\nplot!(p3, gy, gz, label=\"True state\", linewidth=2)\nplot!(p3, iy, iz, label=\"Inferred Mean\", linewidth=2)\n\n# Combine plots with improved layout\nplot(p1, p2, p3, size=(900, 250), layout=(1,3), margin=5Plots.mm)","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/integrating_neural_networks_with_flux.jl/Project.toml`\n  [587475ba] Flux v0.16.5\n⌅ [f6369f11] ForwardDiff v0.10.39\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [37e2e46d] LinearAlgebra v1.11.0\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n","category":"page"},{"location":"categories/advanced_examples/integrating_neural_networks_with_flux.jl/","page":"Integrating Neural Networks With Flux.Jl","title":"Integrating Neural Networks With Flux.Jl","text":"","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#ODE-Parameter-Estimation","page":"Ode Parameter Estimation","title":"ODE Parameter Estimation","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"In this notebook we will explore how we can solve and learn the parameters of an ODE simultaneously using RxInfer. To illustrate how we can can utilize RxInfer, we will take Lotka-Volterra differential equation as an example. We will explore three different alternatives to parameter estimation. The first alternative will demonstrate how we can use free energy to obtain point estimates. The second alternative will demonstrate how we can use a prior distribution on the parameters to obtain a posterior estimate for the unknown parameters of the ODE. The second alternative will do parameter learning in two stages. The first stage will obtain the initialization for the prior hyper-parameters and then use these initial values of the prior to obtain the posterior by message passing. The third alternative will use purely message passing. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"using RxInfer, Optim, LinearAlgebra, Plots,  StaticArrays, StableRNGs","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Introduction-to-Lotka-Volterra-Equations","page":"Ode Parameter Estimation","title":"Introduction to Lotka-Volterra Equations","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"The Lotka-Volterra equations, are a pair of first-order nonlinear differential equations frequently used to describe the dynamics of biological systems in which two species interact: one as a predator and the other as prey. The equations are defined as follows:","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Prey Population Dynamics:   fracdxdt = alpha x - beta xy","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Predator Population Dynamics:   fracdydt = -gamma y + delta xy","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"In this ODE, x is the population of the prey (e.g., rabbits), y is the population of the predator (e.g., foxes), alpha represents the maximum growth rate of the prey, beta is the rate of predation, gamma is the predator's per capita death rate and delta is the growth rate of the predator population based on the availability of prey.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function lotka_volterra(u, z, p, t)\n    α, β, δ, γ = p[SA[1,2,3,4]]\n    x, y = u[SA[1, 2]]\n    du1 = α * x - β * x * y\n    du2 = -δ * y + γ * x * y\n\n    return [du1, du2]\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#The-Runge-Kutta-4th-Order-(RK4)-Method","page":"Ode Parameter Estimation","title":"The Runge-Kutta 4th Order (RK4) Method","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"The Runge-Kutta 4th order method is one of the most widely used numerical techniques for solving ordinary differential equations (ODEs). For a system of the form:","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"fracdxdt = f(x t)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"where x can be a scalar or vector-valued function, RK4 provides a numerical approximation with local truncation error of order O(h^5) and global error of order O(h^4).","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Algorithm","page":"Ode Parameter Estimation","title":"Algorithm","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Given the current state x_n at time t_n, RK4 computes the state at t_n+1 = t_n + dt using four intermediate evaluations:","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"beginaligned\nk_1 = f(x_n t_n) \nk_2 = f(x_n + fracdt2k_1 t_n + fracdt2) \nk_3 = f(x_n + fracdt2k_2 t_n + fracdt2) \nk_4 = f(x_n + dtk_3 t_n + dt)\nendaligned","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"The solution is then advanced using a weighted average of these evaluations:","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"x_n+1 = x_n + fracdt6(k_1 + 2k_2 + 2k_3 + k_4)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function rk4_step(f::Function, x, u, θ, t, dt; supersample = 1)\n    \n    @inbounds for i in 1:supersample\n        dt_i = dt/supersample\n        k1 = f(x, u, θ, t)\n        k2 = f(x + dt_i/2*k1, u, θ, t + dt_i/2)\n        k3 = f(x + dt_i/2*k2, u, θ, t + dt_i/2)\n        k4 = f(x + dt_i*k3, u, θ, t + dt_i)\n        x += dt_i/6*(k1 + 2*k2 + 2*k3 + k4)\n    end\n    return x\n\nend\n\nfunction lotka_volterra_rk4(x, θ, t, dt, supersample = 1)\n    return rk4_step(lotka_volterra, x, 0, θ, t, dt)\n  \nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"lotka_volterra_rk4 (generic function with 2 methods)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Data-Generation","page":"Ode Parameter Estimation","title":"Data Generation","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Lotka Volterra data is generated using the RK4 method. The data is then corrupted with noise to simulate real-world observations. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"DISCLAIMER: Since Lotka-Volterra equations model prey and predator dynamics, adding a Gaussian noise is not realistic. Although adding other noise forms are possible it will complicate the inference process. Therefore, we will use Gaussian noise for instructive purposes. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function generate_data(θ; x = ones(2), t =0.0, dt = 0.001, n = 1000, v = 1, seed = 123)\n    rng = StableRNG(seed)\n    data = Vector{Vector{Float64}}(undef, n)\n    ts = Vector{Float64}(undef, n)\n    for i in 1:n\n        data[i] = lotka_volterra_rk4(x, θ, t, dt)\n        x = data[i]\n        t += dt\n        ts[i] = t\n    end\n    noisy_data = map(data) do d\n        noise = sqrt(v) * [randn(rng), randn(rng)]\n        d + noise\n    end\n    return data, noisy_data, ts\nend\n\ndt = 0.1 # sample_interval\nnoisev = 0.35\nn = 10000\ntrue_params = [1.0, 1.5, 3.0, 1.0]\ndata_long, noisy_data_long, ts_long = generate_data(true_params,dt = dt, n = n, v = noisev);\n\n## We create a smaller dataset for the global parameter optimization. Utilizing the entire dataset for the global optimization will take too much time. \nn_train = 100\ndata = data_long[1:n_train]\nnoisy_data = noisy_data_long[1:n_train]\nts = ts_long[1:n_train];","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Data-Visualization","page":"Ode Parameter Estimation","title":"Data Visualization","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"p = plot(layout=(2,1))\nplot!(subplot=1, ts, [d[1] for d in data], label=\"True x₁\", color=:blue)\nplot!(subplot=1, ts, [d[1] for d in noisy_data], seriestype=:scatter, label=\"Noisy x₁\", color=:blue, alpha=0.3, markersize=1.3)\nplot!(subplot=2, ts, [d[2] for d in data], label=\"True x₂\", color=:red)\nplot!(subplot=2, ts, [d[2] for d in noisy_data], seriestype=:scatter, label=\"Noisy x₂\", color=:red, alpha=0.3, markersize=1.3)\nxlabel!(\"Time\")\nylabel!(subplot=1, \"Prey Population\")\nylabel!(subplot=2, \"Predator Population\")","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#First-Alternative:-Global-Parameter-Optimization","page":"Ode Parameter Estimation","title":"First Alternative: Global Parameter Optimization","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"In the first alternative we will construct one time-segment of Lotka-Volterra equation. We will use lotka_volterra_rk4 function to create non-linear node. This function was defined earlier to numerically solve the Lotka-Volterra equations using the 4th order Runge-Kutta method. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"@model function lotka_volterra_model_without_prior(obs, mprev, Vprev, dt, t, θ)\n    xprev ~ MvNormalMeanCovariance(mprev, Vprev)\n    x     := lotka_volterra_rk4(xprev, θ, t, dt)\n    obs   ~ MvNormalMeanCovariance(x,  noisev * diageye(length(mprev)))\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Non-linear deterministic nodes require meta specification that will determine the type of message approximations to be used. In this case, we can use the Linearization method that will trigger an Extended Kalman Filter (EKF) type of approximation or the Unscented method that will trigger an Unscented Kalman Filter (UKF) type of approximation. Moreover, because we are using RxInfer in an online setting we need to specify how the mean and covariance of the Gaussian distribution will be updated. We do this by using the @autoupdates macro and initialize using the @initialization macro. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"delta_meta = @meta begin\n    lotka_volterra_rk4() ->  Linearization()\nend\n\nautoupdates_without_prior = @autoupdates begin\n    mprev, Vprev= mean_cov(q(x))\nend\n\n@initialization function initialize_without_prior(mx, Vx)\n    q(x) = MvNormalMeanCovariance(mx, Vx)\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Free-Energy-Computation","page":"Ode Parameter Estimation","title":"Free Energy Computation","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"We will now define the free energy function that will be minimized to infer the parameters of the model. Since the parameters of the model are not constrained to be positive, we will use the exp function to transform the parameters to the positive domain. We will set the free energy to true to keep track of the free energy values. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function compute_free_energy_without_prior(θ ; mx = ones(2), Vx = 1e-6 * diageye(2))\n    θ = exp.(θ)\n    result = infer(\n        model = lotka_volterra_model_without_prior(dt = dt, θ = θ),\n        data = (obs = noisy_data, t= ts),\n        initialization = initialize_without_prior(mx, Vx),\n        meta = delta_meta,\n        autoupdates = autoupdates_without_prior,\n        keephistory = length(noisy_data),\n        free_energy = true\n    )\n    return sum(result.free_energy_final_only_history)\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Now we are ready to perform the parameter inference by minimizing the free energy function. We will use the optimize function from the Optim package to perform the optimization. We will use the NelderMead method as the optimizer as it doesn't require gradient information and is faster.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"res_without_prior  = optimize(compute_free_energy_without_prior, zeros(4), NelderMead(), Optim.Options(show_trace = true, show_every = 300));","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Iter     Function value    √(Σ(yᵢ-ȳ)²)/n \n------   --------------    --------------\n     0     1.274436e+03     9.650180e+00\n * time: 0.0003399848937988281","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"θ_minimizer_without_prior = exp.(res_without_prior.minimizer)\nprintln(\"\\nEstimated point mass valued parameters:\")\nfor (i, (name, val)) in enumerate(zip([\"α\", \"β\", \"γ\", \"δ\"], θ_minimizer_without_prior))\n    println(\" * $name: $(round(val, digits=3))\")\nend\n\nprintln(\"\\nActual parameters used to generate data:\")\nfor (i, (name, val)) in enumerate(zip([\"α\", \"β\", \"γ\", \"δ\"], true_params))\n    println(\" * $name: $(round(val, digits=3))\")\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Estimated point mass valued parameters:\n * α: 0.994\n * β: 1.491\n * γ: 3.054\n * δ: 0.997\n\nActual parameters used to generate data:\n * α: 1.0\n * β: 1.5\n * γ: 3.0\n * δ: 1.0","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Second-Alternative:-RxInfer-Model-with-Prior-on-the-Parameters","page":"Ode Parameter Estimation","title":"Second Alternative: RxInfer Model with Prior on the Parameters","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"We will now define the corresponding RxInfer model with the prior distribution on the parameters. For this, we will use the @model macro to create a time segment for the ODE using the deterministic ODE solver lotka_volterra_rk4 as a non-linear node in the RxInfer model. For the prior distribution of the parameters, we will use a multivariate Gaussian distribution with mean mθ and covariance Vθ that will be initialized using the initialize macro.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"@model function lotka_volterra_model(obs, mprev, Vprev, dt, t, mθ, Vθ)\n    θ     ~ MvNormalMeanCovariance(mθ, Vθ)\n    xprev ~ MvNormalMeanCovariance(mprev, Vprev)\n    x     := lotka_volterra_rk4(xprev, θ, t, dt)\n    obs   ~ MvNormalMeanCovariance(x,  noisev * diageye(length(mprev)))\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"autoupdates = @autoupdates begin\n    mprev, Vprev= mean_cov(q(x))\n    mθ, Vθ = mean_cov(q(θ))\nend\n\n@initialization function initialize(mx, Vx, mθ, Vθ)\n    q(x) = MvNormalMeanCovariance(mx, Vx)\n    q(θ) = MvNormalMeanCovariance(mθ, Vθ)\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Prior-Initialization-by-means-of-Free-Energy-Minimization","page":"Ode Parameter Estimation","title":"Prior Initialization by means of Free Energy Minimization","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"We will now define the free energy function that will be minimized to infer the initial hyper-parameters of the prior distribution. Since we have 4 parameters, we will initialize the mean of the prior distribution with 4 elements and the diagonal elements of the covariance matrix. Again, we will use the exp function to transform the parameters to the positive domain. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function compute_free_energy(θ ; mx = ones(2), Vx = 1e-6 * diageye(2))\n    θ = exp.(θ)\n    mθ = θ[1:4]\n    Vθ = Diagonal(θ[5:end])\n    result = infer(\n        model = lotka_volterra_model(dt = dt,),\n        data = (obs = noisy_data, t = ts),\n        initialization = initialize(mx, Vx, mθ, Vθ),\n        meta = delta_meta,\n        autoupdates = autoupdates,\n        keephistory = length(noisy_data),\n        free_energy = true\n    )\n    return sum(result.free_energy_final_only_history)\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Parameter-Inference","page":"Ode Parameter Estimation","title":"Parameter Inference","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"We will now perform the parameter inference by minimizing the free energy function. We will use the optimize function from the Optim package to perform the optimization. We will use the NelderMead method as the optimizer as it doesn't require gradient information and is faster.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"res = optimize(compute_free_energy, [zeros(4); 0.1ones(4)], NelderMead(), Optim.Options(show_trace = true, show_every = 300));","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Iter     Function value    √(Σ(yᵢ-ȳ)²)/n \n------   --------------    --------------\n     0     2.814831e+02     2.192538e-01\n * time: 0.00418400764465332\n   300     2.714613e+02     1.554900e-03\n * time: 6.108357906341553\n   600     2.712411e+02     6.805548e-07\n * time: 12.400959014892578","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"θ_minimizer = exp.(res.minimizer)\nmθ_init = θ_minimizer[1:4]\nVθ_init = Diagonal(θ_minimizer[5:end])\n\nprintln(\"\\nEstimated initialization parameters for the prior distribution:\")\nfor (i, (name, val, var)) in enumerate(zip([\"α\", \"β\", \"γ\", \"δ\"], mθ_init, θ_minimizer[5:8]))\n    println(\" * $name: $(round(val, digits=3)) ± $(round(sqrt(var), digits=3))\")\nend\n\nprintln(\"\\nActual parameters used to generate data:\")\nfor (i, (name, val)) in enumerate(zip([\"α\", \"β\", \"γ\", \"δ\"], true_params))\n    println(\" * $name: $(round(val, digits=3))\")\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Estimated initialization parameters for the prior distribution:\n * α: 2.279 ± 1.275\n * β: 1.91 ± 2.119\n * γ: 1.636 ± 2.228\n * δ: 0.962 ± 0.693\n\nActual parameters used to generate data:\n * α: 1.0\n * β: 1.5\n * γ: 3.0\n * δ: 1.0","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Having estimated the initial hyper-parameters of the prior distribution, we can now perform the parameter inference by online message passing. We will use the infer function to perform the inference. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"result = infer(\n    model = lotka_volterra_model(dt = dt,),\n    data = (obs = noisy_data_long, t= ts_long),\n    initialization = initialize(ones(2), 1e-6 * diageye(2), mθ_init, Vθ_init),\n    meta = delta_meta,\n    autoupdates = autoupdates,\n    keephistory = length(noisy_data_long),\n    free_energy = true\n);","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"mθ_posterior = mean.(result.history[:θ])\nVθ_posterior = var.(result.history[:θ])\n\np = plot(layout=(4,1), size=(800,1000), legend=:right)\n\nparam_names = [\"α\", \"β\", \"γ\", \"δ\"]\n\nfor i in 1:4\n    means = [m[i] for m in mθ_posterior]\n    stds = [2sqrt(v[i]) for v in Vθ_posterior]\n    \n    plot!(p[i], means, ribbon=stds, label=\"Posterior\", subplot=i)\n    hline!(p[i], [true_params[i]], label=\"True value\", linestyle=:dash, color=:red, subplot=i)\n    \n    title!(p[i], param_names[i], subplot=i)\n    if i == 4 \n        xlabel!(p[i], \"Time step\", subplot=i)\n    end\nend\n\n# Place legend at top right for all subplots\nplot!(p, legend=:topright)\n\ndisplay(p)\nfinal_means = last(mθ_posterior)\nfinal_vars = last(Vθ_posterior)\nfinal_stds = sqrt.(final_vars)\n\n# Print results\nprintln(\"\\nFinal Parameter Estimates:\")\nfor (param, mean, std) in zip(param_names, final_means, final_stds)\n    println(\"$param: $mean ± $(std)\")\nend\n\n# Get final covariance matrix\nfinal_cov = cov(last(result.history[:θ]))\nprintln(\"\\nFinal Parameter Covariance Matrix:\")\ndisplay(final_cov)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Final Parameter Estimates:\nα: 0.9873125810392338 ± 0.022683193282629927\nβ: 1.4928568988039574 ± 0.026900704209026783\nγ: 3.0329956989098488 ± 0.12553964480470822\nδ: 1.0192425653899293 ± 0.03383241259684531\n\nFinal Parameter Covariance Matrix:\n4×4 Matrix{Float64}:\n 0.000514527   0.00038494    8.38569e-5   3.78402e-5\n 0.00038494    0.000723648  -8.1867e-5   -4.44924e-5\n 8.38569e-5   -8.1867e-5     0.0157602    0.00373872\n 3.78402e-5   -4.44924e-5    0.00373872   0.00114463","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"from = 1\nskip = 1        \nto = 500\n\n# Get state estimates and variances\nmx = mean.(result.history[:x])\nVx = var.(result.history[:x])\n\n# Plot state estimates with uncertainty bands\np1 = plot(ts_long[from:skip:to] , getindex.(mx, 1)[from:skip:to], ribbon=2*sqrt.(getindex.(Vx, 1)[from:skip:to]), \n          label=\"Prey estimate\", legend=:topright)\nscatter!(p1, ts_long[from:skip:to], getindex.(noisy_data_long, 1)[from:skip:to], label=\"Noisy prey observations\", alpha=0.5,ms=1)\nplot!(p1, ts_long[from:skip:to], getindex.(data_long, 1)[from:skip:to], label=\"True prey\", linestyle=:dash)\ntitle!(p1, \"Prey Population\")\n\np2 = plot(ts_long[from:skip:to], getindex.(mx, 2)[from:skip:to], ribbon=2*sqrt.(getindex.(Vx, 2)[from:skip:to]), \n          label=\"Predator estimate\", legend=:topright)\nscatter!(p2, ts_long[from:skip:to], getindex.(noisy_data_long, 2)[from:skip:to], label=\"Noisy predator observations\", alpha=0.5, ms=1)\nplot!(p2, ts_long[from:skip:to], getindex.(data_long, 2)[from:skip:to] , label=\"True predator\", linestyle=:dash)\ntitle!(p2, \"Predator Population\")\n\nplot(p1, p2, layout=(2,1), size=(1000,600))","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Third-Alternative:-RxInfer-Model-with-Exponential-Transformation-on-the-Parameters","page":"Ode Parameter Estimation","title":"Third Alternative: RxInfer Model with Exponential Transformation on the Parameters","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"So far we have used the exp function to transform the parameters to the positive domain and computed free energy. This transformation was done outside of @model macro. In this approach, we will use the exp function to transform the parameters to the positive domain but within the @model macro. We will then use the Unscented method to approximate the non-linear deterministic node. This approach is more computationally efficient than the previous one, however it may suffer from accuracy issues as we may not have a good hyper-parameter initialization. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"NOTE: We can not use exp.() inside the @model macro as the model macro doesn't support broadcasting yet. So we need to define a function that will apply the exp function to the parameters. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"lotka_volterra_rk4_transformed(x,θ,t,dt) = lotka_volterra_rk4(x, exp.(θ), t, dt)## This function is used to apply the exp function to the parameters within the @model macro\n\n@model function lotka_volterra_model2(obs, mprev, Vprev, dt, t, mθ, Vθ)\n    θ     ~ MvNormalMeanCovariance(mθ, Vθ)\n    xprev ~ MvNormalMeanCovariance(mprev, Vprev)\n    x     := lotka_volterra_rk4_transformed(xprev, θ, t, dt)\n    obs   ~ MvNormalMeanCovariance(x,  noisev * diageye(length(mprev)))\nend\n\ndelta_meta2 = @meta begin\n    lotka_volterra_rk4_transformed() ->  Unscented()\nend\n\nautoupdates2 = @autoupdates begin\n    mprev, Vprev= mean_cov(q(x))\n    mθ, Vθ = mean_cov(q(θ))\nend\n\n@initialization function initialize2(mx, Vx, mθ, Vθ)\n    q(x) = MvNormalMeanCovariance(mx, Vx)\n    q(θ) = MvNormalMeanCovariance(mθ, Vθ)\nend\n\n\nresult2  = infer(\n    model = lotka_volterra_model2(dt = dt,),\n    data = (obs = noisy_data_long, t= ts_long),\n    initialization = initialize2(ones(2),  1e-6diageye(2), zeros(4), 0.1*diageye(4)),\n    meta = delta_meta2,\n    autoupdates = autoupdates2,\n    keephistory = length(noisy_data_long),\n    free_energy = true\n)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"RxInferenceEngine:\n  Posteriors stream    | enabled for (θ, xprev, x)\n  Free Energy stream   | enabled\n  Posteriors history   | available for (θ, xprev, x)\n  Free Energy history  | available\n  Enabled events       | [  ]","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"mθ =  mean.(result2.history[:θ])\nVθ = cov.(result2.history[:θ])\nexpf(θ) = exp.(θ)\nθdists = map((m,v) -> MvNormalMeanCovariance(m, v), mθ, Vθ)\nθ_exp_dists = map(θdist -> ReactiveMP.approximate(Unscented(), expf, (θdist,)), θdists)\n\nmθ_exp =  mean.(θ_exp_dists)\nVθ_exp = cov.(θ_exp_dists)\n\n# Plot the inferred parameters with uncertainty\np1 = plot(ts_long, getindex.(mθ_exp, 1), ribbon=2*sqrt.(getindex.(Vθ_exp, 1,1)), label=\"α\", legend=:topright)\nplot!(p1, ts_long, fill(true_params[1], length(ts_long)), label=\"True α\", linestyle=:dash)\ntitle!(p1, \"Parameter α\")\n\np2 = plot(ts_long, getindex.(mθ_exp, 2), ribbon=2*sqrt.(getindex.(Vθ_exp, 2,2)), label=\"β\", legend=:topright)\nplot!(p2, ts_long, fill(true_params[2], length(ts_long)), label=\"True β\", linestyle=:dash)\ntitle!(p2, \"Parameter β\")\n\np3 = plot(ts_long, getindex.(mθ_exp, 3), ribbon=2*sqrt.(getindex.(Vθ_exp, 3,3)), label=\"γ\", legend=:topright)\nplot!(p3, ts_long, fill(true_params[3], length(ts_long)), label=\"True γ\", linestyle=:dash)\ntitle!(p3, \"Parameter γ\")\n\np4 = plot(ts_long, getindex.(mθ_exp, 4), ribbon=2*sqrt.(getindex.(Vθ_exp, 4,4)), label=\"δ\", legend=:topright)\nplot!(p4, ts_long, fill(true_params[4], length(ts_long)), label=\"True δ\", linestyle=:dash)\ntitle!(p4, \"Parameter δ\")\n\nplot(p1, p2, p3, p4, layout=(4,1), size=(1000,800))","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"# Print final parameter estimates and covariance\nfinal_means = last(mθ_exp)\nfinal_vars = diag(last(Vθ_exp))\nfinal_stds = sqrt.(final_vars)\n\n# Print results\nprintln(\"\\nFinal Parameter Estimates:\")\nfor (param, mean, std) in zip(param_names, final_means, final_stds)\n    println(\"$param: $mean ± $(std)\")\nend\n\nprintln(\"\\nFinal parameter covariance matrix:\")\ndisplay(last(Vθ_exp))","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Final Parameter Estimates:\nα: 1.0466420644370373 ± 0.023551898981352345\nβ: 1.5267272962373681 ± 0.027682602680033756\nγ: 2.7961085961433128 ± 0.12150864747658563\nδ: 0.9428781820606673 ± 0.032860835936904864\n\nFinal parameter covariance matrix:\n4×4 Matrix{Float64}:\n 0.000554692   0.000437206   8.94158e-5   3.93331e-5\n 0.000437206   0.000766326  -7.64945e-5  -5.57011e-5\n 8.94158e-5   -7.64945e-5    0.0147644    0.00347717\n 3.93331e-5   -5.57011e-5    0.00347717   0.00107983","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"\n# Get state estimates and variances\nmx = mean.(result2.history[:x])\nVx = var.(result2.history[:x])\n\n# Plot state estimates with uncertainty bands\np1 = plot(ts_long[from:skip:to] , getindex.(mx, 1)[from:skip:to], ribbon=2*sqrt.(getindex.(Vx, 1)[from:skip:to]), \n          label=\"Prey estimate\", legend=:topright)\nscatter!(p1, ts_long[from:skip:to], getindex.(noisy_data_long, 1)[from:skip:to], label=\"Noisy prey observations\", alpha=0.5,ms=1)\nplot!(p1, ts_long[from:skip:to], getindex.(data_long, 1)[from:skip:to], label=\"True prey\", linestyle=:dash)\ntitle!(p1, \"Prey Population\")\n\np2 = plot(ts_long[from:skip:to], getindex.(mx, 2)[from:skip:to], ribbon=2*sqrt.(getindex.(Vx, 2)[from:skip:to]), \n          label=\"Predator estimate\", legend=:topright)\nscatter!(p2, ts_long[from:skip:to], getindex.(noisy_data_long, 2)[from:skip:to], label=\"Noisy predator observations\", alpha=0.5, ms=1)\nplot!(p2, ts_long[from:skip:to], getindex.(data_long, 2)[from:skip:to] , label=\"True predator\", linestyle=:dash)\ntitle!(p2, \"Predator Population\")\n\nplot(p1, p2, layout=(2,1), size=(1000,600))","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/ode_parameter_estimation/Project.toml`\n  [be0214bd] NonlinearSolveBase v2.0.0\n  [429524aa] Optim v1.13.2\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [90137ffa] StaticArrays v1.9.15\n  [37e2e46d] LinearAlgebra v1.11.0\n","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Predicting-Bike-Rental-Demand","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Important Note: This notebook primarily aims to show how to manage missing data and generate predictions using a model. It does not serve as a comprehensive guide to building the most advanced model for this dataset or showcase the best practices in feature engineering. To keep explanations clear, we will use simplified assumptions and straightforward models. For applications in the real world, you should employ more sophisticated approaches and feature engineering methods. However, we will present a more complex model towards the end of this notebook for you to explore, albeit with less detailed guidance.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using RxInfer","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Preamble:-Enabling-Predictions","page":"Predicting Bike Rental Demand","title":"Preamble: Enabling Predictions","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"RxInfer.jl facilitates predictions in two primary ways. ","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Implicit Prediction: By adding missing instances directly into the data, which are then treated as regular observations during inference.\nExplicit Prediction: By defining a separate data variable in the model. This approach doesn't necessitate passing missing instances as the data variable but does require specifying the predictvar argument in the inference function.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Example","page":"Predicting Bike Rental Demand","title":"Example","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Consider the following model:","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"@model function example_model(y)\n\n    h ~ NormalMeanPrecision(0, 1.0)\n    x ~ NormalMeanPrecision(h, 1.0)\n    y ~ NormalMeanPrecision(x, 10.0)\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Implicit Prediction\nresult = infer(model = example_model(), data = (y = missing,))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (h, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Explicit Prediction\nresult = infer(model = example_model(), predictvars = (y = KeepLast(),))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (h, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Both approaches yield the same results, but the choice depends on personal preferences and the model's structure. In scenarios with a clear distinction between observed and predicted variables, the explicit method is preferable. However, our subsequent example will not differentiate between observations and predictions, as it utilizes a state space representation.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using CSV, DataFrames, Plots","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Objective","page":"Predicting Bike Rental Demand","title":"Objective","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This example aims to simultaneously learn the dynamics of the feature space and predict hourly bike rental demand through reactive message passing, a signature approach of RxInfer.jl.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Dataset-Source","page":"Predicting Bike Rental Demand","title":"Dataset Source","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Data for this example study is sourced from the Kaggle Bike Count Prediction Dataset. For the purpose of this example, the original dataset from Kaggle has been adapted by removing categorical variables such as season, holiday, and working day. Additionally we take only 500 entries. This modification allows us to focus on modeling the continuous variables without additional complexities of handling categorical data. Nevertheless, this extension is feasible within RxInfer.jl.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Load the data\ndf = CSV.read(\"modified_bicycle.csv\", DataFrame)\ndf[1:10, :]","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"10×6 DataFrame\n Row │ datetime            temp     atemp    humidity  windspeed  count\n     │ String31            Float64  Float64  Float64   Float64    Int64\n─────┼──────────────────────────────────────────────────────────────────\n   1 │ 2011-01-01 0:00:00     9.84   14.395      81.0     0.0        16\n   2 │ 2011-01-01 1:00:00     9.02   13.635      80.0     0.0        40\n   3 │ 2011-01-01 2:00:00     9.02   13.635      80.0     0.0        32\n   4 │ 2011-01-01 3:00:00     9.84   14.395      75.0     0.0        13\n   5 │ 2011-01-01 4:00:00     9.84   14.395      75.0     0.0         1\n   6 │ 2011-01-01 5:00:00     9.84   12.88       75.0     6.0032      1\n   7 │ 2011-01-01 6:00:00     9.02   13.635      80.0     0.0         2\n   8 │ 2011-01-01 7:00:00     8.2    12.88       86.0     0.0         3\n   9 │ 2011-01-01 8:00:00     9.84   14.395      75.0     0.0         8\n  10 │ 2011-01-01 9:00:00    13.12   17.425      76.0     0.0        14","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# we reserve few samples for prediction\nn_future = 24\n\n# `x` is a sequence of observed features\nX = Union{Vector{Float64}, Missing}[[row[i] for i in 2:(ncol(df))-1] for row in eachrow(df)][1:end-n_future]\n\n# `y` is a sequence of \"count\" bicycles\ny = Union{Float64, Missing}[df[:, \"count\"]...][1:end-n_future]\n\nstate_dim = length(X[1]); # dimensionality of feature space","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Generative-Model-with-Priors","page":"Predicting Bike Rental Demand","title":"Generative Model with Priors","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"We present a generative model that delineates the latent dynamics of feature evolution, represented by mathbfh_t, and their link to the bike rental counts, mathbfy_t.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Equations-and-Priors","page":"Predicting Bike Rental Demand","title":"Equations and Priors","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Feature Dynamics with Prior:\nPrior: mathbfa sim mathcalN(mathbf0 mathbfI)\nDynamics: mathbfh_t sim mathcalN(mathbfA h_t-1 mathbfQ)\nmathbfA\nis the transition matrix, reshaped from the prior vector mathbfa, and mathbfQ represents process noise.\nNoisy Observations:\nmathbfx_t sim mathcalN(mathbfh_t mathbfP)\nRepresents the observed noisy state of the features.\nCount Prediction with Prior:\nPrior: boldsymboltheta sim mathcalN(mathbf0 mathbfI)\nPrediction: y_t sim mathcalN(textsoftplus(boldsymboltheta^topmathbfh_t) sigma^2)\nModels the bike rental count as influenced by a non-linear transformation of the hidden state.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Interpretation","page":"Predicting Bike Rental Demand","title":"Interpretation","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This framework aims to simultaneously infer the transition matrix mathbfA and the regression parameters boldsymboltheta, providing a comprehensive view of the feature space dynamics and the count prediction.\nBy employing Gaussian priors on both mathbfa and boldsymboltheta, we incorporate beliefs about their distributions.\nThe inference process aims to discover these underlying dynamics, enabling predictions of both features mathbfx_t and counts y_t.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# # We augument the dataset with missing entries for 24 hours ahead\nappend!(X, repeat([missing], n_future))\nappend!(y, repeat([missing], n_future));","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Function to perform the state transition in the model.\n# It reshapes vector `a` into a matrix and multiplies it with vector `x` to simulate the transition.\nfunction transition(a, x)\n    nm, n = length(a), length(x)\n    m = nm ÷ n  # Calculate the number of rows for reshaping 'a' into a matrix\n    A = reshape(a, (m, n))  \n    return A * x\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"transition (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# The dotsoftplus function combines a dot product and softplus transformation.\n# While useful for ensuring positivity, it may not be the optimal choice for all scenarios,\n# especially if the data suggests other forms of relationships or distributions.\nimport StatsFuns: softplus\ndotsoftplus(a, x) = softplus(dot(a, x))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"dotsoftplus (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# model definction\n@model function bicycle_ssm(x, y, h0, θ0, a0, Q, s)\n\n    a ~ a0\n    θ ~ θ0\n    h_prior ~ h0\n\n    h_prev = h_prior\n    for i in eachindex(y)\n        \n        h[i] ~ MvNormal(μ=transition(a, h_prev), Σ=Q)\n        x[i] ~ MvNormal(μ=h[i], Σ=diageye(state_dim))\n        y[i] ~ Normal(μ=dotsoftplus(θ, h[i]), σ²=s)\n        h_prev = h[i]\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# In this example, we opt for a basic Linearization approach for the transition and dotsoftplus functions.\n# However, alternative methods like Unscented or CVI approximations can also be considered.\nbicycle_ssm_meta = @meta begin \n    transition() -> Linearization()\n    dotsoftplus() -> Linearization()\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Meta: \n  transition() -> ReactiveMP.Linearization()\n  dotsoftplus() -> ReactiveMP.Linearization()","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# prior_h: Based on first observation, assuming initial state is similar with equal variance.\nprior_h = MvNormalMeanCovariance(X[1], diageye(state_dim))\n# prior_θ, prior_a: No initial bias, parameters independent with equal uncertainty.\nprior_θ = MvNormalMeanCovariance(zeros(state_dim), diageye(state_dim))\nprior_a = MvNormalMeanCovariance(zeros(state_dim^2), diageye(state_dim^2));","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Note that, in contrast with other examples, we wrap the data y in an UnfactorizedData struct. This is to indicate to the inference engine that we want to infer a joint posterior distribution over the missing values in y and the latent variables. More information on this can be found in the documentation on variational constraints.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# the deterministic relationsships (transition) and (dotsoftplus) will induce loops in the graph representation of our model, this necessiates the initialization of the messages\nimessages = @initialization begin\n    μ(h) = prior_h\n    μ(a) = prior_a\n    μ(θ) = prior_θ\nend\n# Assumptions about the model parameters:\n# Q: Process noise based on observed features' variance, assuming process variability reflects observed features variability.\n# s: Observation noise based on observed data variance, directly estimating variance in the data, important for predictions\nbicycle_model = bicycle_ssm(h0=prior_h, θ0=prior_θ, a0=prior_a, Q=var(filter(!ismissing, X)).*diageye(state_dim), s=var(filter(!ismissing, y)))\n\nresult = infer(\n    model = bicycle_model,\n    data  = (x=X, y=UnfactorizedData(y)), \n    options = (limit_stack_depth = 500, ), \n    returnvars = KeepLast(),\n    predictvars = KeepLast(),\n    initialization = imessages,\n    meta = bicycle_ssm_meta,\n    iterations = 20,\n    showprogress=true,\n)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (a, h, h_prior, θ)\n  Predictions      | available for (y, x)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# For a sake of this example, we extract only predictions\nmean_y, cov_y = mean.(result.predictions[:y]), cov.(result.predictions[:y])\nmean_x, cov_x = mean.(result.predictions[:x]), var.(result.predictions[:x])\n\nmean_x1, cov_x1 = getindex.(mean_x, 1), getindex.(cov_x, 1)\nmean_x2, cov_x2 = getindex.(mean_x, 2), getindex.(cov_x, 2)\nmean_x3, cov_x3 = getindex.(mean_x, 3), getindex.(cov_x, 3)\nmean_x4, cov_x4 = getindex.(mean_x, 4), getindex.(cov_x, 4);","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"slice = (300, length(y))\ndata = df[:, \"count\"][length(y)-n_future:length(y)]\n\np = scatter(y, \n            color=:darkblue, \n            markerstrokewidth=0,\n            label=\"Observed Count\", \n            alpha=0.6)\n\n# Plotting the mean prediction with variance ribbon\nplot!(mean_y, ribbon=sqrt.(cov_y), \n      color=:orange, \n      fillalpha=0.3,\n      label=\"Predicted Mean ± Std Dev\")\n\n# Adding a vertical line to indicate the start of the future prediction\nvline!([length(y)-n_future], \n       label=\"Prediction Start\", \n       linestyle=:dash, \n       linecolor=:green)\n\n# Future (unobserved) data\nplot!(length(y)-n_future:length(y), data, label=\"Future Count\")\n\n# Adjusting the limits\nxlims!(slice)\n\n# Enhancing the plot with titles and labels\ntitle!(\"Bike Rental Demand Prediction\")\nxlabel!(\"Time\")\nylabel!(\"Bike Count\")\n\n# Adjust the legend\nlegend=:topright\n\n# Show the final plot\ndisplay(p)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using Plots\n\n# Define a color palette\npalette = cgrad(:viridis)\n\n# Plot the hidden states with observations\np1 = plot(mean_x1, ribbon=sqrt.(cov_x1), color=palette[1], label=\"Hidden State 1\", legend=:topleft)\nplot!(df[!, :temp], color=:grey, label=\"Temperature\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Temperature vs Hidden State 1\")\n\np2 = plot(mean_x2, ribbon=sqrt.(cov_x2), color=palette[2], label=\"Hidden State 2\", legend=:topleft)\nplot!(df[!, :atemp], color=:grey, label=\"Feels-Like Temp\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Feels-Like Temp vs Hidden State 2\")\n\np3 = plot(mean_x3, ribbon=sqrt.(cov_x3), color=palette[3], label=\"Hidden State 3\", legend=:topleft)\nplot!(df[!, :humidity], color=:grey, label=\"Humidity\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Humidity vs Hidden State 3\")\n\np4 = plot(mean_x4, ribbon=sqrt.(cov_x4), color=palette[4], label=\"Hidden State 4\", legend=:topleft)\nplot!(df[!, :windspeed], color=:grey, label=\"Windspeed\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Windspeed vs Hidden State 4\")\n\nfor p in [p1, p2, p3, p4]\n    xlims!(p, first(slice), last(slice))\nend\n\nplot(p1, p2, p3, p4, layout=(2, 2), size=(800, 400))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Improving-the-model","page":"Predicting Bike Rental Demand","title":"Improving the model","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"While our current model's predictions may not closely match real-world data, it's important to recognize that certain assumptions and simplifications were made that might have affected the results. The model is essentially a theoretical framework, highlighting the ability to simultaneously deduce states, parameters, and predictions, with an emphasis on the analysis's predictive aspect.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"To enhance the model and refine its predictions, we can employ variational message passing. This method enables us to eliminate loops within the model by substituting initial messages with initial marginal distributions. This is achieved by utilizing ContinuousTransition (also referred to as CTransition) and SoftDot (aka softdot) nodes. These nodes facilitate the variational approximation of the transition matrix and the dot product, respectively.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"transformation = a -> reshape(a, state_dim, state_dim)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"#31 (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# model definction\n@model function bicycle_ssm_advanced(x, y, h0, θ0, a0, P0, γ0)\n\n    a ~ a0\n    θ ~ θ0\n    h_prior ~ h0\n    P ~ P0\n    γ ~ γ0\n\n    h_prev = h_prior\n    for i in eachindex(y)\n        \n        h[i] ~ CTransition(h_prev, a, P)\n        x[i]  ~ MvNormal(μ=h[i], Λ=diageye(state_dim))\n        _y[i] ~ softdot(θ, h[i], γ)\n        y[i] ~ Normal(μ=softplus(_y[i]), γ=1e4)\n        h_prev = h[i]\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"bicycle_ssm_advanced_meta = @meta begin \n    softplus() -> Linearization()\n    CTransition() -> CTMeta(transformation)\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Meta: \n  softplus() -> ReactiveMP.Linearization()\n  ReactiveMP.ContinuousTransition() -> ReactiveMP.ContinuousTransitionMeta{\nMain.anonymous.var\"#31#32\"}(Main.anonymous.var\"#31#32\"())","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"bicycle_ssm_advanced_constraints = @constraints begin\n    q(h_prior, h, a, P, γ, _y, y, θ) = q(h_prior, h)q(a)q(P)q(γ)q(_y, y)q(θ)\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Constraints: \n  q(h_prior, h, a, P, γ, _y, y, θ) = q(h_prior, h)q(a)q(P)q(γ)q(_y, y)q(θ)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"prior_P = ExponentialFamily.WishartFast(state_dim+2, inv.(var(filter(!ismissing, X))) .* diageye(state_dim))\nprior_a = MvNormalMeanPrecision(ones(state_dim^2), diageye(state_dim^2));\n\nprior_γ = GammaShapeRate(1.0, var(filter(!ismissing, y)))\nprior_h = MvNormalMeanPrecision(X[1], diageye(state_dim))\nprior_θ = MvNormalMeanPrecision(ones(state_dim), diageye(state_dim))\n\nimarginals = @initialization begin \n    q(h) = prior_h\n    q(a) = prior_a\n    q(P) = prior_P\n    q(γ) = prior_γ\n    q(θ) = prior_θ\nend\n\nbicycle_model_advanced = bicycle_ssm_advanced(h0=prior_h, θ0=prior_θ, a0=prior_a, P0=prior_P, γ0=prior_γ)\n\nresult_advanced = infer(\n    model = bicycle_model_advanced,\n    data  = (x=X, y=y), \n    options = (limit_stack_depth = 500, ), \n    returnvars = KeepLast(),\n    predictvars = KeepLast(),\n    initialization = imarginals,\n    constraints = bicycle_ssm_advanced_constraints,\n    meta = bicycle_ssm_advanced_meta,\n    iterations = 10,\n    showprogress=true,\n)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (a, P, _y, γ, h, h_prior, θ)\n  Predictions      | available for (y, x)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# For a sake of this example, we extract only predictions\nmean_y, cov_y = mean.(result_advanced.predictions[:y]), cov.(result_advanced.predictions[:y])","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"([16.000001019201765, 39.9999999771452, 32.00000029690829, 13.0000012305664\n04, 1.000001735201384, 1.0000019345942357, 2.0000015515352927, 3.0000014149\n59412, 8.000001680637501, 14.000001845050349  …  30.82842295140484, 30.4929\n08849041825, 30.18361227546638, 29.89498617419104, 29.622442386942147, 29.3\n6222177840814, 29.111291014209346, 28.86726225380032, 28.628327961902706, 2\n8.393168163892906], [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001\n, 0.0001, 0.0001, 0.0001  …  0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001\n, 0.0001, 0.0001, 0.0001, 0.0001])","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"slice = (300, length(y))\ndata = df[:, \"count\"][length(y)-n_future:length(y)]\n\npa = scatter(y, \n            color=:darkblue, \n            markerstrokewidth=0,\n            label=\"Observed Count\", \n            alpha=0.6)\n\n# Plotting the mean prediction with variance ribbon\nplot!(mean_y, ribbon=sqrt.(cov_y), \n      color=:orange, \n      fillalpha=0.3,\n      label=\"Predicted Mean ± Std Dev\")\n\n# Adding a vertical line to indicate the start of the future prediction\nvline!([length(y)-n_future], \n       label=\"Prediction Start\", \n       linestyle=:dash, \n       linecolor=:green)\n\n# Future (unobserved) data\nplot!(length(y)-n_future:length(y), data, label=\"Future Count\")\n\n# Adjusting the limits\nxlims!(slice)\n\n# Enhancing the plot with titles and labels\ntitle!(\"Advanced model\")\nxlabel!(\"Time\")\nylabel!(\"Bike Count\")\n\n# Adjust the legend\nlegend=:topright\n\n# Show the final plot\nplot(pa, p, size=(800, 400))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/predicting_bike_rental_demand/Project.toml`\n  [336ed68f] CSV v0.10.15\n  [a93c6f00] DataFrames v1.8.0\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [4c63d2b9] StatsFuns v1.5.0\n","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Infinite-Data-Stream","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"This example shows the capabilities of RxInfer to perform Bayesian inference on real-time signals. As usual, first, we start with importing necessary packages:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"using RxInfer, Plots, Random, StableRNGs","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"For demonstration purposes we will create a synthetic environment that has a hidden underlying signal, which we cannot observer directly. Instead, we will observe a noised realisation of this hidden signal:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"mutable struct Environment\n    rng                   :: AbstractRNG\n    current_state         :: Float64\n    observation_precision :: Float64\n    history               :: Vector{Float64}\n    observations          :: Vector{Float64}\n    \n    Environment(current_state, observation_precision; seed = 123) = begin \n         return new(StableRNG(seed), current_state, observation_precision, [], [])\n    end\nend\n\nfunction getnext!(environment::Environment)\n    environment.current_state = environment.current_state + 1.0\n    nextstate  = 10sin(0.1 * environment.current_state)\n    observation = rand(NormalMeanPrecision(nextstate, environment.observation_precision))\n    push!(environment.history, nextstate)\n    push!(environment.observations, observation)\n    return observation\nend\n\nfunction gethistory(environment::Environment)\n    return environment.history\nend\n\nfunction getobservations(environment::Environment)\n    return environment.observations\nend","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"getobservations (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Model-specification","page":"Infinite Data Stream","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"We assume that we don't know the shape of our signal in advance. So we try to fit a simple gaussian random walk with unknown observation noise:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"@model function kalman_filter(x_prev_mean, x_prev_var, τ_shape, τ_rate, y)\n    x_prev ~ Normal(mean = x_prev_mean, variance = x_prev_var)\n    τ ~ Gamma(shape = τ_shape, rate = τ_rate)\n\n    # Random walk with fixed precision\n    x_current ~ Normal(mean = x_prev, precision = 1.0)\n    y ~ Normal(mean = x_current, precision = τ)\n    \nend\n\n# We assume the following factorisation between variables \n# in the variational distribution\n@constraints function filter_constraints()\n    q(x_prev, x_current, τ) = q(x_prev, x_current)q(τ)\nend","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"filter_constraints (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Prepare-environment","page":"Infinite Data Stream","title":"Prepare environment","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"initial_state         = 0.0\nobservation_precision = 0.1","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"0.1","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"After we have created the environment we can observe how our signal behaves:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"testenvironment = Environment(initial_state, observation_precision);\n\nanimation = @animate for i in 1:100\n    getnext!(testenvironment)\n    \n    history = gethistory(testenvironment)\n    observations = getobservations(testenvironment)\n    \n    p = plot(size = (1000, 300))\n    \n    p = plot!(p, 1:i, history[1:i], label = \"Hidden signal\")\n    p = scatter!(p, 1:i, observations[1:i], ms = 4, alpha = 0.7, label = \"Observation\")\nend\n\ngif(animation, \"infinite-data-stream.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Filtering-on-static-dataset","page":"Infinite Data Stream","title":"Filtering on static dataset","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"RxInfer is flexible and allows for running inference both on real-time and static datasets. In the next section we show how to perform the filtering procedure on a static dataset. We also will verify our inference procedure by checking on the Bethe Free Energy values:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"n                  = 300\nstatic_environment = Environment(initial_state, observation_precision);\n\nfor i in 1:n\n    getnext!(static_environment)\nend\n\nstatic_history      = gethistory(static_environment)\nstatic_observations = getobservations(static_environment);\nstatic_datastream   = from(static_observations) |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_static(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    init = @initialization begin\n        q(x_current) = NormalMeanVariance(0.0, 1e3) \n        q(τ) = GammaShapeRate(1.0, 1.0)\n    end\n\n    engine = infer(\n        model          = kalman_filter(),\n        constraints    = filter_constraints(),\n        datastream     = datastream,\n        autoupdates    = autoupdates,\n        returnvars     = (:x_current, ),\n        keephistory    = 10_000,\n        historyvars    = (x_current = KeepLast(), τ = KeepLast()),\n        initialization = init,\n        iterations     = 10,\n        free_energy    = true,\n        autostart      = true,\n    )\n    \n    return engine\nend","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_static (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"result = run_static(static_environment, static_datastream);","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"static_inference = @animate for i in 1:n\n    estimated = result.history[:x_current]\n    p = plot(1:i, mean.(estimated[1:i]), ribbon = var.(estimated[1:n]), label = \"Estimation\")\n    p = plot!(static_history[1:i], label = \"Real states\")    \n    p = scatter!(static_observations[1:i], ms = 2, label = \"Observations\")\n    p = plot(p, size = (1000, 300), legend = :bottomright)\nend\n\ngif(static_inference, \"infinite-data-stream-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"plot(result.free_energy_history, label = \"Bethe Free Energy (averaged)\")","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Filtering-on-realtime-dataset","page":"Infinite Data Stream","title":"Filtering on realtime dataset","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"Next lets create a \"real\" infinite stream. We use timer() observable from Rocket.jlto emulate real-world scenario. In our example we are going to generate a new data point every ~41ms (24 data points per second). For demonstration purposes we force stop after n data points, but there is no principled limitation to run inference indefinite:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_and_plot(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    posteriors = []\n    \n    plotfn = (q_current) -> begin \n        IJulia.clear_output(true)\n        \n        push!(posteriors, q_current)\n\n        p = plot(mean.(posteriors), ribbon = var.(posteriors), label = \"Estimation\")\n        p = plot!(gethistory(environment), label = \"Real states\")    \n        p = scatter!(getobservations(environment), ms = 2, label = \"Observations\")\n        p = plot(p, size = (1000, 300), legend = :bottomright)\n\n        display(p)\n    end\n    \n    init = @initialization begin\n        q(x_current) = NormalMeanVariance(0.0, 1e3)\n        q(τ) = GammaShapeRate(1.0, 1.0)\n    end\n\n    engine = infer(\n        model         = kalman_filter(),\n        constraints   = filter_constraints(),\n        datastream    = datastream,\n        autoupdates   = autoupdates,\n        returnvars    = (:x_current, ),\n        initialization = init,\n        iterations    = 10,\n        autostart     = false,\n    )\n    \n    qsubscription = subscribe!(engine.posteriors[:x_current], plotfn)\n    \n    RxInfer.start(engine)\n    \n    return engine\nend","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_and_plot (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"# This example runs in our documentation pipeline, which does not support \"real-time\" execution context\n# We skip this code if run not in Jupyter notebook (see below an example with gif)\nengine = nothing \nif isdefined(Main, :IJulia)\n    timegen      = 41 # 41 ms\n    environment  = Environment(initial_state, observation_precision);\n    observations = timer(timegen, timegen) |> map(Float64, (_) -> getnext!(environment)) |> take(n) # `take!` automatically stops after `n` observations\n    datastream   = observations |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));\n    engine = run_and_plot(environment, datastream)\nend;","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"The plot above is fully interactive and we can stop and unsubscribe from our datastream before it ends:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"if !isnothing(engine) && isdefined(Main, :IJulia)\n    RxInfer.stop(engine)\n    IJulia.clear_output(true)\nend;","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/infinite_data_stream/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"page"},{"location":"categories/problem_specific/universal_mixtures/#Universal-Mixtures","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"section"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"using RxInfer, Distributions, Random, Plots","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane are having a coin toss competition. Before they start, they both have the feeling that something is not right. The coin is unbalanced and favors one side over the other. However, both John and Jane do not know which side is being favored. John thinks that the coin favors heads and Jane thinks tails. Coincidentally, both John and Jane have a strong mathematics background and are aware of the appropriate likelihood function for this experiment","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y_i mid theta) = mathrmBer(y_i mid theta)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"where y_i in 01 are the outcomes of the coin tosses, i.e. heads or tails, and where theta is the coin parameter. They express their gut feeling about the fairness of the coin in terms of a prior distribution over the coin parameter theta, which represents the probability of the coin landing on heads. Based on their gut feeling and the support of thetain01 they come up with the prior beliefs:","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textJohn) = mathrmBeta(theta mid 7 2)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textJane) = mathrmBeta(theta mid 2 7)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior beliefs\")\nplot!(rθ, (x) -> pdf(Beta(7.0, 2.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) John\", c=1)\nplot!(rθ, (x) -> pdf(Beta(2.0, 7.0), x), fillalpha=0.3, fillrange = 0, label=\"p(θ) Jane\", c=3,)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane really want to clear the odds and decide to perform a lengthy experiment. They toss the unbalanced coin N = 10 times, because their favorite TV show is cancelled anyway and therefore they have plenty of time. ","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"true_coin = Bernoulli(0.25)\nnr_throws = 10\ndataset = Int.(rand(MersenneTwister(42), true_coin, nr_throws))\nnr_heads, nr_tails = sum(dataset), nr_throws-sum(dataset)\nprintln(\"experimental outcome: \\n - heads: \", nr_heads, \"\\n - tails: \", nr_tails);","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"experimental outcome: \n - heads: 5\n - tails: 5","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"For computing the posterior beliefs p(theta mid y) about the parameter theta, they will perform probabilistic inference in the model based on Bayes' rule. Luckily everything is tractable and therefore they can resort to exact inference. They decide to outsource these tedious computations using RxInfer.jl and specify the following models:","category":"page"},{"location":"categories/problem_specific/universal_mixtures/#John's-model:","page":"Universal Mixtures","title":"John's model:","text":"","category":"section"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta mid textJohn) = p(theta mid textJohn) prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_john(y)\n\n    # specify John's prior model over θ\n    θ ~ Beta(7.0, 2.0)\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"categories/problem_specific/universal_mixtures/#Jane's-model:","page":"Universal Mixtures","title":"Jane's model:","text":"","category":"section"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta mid textJane) = p(theta mid textJane) prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_jane(y)\n\n    # specify Jane's prior model over θ\n    θ ~ Beta(2.0, 7.0)\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Now it is time to figure out whose prior belief was the best and who was actually right. They perform probabilistic inference automatically and compute the Bethe free energy to compare eachothers models. For acyclic models, the Bethe free energy mathrmF_B bounds the model evidence p(y) as ","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"mathrmF_Bpq geq - ln p(y)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_john = infer(\n    model = beta_model_john(), \n    data  = (y = dataset, ),\n    free_energy = true,\n)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[8.28853]","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_jane = infer(\n    model = beta_model_jane(), \n    data  = (y = dataset, ),\n    free_energy = true\n)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[8.28853]","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"From these results, they agree that Jane her gut feeling was right all along, as her Bethe free energy is lower and therefore her model evidence is higher. Nonetheless, after the 10 throws, they now have a better idea about the underlying theta parameter. They formulate this through the posterior distributions p(theta mid y textJohn) and p(theta mid y textJane):","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior beliefs\")\nplot!(rθ, (x) -> pdf(result_john.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) John\", c=1)\nplot!(rθ, (x) -> pdf(result_jane.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"p(θ|y) Jane\", c=3,)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"What John and Jane did not know, was that Mary, their neighbour, was overhearing their conversation. She was also curious, but could not see the coin. She did not really know how to formulate a prior distribution over theta, so instead she combined both John and Jane their prior beliefs. She had the feeling that John his assessment was more correct, as he was often going to the casino. As a result, she mixed the prior beliefs of John and Jane with proportions 0.7 and 0.3, respectively. Her model for the environment is specified as","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta c mid textMary) = p(c mid textMary)  p(theta mid textJohn)^c p(theta mid textJane)^1-c prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"where c describes the probability of John being correct as ","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(c mid textMary) = mathrmBer(c mid 07)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"The predictive distribution p(theta mid textMary) for theta (similar to the prior beliefs of John and Jane) she obtained from the marginalisation over c as","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textMary) = sum_cin01 p(cmidtextMary) p(theta mid textJohn)^c p(theta mid textJane)^1-c = 07 cdot p(theta mid textJohn) + 03 cdot p(theta mid textJane)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior belief\")\nplot!(rθ, (x) -> pdf(MixtureDistribution([Beta(2.0, 7.0), Beta(7.0, 2.0)], [ 0.3, 0.7 ]), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) Mary\", c=1)\nplot!(rθ, (x) -> 0.7*pdf(Beta(7.0, 2.0), x), c=3, label=\"\")\nplot!(rθ, (x) -> 0.3*pdf(Beta(2.0, 7.0), x), c=3, label=\"\")","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"She was also interested in the results and used the new Mixture node and addons in ReactiveMP.jl. She specified her model as follows and performed inference in this model:","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_mary(y)\n\n    # specify John's and Jane's prior models over θ\n    θ_jane ~ Beta(2.0, 7.0)\n    θ_john ~ Beta(7.0, 2.0)\n\n    # specify initial guess as to who is right\n    john_is_right ~ Bernoulli(0.7) \n\n    # specify mixture prior Distribution\n    θ ~ Mixture(switch = john_is_right, inputs = [θ_jane, θ_john])\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"This Mixture node updates the belief over c on the performance of the individual models of both John and Jane using so-called scale factors, as introduced in Nguyen et al.. The specific update rules for this node can be found here.","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_mary = infer(\n    model = beta_model_mary(), \n    data  = (y = dataset, ),\n    returnvars = (θ = KeepLast(), θ_john = KeepLast(), θ_jane = KeepLast(), john_is_right = KeepLast()),\n    addons = AddonLogScale(),\n    postprocess = UnpackMarginalPostprocess(),\n)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (john_is_right, θ_john, θ, θ_jane)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Mary was happy, with her mixture prior, she beat John in terms of performance. However, it was not the best decision to think that John was right. In fact, after the experiment there was only a minor possibility remaining that John was right. Her posterior distribution over theta also changed, and as expected the estimate from Jane was more prominent.","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior belief\")\nplot!(rθ, (x) -> pdf(result_mary.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) Mary\", c=1)\nplot!(rθ, (x) -> result_mary.posteriors[:θ].weights[1] * pdf(component(result_mary.posteriors[:θ], 1), x), label=\"\", c=3)\nplot!(rθ, (x) -> result_mary.posteriors[:θ].weights[2] * pdf(component(result_mary.posteriors[:θ], 2), x), label=\"\", c=3)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/universal_mixtures/Project.toml`\n  [31c24e10] Distributions v0.25.121\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/#Nonlinear-Sensor-Fusion","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"section"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"using RxInfer, Random, LinearAlgebra, Distributions, Plots, StatsPlots, Optimisers\nusing DataFrames, DelimitedFiles, StableRNGs","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In a secret ongoing mission to Mars, NASA has deployed its custom lunar roving vehicle, called WALL-E, to explore the area and to discover hidden minerals. During one of the solar storm, WALL-E's GPS unit got damaged, preventing it from accurately locating itself. The engineers at NASA were devastated as they developed the project over the past couple of years and spend most of their funding on it. Without being able to locate WALL-E, they were unable to complete their mission.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"A smart group of engineers came up with a solution to locate WALL-E. They decided to repurpose 3 nearby satelites as beacons for WALL-E, allowing it to detect its relative location to these beacons. However, these satelites were old and therefore WALL-E was only able to obtain noisy estimates of its distance to these beacons. These distances were communicated back to earth, where the engineers tried to figure our WALL-E's location. Luckily they knew the locations of these satelites and together with the noisy estimates of the distance to WALL-E they can infer the exact location of the moving WALL-E.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"To illustrate these noisy measurements, the engineers decided to plot them:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# fetch measurements\nbeacon_locations = readdlm(\"data/beacons.txt\")\ndistances = readdlm(\"data/distances.txt\")\nposition = readdlm(\"data/position.txt\")\nnr_observations = size(distances, 1);","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual location of WALL-E\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\")\n\n# plot noisy distance measurements\np2 = plot(distances, legend=:topleft, linewidth=3, label=[\"distance to beacon 1\" \"distance to beacon 2\" \"distance to beacon 3\"])\nxlabel!(\"time [sec]\"), ylabel!(\"distance [m]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In order to track the location of WALL-E based on the noisy distance measurements to the beacon, the engineers developed a probabilistic model for the movements for WALL-E and the distance measurements that followed from this. The engineers assumed that the position of WALL-E at time t, denoted by z_t, follows a 2-dimensional normal random walk:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(z_t mid z_t - 1) = mathcalN(z_t mid z_t-1mathrmI_2)\nendaligned","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"where mathrmI_2 denotes the 2-dimensional identity matrix. From the current position of WALL-E, we specify our noisy distance measurements y_t as a noisy set of the distances between WALL-E and the beacons, specified by s_i:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(y_t mid z_t)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixmathrmI_3 right  right)\nendaligned","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers are smart enough to automate the probabilistic inference procedure using RxInfer.jl. They specify the probabilistic model as:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# function to compute distance to beacons\nfunction compute_distances(z)    \n    distance1 = norm(z - beacon_locations[1,:])\n    distance2 = norm(z - beacon_locations[2,:])\n    distance3 = norm(z - beacon_locations[3,:])\n    distances = [distance1, distance2, distance3]\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model(y, W, R)\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:length(y)\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], W)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Due to non-linearity, exact probabilistic inference is intractable in this model. Therefore we resort to Conjugate-Computational Variational Inference (CVI) following the paper Probabilistic programming with stochastic variational message passing. This requires setting the @meta macro in RxInfer.jl.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Please note that we permit improper messages within the CVI procedure in this example by providing Val(false) to CVI constructor:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"    compute_distances(z) -> CVI(..., Val(false), ...)","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"This move may lead to numerical instabilities in other scenarios, however dissallowing improper messages in this case can lead to a biased estimates of posterior distribution. So, as a rule of thumb, you should try the default setting, and if it fails to find an unbiased result, enable improper messages.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_model_meta(nr_samples, nr_iterations, rng)\n    compute_distances(z) -> CVI(rng, nr_samples, nr_iterations, Optimisers.Descent(0.1), ForwardDiffGrad(), 1, Val(false), false)\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"NOTE: You can try out different meta for approximating the nonlinearity, e.g.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_linear_meta()\n    compute_distances(z) -> Linearization()\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_unscented_meta()\n    compute_distances(z) -> Unscented()\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"init = @initialization begin \n    μ(z) = MvNormalMeanPrecision(ones(2), 0.1 * diageye(2))\nend","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Initial state: \n  μ(z) = MvNormalMeanPrecision(\nμ: [1.0, 1.0]\nΛ: [0.1 0.0; 0.0 0.1]\n)","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_fast = infer(\n    model = random_walk_model(W = diageye(2), R = diageye(3)),\n    meta = random_walk_model_meta(1, 3, StableRNG(42)), # or random_walk_unscented_meta()\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initialization = init,\n);","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_accuracy = infer(\n    model = random_walk_model(W = diageye(2), R = diageye(3)),\n    meta = random_walk_model_meta(1000, 100, StableRNG(42)),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initialization = init,\n);","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"After running this fast inference procedure, the engineers plot the results and evaluate the performance:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_fast.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Fast (1 sample, 3 iterations)\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot beacon and actual and estimated location of WALL-E (accurate inference)\np2 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_accuracy.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Accurate (1000 samples, 100 iterations)\"); p2.series_list[end][:label] = \"estimated location ±2σ\"\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers were very happy with the solution, as it meant that the Mars mission could continue. However, they noted that the estimates began to deviate after WALL-E moved further away from the beacons. They deemed this was likely due to the noise in the distance measurements. Therefore, the engineers decided to adapt the model, such that they would also infer the process and observation noise precision matrices, Q and R respectively. They did this by adding Wishart priors to those matrices:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(Q) = mathcalW(Q mid 3 mathrmI_2) \n  p(R) = mathcalW(R mid 4 mathrmI_3) \n  p(z_t mid z_t - 1 Q) = mathcalN(z_t mid z_t-1 Q^-1)\n  p(y_t mid z_t R)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixR^-1 right  right)\nendaligned","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model_wishart(y)\n    # set priors on precision matrices\n    Q ~ Wishart(3, diageye(2))\n    R ~ Wishart(4, diageye(3))\n\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:length(y)\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], Q)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"meta = @meta begin \n    compute_distances(z) -> CVI(StableRNG(42), 1000, 100, Optimisers.Descent(0.01), ForwardDiffGrad(), 1, Val(false), false)\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Because of the added complexity with the Wishart distributions, the engineers simplify the problem by employing a structured mean-field factorization:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"constraints = @constraints begin\n    q(z, Q, R) = q(z)q(Q)q(R)\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"init = @initialization begin \n    μ(z) = MvNormalMeanPrecision(zeros(2), 0.01 * diageye(2))\n    q(R) = Wishart(4, diageye(3))\n    q(Q) = Wishart(3, diageye(2))\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers run the inference procedure again and decide to track the inference performance using the Bethe free energy.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_wishart = infer(\n    model = random_walk_model_wishart(),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = constraints,\n    meta = meta,\n    initialization = init,\n);","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"They plot the new estimates and the performance over time, and luckily WALL-E is found!","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_wishart.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot bethe free energy performance\np2 = plot(results_wishart.free_energy[2:end], label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/nonlinear_sensor_fusion/Project.toml`\n  [a93c6f00] DataFrames v1.8.0\n  [8bb1440f] DelimitedFiles v1.9.1\n  [31c24e10] Distributions v0.25.121\n  [3bd65402] Optimisers v0.4.6\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n  [f3b207a7] StatsPlots v0.15.8\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/#Recurrent-Switching-Linear-Dynamical-System","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"","category":"section"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"This is an experimental example of a Recurrent Switching Linear Dynamical System (RSLDS) model. The notebook requires patches to RxInfer and ReactiveMP, which are condensed in the hidden blocks below.","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"details: Hidden block of RxInfer & ReactiveMP patches and extensions - click to expand\n\nusing ExponentialFamily, RxInfer, BayesBase, GraphPPL\nimport ReactiveMP: AbstractFactorNode, NodeInterface, IndexedNodeInterface, FactorNodeActivationOptions, Marginalisation,\n Deterministic, PredefinedNodeFunctionalForm,FunctionalDependencies, collect_functional_dependencies, activate!, functional_dependencies, \n collect_latest_messages, collect_latest_marginals, marginalrule, rule, name, getinboundinterfaces, clustername, getdependecies,\n messagein, ManyOf, getvariable\nimport ExponentialFamily: getnaturalparameters, exponential_family_typetag\nexport Gate, GateNode\n\n# Mixture Functional Form\nstruct Gate{N} end\n\nReactiveMP.as_node_symbol(::Type{<:Gate}) = :Gate\nReactiveMP.interfaces(::Type{<:Gate}) = Val((:out, :switch, :inputs))\nReactiveMP.alias_interface(::Type{<:Gate}, ::Int64, name::Symbol) = name\nReactiveMP.is_predefined_node(::Type{<:Gate}) = ReactiveMP.PredefinedNodeFunctionalForm()\nReactiveMP.sdtype(::Type{<:Gate}) = ReactiveMP.Deterministic()\nReactiveMP.collect_factorisation(::Type{<:Gate}, factorization) = GateNodeFactorisation()\n\nstruct GateNodeFactorisation end\n\nstruct GateNode{N} <: ReactiveMP.AbstractFactorNode\n    out    :: ReactiveMP.NodeInterface\n    switch :: ReactiveMP.NodeInterface\n    inputs :: NTuple{N, ReactiveMP.IndexedNodeInterface}\nend \n\nReactiveMP.functionalform(factornode::GateNode{N}) where {N} = Gate{N}\nReactiveMP.getinterfaces(factornode::GateNode) = (factornode.out, factornode.switch, factornode.inputs...)\nReactiveMP.sdtype(factornode::GateNode) = ReactiveMP.Deterministic()\n\nReactiveMP.interfaceindices(factornode::GateNode, iname::Symbol)                       = (ReactiveMP.interfaceindex(factornode, iname),)\nReactiveMP.interfaceindices(factornode::GateNode, inames::NTuple{N, Symbol}) where {N} = map(iname -> ReactiveMP.interfaceindex(factornode, iname), inames)\n\nReactiveMP.interfaceindex(factornode::GateNode, iname::Symbol) = begin\n    if iname === :out\n        return 1\n    elseif iname === :switch\n        return 2\n    elseif iname === :inputs\n        return 3\n    end\nend\n\nReactiveMP.factornode(::Type{<:Gate}, interfaces, factorization) = begin\n    outinterface = interfaces[findfirst(((name, variable),) -> name == :out, interfaces)]\n    switchinterface = interfaces[findfirst(((name, variable),) -> name == :switch, interfaces)]\n    inputinterfaces = filter(((name, variable),) -> name == :inputs, interfaces)\n    N = length(inputinterfaces)\n    return GateNode(ReactiveMP.NodeInterface(outinterface...), ReactiveMP.NodeInterface(switchinterface...), ntuple(i -> ReactiveMP.IndexedNodeInterface(i, ReactiveMP.NodeInterface(inputinterfaces[i]...)), N))\n    \nend\n\nstruct GateNodeInboundInterfaces end\n\nReactiveMP.getinboundinterfaces(::GateNode) = GateNodeInboundInterfaces()\nReactiveMP.clustername(::GateNodeInboundInterfaces) = :switch_inputs\n\n\nstruct GateNodeFunctionalDependencies <: FunctionalDependencies end\n\nReactiveMP.collect_functional_dependencies(::GateNode, ::Nothing) = GateNodeFunctionalDependencies()\nReactiveMP.collect_functional_dependencies(::GateNode, ::GateNodeFunctionalDependencies) = GateNodeFunctionalDependencies()\nReactiveMP.collect_functional_dependencies(::GateNode, ::Any) =\n    error(\"The functional dependencies for GateNode must be either `Nothing` or `GateNodeFunctionalDependencies`\")\n\nReactiveMP.activate!(factornode::GateNode, options::FactorNodeActivationOptions) = begin\n    dependencies = ReactiveMP.collect_functional_dependencies(factornode, ReactiveMP.getdependecies(options))\n    return ReactiveMP.activate!(dependencies, factornode, options)\nend\n\n\nReactiveMP.functional_dependencies(::GateNodeFunctionalDependencies, factornode::GateNode{N}, interface, iindex::Int) where {N} = begin\n    message_dependencies = if iindex === 1\n        # output depends on input messages:\n        (factornode.inputs, )\n    elseif iindex === 2\n        # switch depends on:\n        (factornode.out, factornode.inputs)\n    elseif 2 < iindex <= N + 2\n        # k'th input depends on:\n        (factornode.out, )\n    else\n        error(\"Bad index in functional_dependencies for MixtureNode\")\n    end\n\n    marginal_dependencies = if iindex === 1\n        # output depends on:\n        (factornode.switch, )\n    elseif iindex === 2\n        # switch depends on:\n        ( )\n    elseif 2 < iindex <= N + 2\n        # k'th input depends on:\n        (factornode.switch,)\n    else\n        error(\"Bad index in functional_dependencies for GateNode\")\n    end\n\n    return message_dependencies, marginal_dependencies\nend\n\n\nReactiveMP.collect_latest_messages(::GateNodeFunctionalDependencies, factornode::GateNode{N}, messages::Tuple{NodeInterface}) where {N} = begin\n    outputinterface = messages[1]\n\n    msgs_names = Val{(name(outputinterface),)}()\n    msgs_observable = combineLatestUpdates((messagein(outputinterface),), PushNew())\n    return msgs_names, msgs_observable\nend\n\nReactiveMP.collect_latest_marginals(::GateNodeFunctionalDependencies, factornode::GateNode{N}, marginals::Tuple{NodeInterface}) where {N} = begin\n    switchinterface = marginals[1]\n\n    marginal_names = Val{(name(switchinterface),)}()\n    marginal_observable = combineLatestUpdates((\n        getmarginal(getvariable(switchinterface), IncludeAll()),\n    ), PushNew())\n\n    return marginal_names, marginal_observable\nend\n\nReactiveMP.collect_latest_marginals(::GateNodeFunctionalDependencies, factornode::GateNode{N}, marginals::NTuple{N,IndexedNodeInterface}) where {N} = begin\n    inputsinterfaces = marginals\n    \n    marginal_names = Val{(name(first(inputsinterfaces)),)}()\n    marginal_observable = combineLatest(map(input -> getmarginal(getvariable(input), IncludeAll()), inputsinterfaces), PushNew()) |> map_to((ManyOf(map(input -> getmarginal(getvariable(input), IncludeAll()), inputsinterfaces)),))\n    \n    return marginal_names, marginal_observable\nend\n\nReactiveMP.collect_latest_messages(::GateNodeFunctionalDependencies, factornode::GateNode{N}, messages::Tuple{NodeInterface, NTuple{N, IndexedNodeInterface}}) where {N} = begin\n    output_or_switch_interface = messages[1]\n    inputsinterfaces = messages[2]\n\n    msgs_names = Val{(name(output_or_switch_interface), name(inputsinterfaces[1]))}()\n    msgs_observable =\n        combineLatest(\n            (messagein(output_or_switch_interface), combineLatest(map(input -> messagein(input), inputsinterfaces), PushNew())),\n            PushNew()\n        ) |> map_to((\n            messagein(output_or_switch_interface), \n            ManyOf(map(input -> messagein(input), inputsinterfaces))\n        ))\n    \n    return msgs_names, msgs_observable\nend\n\nReactiveMP.collect_latest_messages(::GateNodeFunctionalDependencies, factornode::GateNode{N}, messages::Tuple{NTuple{N,IndexedNodeInterface}}) where {N} = begin\n    inputsinterfaces = messages[1]\n    \n    msgs_names = Val{(name(first(inputsinterfaces)),)}()\n    msgs_observable = combineLatest(map(input -> messagein(input), inputsinterfaces), PushNew()) |> map_to((ManyOf(map(input -> messagein(input), inputsinterfaces)),))\n    \n    return msgs_names, msgs_observable\nend\n\n\nReactiveMP.marginalrule(fform::Type{<:Gate}, on::Val{:switch_inputs}, mnames::Any, messages::Any, qnames::Nothing, marginals::Nothing, meta::Nothing, __node::Any) = begin\n    # m_out = getdata(messages[1])\n    m_switch = getdata(messages[2])\n    m_inputs = getdata.(messages[3:end])\n\n\n    return FactorizedJoint((m_inputs..., m_switch))\nend\n\nReactiveMP.@rule Gate(:out, Marginalisation) (q_switch::Any, m_inputs::ManyOf{N, Any}) where {N} = begin\n    return MixtureDistribution(collect(m_inputs), probvec(q_switch))\nend\n\n\nReactiveMP.@rule Gate(:switch, Marginalisation) (m_out::Any, m_inputs::ManyOf{N, Any}) where {N} = begin\n    logscales = map(input -> compute_logscale(prod(GenericProd(),m_out,input), m_out, input), m_inputs)\n    p = softmax(collect(logscales))\n    return Multinomial(1, p)\nend\n\n\nReactiveMP.@rule Gate((:inputs, k), Marginalisation) (m_out::Any, q_switch::Any,) = begin\n    z = probvec(q_switch)[k]\n    ef_out = convert(ExponentialFamilyDistribution, m_out)\n    η      = getnaturalparameters(ef_out)\n    ef_opt = ExponentialFamilyDistribution(exponential_family_typetag(ef_out), η * z)\n\n    return convert(Distribution, ef_opt)\nend\n\n\nReactiveMP.@rule typeof(*)(:out, Marginalisation) (m_A::PointMass, m_in::MixtureDistribution, meta::Any) = begin \n    comps = BayesBase.components(m_in)\n    new_components = similar(comps)\n    @inbounds for (i,component) in enumerate(comps)\n        new_components[i] = @call_rule typeof(*)(:out, Marginalisation) (m_A = m_A, m_in = component, meta = meta)\n    end\n    dist = MixtureDistribution(new_components, BayesBase.weights(m_in))\n    return dist\nend\n\nReactiveMP.@rule typeof(dot)(:out, Marginalisation) (m_in1::MixtureDistribution, m_in2::PointMass, meta::Any) = begin \n    comps = BayesBase.components(m_in1)\n    new_components = []\n    @inbounds for (i, component) in enumerate(comps)\n        push!(new_components, @call_rule typeof(dot)(:out, Marginalisation) (m_in1 = component, m_in2 = m_in2, meta = meta))\n    end\n    \n    mixture = MixtureDistribution(new_components, BayesBase.weights(m_in1))\n\n    return mixture\nend\n\n\n\n@rule typeof(dot)(:in1, Marginalisation) (m_out::MixtureDistribution, m_in2::PointMass, meta::Any) = begin \n    comps = BayesBase.components(m_out)\n    weights = BayesBase.weights(m_out)\n    new_comps = []\n    for (comp, weight) in zip(comps, weights)\n        new_comp = @call_rule typeof(dot)(:in1, Marginalisation) (m_out = comp, m_in2 = m_in2, meta = meta)\n        push!(new_comps, new_comp)\n    end\n    return MixtureDistribution(new_comps, weights)\nend\n\nfunction BayesBase.prod(::BayesBase.UnspecifiedProd, left::GaussianDistributionsFamily, right::MixtureDistribution)\n    comps = BayesBase.components(right)\n    weights = BayesBase.weights(right)\n    new_comps = []\n    for comp in comps\n        new_comp = prod(GenericProd(),left, comp)\n        push!(new_comps, new_comp)\n    end\n    \n    return MixtureDistribution(new_comps, weights)\nend\n\nBayesBase.prod(::BayesBase.UnspecifiedProd, left::MixtureDistribution, right::GaussianDistributionsFamily) = prod(GenericProd(),right, left)\nBayesBase.paramfloattype(::MixtureDistribution) = Float64\n\n\nimport ExponentialFamily.LogExpFunctions: logsumexp\nfunction BayesBase.prod(::GenericProd, left::Categorical, right::Multinomial)\n    @assert right.n == 1\n    right_cat = Categorical(right.p)\n\n    p = prod(GenericProd(), left, right_cat).p \n    return Multinomial(1, p)\n\nend\n\n\n\nBayesBase.prod(::GenericProd, left::Multinomial, right::Categorical) = prod(GenericProd(), right, left)\nfunction BayesBase.prod(::GenericProd, left::Multinomial, right::Multinomial) \n    @assert left.n == right.n\n\n    p = left.p .* right.p\n    p = p ./ sum(p)\n    return Multinomial(left.n, p)\nend\n\nBayesBase.prod(::BayesBase.UnspecifiedProd, left::Multinomial, right::Multinomial) = prod(GenericProd(), left, right)\n\n\nfunction BayesBase.compute_logscale(dist1::Multinomial, dist2::Multinomial, dist3::Multinomial) \n    logp1 = log.(dist1.p) - log(dist1.p[end])\n    logp2 = log.(dist2.p) - log(dist2.p[end])\n    logp3 = log.(dist3.p) - log(dist3.p[end])\n    return logsumexp(logp1) - logsumexp(logp2) - logsumexp(logp3)\nend\n\nBayesBase.compute_logscale(d1::ExponentialFamily.WishartFast, d2::ExponentialFamily.WishartFast, d3::ExponentialFamily.WishartFast) = begin\n    return logpartition(convert(ExponentialFamilyDistribution, d1)) - logpartition(convert(ExponentialFamilyDistribution, d2)) - logpartition(convert(ExponentialFamilyDistribution, d3))\nend\n\n\nExponentialFamily.probvec(d::Multinomial) = d.p\n\n@rule ContinuousTransition(:W, Marginalisation) (q_y_x::MultivariateNormalDistributionsFamily, q_a::MixtureDistribution, meta::Any) = begin \n    q_a_normal = convert(promote_variate_type(typeof(mean(q_a)), NormalMeanPrecision), mean(q_a), precision(q_a))\n    return @call_rule ContinuousTransition(:W, Marginalisation) (q_y_x = q_y_x, q_a = q_a_normal, meta = meta)\nend\n\n@rule ContinuousTransition(:y, Marginalisation) (m_x::MultivariateNormalDistributionsFamily, q_a::MixtureDistribution, q_W::Any, meta::Any) = begin \n    q_a_normal = convert(promote_variate_type(typeof(mean(q_a)), NormalMeanPrecision), mean(q_a), precision(q_a))\n    return @call_rule ContinuousTransition(:y, Marginalisation) (m_x = m_x, q_a = q_a_normal, q_W = q_W, meta = meta)\nend\n\n\n@rule ContinuousTransition(:a, Marginalisation) (q_y_x::MultivariateNormalDistributionsFamily, q_a::MixtureDistribution, q_W::Any, meta::Any) = begin \n    q_a_normal = convert(promote_variate_type(typeof(mean(q_a)), NormalMeanPrecision), mean(q_a), precision(q_a))\n    return @call_rule ContinuousTransition(:a, Marginalisation) (q_y_x = q_y_x, q_a = q_a_normal, q_W = q_W, meta = meta)\nend\n\n@rule ContinuousTransition(:x, Marginalisation) (m_y::MultivariateNormalDistributionsFamily , q_a::MixtureDistribution, q_W::Any, meta::Any) = begin \n    q_a_normal = convert(promote_variate_type(typeof(mean(q_a)), NormalMeanPrecision), mean(q_a), precision(q_a))\n    return @call_rule ContinuousTransition(:x, Marginalisation) (m_y = m_y, q_a = q_a_normal, q_W = q_W, meta = meta)\nend\n\n@rule ContinuousTransition(:y, Marginalisation) (m_x::MixtureDistribution, q_a::MultivariateNormalDistributionsFamily, q_W::Any, meta::Any) = begin \n    m_x_normal = convert(promote_variate_type(typeof(mean(m_x)), NormalMeanPrecision), mean(m_x), precision(m_x))\n    return @call_rule ContinuousTransition(:y, Marginalisation) (m_x = m_x_normal, q_a = q_a, q_W = q_W, meta = meta)\nend\n\n\n@marginalrule ContinuousTransition(:y_x) (m_y::MultivariateNormalDistributionsFamily, m_x::MixtureDistribution, q_a::MultivariateNormalDistributionsFamily, q_W::Any, meta::Any) = begin \n    m_x_normal = convert(promote_variate_type(typeof(mean(m_x)), NormalMeanPrecision), mean(m_x), precision(m_x))\n    return @call_marginalrule ContinuousTransition(:y_x) (m_y = m_y, m_x = m_x_normal, q_a = q_a, q_W = q_W, meta = meta)\nend\n\n@rule typeof(+)(:out, Marginalisation) (m_in1::MultivariateNormalDistributionsFamily, m_in2::MixtureDistribution, ) = begin \n    return @call_rule typeof(+)(:out, Marginalisation) (m_in1 = m_in1, m_in2 = convert(promote_variate_type(typeof(mean(m_in2)), NormalMeanPrecision), mean(m_in2), precision(m_in2)))\nend\n\n@rule typeof(+)(:in1, Marginalisation) (m_out::MultivariateNormalDistributionsFamily, m_in2::MixtureDistribution, ) = begin \n    return @call_rule typeof(+)(:in1, Marginalisation) (m_out = convert(promote_variate_type(typeof(mean(m_out)), NormalMeanPrecision), mean(m_out), precision(m_out)), m_in2 = convert(promote_variate_type(typeof(mean(m_in2)), NormalMeanPrecision), mean(m_in2), precision(m_in2)))\nend\n\n@rule DiscreteTransition(:out, Marginalisation) (m_in::Multinomial, q_a::DirichletCollection, ) = begin \n    @assert m_in.n == 1\n    p = probvec(m_in)\n    m_in_cat = Categorical(p)\n    return @call_rule DiscreteTransition(:out, Marginalisation) (m_in = m_in_cat, q_a = q_a)\nend\n\n@rule DiscreteTransition(:in, Marginalisation) (m_out::Multinomial, q_a::DirichletCollection, ) = begin \n    @assert m_out.n == 1\n    p = probvec(m_out)\n    m_out_cat = Categorical(p)\n    return @call_rule DiscreteTransition(:in, Marginalisation) (m_out = m_out_cat, q_a = q_a)\nend\n\n@marginalrule DiscreteTransition(:out_in) (m_out::Multinomial, m_in::Multinomial, q_a::DirichletCollection, ) = begin \n    @assert m_out.n == 1 && m_in.n == 1\n    p_out = probvec(m_out)\n    p_in = probvec(m_in)\n    m_out_cat = Categorical(p_out)\n    m_in_cat = Categorical(p_in)\n    return @call_marginalrule DiscreteTransition(:out_in) (m_out = m_out_cat, m_in = m_in_cat, q_a = q_a)\nend\n\n\nBase.length(d::MixtureDistribution) = length(d.components)\nBase.ndims(d::MixtureDistribution) = first(size(first(d.components)))\n\nExponentialFamily.probvec(d::Multinomial) = d.p\n\nBayesBase.entropy(d::MixtureDistribution) = mapreduce((c,w) -> w * BayesBase.entropy(c), +, d.components, d.weights)\n\nBayesBase.mean(f::F, itr::MixtureDistribution) where {F} = mapreduce((c,w) -> w * mean(f, c), +, itr.components, itr.weights)\n\nfunction create_P_matrix(n_switches)\n    P = zeros(n_switches, n_switches)\n    for i in 1:n_switches\n        P[i,:] = 0.5 * ones(n_switches)\n        P[i,i] = 1.0\n    end\n    return P\nend\n\nfunction BayesBase.mean(mixture::MixtureDistribution)\n    component_means = mean.(BayesBase.components(mixture))\n    component_weights = BayesBase.weights(mixture)\n    return mapreduce((m,w) -> w*m, +, component_means, component_weights)\nend\n\nfunction BayesBase.cov(mixture::MixtureDistribution)\n    component_cov = cov.(BayesBase.components(mixture))\n    component_means = mean.(BayesBase.components(mixture))\n    component_weights = BayesBase.weights(mixture)\n    mixture_mean = mean(mixture)\n    return mapreduce((v,m,w) -> w*(v + m*m'), +, component_cov, component_means, component_weights) - mixture_mean*mixture_mean'\nend\n\nBayesBase.precision(mixture::MixtureDistribution) = inv(cov(mixture))\n\nfunction BayesBase.var(mixture::MixtureDistribution)\n    component_vars = var.(BayesBase.components(mixture))\n    component_means = mean.(BayesBase.components(mixture))\n    component_weights = BayesBase.weights(mixture)\n    mixture_mean = mean(mixture)\n    return mapreduce((v,m,w) -> w*(v + m.^2), +, component_vars, component_means, component_weights) - mixture_mean.^2\nend\n","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"details: Hidden block of RSLDS Model Specification - click to expand\n\nimport ExponentialFamily: softmax\n\n\"\"\"\n    RSLDSHyperparameters{T}\n\nStructure containing hyperparameters for the Recurrent Switching Linear Dynamical System (RSLDS) model.\n\n# Fields\n- `a_w::T = 2.0`: Shape parameter for the Gamma prior on precision parameter w (when n_switches=1)\n- `b_w::T = 2.0`: Rate parameter for the Gamma prior on precision parameter w (when n_switches=1)\n- `Ψ_w::Matrix{T}`: Scale matrix for the Wishart prior on precision matrix w (when n_switches>1)\n- `Ψ_R::Union{Matrix{T}, T}`: Scale matrix/parameter for the Wishart/Gamma prior on observation precision\n- `ν_R::T`: Degrees of freedom for the Wishart prior on observation precision\n- `α::Matrix{T}`: Parameter matrix for the Dirichlet prior on transition probabilities\n- `C::Matrix{T}`: Observation matrix mapping latent states to observations\n\"\"\"\nBase.@kwdef struct RSLDSHyperparameters{T} \n   a_w::T = 2.0\n   b_w::T = 2.0\n   Ψ_w::Matrix{T}\n   Ψ_R::Union{Matrix{T}, T}\n   ν_R::T\n   α::Matrix{T} \n   C::Matrix{T}\nend\n\n\"\"\"\n    get_hyperparameters(hyperparameters::RSLDSHyperparameters)\n\nExtract all hyperparameters from the RSLDSHyperparameters structure.\n\n# Arguments\n- `hyperparameters::RSLDSHyperparameters`: Structure containing the hyperparameters\n\n# Returns\nA tuple containing all hyperparameters in the order: a_w, b_w, Ψ_w, Ψ_R, ν_R, α, C\n\"\"\"\nfunction get_hyperparameters(hyperparameters::RSLDSHyperparameters)\n    return hyperparameters.a_w, hyperparameters.b_w, hyperparameters.Ψ_w, hyperparameters.Ψ_R, hyperparameters.ν_R, hyperparameters.α, hyperparameters.C\nend\n\n\"\"\"\n    default_hyperparameters(n_switches, obs_dim, dim_latent)\n\nCreate a default set of hyperparameters for the RSLDS model.\n\n# Arguments\n- `n_switches`: Number of switching states in the model\n- `obs_dim`: Dimension of the observation space\n- `dim_latent`: Dimension of the latent state space\n\n# Returns\nAn RSLDSHyperparameters structure with default values\n\"\"\"\nfunction default_hyperparameters(n_switches, obs_dim, dim_latent)\n    return RSLDSHyperparameters(\n        a_w = 2.0,\n        b_w = 2.0,\n        Ψ_w = diageye(n_switches),\n        Ψ_R = diageye(obs_dim),\n        ν_R = obs_dim + 2.0,\n        α = ones(n_switches+1, n_switches+1),\n        C = diageye(obs_dim,dim_latent)\n    )\nend\n\n\n@model function rslds_model_learning(obs,n_obs,n_switches, dim_latent, η, Ψ, hyperparameters, learn_observation_covariance)\n    local H,A,Λ,u\n    transformation  = (x) -> reshape(x, (dim_latent, dim_latent))\n    transformation2 = (x) -> reshape(x, (n_switches, dim_latent))\n    ##Hyperparameters\n    a_w, b_w, Ψ_w, Ψ_R,ν_R, α, C = get_hyperparameters(hyperparameters)\n    ## Priors on the parameters \n    if n_switches == 1\n        w ~ GammaShapeRate(a_w, b_w)\n    else\n        w ~ Wishart(n_switches+2,Ψ_w)\n    end \n\n    if learn_observation_covariance\n        if n_obs == 1\n            R ~ GammaShapeRate(ν_R, Ψ_R)\n        else\n            R ~ Wishart(ν_R, Ψ_R) \n        end\n    else\n        R = Ψ_R\n    end\n    \n    for k in 1:n_switches+1\n        H[k] ~ MvNormalMeanCovariance(zeros(dim_latent^2), diageye(dim_latent^2))\n        Λ[k] ~ Wishart(dim_latent+2, diageye(dim_latent))\n    end\n    P ~ DirichletCollection(α)\n    ϕ ~ MvNormalMeanCovariance(zeros(dim_latent*n_switches), diageye(dim_latent*n_switches))\n    ## States Initialisation \n    x[1] ~ MvNormalMeanCovariance(zeros(dim_latent), diageye(dim_latent))\n    for t in eachindex(obs)  \n        ## Recurrent Layer\n        if n_switches == 1\n            u[t] ~ softdot(ϕ, x[t], w)\n        else\n            u[t] ~ ContinuousTransition(x[t], ϕ, w) where {meta = CTMeta(transformation2)}\n        end     \n        s[t] ~ MultinomialPolya(1, u[t]) where {dependencies = RequireMessageFunctionalDependencies(ψ = convert(promote_variate_type(typeof(η), NormalWeightedMeanPrecision), η, Ψ))}   \n        s[t+1] ~ DiscreteTransition(s[t], P)\n        ##Transition Layer\n        A[t] := Gate(switch=s[t+1], inputs=H)\n        B[t] := Gate(switch=s[t+1], inputs=Λ)\n        x[t+1] ~ ContinuousTransition(x[t], A[t], B[t]) where {meta = CTMeta(transformation)}\n        ## Observation Layer\n        obs[t] ~ MvNormalMeanPrecision(C*x[t+1], R)\n    end\nend\n\n@constraints  function rslds_learning_constraints(learn_observation_covariance)\n    if learn_observation_covariance\n        q(x,s,u,ϕ,w,P,H,A,Λ,B,R) = q(x,u)q(A)q(s)q(ϕ)q(w)q(P)q(H)q(Λ)q(B)q(R)\n    else\n        q(x,s,u,ϕ,w,P,H,A,Λ,B) = q(x,u)q(A)q(s)q(ϕ)q(w)q(P)q(H)q(Λ)q(B)\n    end\nend\n\n@initialization function rslds_learning_initmarginals(n_switches, dim_latent, obs_dim, learn_observation_covariance; rng = StableRNG(42))    \n    q(x) = vague(MvNormalWeightedMeanPrecision, dim_latent)\n    q(s) = Multinomial(1,softmax(randn(rng, n_switches+1)))\n    q(ϕ) = vague(MvNormalWeightedMeanPrecision, dim_latent*(n_switches))\n    if n_switches == 1\n        q(w) = vague(GammaShapeRate)\n    else\n        q(w) = vague(Wishart, n_switches)   \n    end\n    q(A) = vague(MvNormalWeightedMeanPrecision, dim_latent^2)\n    q(P) = DirichletCollection(ones(n_switches+1,n_switches+1))\n    q(Λ) = vague(Wishart, dim_latent)\n    q(H) = vague(MvNormalWeightedMeanPrecision, dim_latent^2)\n    q(B) = vague(Wishart, dim_latent)\n    if learn_observation_covariance\n        if obs_dim == 1\n            q(R) = vague(GammaShapeRate)\n        else\n            q(R) = Wishart(obs_dim+2, diageye(obs_dim))\n        end\n    end\nend;\n\n\n\n\"\"\"\n    fit_rslds(data, n_switches, dim_latent, n_obs; kwargs...)\n\nFit a Recurrent Switching Linear Dynamical System (RSLDS) model to the provided data.\n\n# Arguments\n- `data`: Time series observation data\n- `n_switches`: Number of switching states in the model. Note: The user provides the total number of states,\n  but internally we use (n_switches-1) because the MultinomialPolya distribution adds an extra dimension\n  to represent the recurrent influence on state transitions.\n- `dim_latent`: Dimension of the latent state space\n- `n_obs`: Dimension of the observation space\n\n# Keyword Arguments\n- `iterations::Int = 60`: Number of inference iterations\n- `η = nothing`: Mean parameter for the functional dependency in MultinomialPolya\n- `Ψ = nothing`: Precision parameter for the functional dependency in MultinomialPolya\n- `hyperparameters = nothing`: Custom hyperparameters for the model\n- `progress::Bool = false`: Whether to show progress during inference\n- `learn_observation_covariance::Bool = false`: Whether to learn the observation covariance\n\n# Returns\nThe result of the inference procedure\n\"\"\"\nfunction fit_rslds(data, n_switches, dim_latent, n_obs; iterations = 60, η = nothing, Ψ = nothing, hyperparameters = nothing, progress = false, learn_observation_covariance = false)\n    @assert n_switches > 1 \"n_switches must be greater than 1\"\n    # We subtract 1 from n_switches because the MultinomialPolya distribution\n    # internally adds an extra dimension to represent the recurrent influence\n    # on state transitions. This convention allows the model to maintain the\n    # correct dimensionality while incorporating the recurrent dynamics.\n    n_switches = n_switches - 1\n\n    if hyperparameters === nothing\n        hyperparameters = default_hyperparameters(n_switches, length(data[1]), dim_latent)\n    end\n\n    if η === nothing\n        if n_switches == 1\n            η = 0.0\n        else\n            η = zeros(n_switches)\n        end\n    end\n    if Ψ === nothing\n        if n_switches == 1\n            Ψ = 0.0001\n        else\n            Ψ = 0.0001*diageye(n_switches)\n        end\n    end\n    model = rslds_model_learning(n_obs = n_obs, n_switches = n_switches, dim_latent = dim_latent, η = η, Ψ = Ψ, hyperparameters = hyperparameters, learn_observation_covariance = learn_observation_covariance)\n    constraints = rslds_learning_constraints(learn_observation_covariance)\n    initmarginals = rslds_learning_initmarginals(n_switches, dim_latent, n_obs, learn_observation_covariance)\n    \n    return infer(model = model, data = (obs=data, ), constraints = constraints, initialization = initmarginals, iterations = iterations,\n    showprogress = progress,\n    returnvars = KeepEach(),\n    free_energy = true,\n    options = (limit_stack_depth = 100,)\n    )\nend\n\n# \n\nfunction states_to_categorical(states)\n    return [argmax(states[t].p) for t in 1:length(states)]\nend\n","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"details: Hidden block of Generating Synthetic Data - click to expand\n\nusing StableRNGs\n\nfunction generate_switching_data(T, A1, A2, c, Q, R, x_0;rng = StableRNG(42))\n    # Initialize arrays to store states and observations\n    x = zeros(2, T)  # State matrix: 2 dimensions × T timesteps\n    y = zeros(2, T)  # Observation matrix: 2 dimensions × T timesteps\n    \n    # Set initial state\n    x[:,1] = x_0\n    \n    # Generate state transitions and observations\n    for t in 2:T\n        # Switch dynamics multiple times through the sequence\n        if t < T/3 || (t >= T/2 && t < 3T/4)\n            x[:,t] = A2 * x[:,t-1] + rand(rng,MvNormal(zeros(2), Q))  # First regime\n        else\n            x[:,t] = A1 * x[:,t-1] + rand(rng,MvNormal(zeros(2), Q))  # Second regime\n        end\n        \n        # Generate observation from current state\n        y[:,t] = c * x[:,t] + rand(rng,MvNormal(zeros(2), R))\n    end\n\n    return x, y\nend\n        \n\n# System parameters\nT = 500  # Time horizon\nθ = π / 15  # Rotation angle\n\n# Define system matrices\nA1 = [cos(θ) -sin(θ); sin(θ) cos(θ)]    # Rotation matrix\nA2 = [0.4 -0.01; 0.01 0.2]         \nc = [0.6 -0.02; -0.02 0.3]                   # Observation/distortion matrix\n\n# Noise parameters\nQ = [1.0 0.0; 0.0 1.0]                   # State noise covariance\nR =  [1.0 0.0; 0.0 1.0]            # Observation noise variance\nx_0 = [0.0, 0.0]                         # Initial state vector\n\n# Generate synthetic data\nx, y = generate_switching_data(T, A1, A2, c, Q, R, x_0)\ny = [y[:,i] for i in 1:T]\nx = [x[:,i] for i in 1:T]\n","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"hyperparameters = RSLDSHyperparameters(\n    a_w = 0.01,\n    b_w = 0.01,\n    Ψ_w = 10.0*diageye(2), # n-1  \n    Ψ_R = inv(R),\n    ν_R = 4.0,\n    α = ones(2,2), # n\n    C = c\n)","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"Main.anonymous.RSLDSHyperparameters{Float64}(0.01, 0.01, [10.0 0.0; 0.0 10.\n0], [1.0 0.0; 0.0 1.0], 4.0, [1.0 1.0; 1.0 1.0], [0.6 -0.02; -0.02 0.3])","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"rslds_result = fit_rslds(y, 2, 2, 2; iterations = 150, hyperparameters = hyperparameters, progress = true)","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"Inference results:\n  Posteriors       | available for (ϕ, w, P, A, s, H, Λ, B, u, x)\n  Free Energy:     | Real[386913.0, 75580.8, 2809.27, 2010.43, 1920.47, 186\n5.65, 1893.55, 1933.31, 1972.41, 2010.39  …  1724.41, 1725.22, 1726.01, 172\n6.79, 1727.56, 1728.32, 1729.06, 1729.79, 1730.51, 1731.22]","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"using Plots","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"switching_state_posterior = rslds_result.posteriors[:s][end];\nstates = states_to_categorical(switching_state_posterior);\nscatter(states, label=\"Estimated Regimes\", color=\"blue\", linewidth=2)","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"(Image: )","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"continuous_state_posterior = rslds_result.posteriors[:x][end];\nindex = 1\nfrom = 1\nto = 500\n\nm_continuous = getindex.(mean.(continuous_state_posterior), index);\nvar_continuous = getindex.(var.(continuous_state_posterior), index);\nplot(m_continuous[from+1:to], ribbon=sqrt.(var_continuous[from+1:to]), label=\"Estimated States\", color=\"blue\",fillalpha=0.2, linewidth=2)\nplot!(getindex.(x,index)[from:to], label=\"True States\", color=\"green\", linewidth=1)\nscatter!(getindex.(y,index)[from:to], label=\"Observed Data\", color=\"black\", ms=1.3)\nlens!([10,50],[-3, 3],inset = (1, bbox(0.07, 0.6, 0.3, 0.3)), )","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"(Image: )","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"index = 2\nm_continuous = getindex.(mean.(continuous_state_posterior), index);\nvar_continuous = getindex.(var.(continuous_state_posterior), index);\nplot(m_continuous[from+1:to], ribbon=sqrt.(var_continuous[from+1:to]), label=\"Estimated States\", color=\"blue\",fillalpha=0.2, linewidth=2)\nplot!(getindex.(x,index)[from:to], label=\"True States\", color=\"green\", linewidth=1)\nscatter!(getindex.(y,index)[from:to], label=\"Observed Data\", color=\"black\", ms=1.3)\nlens!([350,400],[-3, 3],inset = (1, bbox(0.07, 0.6, 0.3, 0.3)), )","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"(Image: )","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"println(\"Estimated continuous transition matrices:\")\nprintln(\"----------------------------------------\")\nfor i in 1:length(rslds_result.posteriors[:H][end])\n    println(\"Matrix $i:\")\n    println(reshape(mean(rslds_result.posteriors[:H][end][i]), 2, 2))\n    println()\nend","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"Estimated continuous transition matrices:\n----------------------------------------\nMatrix 1:\n[0.9801827287287421 -0.20963425131252444; 0.190504731281102 0.9791493851524\n09]\n\nMatrix 2:\n[0.5040325201488747 -0.32130428525440186; 0.1078101331089597 0.535126629567\n765]","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"println(\"Estimated discrete transition matrix for HMM layer:\")\nprintln(\"----------------------------------------\")\nprintln(mean(rslds_result.posteriors[:P][end]))","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"Estimated discrete transition matrix for HMM layer:\n----------------------------------------\n[0.9866080421546886 0.00863621309969763; 0.013391957845311423 0.99136378690\n03024]","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/experimental_examples/recurrent_switching_linear_dynamical_system/Project.toml`\n  [b4ee3484] BayesBase v1.5.7\n  [62312e5e] ExponentialFamily v2.1.0\n  [b3f8163a] GraphPPL v4.6.4\n  [91a5bcdd] Plots v1.41.1\n  [a194aa59] ReactiveMP v5.6.0\n  [86711068] RxInfer v4.6.0\n  [860ef19b] StableRNGs v1.0.3\n","category":"page"},{"location":"categories/experimental_examples/recurrent_switching_linear_dynamical_system/","page":"Recurrent Switching Linear Dynamical System","title":"Recurrent Switching Linear Dynamical System","text":"","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Assessing-People’s-Skills","page":"Assessing People Skills","title":"Assessing People’s Skills","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"The goal of this demo is to demonstrate the use of the @node and @rule macros, which allow the user to define custom factor nodes and associated update rules respectively. We will introduce these macros in the context of a root cause analysis on a student's test results. This demo is inspired by Chapter 2 of \"Model-Based Machine Learning\" by Winn et al.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Problem-Statement","page":"Assessing People Skills","title":"Problem Statement","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We consider a student who takes a test that consists of three questions. Answering each question correctly requires a combination of skill and attitude. More precisely, has the student studied for the test, and have they partied the night before?","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We model the result for question i as a continuous variable r_iin01, and skill/attitude as a binary variable s_i in 0 1, where s_1 represents whether the student has partied, and s_2 and s_3 represent whether the student has studied the chapters for the corresponding questions.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We assume the following logic:","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"If the student is alert (has not partied), then they will score on the first question;\nIf the student is alert or has studied chapter two, then they will score on question two;\nIf the student can answer question two and has studied chapter three, then they will score on question three.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Generative-Model-Definition","page":"Assessing People Skills","title":"Generative Model Definition","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"To model the probability for correct answers, we assume a latent state variable t_i in 01. The dependencies between the variables can then be modeled by the following Bayesian network:","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"(s_1)   (s_2)   (s_3)\n  |       |       |\n  v       v       v\n(t_1)-->(t_2)-->(t_3)\n  |       |       |\n  v       v       v\n(r_1)   (r_2)   (r_3)","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"As prior beliefs, we assume that a student is equally likely to study/party or not: s_i sim Ber(05) for all i. Next, we model the domain logic as beginaligned   t_1 = s_1\n  t_2 = t_1  s_2\n  t_3 = t_2  s_3 endaligned For the scoring results we might not have a specific forward model in mind. However, we can define a backward mapping, from continuous results to discrete latent variables, as  t_i sim Ber(s_i) for all i.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Custom-Nodes-and-Rules","page":"Assessing People Skills","title":"Custom Nodes and Rules","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"The backward mapping from results to latents is quite specific to our application. Moreover, it does not define a proper generative forward model. In order to still define a full generative model for our application, we can define a custom Score node and define an update rule that implements the backward mapping from scores to latents as a message.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"In RxInfer, the @node macro defines a factor node. This macro accepts the new node type, an indicator for a stochastic or deterministic relationship, and a list of interfaces.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"using RxInfer, Random\n\n# Create Score node\nstruct Score end\n\n@node Score Stochastic [out, in]","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We can now define the backward mapping as a sum-product message through the @rule macro. This macro accepts the node type, the (outbound) interface on which the message is sent, any relevant constraints, and the message/distribution types on the remaining (inbound) interfaces.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"# Adding update rule for the Score node\n@rule Score(:in, Marginalisation) (q_out::PointMass,) = begin\n    return Bernoulli(mean(q_out))\nend","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Generative-Model-Specification","page":"Assessing People Skills","title":"Generative Model Specification","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We can now build the full generative model.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"# GraphPPL.jl exports the `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function skill_model(r)\n\n    local s\n    # Priors\n    for i in eachindex(r)\n        s[i] ~ Bernoulli(0.5)\n    end\n\n    # Domain logic\n    t[1] ~ ¬s[1]\n    t[2] ~ t[1] || s[2]\n    t[3] ~ t[2] && s[3]\n\n    # Results\n    for i in eachindex(r)\n        r[i] ~ Score(t[i])\n    end\nend","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Inference-Specification","page":"Assessing People Skills","title":"Inference Specification","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"Let us assume that a student scored very low on all questions and set up and execute an inference algorithm.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"test_results = [0.1, 0.1, 0.1]\ninference_result = infer(\n    model=skill_model(),\n    data=(r=test_results,)\n)","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"Inference results:\n  Posteriors       | available for (s, t)","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Results","page":"Assessing People Skills","title":"Results","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"# Inspect the results\nmap(params, inference_result.posteriors[:s])","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"3-element Vector{Tuple{Float64}}:\n (0.9872448979591837,)\n (0.06377551020408162,)\n (0.4719387755102041,)","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"These results suggest that this particular student was very likely out on the town last night.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/assessing_people_skills/Project.toml`\n  [86711068] RxInfer v4.6.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"","category":"page"},{"location":"how_to_contribute/#Contributing-to-RxInfer-Examples","page":"How to contribute","title":"Contributing to RxInfer Examples","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"We welcome contributions from the community! This guide will help you understand how to add new examples or improve existing ones in the RxInfer Examples collection. Here are the steps to follow to add a new example:","category":"page"},{"location":"how_to_contribute/#Location-and-Structure","page":"How to contribute","title":"Location and Structure","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Create a new Jupyter notebook in the appropriate category folder. Use examples/Basic Examples/ for fundamental concepts, examples/Advanced Examples/ for complex applications, or examples/Problem Specific/ for domain-specific use cases.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"note: Note\nYou can also introduce a new category by creating a new folder in the examples/ directory. In this case, you should also add a new entry in the docs/make.jl file.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Each example at the very least should have a clear, descriptive title, a meta.jl file in the same directory, a local Project.toml for dependencies, and any required data files.","category":"page"},{"location":"how_to_contribute/#Notebook-Guidelines","page":"How to contribute","title":"Notebook Guidelines","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"First Cell Requirements The first cell must be a markdown cell. It should contain ONLY the title as # <title>. The title should be descriptive and unique (avoid \"Overview\").\nEnvironment Setup The notebook will use the environment specified in the Project.toml file. Add any additional dependencies to the local project.\nContent Structure The notebook should have a clear introduction and problem description, model specification with explanations, inference procedure details, results analysis and visualization, and comprehensive comments for readability.\nSelf-Contained Code\nExamples must be fully self-contained without using include() statements\nAll code should be directly in the notebook cells\nDo not reference external Julia files\nUsers should be able to reproduce examples by simply copying and pasting from the documentation","category":"page"},{"location":"how_to_contribute/#Mathematical-Content","page":"How to contribute","title":"Mathematical Content","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"note: Note\nThe automatic rendering of equations is handled by the make.jl script and it does not understand the spaces after the $$ or $. Below are the rules for formatting equations.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Equation Formatting\nSome text\n\n$$\\begin{aligned}\n<latex equations here>\n\\end{aligned}$$\n\nSome other text\nDo not add spaces before or after the $$ or $\nEquation Rules\nNo space after opening $$ or $\nSeparate display equations with empty lines\nInline equations use single $...$, e.g. $$a + b$$ and not $$ a + b $$","category":"page"},{"location":"how_to_contribute/#Hidden/Collapsible-Code-Blocks","page":"How to contribute","title":"Hidden/Collapsible Code Blocks","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"You can hide complex or supplementary code blocks behind collapsible sections to improve readability while still making all code available to users.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Creating Hidden Blocks To create a collapsible code block, add special marker comments within your code blocks:\n### EXAMPLE_HIDDEN_BLOCK_START(Custom summary text) ###\n# This code will be hidden by default\nfunction complex_function()\n    # Implementation details\n    return result\nend\n\nnothing # to suppress the output in the notebook\n### EXAMPLE_HIDDEN_BLOCK_END ###\nImportant to note that these comments must be on the first and the last line of the code block respectively. If you want to suppress the output of the code block entirely, add nothing at the end of the code block.\nBest Practices\nUse hidden blocks for implementation details that would distract from the main tutorial flow\nProvide a descriptive summary that explains what the hidden code does\nEnsure the code within hidden blocks still runs correctly - it just gets hidden in the display\nUse for auxiliary functions, data processing, or complex implementations\nResult in Documentation The code block will be rendered as a collapsible \"details\" element with your custom summary text as the clickable header.","category":"page"},{"location":"how_to_contribute/#Visualization-and-figures","page":"How to contribute","title":"Visualization and figures","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"All plots rendered with Plots.jl should display automatically\nAsset figures can be saved in the same directory as the notebook and referenced with ![](figure-name.png)\nSpecial figures (e.g., GIFs) should be saved to generated-in-the-notebook.gif in the same directory as the notebook\nReference saved figures as markdown with ![](generated-in-the-notebook.gif) right after the cell that generated it","category":"page"},{"location":"how_to_contribute/#Metadata-Requirements","page":"How to contribute","title":"Metadata Requirements","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Create a meta.jl file in your example's directory with:","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"return (\n    title = \"Your Example Title\",\n    description = \"\"\"\n    A clear description of what the example demonstrates.\n    \"\"\",\n    tags = [\"category\", \"relevant\", \"tags\", \"here\"]\n)","category":"page"},{"location":"how_to_contribute/#Testing-Your-Example","page":"How to contribute","title":"Testing Your Example","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"note: Note\nNote that building the examples locally requires Weave.jl package to be installed globally in your Julia environment. Use julia -e 'using Pkg; Pkg.add(\"Weave\")' to install it.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Local Testing\n# Test all examples\nmake examples\n\n# Test specific example\nmake example FILTER=YourNotebookName\n\n# Render the documentation\nmake docs\n\n# Preview the documentation\nmake preview\nBuild Caching\nThe build system caches the results of example compilation. If you make changes to an example and still see old errors after rebuilding:\n# Clear all build caches and artifacts\nmake clean\n\n# Then rebuild\nmake examples\nCommon Issues\nEnsure all dependencies are in Project.toml\nVerify plots display correctly\nTest with a clean environment\nIf errors persist after fixing, run make clean to clear cached builds","category":"page"},{"location":"how_to_contribute/#Important-Notes","page":"How to contribute","title":"Important Notes","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"note: Plotting Package Preference\nPlease use Plots.jl instead of PyPlot. PyPlot's installation significantly impacts CI build times.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"warning: Documentation Generation\nEnsure your notebook renders correctly in the documentation by:Following equation formatting rules\nUsing proper cell types\nIncluding all necessary resources","category":"page"},{"location":"how_to_contribute/#Getting-Help","page":"How to contribute","title":"Getting Help","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"If you're unsure about anything:","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Check existing examples for reference\nOpen an issue for guidance\nAsk in the discussions section","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Your contributions help make RxInfer.jl better for everyone!","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/#Latent-Vector-Autoregressive-Model","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"","category":"section"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"This is an experimental example of a Latent Vector Autoregressive Model (LVAR).","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"using RxInfer, Random, LinearAlgebra","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"At first let us define auxiliary functions for priors, c and b variables","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"function generate_ar_process(order, θ, n_samples; σ²=1.0)\n    x = zeros(n_samples)\n    # Initialize with random noise\n    x[1:order] = randn(order) * sqrt(σ²)\n    \n    for t in (order+1):n_samples\n        # AR equation: x[t] = θ₁x[t-1] + θ₂x[t-2] + ... + θₚx[t-p] + ε[t]\n        x[t] = sum(θ[i] * x[t-i] for i in 1:order) + randn() * sqrt(σ²)\n    end\n    return x\nend\n\n# Set random seed for reproducibility\nRandom.seed!(42)\n\n# Define orders for each process\norders = 5 .* ones(Int, 20)\nn_samples = 120\nn_missing = 20\n\nn_ar_processes = length(orders)\nprocesses = []\n\n# Generate AR parameters and data for each process\nfor (i, order) in enumerate(orders)\n    # Generate stable AR parameters (using a simple method)\n    θ = 0.5 .^ (1:order)  # This ensures stability by having decreasing coefficients\n    \n    # Generate the AR process\n    x = generate_ar_process(order, θ, n_samples)\n    push!(processes, x)\nend\n\n# Convert to the format needed for the model\ntrue_data = [[processes[j][i] for j in 1:n_ar_processes] for i in 1:n_samples]\nobservations = Any[[true_data[i][j] .+ randn() for j in 1:n_ar_processes] for i in 1:n_samples]\n\ntraining_set = deepcopy(observations[1:n_samples-n_missing])\n\n# Extend observations with missing values\nfor i in n_samples-n_missing:n_samples\n    push!(training_set, missing)\nend","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"function form_priors(orders)\n    priors = (x = [], γ = [], θ = [])\n    for k in 1:length(orders)\n        push!(priors[:γ], GammaShapeRate(1.0, 1.0))\n        push!(priors[:x], MvNormalMeanPrecision(zeros(orders[k]), diageye(orders[k])))\n        push!(priors[:θ], MvNormalMeanPrecision(zeros(orders[k]), diageye(orders[k])))\n    end\n    return priors\nend\n\nfunction form_c_b(y, orders)\n    c = Any[]\n    b = Any[]\n    for k in 1:length(orders)\n        _c = ReactiveMP.ar_unit(Multivariate, orders[k])\n        _b = zeros(length(y[1])); _b[k] = 1.0\n        push!(c, _c)\n        push!(b, _b)\n    end\n    return c, b\nend","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"form_c_b (generic function with 1 method)","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"Next, we define a sub-model for a single AR-process","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"@model function AR_sequence(x, index, length, priors, order)\n    γ ~ priors[:γ][index]\n    θ ~ priors[:θ][index]\n    x_prev ~ priors[:x][index]\n    for i in 1:length\n        x[index, i] ~ AR(x_prev, θ, γ) where {\n            meta = ARMeta(Multivariate, order, ARsafe())\n        }\n        x_prev = x[index, i]\n    end\nend","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"Next, we define a tricky dot sequence","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"@model function dot_sequence(out, k, i, orders, x, c, b)\n    if k === length(orders)\n        out ~ b[k] * dot(c[k], x[k, i])\n    else \n        next ~ dot_sequence(k = k + 1, i = i, orders = orders, x = x, c = c, b = b)\n        out  ~ b[k] * dot(c[k], x[k, i]) + next\n    end\nend","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"And here is our final model spec","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"@model function LVAR(y, orders)\n\n    priors   = form_priors(orders)\n    c, b     = form_c_b(y, orders)\n    y_length = length(y)\n    \n    local x # `x` is being initialized in the loop within submodels\n    for k in 1:length(orders)\n        x ~ AR_sequence(index  = k, length = y_length, priors = priors, order  = orders[k])\n    end\n\n    τ ~ GammaShapeRate(1.0, 1.0)\n    for i in 1:y_length\n        μ[i] ~ dot_sequence(k = 1, i = i, orders = orders, x = x, c = c, b = b)\n        y[i] ~ MvNormalMeanScalePrecision(μ[i], τ)\n    end\nend","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"@constraints function lvar_constraints()\n    for q in AR_sequence\n        # This requires patch in GraphPPL though, see https://github.com/ReactiveBayes/GraphPPL.jl/issues/262\n        # A workaround is to use `constraints = MeanField()` in the `infer` function and initializing `q(x)` instead of `μ(x)`\n        q(x, x_prev, γ, θ) = q(x, x_prev)q(γ)q(θ)\n    end\n    q(μ, τ) = q(μ)q(τ)\nend","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"lvar_constraints (generic function with 1 method)","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"@initialization function lvar_init(orders)\n    # This is a problem still\n    for init in AR_sequence\n        q(γ) = GammaShapeRate(1.0, 1.0) \n        q(θ) = MvNormalMeanPrecision(zeros(orders[1]), diageye(orders[1])) # `orders[1]` is sad... needs to be fixed\n    end\n    q(τ) = GammaShapeRate(1.0, 1.0)\n    μ(x) = MvNormalMeanPrecision(zeros(orders[1]), diageye(orders[1]))\nend","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"lvar_init (generic function with 1 method)","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"mresult = infer(\n    model          = LVAR(orders = orders), \n    data           = (y = training_set, ), \n    constraints    = lvar_constraints(), \n    initialization = lvar_init(orders), \n    returnvars = KeepLast(), \n    options = (limit_stack_depth = 100, ),\n    showprogress = true,\n    iterations = 30,\n)","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"Inference results:\n  Posteriors       | available for (μ, τ, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"## Plot results\n\nusing Plots\n\ntheme(:default)\n\ncombined_plot = plot(layout = (3, 1), size = (600, 800), legend = :topleft)\n\n# Plotting options\nmarker_alpha = 0.7 \nmarker_size = 5  \nribbon_alpha = 0.3\nobservation_color = :green\n\n# Define the training range indices\ntrain_indices = 1:(n_samples - n_missing)\n# Extract observations for the training range\ntrain_observations = observations[train_indices]\n\n# Plot for index 5 (Subplot 1)\nindex = 5\nplot!(combined_plot[1], getindex.(mean.(mresult.predictions[:y][end]), index), ribbon = getindex.(diag.(cov.(mresult.predictions[:y][end])), index), fillalpha=ribbon_alpha, label = \"Inferred $(index)\")\nplot!(combined_plot[1], getindex.(true_data, index), label = \"True $(index)\")\n# Plot only existing observations using train_indices as x-values\nscatter!(combined_plot[1], train_indices, getindex.(train_observations, index), label = \"Observations $(index)\", marker=:xcross, markeralpha=marker_alpha, markersize=marker_size, color=observation_color)\nvline!(combined_plot[1], [n_samples-n_missing], label=\"training/test split\", linestyle=:dash, color=:black)\nplot!(combined_plot[1], title = \"LVAR $(index)\")\n\n# Plot for index 10 (Subplot 2)\nindex = 10\nplot!(combined_plot[2], getindex.(mean.(mresult.predictions[:y][end]), index), ribbon = getindex.(diag.(cov.(mresult.predictions[:y][end])), index), fillalpha=ribbon_alpha, label = \"Inferred $(index)\")\nplot!(combined_plot[2], getindex.(true_data, index), label = \"True $(index)\")\n# Plot only existing observations\nscatter!(combined_plot[2], train_indices, getindex.(train_observations, index), label = \"Observations $(index)\", marker=:xcross, markeralpha=marker_alpha, markersize=marker_size, color=observation_color)\nvline!(combined_plot[2], [n_samples-n_missing], label=\"\", linestyle=:dash, color=:black) # No label for subsequent vlines\nplot!(combined_plot[2], title = \"LVAR $(index)\")\n\n# Plot for index 20 (Subplot 3)\nindex = 15\nplot!(combined_plot[3], getindex.(mean.(mresult.predictions[:y][end]), index), ribbon = getindex.(diag.(cov.(mresult.predictions[:y][end])), index), fillalpha=ribbon_alpha, label = \"Inferred $(index)\")\nplot!(combined_plot[3], getindex.(true_data, index), label = \"True $(index)\")\n# Plot only existing observations\nscatter!(combined_plot[3], train_indices, getindex.(train_observations, index), label = \"Observations $(index)\", marker=:xcross, markeralpha=marker_alpha, markersize=marker_size, color=observation_color)\nvline!(combined_plot[3], [n_samples-n_missing], label=\"\", linestyle=:dash, color=:black) # No label for subsequent vlines\nplot!(combined_plot[3], title = \"LVAR $(index)\")\n\n# Display the combined plot\ncombined_plot","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"(Image: )","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/experimental_examples/latent_vector_autoregressive_model/Project.toml`\n  [91a5bcdd] Plots v1.41.1\n  [86711068] RxInfer v4.6.0\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/experimental_examples/latent_vector_autoregressive_model/","page":"Latent Vector Autoregressive Model","title":"Latent Vector Autoregressive Model","text":"","category":"page"}]
}
