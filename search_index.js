var documenterSearchIndex = {"docs":
[{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#Conjugate-Computational-Variational-Message-Passing-(CVI)","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"using RxInfer, Random, LinearAlgebra, Plots, Optimisers, StableRNGs, SpecialFunctions","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In this notebook, the usage of Conjugate-NonConjugate Variational Inference (CVI) will be described. The implementation of CVI follows the paper Probabilistic programming with stochastic variational message passing.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"This notebook will first describe an example in which CVI is used, then it discusses several limitations, followed by an explanation on how to extend upon CVI.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#An-example:-nonlinear-dynamical-system","page":"Conjugate-Computational Variational Message Passing","title":"An example: nonlinear dynamical system","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"A group of researchers is performing a tracking experiment on some moving object along a 1-dimensional trajectory. The object is moving at a constant velocity, meaning that its position increases constantly over time. However, the researchers do not have access to the absolute position z_t at time t. Instead they have access to the observed squared distance y_t between the object and some reference point s. Because of budget cuts, the servo moving the object and the measurement devices are quite outdated and therefore lead to noisy measurements:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# data generating process\nnr_observations = 50\nreference_point = 53\nhidden_location = collect(1:nr_observations) + rand(StableRNG(124), NormalMeanVariance(0.0, sqrt(5)), nr_observations)\nmeasurements = (hidden_location .- reference_point).^2 + rand(MersenneTwister(124), NormalMeanVariance(0.0, 5), nr_observations);","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# plot hidden location and reference frame\np1 = plot(1:nr_observations, hidden_location, linewidth=3, legend=:topleft, label=\"hidden location\")\nhline!([reference_point], linewidth=3, label=\"reference point\")\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot measurements\np2 = scatter(1:nr_observations, measurements, linewidth=3, label=\"measurements\")\nxlabel!(\"time [sec]\"), ylabel!(\"squared distance [cm2]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The researchers are interested in quantifying this noise and in tracking the unobserved location of the object. As a result of this uncertainty, the researchers employ a probabilistic modeling approach. They formulate the probabilistic model","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"beginaligned\n p(tau)  = Gamma(tau mid alpha_tau beta_tau)\n p(gamma)  = Gamma(gamma mid alpha_gamma beta_gamma)\n p(z_t mid z_t - 1 tau)  = mathcalN(z_t mid z_t - 1 + 1 tau^-1)\n p(y_t mid z_t gamma)  = mathcalN(y_t mid (z_t - s)^2 gamma^-1)\nendaligned","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"where the researchers put priors on the process and measurement noise parameters, tau and gamma, respectively. They do this, because they do not know the accuracy of their devices.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The researchers have recently heard of this cool probabilistic programming package RxInfer.jl. They decided to give it a try and create the above model as follows:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"function compute_squared_distance(z)\n    (z - reference_point)^2\nend;","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@model function measurement_model(y)\n\n    # set priors on precision parameters\n    τ ~ Gamma(shape = 0.01, rate = 0.01)\n    γ ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    # specify estimate of initial location\n    z[1] ~ Normal(mean = 0, precision = τ)\n    y[1] ~ Normal(mean = compute_squared_distance(z[1]), precision = γ)\n\n    # loop over observations\n    for t in 2:length(y)\n\n        # specify state transition model\n        z[t] ~ Normal(mean = z[t-1] + 1, precision = τ)\n\n        # specify non-linear observation model\n        y[t] ~ Normal(mean = compute_squared_distance(z[t]), precision = γ)\n        \n    end\n\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"But here is the problem, our compute_squared_distance function is already compelx enough such that the exact Bayesian inference is intractable in this model. But the researchers knew that the RxInfer.jl supports a various collection of approximation methods for exactly such cases. One of these approximations is called CVI. CVI allows us to perform probabilistic inference around the non-linear measurement function. In general, for any (non-)linear relationship y = f(x1, x2, ..., xN) CVI can be employed, by specifying the function f and by adding this relationship inside the @model macro as y ~ f(x1, x2, ...,xN). The @model macro will generate a factor node with node function p(y | x1, x2, ..., xN) = δ(y - f(x1, x2, ...,xN)).","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The use of this non-linearity requires us to specify that we would like to use CVI. This can be done by specifying the metadata using the @meta macro as:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@meta function measurement_meta(rng, nr_samples, nr_iterations, optimizer)\n    compute_squared_distance() -> CVI(rng, nr_samples, nr_iterations, optimizer)\nend;","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In general, for any (non-)linear function f(), CVI can be enabled with the @meta macro as:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@meta function model_meta(...)\n    f() -> CVI(args...)\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"See ?CVI for more information about the args....","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In our model, the z variables are connected to the non-linear node function. So in order to run probabilstic inference with CVI we need to enforce a constraint on the joint posterior distribution. Specifically, we need to create a factorization in which the variables that are directly connected to non-linearities are assumed to be independent from the rest of the variables.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In the above example, we will assume the following posterior factorization:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@constraints function measurement_constraints()\n    q(z, τ, γ) = q(z)q(τ)q(γ)\nend;","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"This constraint can be explained by the set of two constraints, one for getting CVI to run, and one for assuming a mean-field factorization around the normal node as ","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@constraints function posterior_constraints() begin\n    q(z, γ) = q(z)q(γ) # CVI\n    q(z, τ) = q(z)q(τ) # the mean-field assumption around normal node\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Because the engineers are using RxInfer.jl, they can automate the inference procedure. They track the inference performance using the Bethe free energy.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"initialization = @initialization begin\n    μ(z) = NormalMeanVariance(0, 5)\n    q(z) = NormalMeanVariance(0, 5)\n    q(τ) = GammaShapeRate(1e-12, 1e-3)\n    q(γ) = GammaShapeRate(1e-12, 1e-3)\nend\n\nresults = infer(\n    model = measurement_model(),\n    data = (y = measurements,),\n    iterations = 50,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = measurement_constraints(),\n    meta = measurement_meta(StableRNG(42), 1000, 1000, Optimisers.Descent(0.001)),\n    initialization = initialization\n)","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Inference results:\n  Posteriors       | available for (z)\n  Free Energy:     | Real[506.224, 327.426, 325.122, 320.962, 315.966, 312.\n922, 311.014, 309.621, 308.792, 308.776  …  306.958, 306.514, 306.427, 306.\n842, 306.876, 306.751, 306.631, 306.762, 306.91, 306.734]","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# plot estimates for location\np1 = plot(collect(1:nr_observations), hidden_location, label = \"hidden location\", legend=:topleft, linewidth=3, color = :red)\nplot!(map(mean, results.posteriors[:z]), label = \"estimated location (±2σ)\", ribbon = map(x -> 2*std(x), results.posteriors[:z]), fillalpha=0.5, linewidth=3, color = :orange)\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot Bethe free energy\np2 = plot(results.free_energy, linewidth=3, label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size = (1200, 500))","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#Requirements","page":"Conjugate-Computational Variational Message Passing","title":"Requirements","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"There are several main requirements for the CVI procedure to satisfy:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The out interface of the non-linearity must be independently factorized with respect to other variables in the model.\nThe messages on input interfaces (x1, x2, ..., xN) are required to be from the exponential family of distributions.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"In RxInfer, you can satisfy the first requirement by using appropriate factor nodes (Normal, Gamma, Bernoulli, etc) and second requirement by specifying the @constraints macro. In general you can specify this procedure as","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"@model function model(...)\n    ...\n    y ~ f(x1, x2, ..., xN)\n    ... ~ Node2(z1,..., y, zM) # some node that is using the out interface of the non-linearity\n    ... \nend\n\n@constraints function constraints_meta() begin\n    q(y, z1, ..., zn) = q(y)q(z1,...,zM)\n    ...\nend;\n\n@meta function model_meta(...)\n    f() -> CVI(rng, nr_samples, nr_iterations, optimizer))\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Note that not all exponential family distributions are implemented.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#Extensions","page":"Conjugate-Computational Variational Message Passing","title":"Extensions","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/#Using-a-custom-optimizer","page":"Conjugate-Computational Variational Message Passing","title":"Using a custom optimizer","text":"","category":"section"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"CVI only supports Optimisers optimizers out of the box.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Below an explanation on how to extend to it to a custom optimizer.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Suppose we have CustomDescent structure which we want to use inside CVI for optimization.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"To do so, we need to implement ReactiveMP.cvi_update!(opt::CustomDescent, λ, ∇).","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"ReactiveMP.cvi_update! incapsulates the gradient step:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"opt is used to select your optimizer structure\nλ is the current value\n∇ is a gradient value computed inside CVI.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"struct CustomDescent \n    learning_rate::Float64\nend\n\n# Must return an optimizer and its initial state\nfunction ReactiveMP.cvi_setup(opt::CustomDescent, q)\n     return (opt, nothing)\nend\n\n# Must return an updated (opt, state) and an updated λ (can use new_λ for inplace operation)\nfunction ReactiveMP.cvi_update!(opt_and_state::Tuple{CustomDescent, Nothing}, new_λ, λ, ∇)\n    opt, _ = opt_and_state\n    λ̂ = vec(λ) - (opt.learning_rate .* vec(∇))\n    copyto!(new_λ, λ̂)\n    return opt_and_state, new_λ\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Let's try to apply it to a model: beginaligned  p(x)  = mathcalN(0 1)\n p(y_imid x)  = mathcalN(y_i mid x^2 1)\nendaligned","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Let's generate some synthetic data for the model","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# generate data\ny = rand(StableRNG(123), NormalMeanVariance(19^2, 10), 1000)\nhistogram(y)","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Again we can create the corresponding model as:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"# specify non-linearity\nf(x) = x ^ 2\n\n# specify model\n@model function normal_square_model(y)\n    # describe prior on latent state, we set an arbitrary prior \n    # in a positive domain\n    x ~ Normal(mean = 5, precision = 1e-3)\n    # transform latent state\n    mean := f(x)\n    # observation model\n    y .~ Normal(mean = mean, precision = 0.1)\nend\n\n# specify meta\n@meta function normal_square_meta(rng, nr_samples, nr_iterations, optimizer)\n    f() ->  CVI(rng, nr_samples, nr_iterations, optimizer)\nend","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"normal_square_meta (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"We will use the inference function from ReactiveMP to run inference, where we provide an instance of the CustomDescent structure in our meta macro function:","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"res = infer(\n    model = normal_square_model(),\n    data = (y = y,),\n    iterations = 5,\n    free_energy = true,\n    meta = normal_square_meta(StableRNG(123), 1000, 1000, CustomDescent(0.001)),\n    free_energy_diagnostics = nothing\n)\n\nmean(res.posteriors[:x][end])","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"18.992828536095285","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"The mean inferred value of x is indeed close to 19, which was used to generate the data. Inference is working! ","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"p1 = plot(mean.(res.posteriors[:x]), ribbon = 3std.(res.posteriors[:x]), label = \"Posterior estimation\", ylim = (0, 40))\np2 = plot(res.free_energy, label = \"Bethe Free Energy\")\n\nplot(p1, p2, layout = @layout([ a b ]))","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Note: x^2 can not be inverted; the sign information can be lost: -19 and 19 are both equally good solutions.","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/conjugate-computational_variational_message_passing/","page":"Conjugate-Computational Variational Message Passing","title":"Conjugate-Computational Variational Message Passing","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/conjugate-computational_variational_message_passing/Project.toml`\n⌃ [3bd65402] Optimisers v0.3.4\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [276daf66] SpecialFunctions v2.5.0\n  [860ef19b] StableRNGs v1.0.2\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Motion-Planning-of-Robotic-Arm-in-Joint-Space","page":"Robotic Arm","title":"Motion Planning of Robotic Arm in Joint Space","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"This example demonstrates motion planning for a robotic arm using RxInfer. It's important to understand that the probabilistic inference for motion planning occurs in joint space rather than Cartesian space:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Joint Space: The space of all possible joint angles (θ₁, θ₂, θ₃, ...) of the robotic arm. Our inference model directly plans trajectories in this space, finding optimal joint angle sequences and the control torques needed to achieve them.\nCartesian Space: The 3D space (x, y, z) where the end effector operates. While our targets are specified in Cartesian space, they are translated to joint space targets using inverse kinematics before planning begins.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"This approach has several advantages:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"It directly models the physical dynamics of the arm's joints\nIt respects the arm's natural degrees of freedom\nIt allows for more efficient inference in the space where control actually happens","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"The workflow is:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Specify target positions in Cartesian space (user-friendly)\nConvert targets to joint angles using inverse kinematics\nUse RxInfer to plan optimal trajectories between joint configurations\nVisualize the resulting motion in Cartesian space using forward kinematics","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Note: These examples demonstrate the use of RxInfer for motion planning for a robotic arm. The animations show the inferred trajectories from probabilistic inference, rather than simulated executions. For more realistic simulations the model would need to be extended with a reactive environment that responds to the robotic arm's actions during plan execution. If you're interested in collaborating on a more realistic implementation, please open a discussion and let's work on it together!","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"using RxInfer, LinearAlgebra, Plots","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"The next couple of blocks are spent on defining the structures that form the foundation of our 3D robotic arm simulation. This is boring but important stuff, as we need to define the state and environment of our robotic arm before doing any inference.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Defining-Structures","page":"Robotic Arm","title":"Defining Structures","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"The structures defined below form the foundation of our 3D robotic arm simulation:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Environment: Encapsulates physical properties of the world, such as gravity, that affect the arm's dynamics. This allows us to simulate different environmental conditions.\nRoboticArm3D{N}: Represents a robotic arm with N links in 3D space. The type parameter N ensures type safety and consistency across the codebase. Properties include:\nPhysical dimensions (link lengths)\nMass distribution (important for dynamics calculations)\nTorque limits (physical constraints of the motors)\nThe parametric type allows for compile-time optimizations and type checking.\nArmState3D: Captures the complete state of the arm at any moment, including:\nJoint angles (position)\nJoint velocities (motion)\nThis state representation is crucial for both forward dynamics (predicting motion) and inverse kinematics (planning motion) that we will define later.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"\"\"\"\n    Environment(; gravitational_constant::Float64 = 9.81)\n\nStructure containing environmental properties.\n\"\"\"\nBase.@kwdef struct Environment\n    gravitational_constant::Float64 = 9.81\nend\nget_gravity(env::Environment) = env.gravitational_constant\n\n\"\"\"\nRoboticArm3D(num_links, link_lengths, link_masses, joint_torque_limits)\n\nStructure containing properties of a 3D robotic arm.\n\"\"\"\nBase.@kwdef struct RoboticArm3D{N}\n    num_links::Int64 = N                    # Number of links in the arm\n    link_lengths::Vector{Float64}           # Length of each link\n    link_masses::Vector{Float64}            # Mass of each link\n    joint_torque_limits::Vector{Float64}    # Maximum torque for each joint\n    \n    function RoboticArm3D{N}(num_links, link_lengths, link_masses, joint_torque_limits) where {N}\n        @assert num_links == N \"Number of links must match type parameter\"\n        @assert length(link_lengths) == N \"Length of link_lengths must match number of links\"\n        @assert length(link_masses) == N \"Length of link_masses must match number of links\"\n        @assert length(joint_torque_limits) == 2*N \"Length of joint_torque_limits must match 2*number of links (2 DOF per joint)\"\n        new{N}(num_links, link_lengths, link_masses, joint_torque_limits)\n    end\nend\n\n# Constructor that infers N from the number of links\nfunction RoboticArm3D(;\n    num_links::Int64,\n    link_lengths::Vector{Float64},\n    link_masses::Vector{Float64},\n    joint_torque_limits::Vector{Float64}\n)\n    RoboticArm3D{num_links}(num_links, link_lengths, link_masses, joint_torque_limits)\nend\n\nfunction get_properties(arm::RoboticArm3D{N}) where {N}\n    return (arm.num_links, arm.link_lengths, arm.link_masses, arm.joint_torque_limits)\nend\n\n\"\"\"\nArmState3D(joint_angles, joint_velocities)\n\nStructure representing the state of a 3D robotic arm.\nEach joint has 2 angles (pitch and yaw).\n\"\"\"\nstruct ArmState3D\n    joint_angles::Vector{Float64}      # Angles of each joint (2 per joint: pitch, yaw)\n    joint_velocities::Vector{Float64}  # Angular velocities of each joint\nend\n\nfunction get_state(state::ArmState3D)\n    return (state.joint_angles, state.joint_velocities)\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"get_state (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Kinematics-and-Dynamics:-Working-Together","page":"Robotic Arm","title":"Kinematics and Dynamics: Working Together","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Robotic arm control requires three complementary mathematical tools:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#1.-Forward-Kinematics","page":"Robotic Arm","title":"1. Forward Kinematics","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Purpose: Maps joint angles to end effector position in Cartesian space\nInput: Joint angles (θ₁, θ₂, θ₃, ...)\nOutput: End effector position (x, y, z)\nUse cases: Visualization, collision detection, workspace analysis\nMathematical nature: Pure geometric transformation (no physics)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#2.-Inverse-Kinematics","page":"Robotic Arm","title":"2. Inverse Kinematics","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Purpose: Maps desired end effector position to required joint angles\nInput: Target position (x, y, z)\nOutput: Joint angles (θ₁, θ₂, θ₃, ...) that achieve this position\nUse cases: Goal specification, target translation, user interface\nMathematical nature: Solving geometric equations (often multiple solutions)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#3.-State-Transition-(Dynamics)","page":"Robotic Arm","title":"3. State Transition (Dynamics)","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Purpose: Models how the arm's state evolves over time when forces/torques are applied\nInput: Current state (angles, velocities) and control inputs (torques)\nOutput: Next state after a time step\nUse cases: Realistic motion simulation, control design, trajectory optimization\nMathematical nature: Physics-based differential equations (F=ma, τ=Iα)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Why-We-Need-All-Three","page":"Robotic Arm","title":"Why We Need All Three","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"These components work together in a complete robotic arm system:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Goal Translation: Inverse kinematics translates task-space goals (x,y,z positions) into joint-space goals (angles)\nMotion Generation: State transition models how to apply torques to move between joint configurations\nFeedback: Forward kinematics verifies the actual position achieved","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"The inference process (probabilistic planning) uses these components to determine optimal control policies:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"It uses the state transition to predict how controls affect future states\nIt uses inverse kinematics to define the target joint configuration\nIt uses forward kinematics to evaluate progress toward the goal","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Without inverse kinematics, we couldn't translate Cartesian targets into joint angles. Without state transition, we couldn't model realistic physical motion with inertia, gravity, etc. Without forward kinematics, we couldn't visualize the arm or verify its position.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"\"\"\"\n    forward_kinematics_3d(arm, joint_angles)\n\nA direct geometric approach to forward kinematics for a 2-link arm.\nAngles are interpreted as:\n- joint_angles[1]: yaw angle of the first joint (rotation around Z axis)\n- joint_angles[2]: pitch angle of the first joint (rotation around new Y axis)\n- joint_angles[3]: bend angle of the second joint (in the local XZ plane)\n\"\"\"\nfunction forward_kinematics_3d(arm::RoboticArm3D{2}, joint_angles::Vector{Float64})\n    # Extract arm lengths\n    l1, l2 = arm.link_lengths\n    \n    # Extract angles\n    yaw = joint_angles[1]    # Base rotation around Z\n    pitch = joint_angles[2]  # Shoulder pitch\n    bend = joint_angles[3]   # Elbow bend\n    \n    # Initialize positions\n    positions = zeros(Float64, 3, 3)  # Base, shoulder, elbow\n    \n    # Base position\n    positions[:, 1] = [0.0, 0.0, 0.0]\n    \n    # First, calculate the shoulder position after yaw and pitch\n    # The first link points in direction [cos(yaw)*cos(pitch), sin(yaw)*cos(pitch), sin(pitch)]\n    shoulder_dir = [cos(yaw)*cos(pitch), sin(yaw)*cos(pitch), sin(pitch)]\n    positions[:, 2] = positions[:, 1] + l1 * shoulder_dir\n    \n    # For the elbow, we need to bend in the plane perpendicular to the yaw rotation\n    # Create a coordinate system at the shoulder\n    z_axis = shoulder_dir  # Direction of first link\n    y_axis = [-sin(yaw), cos(yaw), 0.0]  # Perpendicular to xz-plane\n    x_axis = cross(y_axis, z_axis)  # Complete right-handed system\n    \n    # Calculate direction of second link after bend\n    elbow_dir = cos(bend) * z_axis + sin(bend) * x_axis\n    positions[:, 3] = positions[:, 2] + l2 * elbow_dir\n    \n    return positions\nend\n\n\"\"\"\n    inverse_kinematics_3d(arm, target_position)\n\nA direct geometric inverse kinematics solver for a 2-link arm.\n\"\"\"\nfunction inverse_kinematics_3d(arm::RoboticArm3D{2}, target_position)\n    # Extract arm lengths\n    l1, l2 = arm.link_lengths\n    \n    # Extract target coordinates\n    x, y, z = target_position\n    \n    # Calculate distance to target\n    dist = norm(target_position)\n    \n    # Special case: if target is exactly at origin or too close to it\n    if dist < 0.1\n        # Return a safe default position slightly away from the origin\n        return [0.0, 0.3, 0.3, 0.0]  # Small angles that position arm in a safe configuration\n    end\n    \n    # Check if target is reachable\n    if dist > l1 + l2\n        @warn \"Target is out of reach, using closest possible solution\"\n        # Scale target to be at maximum reach\n        scale_factor = (l1 + l2 * 0.99) / dist\n        x *= scale_factor\n        y *= scale_factor\n        z *= scale_factor\n        # Recalculate distance\n        dist = norm([x, y, z])\n    elseif dist < abs(l1 - l2) + 0.05  # Added small margin to prevent numerical issues\n        @warn \"Target is too close, using closest possible solution\"\n        # Scale target to minimum reach\n        scale_factor = (abs(l1 - l2) * 1.05) / dist  # Increased margin\n        x *= scale_factor\n        y *= scale_factor\n        z *= scale_factor\n        # Recalculate distance\n        dist = norm([x, y, z])\n    end\n    \n    # Calculate yaw angle (rotation in the XY plane)\n    # Handle the case where both x and y are close to zero\n    if abs(x) < 1e-6 && abs(y) < 1e-6\n        yaw = 0.0  # Default yaw when target is directly above/below\n    else\n        yaw = atan(y, x)\n    end\n    \n    # Project the target onto the plane defined by the yaw angle\n    # This gives us the radial distance in the direction of the yaw\n    r = sqrt(x^2 + y^2)\n    \n    # Now we have a 2D problem in the RZ plane (where R is the radial distance)\n    # Apply the law of cosines to find the elbow angle\n    cos_elbow = (r^2 + z^2 - l1^2 - l2^2) / (2 * l1 * l2)\n    # Ensure the value is within valid range for acos\n    cos_elbow = clamp(cos_elbow, -1.0, 1.0)\n    elbow = acos(cos_elbow)\n    \n    # Find the angle between the first link and the line to the target\n    # Handle case where r is very small\n    if r < 1e-6\n        if z >= 0\n            # Target is directly above, point straight up\n            pitch = π/2\n        else\n            # Target is directly below, point straight down\n            pitch = -π/2\n        end\n    else\n        cos_alpha = (l1^2 + r^2 + z^2 - l2^2) / (2 * l1 * sqrt(r^2 + z^2))\n        cos_alpha = clamp(cos_alpha, -1.0, 1.0)\n        alpha = acos(cos_alpha)\n        \n        # Calculate pitch angle (elevation from XY plane)\n        # It's the sum of the angle to the target and alpha\n        pitch = atan(z, r) + alpha\n    end\n    \n    # Return the joint angles: [yaw, pitch, elbow]\n    return [yaw, pitch, elbow, 0.0]\nend\n\n\"\"\"\n    state_transition_3d(state, action, arm, environment, dt)\n\nState transition function for the 3D robotic arm, modeling the physics of motion.\n\"\"\"\nfunction state_transition_3d(state, action, arm, environment, dt)\n    # Extract state components (angles and velocities)\n    n = length(state) ÷ 2\n    θ = state[1:n]\n    ω = state[n+1:end]\n    \n    # Extract physical parameters\n    g = get_gravity(environment)\n    num_links, link_lengths, link_masses, _ = get_properties(arm)\n    \n    # Initialize next state with current values\n    θ_next = copy(θ)\n    ω_next = copy(ω)\n    \n    # Apply simple physics for each joint\n    for i in 1:n\n        # Calculate acceleration: torque = I*α, so α = torque/I\n        # Using a simplified moment of inertia model\n        joint_idx = (i + 1) ÷ 2  # Convert to link index (1-indexed)\n        moment_of_inertia = link_masses[min(joint_idx, num_links)] * (link_lengths[min(joint_idx, num_links)]^2) / 3.0\n        \n        # Net torque = control torque - friction\n        # Gravity compensation is already in the action\n        friction = 0.1 * ω[i]\n        net_torque = action[i] - friction\n        \n        # Calculate angular acceleration\n        α = net_torque / moment_of_inertia\n        \n        # Update velocity and position using basic Euler integration\n        ω_next[i] = ω[i] + α * dt\n        θ_next[i] = θ[i] + ω_next[i] * dt\n    end\n    \n    # Combine angles and velocities\n    return vcat(θ_next, ω_next)\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Main.anonymous.state_transition_3d","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Visualization-Functions","page":"Robotic Arm","title":"Visualization Functions","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"This is the most boring part, but it's necessary to visualize the arm and its motion. This is where forward kinematics and inverse kinematics become handy.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"function plot_arm_3d!(p, arm::RoboticArm3D{2}, joint_angles; color=:black)\n    # Calculate positions using the kinematics\n    positions = forward_kinematics_3d(arm, joint_angles)\n    \n    # Add a more substantial base platform\n    θ = range(0, 2π, length=30)\n    base_radius = 0.25\n    base_height = 0.05\n    \n    # Base platform - top circle\n    base_x = base_radius .* cos.(θ)\n    base_y = base_radius .* sin.(θ)\n    base_z = zeros(length(θ)) .+ base_height\n    plot!(p, base_x, base_y, base_z, linewidth=2, color=:darkgray, \n          fill=true, fillcolor=:darkgray, fillalpha=0.7, label=false)\n    \n    # Base platform - bottom circle\n    base_x_bottom = base_radius .* cos.(θ)\n    base_y_bottom = base_radius .* sin.(θ)\n    base_z_bottom = zeros(length(θ))\n    plot!(p, base_x_bottom, base_y_bottom, base_z_bottom, linewidth=2, color=:darkgray, \n          fill=true, fillcolor=:darkgray, fillalpha=0.5, label=false)\n    \n    # Connect top and bottom circles to create cylinder\n    for i in 1:length(θ)\n        plot!(p, [base_x[i], base_x[i]], [base_y[i], base_y[i]], [base_z[i], base_z_bottom[i]],\n              linewidth=1, color=:darkgray, label=false)\n    end\n    \n    # Plot each link of the arm with improved appearance\n    # Link 1: Base to shoulder - create a tapered cylinder effect\n    num_segments = 8\n    for i in 1:num_segments\n        t1 = (i-1)/num_segments\n        t2 = i/num_segments\n        \n        # Interpolate positions\n        x1 = positions[1, 1] * (1-t1) + positions[1, 2] * t1\n        y1 = positions[2, 1] * (1-t1) + positions[2, 2] * t1\n        z1 = positions[3, 1] * (1-t1) + positions[3, 2] * t1\n        \n        x2 = positions[1, 1] * (1-t2) + positions[1, 2] * t2\n        y2 = positions[2, 1] * (1-t2) + positions[2, 2] * t2\n        z2 = positions[3, 1] * (1-t2) + positions[3, 2] * t2\n        \n        # Taper the width from thick to thin\n        width1 = 10 - (i-1) * 0.5\n        width2 = 10 - i * 0.5\n        \n        # Gradient color from dark to light blue\n        color1 = RGB(0.1, 0.3 + t1*0.3, 0.6 + t1*0.3)\n        color2 = RGB(0.1, 0.3 + t2*0.3, 0.6 + t2*0.3)\n        \n        # Draw segment\n        plot!(p, [x1, x2], [y1, y2], [z1, z2],\n              linewidth=width1, color=color1, label=false,\n              seriestype=:path3d, alpha=0.9)\n    end\n    \n    # Link 2: Shoulder to end effector - create a tapered cylinder effect\n    for i in 1:num_segments\n        t1 = (i-1)/num_segments\n        t2 = i/num_segments\n        \n        # Interpolate positions\n        x1 = positions[1, 2] * (1-t1) + positions[1, 3] * t1\n        y1 = positions[2, 2] * (1-t1) + positions[2, 3] * t1\n        z1 = positions[3, 2] * (1-t1) + positions[3, 3] * t1\n        \n        x2 = positions[1, 2] * (1-t2) + positions[1, 3] * t2\n        y2 = positions[2, 2] * (1-t2) + positions[2, 3] * t2\n        z2 = positions[3, 2] * (1-t2) + positions[3, 3] * t2\n        \n        # Taper the width from thick to thin\n        width1 = 8 - (i-1) * 0.5\n        width2 = 8 - i * 0.5\n        \n        # Gradient color from medium to light blue\n        color1 = RGB(0.1, 0.4 + t1*0.4, 0.7 + t1*0.2)\n        color2 = RGB(0.1, 0.4 + t2*0.4, 0.7 + t2*0.2)\n        \n        # Draw segment\n        plot!(p, [x1, x2], [y1, y2], [z1, z2],\n              linewidth=width1, color=color1, label=false,\n              seriestype=:path3d, alpha=0.9)\n    end\n    \n    # Add joint spheres with metallic appearance\n    # Base joint\n    scatter!(p, [positions[1, 1]], [positions[2, 1]], [positions[3, 1]],\n            markersize=12, color=:darkgray, markerstrokewidth=1, \n            markerstrokecolor=:black, label=false)\n    \n    # Middle joint (shoulder) with highlight effect\n    scatter!(p, [positions[1, 2]], [positions[2, 2]], [positions[3, 2]],\n            markersize=10, color=:silver, markerstrokewidth=1, \n            markerstrokecolor=:black, label=false)\n    # Add highlight to middle joint\n    scatter!(p, [positions[1, 2] + 0.02], [positions[2, 2] + 0.02], [positions[3, 2] + 0.02],\n            markersize=3, color=:white, markerstrokewidth=0, \n            label=false)\n    \n    # Plot end effector with a more interesting shape\n    # Main part\n    scatter!(p, [positions[1, 3]], [positions[2, 3]], [positions[3, 3]],\n            markersize=12, markershape=:diamond, color=:crimson, \n            markerstrokewidth=1, markerstrokecolor=:black, label=\"End Effector\")\n    \n    # Add \"gripper\" effect to end effector\n    gripper_length = 0.1\n    gripper_angle1 = atan(positions[2, 3] - positions[2, 2], positions[1, 3] - positions[1, 2])\n    gripper_angle2 = gripper_angle1 + π/2\n    \n    # Gripper part 1\n    plot!(p, [positions[1, 3], positions[1, 3] + gripper_length * cos(gripper_angle1 + π/4)],\n          [positions[2, 3], positions[2, 3] + gripper_length * sin(gripper_angle1 + π/4)],\n          [positions[3, 3], positions[3, 3]],\n          linewidth=3, color=:crimson, label=false)\n    \n    # Gripper part 2\n    plot!(p, [positions[1, 3], positions[1, 3] + gripper_length * cos(gripper_angle1 - π/4)],\n          [positions[2, 3], positions[2, 3] + gripper_length * sin(gripper_angle1 - π/4)],\n          [positions[3, 3], positions[3, 3]],\n          linewidth=3, color=:crimson, label=false)\n    \n    return p\nend\n\nfunction visualize_arm_and_target(arm::RoboticArm3D{N}, joint_angles, target_position) where {N}\n    # Calculate positions using forward kinematics\n    positions = forward_kinematics_3d(arm, joint_angles)\n    \n    # Create plot\n    p = plot(\n        title=\"3D Robotic Arm Visualization\",\n        xlabel=\"X\", ylabel=\"Y\", zlabel=\"Z\",\n        xlim=(-2, 2), ylim=(-2, 2), zlim=(-2, 2),\n        aspect_ratio=:equal,\n        legend=:topright\n    )\n    \n    # Plot the arm\n    for i in 1:arm.num_links\n        plot!(p, [positions[1, i], positions[1, i+1]], \n              [positions[2, i], positions[2, i+1]],\n              [positions[3, i], positions[3, i+1]],\n              linewidth=3, color=:blue, label=(i==1 ? \"Arm\" : false))\n        \n        scatter!(p, [positions[1, i]], [positions[2, i]], [positions[3, i]],\n                markersize=5, color=:black, label=(i==1 ? \"Joints\" : false))\n    end\n    \n    # Plot end effector\n    scatter!(p, [positions[1, end]], [positions[2, end]], [positions[3, end]],\n            markersize=6, color=:red, label=\"End Effector\")\n    \n    # Plot target\n    scatter!(p, [target_position[1]], [target_position[2]], [target_position[3]],\n            markersize=6, markershape=:star, color=:green, label=\"Target\")\n    \n    # Plot base\n    scatter!(p, [0], [0], [0], markersize=8, color=:black, label=\"Base\")\n    \n    # Calculate error\n    error = norm(positions[:, end] - target_position)\n    annotate!(p, 0, 0, 2, text(\"Error: $(round(error, digits=4))\", 10, :black))\n    \n    return p, positions, error\nend\n\n\"\"\"\n    animate_sequential_targets_3d(arm, all_states, all_targets)\n\nAnimate the arm's motion through a sequence of targets.\n\"\"\"\nfunction animate_sequential_targets_3d(arm::RoboticArm3D{N}, all_states::Vector, all_targets::Vector) where {N}\n    num_targets = length(all_targets)\n    \n    # Combine all trajectory segments into one continuous path\n    combined_states = hcat(all_states...)\n    total_frames = size(combined_states, 2)\n    \n    # Calculate the frame indices where we reach each target\n    target_reached_frames = zeros(Int, num_targets)\n    frame_count = 0\n    for i in 1:num_targets\n        frame_count += size(all_states[i], 2)\n        target_reached_frames[i] = frame_count\n    end\n    \n    animation = @animate for k in 1:total_frames\n        # Determine which target we're currently moving towards\n        current_target_idx = 1\n        for i in 1:num_targets\n            if k <= target_reached_frames[i]\n                current_target_idx = i\n                break\n            end\n        end\n        \n        # Get the current joint angles\n        joint_angles = combined_states[:, k]\n        \n        # Calculate camera angle that slowly rotates for better 3D perception\n        camera_angle_x = 30 + 20*sin(k/total_frames*2π)\n        camera_angle_y = 20 + 10*cos(k/total_frames*2π)\n        \n        # Calculate the current end effector position using the kinematics\n        positions = forward_kinematics_3d(arm, joint_angles)\n        current_pos = positions[:, 3]\n        \n        # Calculate distance to current target\n        distance = norm(current_pos - all_targets[current_target_idx])\n        \n        # Calculate overall progress\n        overall_progress = k / total_frames\n        \n        # Create a 3D plot with improved styling\n        p = plot(\n            xlabel=\"X\", ylabel=\"Y\", zlabel=\"Z\",\n            xlim=(-2, 2), ylim=(-2, 2), zlim=(0, 2),\n            title=\"Target: $current_target_idx/$num_targets | Progress: $(round(Int, overall_progress*100))% | Distance: $(round(distance, digits=2))\",\n            legend=:topright, size=(900, 700),\n            camera=(camera_angle_x, camera_angle_y),\n            grid=false,  # Remove grid for cleaner look\n            aspect_ratio=:equal,\n            background_color=:white,\n            foreground_color=:black,\n            guidefontsize=10,\n            titlefontsize=12\n        )\n        \n        # Add a more interesting ground plane with grid pattern\n        x_grid = range(-2, 2, length=20)\n        y_grid = range(-2, 2, length=20)\n        z_grid = zeros(length(x_grid), length(y_grid))\n        surface!(p, x_grid, y_grid, z_grid, color=:aliceblue, alpha=0.2, label=false)\n        \n        # Add grid lines on the ground for better depth perception\n        for x in range(-2, 2, step=0.5)\n            plot!(p, [x, x], [-2, 2], [0.01, 0.01], color=:lightgray, linewidth=1, label=false, alpha=0.3)\n        end\n        for y in range(-2, 2, step=0.5)\n            plot!(p, [-2, 2], [y, y], [0.01, 0.01], color=:lightgray, linewidth=1, label=false, alpha=0.3)\n        end\n        \n        # Plot targets with improved styling\n        for (i, target) in enumerate(all_targets)\n            if i < current_target_idx\n                # Completed targets - we've already reached these\n                scatter!(p, [target[1]], [target[2]], [target[3]],\n                        markersize=8, color=:darkgreen, markershape=:circle, \n                        label=(i==1 ? \"Completed\" : false))\n                \n                # Add a small vertical line connecting target to ground\n                plot!(p, [target[1], target[1]], [target[2], target[2]], [0, target[3]],\n                      linewidth=1, color=:darkgreen, linestyle=:dash, alpha=0.3, label=false)\n            elseif i == current_target_idx\n                # Current target with a glowing effect\n                scatter!(p, [target[1]], [target[2]], [target[3]],\n                        markersize=12, color=:green, markershape=:star, \n                        label=\"Current\")\n                \n                # Add a pulsing effect based on frame number\n                pulse_size = 6 + 3*sin(k/10)\n                scatter!(p, [target[1]], [target[2]], [target[3]],\n                        markersize=pulse_size, color=:green, markershape=:circle, \n                        alpha=0.3, label=false)\n                \n                # Add a vertical line connecting target to ground\n                plot!(p, [target[1], target[1]], [target[2], target[2]], [0, target[3]],\n                      linewidth=1, color=:green, linestyle=:dash, alpha=0.5, label=false)\n            elseif i == current_target_idx + 1\n                # Only show the next target\n                scatter!(p, [target[1]], [target[2]], [target[3]],\n                        markersize=8, color=:gray, markershape=:star, \n                        label=\"Next\")\n                \n                # Add a faint vertical line\n                plot!(p, [target[1], target[1]], [target[2], target[2]], [0, target[3]],\n                      linewidth=1, color=:gray, linestyle=:dash, alpha=0.2, label=false)\n            end\n        end\n        \n        # Add a trail of the end effector's path\n        if k > 1\n            # Get positions from previous frames to create a trail\n            trail_length = min(k-1, 15)  # Shorter trail for less clutter\n            trail_indices = max(1, k-trail_length):k-1\n            \n            # Extract end effector positions for each frame in the trail\n            trail_positions = []\n            for idx in trail_indices\n                trail_joint_angles = combined_states[:, idx]\n                trail_pos = forward_kinematics_3d(arm, trail_joint_angles)[:, 3]\n                push!(trail_positions, trail_pos)\n            end\n            \n            # Extract coordinates for the trail\n            trail_x = [pos[1] for pos in trail_positions]\n            trail_y = [pos[2] for pos in trail_positions]\n            trail_z = [pos[3] for pos in trail_positions]\n            \n            # Plot the trail with a gradient effect\n            if length(trail_x) > 1\n                for i in 1:length(trail_x)-1\n                    # Gradient color from orange to transparent\n                    alpha_val = 0.2 + 0.7 * i / length(trail_x)\n                    plot!(p, [trail_x[i], trail_x[i+1]], \n                          [trail_y[i], trail_y[i+1]],\n                          [trail_z[i], trail_z[i+1]],\n                          linewidth=2 + i/3, color=:orange, linestyle=:solid, \n                          label=false, alpha=alpha_val)\n                end\n            end\n        end\n        \n        # For visual reference, add a shadow of the arm on the XZ plane\n        for i in 1:size(positions, 2)-1\n            plot!(p, [positions[1, i], positions[1, i+1]], \n                  [0, 0],  # Fix Y coordinate to 0 (XZ plane)\n                  [positions[3, i], positions[3, i+1]],\n                  linewidth=2, color=:gray, linestyle=:dash, \n                  label=(i==1 ? \"Shadow\" : false), opacity=0.3)\n        end\n        \n        # Plot the arm with enhanced appearance\n        plot_arm_3d!(p, arm, joint_angles)\n    end\n    \n    gif(animation, \"sequential_targets_3d.gif\", fps=15, show_msg = false)\n    return nothing\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Main.anonymous.animate_sequential_targets_3d","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Model-specification","page":"Robotic Arm","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"@model function robotic_arm_3d_model(arm, environment, initial_state, goal, horizon, dt)\n    # Extract properties\n    g = get_gravity(environment)\n    num_links, _, link_masses, _ = get_properties(arm)\n    \n    # Initial state prior\n    s[1] ~ MvNormal(mean = initial_state, covariance = 1e-5 * I)\n    \n    for i in 1:horizon\n        # Prior on torques - compensate for gravity at each joint\n        # For 3D arm: first joint (yaw) not affected by gravity, \n        # pitch joints affected based on angle\n        gravity_compensation = zeros(2*num_links)\n        for j in 1:num_links\n            if j > 1  # Skip first joint (base yaw)\n                gravity_compensation[2*j-1] = link_masses[j] * g * 0.5  # Pitch compensation\n            end\n        end\n        \n        u[i] ~ MvNormal(μ = gravity_compensation, Σ = diageye(2*num_links))\n        \n        # State transition\n        s[i + 1] ~ MvNormal(\n            μ = state_transition_3d(s[i], u[i], arm, environment, dt),\n            Σ = 1e-10 * I\n        )\n    end\n    \n    # Final state constraint\n    s[end] ~ MvNormal(mean = goal, covariance = 1e-5 * diageye(4*num_links))\nend\n\n\n@meta function robotic_arm_meta()\n    # Approximate the state transition\n    state_transition_3d() -> Unscented()\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"robotic_arm_meta (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Integration-Possibilities","page":"Robotic Arm","title":"Integration Possibilities","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"While this example keeps kinematics separate from the probabilistic model, it's theoretically possible to integrate them directly:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Embedded Forward Kinematics: The model could include forward kinematics as part of its structure, allowing direct optimization in Cartesian space\nEmbedded Inverse Kinematics: The inference process could solve inverse kinematics simultaneously with trajectory optimization","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"For example, we could define a model that directly optimizes for reaching a Cartesian target:","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"@model function direct_cartesian_model(arm, environment, initial_state, target_position, horizon, dt)\n    # Initial state prior\n    s[1] ~ MvNormal(mean = initial_state, covariance = 1e-5 * I)\n    \n    for i in 1:horizon\n        # Control priors\n        u[i] ~ MvNormal(μ = zeros(num_controls), Σ = diageye(num_controls))\n        \n        # State transition\n        s[i + 1] ~ MvNormal(\n            μ = state_transition(s[i], u[i], arm, environment, dt),\n            Σ = 1e-10 * I\n        )\n        \n        # Calculate end effector position using forward kinematics\n        ee_pos[i] := forward_kinematics(arm, s[i][1:num_joints])\n    end\n    \n    # Final position constraint directly in Cartesian space\n    ee_pos[horizon] ~ MvNormal(mean = target_position, covariance = 1e-5 * I)\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"This approach would eliminate the need for separate inverse kinematics calculations but would make the inference problem more complex. For clarity and computational efficiency, this example keeps these components separate.","category":"page"},{"location":"categories/advanced_examples/robotic_arm/#Motion-Planning","page":"Robotic Arm","title":"Motion Planning","text":"","category":"section"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"\"\"\"\n    move_to_target_3d(arm, env, start, target_position, horizon, dt)\n\nPlan motion to reach a target position in 3D space using the RxInfer model.\n\"\"\"\nfunction move_to_target_3d(arm::RoboticArm3D{N}, env::Environment, start::ArmState3D, target_position, horizon, dt) where {N}\n    # Convert ArmState3D to state vector\n    initial_state = vcat(start.joint_angles, start.joint_velocities)\n    \n    # Calculate target joint angles that would reach the target position\n    target_joint_angles = inverse_kinematics_3d(arm, target_position)\n    \n    # Create goal state (target angles and zero velocities)\n    goal_state = vcat(target_joint_angles, zeros(length(target_joint_angles)))\n        \n    # Create and run the inference using the correct API structure\n    results = infer(\n        model = robotic_arm_3d_model(\n            arm = arm,\n            environment = env,\n            horizon = horizon,\n            dt = dt\n        ),\n        data = (\n            initial_state = initial_state,\n            goal = goal_state,\n        ),\n        meta = robotic_arm_meta(),\n        returnvars = (s = KeepLast(), u = KeepLast())\n    )\n    \n    # Extract trajectories - FIXED to handle MvNormalWeightedMeanPrecision\n    states_distributions = results.posteriors[:s]\n    controls_distributions = results.posteriors[:u]\n    \n    # Extract means from the distributions\n    states = [mean(dist) for dist in states_distributions]\n    controls = [mean(dist) for dist in controls_distributions]\n    \n    # Convert to joint angles and velocities\n    n = length(states[1]) ÷ 2\n    joint_angles = [state[1:n] for state in states]\n    joint_velocities = [state[n+1:end] for state in states]\n    \n    return joint_angles, joint_velocities, controls\nend\n\n\n\n\"\"\"\n    run_3d_example()\n\nRun a complete example of 3D motion planning for a robotic arm.\n\"\"\"\nfunction run_3d_example()\n    # Create a 2-link 3D robotic arm\n    arm = RoboticArm3D{2}(\n        num_links = 2,                      # 2-link arm\n        link_lengths = [1.0, 0.8],          # Lengths of links\n        link_masses = [0.5, 0.3],           # Masses of links\n        joint_torque_limits = [5.0, 5.0, 3.0, 3.0]  # Maximum torques (2 per joint)\n    )\n    \n    # Create an environment\n    env = Environment(gravitational_constant = 9.81)\n    \n    # Define an expanded sequence of targets with more points\n    # Avoid the origin (0,0,0) which causes issues\n    targets = [\n        [1.5, 0.0, 0.3],     # Forward\n        [1.0, 1.0, 0.5],     # Forward-right and up\n        [0.0, 1.5, 0.3],     # Right\n        [-0.5, 1.0, 0.0],    # Back-right and down\n        [-1.0, 0.5, 0.8],    # Back and up\n        [-1.0, -0.5, 0.4],   # Back-left and mid-height\n        [-0.5, -1.0, 0.0],   # Back-left and down\n        [0.0, -1.5, 0.3],    # Left\n        [0.8, -0.8, 0.3],    # Forward-left\n        [0.5, 0.0, 1.5],     # Forward and up\n        [0.2, 0.2, 0.3]      # Near home position but not at origin\n    ]\n    \n    # Parameters for motion planning\n    horizon = 10   # Keep horizon at 10 as requested\n    dt = 0.1       # Time step\n    \n    # Initialize the arm state (all zeros)\n    initial_state = ArmState3D(\n        [0.0, 0.3, 0.3, 0.0],  # Start with a slight bend rather than all zeros\n        zeros(4)               # Joint velocities\n    )\n    \n    # Store the states, controls, and targets for later visualization\n    all_states = []\n    all_controls = []\n    current_state = initial_state\n    \n    # Plan motion for each target\n    for (i, target) in enumerate(targets)\n        println(\"\\nPlanning motion to target $i: $target\")\n        \n        # Plan motion to the target\n        θ_trajectory, ω_trajectory, u_trajectory = move_to_target_3d(arm, env, current_state, target, horizon, dt)\n        \n        # Combine all states into a single matrix for visualization\n        states_matrix = hcat(θ_trajectory...)\n        \n        # Update the current state for the next target\n        current_state = ArmState3D(\n            θ_trajectory[end],\n            ω_trajectory[end]\n        )\n        \n        # Store the results\n        push!(all_states, states_matrix)\n        push!(all_controls, hcat(u_trajectory...))\n    end\n    \n    # Animate the motion through all targets\n    animation = animate_sequential_targets_3d(arm, all_states, targets)\n    \n    return arm, all_states, targets, all_controls\nend","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Main.anonymous.run_3d_example","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"arm, states, targets, controls = run_3d_example();","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Planning motion to target 1: [1.5, 0.0, 0.3]\n\nPlanning motion to target 2: [1.0, 1.0, 0.5]\n\nPlanning motion to target 3: [0.0, 1.5, 0.3]\n\nPlanning motion to target 4: [-0.5, 1.0, 0.0]\n\nPlanning motion to target 5: [-1.0, 0.5, 0.8]\n\nPlanning motion to target 6: [-1.0, -0.5, 0.4]\n\nPlanning motion to target 7: [-0.5, -1.0, 0.0]\n\nPlanning motion to target 8: [0.0, -1.5, 0.3]\n\nPlanning motion to target 9: [0.8, -0.8, 0.3]\n\nPlanning motion to target 10: [0.5, 0.0, 1.5]\n\nPlanning motion to target 11: [0.2, 0.2, 0.3]","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/robotic_arm/","page":"Robotic Arm","title":"Robotic Arm","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/robotic_arm/Project.toml`\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [90137ffa] StaticArrays v1.9.13\n","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Solve-GP-regression-by-SDE","page":"Gp Regression By Ssm","title":"Solve GP regression by SDE","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"In this notebook, we solve a GP regression problem by using \"Stochastic Differential Equation\" (SDE). This method is well described in the dissertation \"Stochastic differential equation methods for spatio-temporal Gaussian process regression.\" by Arno Solin and \"Sequential Inference for Latent Temporal Gaussian Process Models\" by Jouni Hartikainen. The idea of the method is as follows.","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Suppose a function f(x) follows a zero-mean Gaussian Process beginaligned f(x) sim mathcalGP(0 k(xx)) endaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"When the dimensionality of x is 1, we can consider f(x) as a stochastic process over time, i.e. f(t). For a certain classses of covariance functions, f(t) is a solution to an m-th order linear stochastic differential equation (SDE) beginaligned a_0 f(t) + a_1 fracd f(t)dt + dots + a_m fracd^m f(t)dt^m = w(t)  endaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where w(t) is a zero-mean white noise process with spectral density Q_c. If we define a vector-valued function mathbff(t) = (f(t) ddt f(t)dots d^m-1dt^m-1f(t)), then we can rewrite the above SDE under the companion form","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nfracd mathbff(t)dt = mathbfF mathbff(t) + mathbfL w(t) quad (1)\nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where mathbfF and mathbfL are defined based on the choice of covariance functions.  From (1), we have the following state-space model: beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) quad(2a) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) quad(2b) \nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where mathbfA_k = exp(mathbfFDelta t_k), with Delta t_k = t_k+1 - t_k, is called the discrete-time state transition matrix, and mathbfQ_k the process noise covariance matrix. For the computation of mathbfQ_k, we will come back later. According to Arno Solin and Jouni Hartikainen's dissertation, the GP regression problem amounts to the inference problem of the above state-space model, and this can be solved by RTS-smoothing. The state-space model starts from  the initial state f_0 sim mathcalN(mathbf0 mathbfP_0). For stationary covariance function, the SDE has a stationary state f_infty sim mathcalN(mathbf0 mathbfP_infty), where mathbfP_infty is the solution to beginaligned fracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0 quad (mathrmLyapunov  equation) endaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"With this stationary condition, the process noise covariance mathbfQ_k is computed as follows beginaligned mathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  endaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"For one-dimensional problem the SDE representation of the GP is defined by the matrices mathbfF  mathbfL  mathbfQ_c  mathbfP_0 and mathbfH. Once we obtain all the matrices, we can do GP regression by implementing RTS-smoothing on the state-space model (2). In this notebook we will particularly use the Matern class of covariance functions for Gaussian Process.","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"using RxInfer, Random, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Create-state-space-model-for-GP-regression","page":"Gp Regression By Ssm","title":"Create state space model for GP regression","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Here we create a state-space model beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) \nendaligned where y_k is the noisy observation of the function f at time t_k, and sigma^2_noise is the noise variance and assumed to be known.","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"@model function gp_regression(y, P, A, Q, H, var_noise)\n    f_prev ~ MvNormal(μ = zeros(length(H)), Σ = P) #initial state\n    for i in eachindex(y)\n        f[i] ~ MvNormal(μ = A[i] * f_prev,Σ = Q[i])\n        y[i] ~ Normal(μ = dot(H , f[i]), var = var_noise)\n        f_prev = f[i]\n    end\nend","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Generate-data","page":"Gp Regression By Ssm","title":"Generate data","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Random.seed!(10)\nn = 100\nσ²_noise = 0.04;\nt = collect(range(-2, 2, length=n)); #timeline\nf_true = sinc.(t); # true process\nf_noisy = f_true + sqrt(σ²_noise)*randn(n); #noisy process\n\npos = sort(randperm(75)[1:2:75]); \nt_obser = t[pos]; # time where we observe data\n\ny_data = Array{Union{Float64,Missing}}(missing, n)\nfor i in pos \n    y_data[i] = f_noisy[i]\nend\n\nθ = [1., 1.]; # store [l, σ²]\nΔt = [t[1]]; # time difference\nappend!(Δt, t[2:end] - t[1:end-1]);","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Let's-visualize-our-data","page":"Gp Regression By Ssm","title":"Let's visualize our data","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"plot(t, f_true, label=\"True process f(t)\")\nscatter!(t_obser, y_data[pos], label = \"Noisy observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Covariance-function:-Matern-3/2","page":"Gp Regression By Ssm","title":"Covariance function: Matern-3/2","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"The Matern is a stationary covariance function and defined as follows beginaligned k(tau) = sigma^2 frac2^1-nuGamma(nu) left(fracsqrt2nutaul right)^nu K_nuleft(fracsqrt2nutaul right) endaligned where  beginaligned sigma^2 textthe magnitude scale hyperparameter\nl textthe characteristic length-scale\nnu textthe smoothness hyperparameter\nK_nu() textthe modified Bessel function of the second kind endaligned When we say the Matern-3/2, we mean nu=32. The matrices for the state space model are computed as follows beginaligned mathbfF = beginpmatrix 0  1\n-lambda^2  -2lambda endpmatrix quad quad mathbfL = beginpmatrix 0  1 endpmatrix quad quad mathbfP_infty = beginpmatrix sigma^2  0  0  lambda^2sigma^2 endpmatrix quad quad mathbfH = beginpmatrix 1  0 endpmatrix quad quad Q_c = 4lambda^3sigma^2 endaligned  where lambda = fracsqrt3l  From these matrices we can define mathbfA_k and mathbfQ_k.","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"λ = sqrt(3)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 1.];\nH = [1., 0.];\nF = [0. 1.; -λ^2 -2λ]\nP∞ = [θ[2] 0.; 0. (λ^2*θ[2]) ]\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"result_32 = infer(\n    model = gp_regression(P = P∞, A = A, Q = Q, H = H, var_noise = σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Inference results:\n  Posteriors       | available for (f, f_prev)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Covariance-function:-Matern-5/2","page":"Gp Regression By Ssm","title":"Covariance function: Matern-5/2","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Now let's try the Matern-5/2 kernel. The matrices for the SDE representation of the Matern-5/2 are:","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nmathbfF = beginpmatrix\n0  1  0\n0  0  1 \n-lambda^3  -3lambda^2  -3lambda\nendpmatrix quad quad mathbfL = beginpmatrix\n0  0  1\nendpmatrix quad quad mathbfH = beginpmatrix\n1  0  0\nendpmatrix quad quad Q_c = frac163 sigma^2 lambda^5 \nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where lambda = sqrt5  l. To find mathbfP_infty, we solve the Lyapunov equation","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nfracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0\nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"of which the solution is","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nvec(mathbfP_infty) = (mathbfI otimes mathbfF + mathbfFotimesmathbfI)^-1 vec(-mathbfLQ_cmathbfL^T)\nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"where vec() is the vectorization operator and otimes denotes the Kronecker product. Now we can find mathbfA_k and mathbfQ_k ","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nmathbfA_k = exp(mathbfFDelta t_k) \nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"beginaligned\nmathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  \nendaligned","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"λ = sqrt(5)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 0., 1.];\nH = [1., 0., 0.];\nF = [0. 1. 0.; 0. 0. 1.;-λ^3 -3λ^2 -3λ]\nQc = 16/3 * θ[2] * λ^5;\n\nI = diageye(3) ; \nvec_P = inv(kron(I,F) + kron(F,I)) * vec(-L * Qc * L'); \nP∞ = reshape(vec_P,3,3);\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"result_52 = infer(\n    model = gp_regression(P = P∞, A = A, Q = Q, H = H, var_noise = σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Inference results:\n  Posteriors       | available for (f, f_prev)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/#Result","page":"Gp Regression By Ssm","title":"Result","text":"","category":"section"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"slicedim(dim) = (a) -> map(e -> e[dim], a)\n\nplot(t, mean.(result_32.posteriors[:f]) |> slicedim(1), ribbon = var.(result_32.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M32\", title = \"Matern-3/2\", legend =false, lw = 2)\nplot!(t, mean.(result_52.posteriors[:f]) |> slicedim(1), ribbon = var.(result_52.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M52\",legend = :bottomleft, title = \"GPRegression by SSM\", lw = 2)\nplot!(t, f_true,label=\"true process\", lw = 2)\nscatter!(t_obser, f_noisy[pos], label=\"Observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"As we can see from the plot, both cases of Matern kernel provide good approximations (small variance) to the true process at the area with dense observations (namely from t = 0 to around 3.5), and when we move far away from this region the approximated processes become less accurate (larger variance). This result makes sense because GP regression exploits the correlation between observations to predict unobserved points, and the choice of covariance functions as well as their hyperparameters might not be optimal. We can increase the accuracy of the approximated processes by simply adding more observations. This way of improvement does not trouble the state-space method much but it might cause computational problem for naive GP regression, because with N observations the complexity of naive GP regression scales with N^3 while the state-space method scales linearly with N.     ","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/gp_regression_by_ssm/","page":"Gp Regression By Ssm","title":"Gp Regression By Ssm","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/gp_regression_by_ssm/Project.toml`\n  [31c24e10] Distributions v0.25.117\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"how_build_works/#How-the-Build-System-Works","page":"How we build the examples","title":"How the Build System Works","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"This document explains the build system for RxInfer.jl Examples. The build process involves two main scripts: examples/make.jl and docs/make.jl, each serving a different purpose.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"tip: Quick Help\nRun make help to see all available build commands and their descriptions:make help","category":"page"},{"location":"how_build_works/#Development-Options","page":"How we build the examples","title":"Development Options","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The build system supports using either the released version of RxInfer.jl or a local development version:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"# Build with released version (default)\nmake examples\n\n# Build with local development version (expects RxInfer.jl next to RxInferExamples.jl)\nmake examples-dev\n\n# Build with specific RxInfer.jl path\nmake examples-dev RXINFER=/path/to/RxInfer.jl\n\n# Build single example with development version\nmake example-dev FILTER=LinearRegression RXINFER=/path/to/RxInfer.jl","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"When using the development version (--use-dev), the build system will:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Look for RxInfer.jl in the specified location\nAdd it as a development dependency to each notebook's environment\nEnsure all notebooks use the same RxInfer version","category":"page"},{"location":"how_build_works/#Overview","page":"How we build the examples","title":"Overview","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The build process happens in two stages:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Converting notebooks to markdown (examples/make.jl)\nBuilding the documentation (docs/make.jl)","category":"page"},{"location":"how_build_works/#Stage-1:-Notebook-Processing-(examples/make.jl)","page":"How we build the examples","title":"Stage 1: Notebook Processing (examples/make.jl)","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"This script handles the conversion of Jupyter notebooks to markdown files.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The notebook processing system:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Converts .ipynb files to .md using Weave.jl\nPreserves each notebook's environment through Project.toml\nOptionally integrates development version of RxInfer.jl\nGenerates figures in the same directory as the notebook\nFixes absolute paths to use relative paths\nAdds contribution notes automatically","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"warning: Self-Contained Examples\nExamples must be self-contained and cannot use include() statements. All code must be directly in the notebook cells to ensure:Examples are reproducible by copying and pasting\nThe build system can properly process all code\nDocumentation remains consistent across different environments","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"For auxiliary file handling, the system copies all supporting files like data files while excluding Manifest.toml files. The original directory structure is maintained throughout this process.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The error handling system checks for error blocks in the output, reports any failed conversions, and provides detailed context when errors occur to help with debugging.","category":"page"},{"location":"how_build_works/#Parallel-Processing","page":"How we build the examples","title":"Parallel Processing","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"The build system leverages Julia's distributed computing capabilities to process multiple notebooks simultaneously. It distributes the workload across available CPU cores. After processing completes, it generates a detailed report showing how many notebooks were processed successfully and which ones failed, if any.","category":"page"},{"location":"how_build_works/#Stage-2:-Documentation-Building-(docs/make.jl)","page":"How we build the examples","title":"Stage 2: Documentation Building (docs/make.jl)","text":"","category":"section"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"This script builds the final documentation and performs several key functions:","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"First, it collects metadata from all examples by reading their meta.jl files. This includes gathering titles, descriptions, and tags for each example, and organizing them into appropriate categories.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Next, it generates the pages needed for the documentation site. This involves creating a comprehensive list of all examples, setting up the navigation structure between pages, and applying consistent HTML styling to the examples list.","category":"page"},{"location":"how_build_works/","page":"How we build the examples","title":"How we build the examples","text":"Finally, it handles the actual documentation building process using Documenter.jl. This includes deploying the built documentation to GitHub Pages and ensuring clean builds by removing old artifacts when needed.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Global-Parameter-Optimisation","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"This notebook demonstrates how to optimize parameters in state space models using external optimization packages, such as Optim.jl and Flux.jl. We utilize RxInfer.jl, a powerful package for inference in probabilistic models.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"By the end of this notebook, you will have practical knowledge of global parameter optimization in state space models. You will learn how to optimize parameters in both univariate and multivariate state space models, and harness the power of external optimization packages such as Optim.jl and Flux.jl.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Univariate-State-Space-Model","page":"Global Parameter Optimisation","title":"Univariate State Space Model","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Let us try use the following simple state space model:","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\n    x_t = x_t-1 + c \n    y_t sim mathcalNleft(x_t p right) \nendaligned","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Our goal is to optimize parameters c and m_x_0.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"@model function smoothing(y, x0, c, P)\n    \n    x_prior ~ Normal(mean = mean(x0), var = var(x0)) \n    x_prev = x_prior\n\n    for i in eachindex(y)\n        x[i] ~ x_prev + c\n        y[i] ~ Normal(mean = x[i], var = P)\n        x_prev = x[i]\n    end\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"rng = MersenneTwister(42)\n\nP      = 1.0\nn      = 250\nc_real = -5.0\ndata   = c_real .+ collect(1:n) + rand(rng, Normal(0.0, sqrt(P)), n);","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# c[1] is C\n# c[2] is μ0\nfunction f(c)\n    x0_prior = NormalMeanVariance(c[2], 100.0)\n    result = infer(\n        model = smoothing(x0 = x0_prior, c = c[1], P = P), \n        data  = (y = data,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"f (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using Optim","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res = optimize(f, ones(2), GradientDescent(), Optim.Options(g_tol = 1e-3, iterations = 100, store_trace = true, show_trace = true, show_every = 10))","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Iter     Function value   Gradient norm \n     0     3.792822e+02     1.799567e+03\n * time: 0.02740192413330078\n    10     3.778753e+02     3.366165e+01\n * time: 8.186949014663696\n    20     3.778724e+02     1.594998e+01\n * time: 16.071810007095337\n    30     3.778722e+02     1.421119e+00\n * time: 23.139429807662964\n    40     3.778722e+02     8.231217e-01\n * time: 29.672371864318848\n    50     3.778722e+02     5.935399e-01\n * time: 35.74333596229553\n    60     3.778722e+02     4.956050e-01\n * time: 41.67148494720459\n    70     3.778722e+02     4.052855e-01\n * time: 47.642579793930054\n    80     3.778722e+02     3.895843e-01\n * time: 53.854053020477295\n    90     3.778722e+02     3.436094e-01\n * time: 59.62544393539429\n   100     3.778722e+02     2.841252e-01\n * time: 65.09936380386353\n * Status: failure (reached maximum number of iterations)\n\n * Candidate solution\n    Final objective value:     3.778722e+02\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 5.66e-05 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.21e-05 ≰ 0.0e+00\n    |f(x) - f(x')|         = 3.10e-08 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 8.21e-11 ≰ 0.0e+00\n    |g(x)|                 = 2.84e-01 ≰ 1.0e-03\n\n * Work counters\n    Seconds run:   65  (vs limit Inf)\n    Iterations:    100\n    f(x) calls:    642\n    ∇f(x) calls:   642","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res.minimizer # Real values are indeed (c = 1.0 and μ0 = -5.0)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"2-element Vector{Float64}:\n  0.9986237539293086\n -4.66345146252166","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", [ 1.0, c_real ])\nprintln(\"Optimized: \", res.minimizer)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Real value vs Optimized\nReal:      [1.0, -5.0]\nOptimized: [0.9986237539293086, -4.66345146252166]","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Multivariate-state-space-model","page":"Global Parameter Optimisation","title":"Multivariate state space model","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Let us consider the multivariate state space model:","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\n    mathbfx_t sim mathcalNleft(mathbfAx_t-1 mathbfQ right) \n    mathbfy_t sim mathcalNleft(mathbfx_t mathbfP right) \nendaligned","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"with prior ","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nmathbfx_0 sim mathcalN(mathbfm_x_0 mathbfV_x_0)\nendaligned","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"and transition matrix ","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nmathbfA = beginbmatrix costheta  -sintheta  sintheta  costheta endbmatrix\nendaligned","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Covariance matrices mathbfV_x_0, mathbfP and mathbfQ are known. Our goal is to optimize parameters mathbfm_x_0 and theta.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"@model function rotate_ssm(y, θ, x0, Q, P)\n    \n    x_prior ~ MvNormal(mean = mean(x0), cov = cov(x0))\n    x_prev = x_prior\n    \n    A = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\n    \n    for i in eachindex(y)\n        x[i] ~ MvNormal(mean = A * x_prev, covariance = Q)\n        y[i] ~ MvNormal(mean = x[i], covariance = P)\n        x_prev = x[i]\n    end\n    \nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Generate data\nfunction generate_rotate_ssm_data()\n    rng = MersenneTwister(1234)\n\n    θ = π / 8\n    A = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\n    Q = Matrix(Diagonal(1.0 * ones(2)))\n    P = Matrix(Diagonal(1.0 * ones(2)))\n\n    n = 300\n\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        \n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(x[i], Q))\n        \n        x_prev = x[i]\n    end\n\n    return θ, A, Q, P, n, x, y\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"generate_rotate_ssm_data (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"θ, A, Q, P, n, x, y = generate_rotate_ssm_data();","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"real₁\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"real₂\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"function f(θ)\n    x0 = MvNormalMeanCovariance([ θ[2], θ[3] ], Matrix(Diagonal(0.01 * ones(2))))\n    result = infer(\n        model = rotate_ssm(θ = θ[1], x0 = x0, Q = Q, P = P), \n        data  = (y = y,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"f (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res = optimize(f, zeros(3), LBFGS(), Optim.Options(f_tol = 1e-14, g_tol = 1e-12, show_trace = true, show_every = 10))","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Iter     Function value   Gradient norm \n     0     2.989629e+03     8.134620e+03\n * time: 5.4836273193359375e-5\n    10     1.151223e+03     4.256316e-09\n * time: 17.089056968688965\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.151223e+03\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 8.06e-09 ≰ 0.0e+00\n    |x - x'|/|x'|          = 6.09e-11 ≰ 0.0e+00\n    |f(x) - f(x')|         = 2.27e-13 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 1.98e-16 ≤ 1.0e-14\n    |g(x)|                 = 4.26e-09 ≰ 1.0e-12\n\n * Work counters\n    Seconds run:   17  (vs limit Inf)\n    Iterations:    10\n    f(x) calls:    50\n    ∇f(x) calls:   50","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", θ)\nprintln(\"Optimized: \", res.minimizer[1])\n\n@show sin(θ), sin(res.minimizer[1])\n@show cos(θ), cos(res.minimizer[1])","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Real value vs Optimized\nReal:      0.39269908169872414\nOptimized: 132.32773692085084\n(sin(θ), sin(res.minimizer[1])) = (0.3826834323650898, 0.37170549491211946)\n(cos(θ), cos(res.minimizer[1])) = (0.9238795325112867, 0.9283507015412529)\n(0.9238795325112867, 0.9283507015412529)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"x0 = MvNormalMeanCovariance([ res.minimizer[2], res.minimizer[3] ], Matrix(Diagonal(100.0 * ones(2))))\n\nresult = infer(\n    model = rotate_ssm(θ = res.minimizer[1], x0 = x0, Q = Q, P = P), \n    data  = (y = y,), \n    free_energy = true\n)\n\nxmarginals = result.posteriors[:x]\n\npx = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"real₁\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"real₂\")\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"inf₁\")\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"inf₂\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Learning-Kalman-filter-with-LSTM-driven-dynamic","page":"Global Parameter Optimisation","title":"Learning Kalman filter with LSTM driven dynamic","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In this example, our focus is on Bayesian state estimation in a Nonlinear State-Space Model. Specifically, we will utilize the time series generated by the Lorenz system as an example. ","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Our objective is to compute the marginal posterior distribution of the latent (hidden) state x_k at each time step k, considering the history of measurements up to that time step:","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"p(x_k  y_1k)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"The above expression represents the probability distribution of the latent state x_k given the measurements y_1k up to time step k.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using RxInfer, BenchmarkTools, Flux, ReverseDiff, Random, Plots, LinearAlgebra, ProgressMeter, JLD, StableRNGs","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Generate-data","page":"Global Parameter Optimisation","title":"Generate data","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Lorenz system equations to be used to generate dataset\nBase.@kwdef mutable struct Lorenz\n    dt::Float64\n    σ::Float64\n    ρ::Float64\n    β::Float64\n    x::Float64\n    y::Float64\n    z::Float64\nend\n\nfunction step!(l::Lorenz)\n    dx = l.σ * (l.y - l.x);         l.x += l.dt * dx\n    dy = l.x * (l.ρ - l.z) - l.y;   l.y += l.dt * dy\n    dz = l.x * l.y - l.β * l.z;     l.z += l.dt * dz\nend\n;","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Dataset\nrng = StableRNG(999)\n\nordered_dataset = []\nordered_parameters = []\nfor σ = 11:15\n    for ρ = 23:27\n        for β_nom = 6:9\n            attractor = Lorenz(0.02, σ, ρ, β_nom/3.0, 1, 1, 1)\n            noise_free_data = [[1.0, 1.0, 1.0]]\n            for i=1:99\n                step!(attractor)\n                push!(noise_free_data, [attractor.x, attractor.y, attractor.z])\n            end\n            push!(ordered_dataset, noise_free_data)\n            push!(ordered_parameters, [σ, ρ, β_nom/3.0])\n        end\n    end\nend\n\nnew_order = collect(1:100)\nshuffle!(rng,new_order)\n\ndataset = [] # noisy dataset\nnoise_free_dataset = [] # noise free dataset\nlorenz_parameters = []\n\nfor i in new_order\n    local data = []\n    push!(noise_free_dataset, ordered_dataset[i])\n    push!(lorenz_parameters, ordered_parameters[i])\n    for nfd in ordered_dataset[i]\n        push!(data,nfd+randn(rng,3))\n    end\n    push!(dataset, data)\nend\n\ntrainset = dataset[1:60]\nvalidset = dataset[61:80]\ntestset = dataset[81:end]\n\nnoise_free_trainset = noise_free_dataset[1:60]\nnoise_free_validset = noise_free_dataset[61:80]\nnoise_free_testset = noise_free_dataset[81:end]\n;","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Data-visualization","page":"Global Parameter Optimisation","title":"Data visualization","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"one_nonoise=noise_free_trainset[1]\none=trainset[1]\ngx, gy, gz = zeros(100), zeros(100), zeros(100)\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nfor i=1:100\n    rx[i], ry[i], rz[i] = one[i][1], one[i][2], one[i][3]\n    gx[i], gy[i], gz[i] = one_nonoise[i][1], one_nonoise[i][2], one_nonoise[i][3]\nend\np1=plot(rx,ry,label=\"Noise observations\")\np1=plot!(gx,gy,label=\"True state\")\nxlabel!(\"x\")\nylabel!(\"y\")\np2=plot(rx,rz,label=\"Noise observations\")\np2=plot!(gx,gz,label=\"True state\")\nxlabel!(\"x\")\nylabel!(\"z\")\np3=plot(ry,rz,label=\"Noise observations\")\np3=plot!(gy,gz,label=\"True state\")\nxlabel!(\"y\")\nylabel!(\"z\")\nplot(p1, p2, p3, size = (800, 200),layout=(1,3))","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Inference","page":"Global Parameter Optimisation","title":"Inference","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We use the following state-space model representation:","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nx_k sim p(x_k  x_k-1) \ny_k sim p(y_k  x_k)\nendaligned","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"where x_k sim p(x_k  x_k-1) represents the hidden dynamics of our system.  The hidden dynamics of the Lorenz system exhibit nonlinearities and hence cannot be solved in the closed form. One manner of solving this problem is by introducing a neural network to approximate the transition matrix of the Lorenz system. ","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nA_k-1=NN(y_k-1) \np(x_k  x_k-1)=mathcalN(x_k  A_k-1x_k-1 Q) \np(y_k  x_k)=mathcalN(y_k  Bx_k R)\nendaligned","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"where NN is the neural network. The input is the observation y_k-1, and output is the trasition matrix A_k-1. B denote distortion or measurment matrix. Q and R are covariance matrices. Note that the hidden state x_k comprises three coordinates, i.e. x_k = (rx_k ry_k rz_k)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"By employing this state-space model representation and utilizing the neural network approximation, we can estimate the hidden dynamics and perform inference in the Lorenz system.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Neural Network model\nmutable struct NN\n    InputLayer\n    OutputLater\n    g\n    params\n    function NN(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n        InputLayer = Dense(W1, b1, relu)\n        Lstm = LSTM(W2_1,W2_2,b2,s2_1)\n        OutputLayer = Dense(W3, b3)\n        g = Chain(InputLayer, OutputLayer);\n        new(InputLayer, OutputLayer, g, (W1,b1,W2_1,W2_2,b2,s2_1,W3,b3))\n    end\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Model-specification","page":"Global Parameter Optimisation","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Note that we treat the trasition matrix A_k-1 as time-varying.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"#State Space Model\n@model function ssm(y, As, Q, B, R)\n    \n    x_prior_mean = zeros(3)\n    x_prior_cov  = Matrix(Diagonal(ones(3)))\n    \n    x[1] ~ MvNormal(mean = x_prior_mean, cov = x_prior_cov)\n    y[1] ~ MvNormal(mean = B * x[1], cov = R)\n    \n    for i in 2:length(y)\n        x[i] ~ MvNormal(mean = As[i - 1] * x[i - 1], cov = Q) \n        y[i] ~ MvNormal(mean = B * x[i], cov = R)\n    end\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We set distortion matrix B and the covariance matrices Q and R as identity matrix.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Q = Matrix(Diagonal(ones(3)))*2\nB = Matrix(Diagonal(ones(3)))\nR = Matrix(Diagonal(ones(3)))\n;","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We use the inference function in the RxInfer.jl. Before that, we need to bulid a function to get the matrix A output by the neural network. And the A is treated as a datavar in the inference function.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"function get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    n = length(data)\n    neural = NN(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    Flux.reset!(neural)\n    As  = map((d) -> Matrix(Diagonal(neural.g(d))), data[1:end-1])\n    return As\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"get_matrix_AS (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"The weights of neural network NN are initialized as follows:","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Initial model parameters\nW1, b1 = randn(5, 3)./100, randn(5)./100\nW2_1, W2_2, b2, s2_1, s2_2 = randn(5 * 4, 5) ./ 100, randn(5 * 4, 5) ./ 100, randn(5*4) ./ 100, zeros(5), zeros(5)\nW3, b3 = randn(3, 5) ./ 100, randn(3) ./ 100\n;","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Before network training, we show the inference results for the hidden states:","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Performance on an instance from the testset before training\nindex = 1\ndata=testset[index]\nn=length(data)\nresult = infer(\n    model = ssm(As = get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q = Q,B = B,R = R), \n    data  = (y = data, ), \n    returnvars = (x = KeepLast(), ),\n    free_energy = true\n)\nx_est=result.posteriors[:x]\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\n\np1 = plot(rx,label=\"Hidden state rx\")\np1 = plot!(rx_est_m,label=\"Inferred states\", ribbon=rx_est_var)\np1 = scatter!(first.(testset[index]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Hidden state ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Hidden state rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index]), label=\"Observations\", markersize=1.0)\n\n\nplot(p1, p2, p3, size = (1000, 300))","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Training-network","page":"Global Parameter Optimisation","title":"Training network","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In this part, we use the Free Energy as the objective function to optimize the weights of network.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# free energy objective to be optimized during training\nfunction fe_tot_est(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    fe_ = 0\n    for train_instance in trainset\n        result = infer(\n            model = ssm(n, get_matrix_AS(train_instance,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q,B,R), \n            data  = (y = train_instance, ), \n            returnvars = (x = KeepLast(), ),\n            free_energy = true\n        )\n        fe_ += result.free_energy[end]\n    end\n    return fe_\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"fe_tot_est (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Training","page":"Global Parameter Optimisation","title":"Training","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Training is a computationally expensive procedure, for the sake of an example we load pre-trained weights\n# Uncomment the following code to train the network manually\n# opt = Flux.Optimise.RMSProp(0.006, 0.95)\n# params = (W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n# @showprogress for epoch in 1:800\n#     grads = ReverseDiff.gradient(fe_tot_est, params);\n#     for i=1:length(params)\n#         Flux.Optimise.update!(opt,params[i],grads[i])\n#     end\n# end","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Test","page":"Global Parameter Optimisation","title":"Test","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Import the weights of neural network that we have trained.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"W1a, b1a, W2_1a, W2_2a, b2a, s2_1a, W3, b3a = load(\"nn_prediction/weights.jld\")[\"data\"];","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Performance on an instance from the testset after training\nindex = 1\ndata = testset[index]\nn = length(data)\nresult = infer(\n    model = ssm(As=get_matrix_AS(data,W1a,b1a,W2_1a,W2_2a,b2a,s2_1a,W3,b3a),Q=Q,B=B,R=R), \n    data  = (y = data, ), \n    returnvars = (x = KeepLast(), ),\n    free_energy = true\n)\nx_est=result.posteriors[:x]\n\ngx, gy, gz = zeros(100), zeros(100), zeros(100)\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    gx[i], gy[i], gz[i] = noise_free_testset[index][i][1], noise_free_testset[index][i][2], noise_free_testset[index][i][3]\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\n\np1 = plot(rx,label=\"Hidden state rx\")\np1 = plot!(rx_est_m,label=\"Inferred states\", ribbon=rx_est_var)\np1 = scatter!(first.(testset[index]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Hidden state ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Hidden state rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index]), label=\"Observations\", markersize=1.0)\n\nplot(p1, p2, p3, size = (1000, 300))","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/#Prediction","page":"Global Parameter Optimisation","title":"Prediction","text":"","category":"section"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In the above instances, the observations during whole time are available. For prediction task, we can only access to the  observations untill k and estimate the future state at time k+1, k+2, dots,k+T.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We can still solve this problem by the trained neural network to approximate the transition matrix. And we can get the one-step prediction in the future. Then, the predicted results are feed into the neural network to generate the transition matrix for the next step, and roll into the future to get the multi-step prediction.","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nA_k=NN(x_k) \np(x_k+1  x_k)=mathcalN(x_k+1  A_kx_k Q) \nendaligned","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"#Define the prediction function\nmultiplyGaussian(A,m,V) = (A * m, A * V * transpose(A))\nsumGaussians(m1,m2,V1,V2) = (m1 + m2, V1 + V2)\n\nfunction runForward(A,B,Q,R,mh_old,Vh_old)\n    mh_1, Vh_1 = multiplyGaussian(A,mh_old,Vh_old)\n    mh_pred, Vh_pred = sumGaussians(mh_1, zeros(length(mh_old)), Vh_1, Q)\nend\n\nfunction g_predict(mh_old,Vh_old,Q)\n    neural = NN(W1a,b1a,W2_1a,W2_2a,b2a,s2_1a,W3,b3a)\n    # Flux.reset!(neural)\n    As  = map((d) -> Matrix(Diagonal(neural.g(d))), [mh_old])\n    As = As[1]\n    return runForward(As,B,Q,R,mh_old,Vh_old), As\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"g_predict (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"After k=75, the observations are not available, and we predict the future state from k=76 to the end","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"tt = 75\nmh = mean(x_est[tt])\nVh = cov(x_est[tt])\nmo_list, Vo_list, A_list = [], [], [] \ninv_Q = inv(Q)\nfor t=1:100-tt\n    (mo, Vo), A_t = g_predict(mh,Vh,inv_Q)\n    push!(mo_list, mo)\n    push!(Vo_list, Vh)\n    push!(A_list, A_t)\n    global mh = mo\n    global Vh = Vo\nend","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Prediction visualization\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\nfor i=1:tt\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\nfor i=tt+1:100\n    ii=i-tt\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mo_list[ii][1], mo_list[ii][2], mo_list[ii][3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = Vo_list[ii][1,1], Vo_list[ii][2,2], Vo_list[ii][3,3]\nend\np1 = plot(rx,label=\"Ground truth rx\")\np1 = plot!(rx_est_m,label=\"Inffered state rx\",ribbon=rx_est_var)\np1 = scatter!(first.(testset[index][1:tt]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Ground truth ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index][1:tt], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Ground truth rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index][1:tt]), label=\"Observations\", markersize=1.0)\n\n\nplot(p1, p2, p3, size = (1000, 300),legend=:bottomleft)","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/global_parameter_optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/global_parameter_optimisation/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n⌃ [587475ba] Flux v0.14.25\n  [4138dd39] JLD v0.13.5\n  [429524aa] Optim v1.11.0\n  [91a5bcdd] Plots v1.40.9\n  [92933f4c] ProgressMeter v1.10.2\n  [37e2e3b7] ReverseDiff v1.15.3\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"","category":"page"},{"location":"categories/basic_examples/coin_toss_model/#Coin-toss-model-(Beta-Bernoulli)","page":"Coin Toss Model","title":"Coin toss model (Beta-Bernoulli)","text":"","category":"section"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"In this example, we are going to perform an exact inference for a coin-toss model that can be represented as:","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"beginaligned\np(theta) = mathrmBeta(thetaa b)\np(y_itheta) = mathrmBernoulli(y_itheta)\nendaligned","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"where y_i in 0 1 is a binary observation induced by Bernoulli likelihood while theta is a Beta prior distribution on the parameter of Bernoulli. We are interested in inferring the posterior distribution of theta.","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"We start with importing all needed packages:","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"using RxInfer, Random","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Let's generate some synthetic observations using Bernoulli distribution for a biased coin-tosses that are independent and identically distributed (IID).","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"rng = MersenneTwister(42)\nn = 500\nθ_real = 0.75\ndistribution = Bernoulli(θ_real)\n\ndataset = float.(rand(rng, Bernoulli(θ_real), n));","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Once we generate the dataset, now we define a coin-toss model using the @model macro from RxInfer.jl","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds a factor graph under the hood\n@model function coin_model(y, a, b)\n\n    # We endow θ parameter of our model with \"a\" prior\n    θ ~ Beta(a, b)\n    # note that, in this particular case, the `Uniform(0.0, 1.0)` prior will also work.\n    # θ ~ Uniform(0.0, 1.0)\n\n    # here, the outcome of each coin toss is governed by the Bernoulli distribution\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(θ)\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Now, once the model is defined, we perform a (perfect) inference:","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"result = infer(\n    model = coin_model(a = 4.0, b = 8.0), \n    data  = (y = dataset,)\n)","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Once the result is calculated, we can focus on the posteriors","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"θestimated = result.posteriors[:θ]","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Distributions.Beta{Float64}(α=377.0, β=135.0)","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"and visualisation of the results","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np = plot(title = \"Inference results\")\n\nplot!(rθ, (x) -> pdf(Beta(4.0, 8.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\nplot!(rθ, (x) -> pdf(θestimated, x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\nvline!([θ_real], label=\"Real θ\")","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/coin_toss_model/","page":"Coin Toss Model","title":"Coin Toss Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/coin_toss_model/Project.toml`\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Bayesian-Linear-Regression-Tutorial","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression Tutorial","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"This notebook is an extensive tutorial on Bayesian linear regression with RxInfer and consists of two major parts:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The first part uses a regular Bayesian Linear Regression on a simple application of fuel consumption for a car with synthetic data.\nThe second part is an adaptation of a tutorial from NumPyro and uses Hierarchical Bayesian linear regression on the OSIC pulmonary fibrosis progression dataset from Kaggle.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"using RxInfer, Random, Plots, StableRNGs, LinearAlgebra, StatsPlots, LaTeXStrings, DataFrames, CSV, GLM","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Part-1.-Bayesian-Linear-Regression","page":"Bayesian Linear Regression","title":"Part 1. Bayesian Linear Regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"John recently purchased a new car and is interested in its fuel consumption rate. He believes that this rate has a linear relationship with speed, and as such, he wants to conduct tests by driving his car on different types of roads, recording both the fuel usage and speed. In order to determine the fuel consumption rate, John employs Bayesian linear regression.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Univariate-regression-with-known-noise","page":"Bayesian Linear Regression","title":"Univariate regression with known noise","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"First, he drives the car on a urban road. John enjoys driving on the well-built, wide, and flat urban roads. Urban roads also offer the advantage of precise fuel consumption measurement with minimal noise. Therefore John models the fuel consumption y_ninmathbbR as a normal distribution and treats x_n as a fixed hyperparameter:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginaligned\np(y_n mid a b) = mathcalN(y_n mid a x_n + b  1)\nendaligned","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The recorded speed is denoted as x_n in mathbbR and the recorded fuel consumption as y_n in mathbbR. Prior beliefs on a and b are informed by the vehicle manual.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginaligned\n    p(a) = mathcalN(a mid m_a v_a) \n    p(b) = mathcalN(b mid m_b v_b) \nendaligned","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Together they form the probabilistic model p(y a b) = p(a)p(b) prod_N=1^N p(y_n mid a b) where the goal is to infer the posterior distributions p(a mid y) and p(bmid y).","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He records the speed and fuel consumption for the urban road which is the xdata and ydata.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function generate_data(a, b, v, nr_samples; rng=StableRNG(1234))\n    x = float.(collect(1:nr_samples))\n    y = a .* x .+ b .+ randn(rng, nr_samples) .* sqrt(v)\n    return x, y\nend;","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"x_data, y_data = generate_data(0.5, 25.0, 1.0, 250)\n\nscatter(x_data, y_data, title = \"Dataset (City road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In order to estimate the two parameters with the recorded data, he uses a RxInfer.jl to create the above described model.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression(x, y)\n    a ~ Normal(mean = 0.0, variance = 1.0)\n    b ~ Normal(mean = 0.0, variance = 100.0)    \n    y .~ Normal(mean = a .* x .+ b, variance = 1.0)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He is delighted that he can utilize the inference function from this package, saving him the effort of starting from scratch and enabling him to obtain the desired results for this road. He does note that there is a loop in his model, namely all a and b variables are connected over all observations, therefore he needs to initialize one of the messages and run multiple iterations for the loopy belief propagation algorithm. It is worth noting that loopy belief propagation is not guaranteed to converge in general and might be highly influenced by the choice of the initial messages in the initialization argument. He is going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"results = infer(\n    model          = linear_regression(), \n    data           = (y = y_data, x = x_data), \n    initialization = @initialization(μ(b) = NormalMeanVariance(0.0, 100.0)), \n    returnvars     = (a = KeepLast(), b = KeepLast()),\n    iterations     = 20,\n    free_energy    = true\n)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (a, b)\n  Free Energy:     | Real[450.062, 8526.84, 4960.42, 2949.02, 1819.14, 1184\n.44, 827.897, 627.595, 515.064, 451.839, 416.313, 396.349, 385.129, 378.821\n, 375.274, 373.279, 372.156, 371.524, 371.167, 370.966]","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He knows the theoretical coefficients and noise for this car from the manual. He is going to compare the experimental solution with theoretical results.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=L\"Prior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a)$\", c=1,)\npra = vline!(pra, [ 0.5 ], label=L\"True $a$\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results.posteriors[:a], x), title=L\"Posterior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a\\mid y)$\", c=2,)\npsa = vline!(psa, [ 0.5 ], label=L\"True $a$\", c = 3)\n\nplot(pra, psa, size = (1000, 200), xlabel=L\"$a$\", ylabel=L\"$p(a)$\", ylims=[0,Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=L\"Prior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"p(b)\", c=1, legend = :topleft)\nprb = vline!(prb, [ 25 ], label=L\"True $b$\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results.posteriors[:b], x), title=L\"Posterior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"p(b\\mid y)\", c=2, legend = :topleft)\npsb = vline!(psb, [ 25 ], label=L\"True $b$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$b$\", ylabel=L\"$p(b)$\", ylims=[0, Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"a = results.posteriors[:a]\nb = results.posteriors[:b]\n\nprintln(\"Real a: \", 0.5, \" | Estimated a: \", mean_var(a), \" | Error: \", abs(mean(a) - 0.5))\nprintln(\"Real b: \", 25.0, \" | Estimated b: \", mean_var(b), \" | Error: \", abs(mean(b) - 25.0))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Real a: 0.5 | Estimated a: (0.501490188462706, 1.9162284531300301e-7) | Err\nor: 0.001490188462705988\nReal b: 25.0 | Estimated b: (24.81264210195605, 0.0040159675312827) | Error\n: 0.18735789804394898","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Based on the Bethe free energy below, John knows that the loopy belief propagation has actually converged after 20 iterations:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# drop first iteration, which is influenced by the `initmessages`\nplot(2:20, results.free_energy[2:end], title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Univariate-regression-with-unknown-noise","page":"Bayesian Linear Regression","title":"Univariate regression with unknown noise","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Afterwards, he plans to test the car on a mountain road. However, mountain roads are typically narrow and filled with small stones, which makes it more difficult to establish a clear relationship between fuel consumption and speed, leading to an unknown level of noise in the regression model. Therefore, he design a model with unknown Inverse-Gamma distribution on the variance. beginaligned p(y_n mid a b s) = mathcalN(y_n mid ax_n + b s)\np(s) = mathcalIG(smidalpha theta)\np(a) = mathcalN(a mid m_a v_a) \np(b) = mathcalN(b mid m_b v_b)  endaligned","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression_unknown_noise(x, y)\n    a ~ Normal(mean = 0.0, variance = 1.0)\n    b ~ Normal(mean = 0.0, variance = 100.0)\n    s ~ InverseGamma(1.0, 1.0)\n    y .~ Normal(mean = a .* x .+ b, variance = s)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"x_data_un, y_data_un = generate_data(0.5, 25.0, 400.0, 250)\n\nscatter(x_data_un, y_data_un, title = \"Dateset with unknown noise (mountain road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"To solve this problem in closed-from we need to resort to a variational approximation. The procedure will be a combination of variational inference and loopy belief propagation. He chooses constraints = MeanField() as a global variational approximation and provides initial marginals with the initialization argument. He is, again, going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"init_unknown_noise = @initialization begin \n    μ(b) = NormalMeanVariance(0.0, 100.0)\n    q(s) = vague(InverseGamma)\nend\n\nresults_unknown_noise = infer(\n    model           = linear_regression_unknown_noise(), \n    data            = (y = y_data_un, x = x_data_un), \n    initialization  = init_unknown_noise, \n    returnvars      = (a = KeepLast(), b = KeepLast(), s = KeepLast()), \n    iterations      = 20,\n    constraints     = MeanField(),\n    free_energy     = true\n)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (a, b, s)\n  Free Energy:     | Real[1657.49, 1192.08, 1142.31, 1135.43, 1129.19, 1125\n.47, 1123.34, 1122.13, 1121.44, 1121.05, 1120.82, 1120.69, 1120.61, 1120.56\n, 1120.53, 1120.52, 1120.5, 1120.5, 1120.49, 1120.49]","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Based on the Bethe free energy below, John knows that his algorithm has converged after 20 iterations:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"plot(results_unknown_noise.free_energy, title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Below he visualizes the obtained posterior distributions for parameters:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=L\"Prior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a)$\", c=1,)\npra = vline!(pra, [ 0.5 ], label=L\"True $a$\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:a], x), title=L\"Posterior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(a)$\", c=2,)\npsa = vline!(psa, [ 0.5 ], label=L\"True $a$\", c = 3)\n\nplot(pra, psa, size = (1000, 200), xlabel=L\"$a$\", ylabel=L\"$p(a)$\", ylims=[0, Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=L\"Prior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(b)$\", c=1, legend = :topleft)\nprb = vline!(prb, [ 25.0 ], label=L\"True $b$\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:b], x), title=L\"Posterior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(b)$\", c=2, legend = :topleft)\npsb = vline!(psb, [ 25.0 ], label=L\"True $b$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$b$\", ylabel=L\"$p(b)$\", ylims=[0, Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(0.001, 400, length = 1000), (x) -> pdf(InverseGamma(1.0, 1.0), x), title=L\"Prior for $s$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(s)$\", c=1, legend = :topleft)\nprb = vline!(prb, [ 200 ], label=L\"True $s$\", c = 3)\npsb = plot(range(0.001, 400, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:s], x), title=L\"Posterior for $s$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(s)$\", c=2, legend = :topleft)\npsb = vline!(psb, [ 200 ], label=L\"True $s$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$s$\", ylabel=L\"$p(s)$\", ylims=[0, Inf])","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He sees that in the presence of more noise the inference result is more uncertain about the actual values for a and b parameters.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"John samples a and b and plot many possible regression lines on the same plot:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"as = rand(results_unknown_noise.posteriors[:a], 100)\nbs = rand(results_unknown_noise.posteriors[:b], 100)\np = scatter(x_data_un, y_data_un, title = \"Linear regression with more noise\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")\nfor (a, b) in zip(as, bs)\n    global p = plot!(p, x_data_un, a .* x_data_un .+ b, alpha = 0.05, color = :red)\nend\n\nplot(p, size = (900, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"From this plot John can see that many lines do fit the data well and there is no definite \"best\" answer to the regression coefficients. He realize that most of these lines, however, resemble a similar angle and shift.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Multivariate-linear-regression","page":"Bayesian Linear Regression","title":"Multivariate linear regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In addition to fuel consumption, he is also interested in evaluating the car's power performance, braking performance, handling stability, smoothness, and other factors. To investigate the car's performance, he includes additional measurements. Essentially, this approach involves performing multiple linear regression tasks simultaneously, using multiple data vectors for x and y with different levels of noise. As in the previous example, he assumes the level of noise to be unknown.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression_multivariate(dim, x, y)\n    a ~ MvNormal(mean = zeros(dim), covariance = 100 * diageye(dim))\n    b ~ MvNormal(mean = ones(dim), covariance = 100 * diageye(dim))\n    W ~ InverseWishart(dim + 2, 100 * diageye(dim))\n    y .~ MvNormal(mean = x .* a .+ b, covariance = W)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"After received all the measurement records, he plots the measurements and performance index:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"dim_mv = 6\nnr_samples_mv = 50\nrng_mv = StableRNG(42)\na_mv = randn(rng_mv, dim_mv)\nb_mv = 10 * randn(rng_mv, dim_mv)\nv_mv = 100 * rand(rng_mv, dim_mv)\n\nx_data_mv, y_data_mv = collect(zip(generate_data.(a_mv, b_mv, v_mv, nr_samples_mv)...));","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p = plot(title = \"Multivariate linear regression\", legend = :topleft)\n\nplt = palette(:tab10)\n\ndata_set_label = [\"\"]\n\nfor k in 1:dim_mv\n    global p = scatter!(p, x_data_mv[k], y_data_mv[k], label = \"Measurement #$k\", ms = 2, color = plt[k])\nend\nxlabel!(L\"$x$\")\nylabel!(L\"$y$\")\np","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Before this data can be used to perform inference, John needs to change its format slightly.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"x_data_mv_processed = map(i -> Diagonal([getindex.(x_data_mv, i)...]), 1:nr_samples_mv)\ny_data_mv_processed = map(i -> [getindex.(y_data_mv, i)...], 1:nr_samples_mv);","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"init = @initialization begin \n    q(W) = InverseWishart(dim_mv + 2, 10 * diageye(dim_mv))\n    μ(b) = MvNormalMeanCovariance(ones(dim_mv), 10 * diageye(dim_mv))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Initial state: \n  q(W) = Distributions.InverseWishart{Float64, PDMats.PDMat{Float64, Matrix\n{Float64}}}(\ndf: 8.0\nΨ: [10.0 0.0 … 0.0 0.0; 0.0 10.0 … 0.0 0.0; … ; 0.0 0.0 … 10.0 0.0; 0.0 0.0\n … 0.0 10.0]\n)\n\n  μ(b) = MvNormalMeanCovariance(\nμ: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\nΣ: [10.0 0.0 … 0.0 0.0; 0.0 10.0 … 0.0 0.0; … ; 0.0 0.0 … 10.0 0.0; 0.0 0.0\n … 0.0 10.0]\n)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"results_mv = infer(\n    model           = linear_regression_multivariate(dim = dim_mv),\n    data            = (y = y_data_mv_processed, x = x_data_mv_processed),\n    initialization  = init,\n    returnvars      = (a = KeepLast(), b = KeepLast(), W = KeepLast()),\n    free_energy     = true,\n    iterations      = 50,\n    constraints     = MeanField()\n)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (a, b, W)\n  Free Energy:     | Real[864.485, 789.026, 769.094, 750.865, 737.67, 724.7\n22, 712.341, 700.865, 690.782, 682.505  …  664.434, 664.434, 664.434, 664.4\n34, 664.434, 664.434, 664.434, 664.434, 664.434, 664.434]","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Again, the algorithm nicely converged, because the Bethe free energy reached a plateau. John also draws the results for the linear regression parameters and sees that the lines very nicely follow the provided data.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p = plot(title = \"Multivariate linear regression\", legend = :topleft, xlabel=L\"$x$\", ylabel=L\"$y$\")\n\n# how many lines to plot\nr = 50\n\ni_a = collect.(eachcol(rand(results_mv.posteriors[:a], r)))\ni_b = collect.(eachcol(rand(results_mv.posteriors[:b], r)))\n\nplt = palette(:tab10)\n\nfor k in 1:dim_mv\n    x_mv_k = x_data_mv[k]\n    y_mv_k = y_data_mv[k]\n\n    for i in 1:r\n        global p = plot!(p, x_mv_k, x_mv_k .* i_a[i][k] .+ i_b[i][k], label = nothing, alpha = 0.05, color = plt[k])\n    end\n\n    global p = scatter!(p, x_mv_k, y_mv_k, label = \"Measurement #$k\", ms = 2, color = plt[k])\nend\n\n# truncate the init step\nf = plot(results_mv.free_energy[2:end], title =\"Bethe free energy convergence\", label = nothing, xlabel = \"Iteration\", ylabel = \"Bethe free energy [nats]\") \n\nplot(p, f, size = (1000, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He needs more iterations to converge in comparison to the very first example, but that is expected since the problem became multivariate and, hence, more difficult.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"i_a_mv = results_mv.posteriors[:a]\n\nps_a = []\n\nfor k in 1:dim_mv\n    \n    local _p = plot(title = L\"Estimated $a_{%$k}$\", xlabel=L\"$a_{%$k}$\", ylabel=L\"$p(a_{%$k})$\", xlims = (-1.5,1.5), xticks=[-1.5, 0, 1.5], ylims=[0, Inf])\n\n    local m_a_mv_k = mean(i_a_mv)[k]\n    local v_a_mv_k = std(i_a_mv)[k, k]\n    \n    _p = plot!(_p, Normal(m_a_mv_k, v_a_mv_k), fillalpha=0.3, fillrange = 0, label=L\"$q(a_{%$k})$\", c=2,)\n    _p = vline!(_p, [ a_mv[k] ], label=L\"True $a_{%$k}$\", c = 3)\n           \n    push!(ps_a, _p)\nend\n\nplot(ps_a...)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"i_b_mv = results_mv.posteriors[:b]\n\nps_b = []\n\nfor k in 1:dim_mv\n    \n    local _p = plot(title = L\"Estimated $b_{%$k}$\", xlabel=L\"$b_{%$k}$\", ylabel=L\"$p(b_{%$k})$\", xlims = (-20,20), xticks=[-20, 0, 20], ylims =[0, Inf])\n    local m_b_mv_k = mean(i_b_mv)[k]\n    local v_b_mv_k = std(i_b_mv)[k, k]\n\n    _p = plot!(_p, Normal(m_b_mv_k, v_b_mv_k), fillalpha=0.3, fillrange = 0, label=L\"$q(b_{%$k})$\", c=2,)\n    _p = vline!(_p, [ b_mv[k] ], label=L\"Real $b_{%$k}$\", c = 3)\n           \n    push!(ps_b, _p)\nend\n\nplot(ps_b...)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"He also checks the noise estimation procedure and sees that the noise variance are currently a bit underestimated. Note here that he neglects the covariance terms between the individual elements, which might result in this kind of behaviour.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"scatter(1:dim_mv, v_mv, ylims=(0, 100), label=L\"True $s_d$\")\nscatter!(1:dim_mv, diag(mean(results_mv.posteriors[:W])); yerror=sqrt.(diag(var(results_mv.posteriors[:W]))), label=L\"$\\mathrm{E}[s_d] \\pm \\sigma$\")\nplot!(; xlabel=L\"Dimension $d$\", ylabel=\"Variance\", title=\"Estimated variance of the noise\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Part-2.-Hierarchical-Bayesian-Linear-Regression","page":"Bayesian Linear Regression","title":"Part 2. Hierarchical Bayesian Linear Regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Disclaimer The tutorial below is an adaptation of the Bayesian Hierarchical Linear Regression tutorial implemented in NumPyro. ","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The original author in NumPyro is Carlos Souza. Updated by Chris Stoafer in NumPyro. Adapted to RxInfer by Dmitry Bagaev.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Probabilistic Machine Learning models can not only make predictions about future data but also model uncertainty. In areas such as personalized medicine, there might be a large amount of data, but there is still a relatively small amount available for each patient. To customize predictions for each person, it becomes necessary to build a model for each individual — considering its inherent uncertainties — and then couple these models together in a hierarchy so that information can be borrowed from other similar individuals [1].","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The purpose of this tutorial is to demonstrate how to implement a Bayesian Hierarchical Linear Regression model using RxInfer. To provide motivation for the tutorial, I will use the OSIC Pulmonary Fibrosis Progression competition, hosted on Kaggle.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# https://www.machinelearningplus.com/linear-regression-in-julia/\n# https://nbviewer.org/github/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_hierarchical_linear_regression.ipynb","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Understanding-the-Task","page":"Bayesian Linear Regression","title":"Understanding the Task","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Pulmonary fibrosis is a disorder characterized by scarring of the lungs, and its cause and cure are currently unknown. In this competition, the objective was to predict the severity of decline in lung function for patients. Lung function is assessed based on the output from a spirometer, which measures the forced vital capacity (FVC), representing the volume of air exhaled.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In medical applications, it is valuable to evaluate a model's confidence in its decisions. As a result, the metric used to rank the teams was designed to reflect both the accuracy and certainty of each prediction. This metric is a modified version of the Laplace Log Likelihood (further details will be provided later).","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Now, let's explore the data and dig deeper into the problem involved.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"dataset = CSV.read(\"hbr/osic_pulmonary_fibrosis.csv\", DataFrame);","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"describe(dataset)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"7×7 DataFrame\n Row │ variable       mean     min                        median   max     \n    ⋯\n     │ Symbol         Union…   Any                        Union…   Any     \n    ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ Patient                 ID00007637202177411956430           ID004266\n372 ⋯\n   2 │ Weeks          31.8618  -5                         28.0     133\n   3 │ FVC            2690.48  827                        2641.0   6399\n   4 │ Percent        77.6727  28.8776                    75.6769  153.145\n   5 │ Age            67.1885  49                         68.0     88      \n    ⋯\n   6 │ Sex                     Female                              Male\n   7 │ SmokingStatus           Currently smokes                    Never sm\noke\n                                                               3 columns om\nitted","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"first(dataset, 5)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"5×7 DataFrame\n Row │ Patient                    Weeks  FVC    Percent  Age    Sex      Sm\noki ⋯\n     │ String31                   Int64  Int64  Float64  Int64  String7  St\nrin ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ ID00007637202177411956430     -4   2315  58.2536     79  Male     Ex\n-sm ⋯\n   2 │ ID00007637202177411956430      5   2214  55.7121     79  Male     Ex\n-sm\n   3 │ ID00007637202177411956430      7   2061  51.8621     79  Male     Ex\n-sm\n   4 │ ID00007637202177411956430      9   2144  53.9507     79  Male     Ex\n-sm\n   5 │ ID00007637202177411956430     11   2069  52.0634     79  Male     Ex\n-sm ⋯\n                                                                1 column om\nitted","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The dataset provided us with a baseline chest CT scan and relevant clinical information for a group of patients. Each patient has an image taken at Week = 0, and they undergo numerous follow-up visits over approximately 1-2 years, during which their Forced Vital Capacity (FVC) is measured. For the purpose of this tutorial, we will only consider the Patient ID, the weeks, and the FVC measurements, discarding all other information. Restricting our analysis to these specific columns allowed our team to achieve a competitive score, highlighting the effectiveness of Bayesian hierarchical linear regression models, especially when dealing with uncertainty, which is a crucial aspect of the problem.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Since this is real medical data, the relative timing of FVC measurements varies widely, as shown in the 3 sample patients below:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patientinfo(dataset, patient_id) = filter(:Patient => ==(patient_id), dataset)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patientinfo (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function patientchart(dataset, patient_id; line_kws = true)\n    info = patientinfo(dataset, patient_id)\n    x = info[!, \"Weeks\"]\n    y = info[!, \"FVC\"]\n\n    p = plot(tickfontsize = 10, margin = 1Plots.cm, size = (400, 400), titlefontsize = 11)\n    p = scatter!(p, x, y, title = patient_id, legend = false, xlabel = \"Weeks\", ylabel = \"FVC\")\n    \n    if line_kws\n        # Use the `GLM.jl` package to estimate linear regression\n        linearFormulae = @formula(FVC ~ Weeks)\n        linearRegressor = lm(linearFormulae, patientinfo(dataset, patient_id))\n        linearPredicted = predict(linearRegressor)\n        p = plot!(p, x, linearPredicted, color = :red, lw = 3)\n    end\n\n    return p\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patientchart (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p1 = patientchart(dataset, \"ID00007637202177411956430\")\np2 = patientchart(dataset, \"ID00009637202177434476278\")\np3 = patientchart(dataset, \"ID00010637202177584971671\")\n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"On average, each of the 176 patients provided in the dataset had 9 visits during which their FVC was measured. These visits occurred at specific weeks within the interval [-12, 133]. The decline in lung capacity is evident, but it also varies significantly from one patient to another.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Our task was to predict the FVC measurements for each patient at every possible week within the [-12, 133] interval, along with providing a confidence score for each prediction. In other words, we were required to fill a matrix, as shown below, with the predicted values and their corresponding confidence scores:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The task was ideal for applying Bayesian inference. However, the vast majority of solutions shared within the Kaggle community utilized discriminative machine learning models, disregarding the fact that most discriminative methods struggle to provide realistic uncertainty estimates. This limitation stems from their typical training process, which aims to optimize parameters to minimize certain loss criteria (such as predictive error). As a result, these models do not inherently incorporate uncertainty into their parameters or subsequent predictions. While some methods may produce uncertainty estimates as a by-product or through post-processing steps, these are often heuristic-based and lack a statistically principled approach to estimate the target uncertainty distribution [2].","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Modelling:-Bayesian-Hierarchical-Linear-Regression-with-Partial-Pooling","page":"Bayesian Linear Regression","title":"Modelling: Bayesian Hierarchical Linear Regression with Partial Pooling","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In a basic linear regression, which is not hierarchical, the assumption is that all FVC decline curves share the same α and β values. This model is known as the \"pooled model.\" On the other extreme, we could assume a model where each patient has a personalized FVC decline curve, and these curves are entirely independent of one another. This model is referred to as the \"unpooled model,\" where each patient has completely separate regression lines.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In this analysis, we will adopt a middle ground approach known as \"Partial pooling.\" Specifically, we will assume that while α's and β's are different for each patient, as in the unpooled case, these coefficients share some similarities. This partial pooling will be achieved by modeling each individual coefficient as being drawn from a common group distribution.:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Mathematically, the model is described by the following equations:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginequation\n    beginaligned\n        mu_alpha sim mathcalN(mathrmmean = 00 mathrmvariance = 2500000) \n        sigma_alpha sim mathcalGamma(mathrmshape = 175 mathrmscale = 4554) \n        mu_beta sim mathcalN(mathrmmean = 00 mathrmvariance = 90) \n        sigma_beta sim mathcalGamma(mathrmshape = 175 mathrmscale = 136) \n        alpha_i sim mathcalN(mathrmmean = mu_alpha mathrmprecision = sigma_alpha) \n        beta sim mathcalN(mathrmmean = mu_beta mathrmprecision = sigma_beta) \n        sigma sim mathcalGamma(mathrmshape = 175 mathrmscale = 4554) \n        mathrmFVC_ij sim mathcalN(mathrmmean = alpha_i + t beta_i mathrmprecision = sigma)\n    endaligned\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"where t is the time in weeks. Those are very uninformative priors, but that's ok: our model will converge!","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Implementing this model in RxInfer is pretty straightforward:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function partially_pooled(patient_codes, weeks, data)\n    μ_α ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of α (intercept)\n    μ_β ~ Normal(mean = 0.0, var = 9.0)      # Prior for the mean of β (slope)\n    σ_α ~ Gamma(shape = 1.75, scale = 45.54) # Prior for the precision of α (intercept)\n    σ_β ~ Gamma(shape = 1.75, scale = 1.36)  # Prior for the precision of β (slope)\n\n    n_codes = length(patient_codes)            # Total number of data points\n    n_patients = length(unique(patient_codes)) # Number of unique patients in the data\n\n    local α # Individual intercepts for each patient\n    local β # Individual slopes for each patient\n\n    for i in 1:n_patients\n        α[i] ~ Normal(mean = μ_α, precision = σ_α) # Sample the intercept α from a Normal distribution\n        β[i] ~ Normal(mean = μ_β, precision = σ_β) # Sample the slope β from a Normal distribution\n    end\n\n    σ ~ Gamma(shape = 1.75, scale = 45.54)   # Prior for the standard deviation of the error term\n    \n    local FVC_est\n\n    for i in 1:n_codes\n        FVC_est[i] ~ α[patient_codes[i]] + β[patient_codes[i]] * weeks[i] # FVC estimation using patient-specific α and β\n        data[i] ~ Normal(mean = FVC_est[i], precision = σ)                # Likelihood of the observed FVC data\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Variational constraints are used in variational methods to restrict the set of functions or probability distributions that the method can explore during optimization. These constraints help guide the optimization process towards more meaningful and tractable solutions. We need variational constraints to ensure that the optimization converges to valid and interpretable solutions, avoiding solutions that might not be meaningful or appropriate for the given problem. By incorporating constraints, we can control the complexity and shape of the solutions, making them more useful for practical applications. We use the @constraints macro from RxInfer to define approriate variational constraints.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@constraints function partially_pooled_constraints()\n    # Assume that `μ_α`, `σ_α`, `μ_β`, `σ_β` and `σ` are jointly independent\n    q(μ_α, σ_α, μ_β, σ_β, σ) = q(μ_α)q(σ_α)q(μ_β)q(σ_β)q(σ)\n    # Assume that `μ_α`, `σ_α`, `α` are jointly independent\n    q(μ_α, σ_α, α) = q(μ_α, α)q(σ_α)\n    # Assume that `μ_β`, `σ_β`, `β` are jointly independent\n    q(μ_β, σ_β, β) = q(μ_β, β)q(σ_β)\n    # Assume that `FVC_est`, `σ` are jointly independent\n    q(FVC_est, σ) = q(FVC_est)q(σ) \nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_constraints (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"These @constraints assume some structural independencies in the resulting variational approximation. For simplicity we can also use constraints = MeanField() in the inference function below. That's all for modelling!","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Inference-in-the-model","page":"Bayesian Linear Regression","title":"Inference in the model","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"A significant achievement of Probabilistic Programming Languages, like RxInfer, is the ability to separate model specification and inference. Once I define my generative model with priors, condition statements, and data likelihoods, I can delegate the challenging inference tasks to RxInfer's inference engine.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Calling the inference engine only takes a few lines of code. Before proceeding, let's assign a numerical Patient ID to each patient code, a task that can be easily accomplished using label encoding.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patient_ids          = dataset[!, \"Patient\"] # get the column of all patients\npatient_code_encoder = Dict(map(((id, patient), ) -> patient => id, enumerate(unique(patient_ids))));\npatient_code_column  = map(patient -> patient_code_encoder[patient], patient_ids)\n\ndataset[!, :PatientCode] = patient_code_column\n\nfirst(patient_code_encoder, 5)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"5-element Vector{Pair{InlineStrings.String31, Int64}}:\n \"ID00197637202246865691526\" => 85\n \"ID00388637202301028491611\" => 160\n \"ID00341637202287410878488\" => 142\n \"ID00020637202178344345685\" => 9\n \"ID00305637202281772703145\" => 127","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function partially_pooled_inference(dataset)\n\n    patient_codes = values(dataset[!, \"PatientCode\"])\n    weeks = values(dataset[!, \"Weeks\"])\n    FVC_obs = values(dataset[!, \"FVC\"]);\n\n    init = @initialization begin \n        μ(α) = vague(NormalMeanVariance)\n        μ(β) = vague(NormalMeanVariance)\n        q(α) = vague(NormalMeanVariance)\n        q(β) = vague(NormalMeanVariance)\n        q(σ) = vague(Gamma)\n        q(σ_α) = vague(Gamma)\n        q(σ_β) = vague(Gamma)\n    end\n\n    results = infer(\n        model = partially_pooled(patient_codes = patient_codes, weeks = weeks),\n        data = (data = FVC_obs, ),\n        options = (limit_stack_depth = 500, ),\n        constraints = partially_pooled_constraints(),\n        initialization = init,\n        returnvars = KeepLast(),\n        iterations = 100\n    )\n    \nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_inference (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We use a hybrid message passing approach combining exact and variational inference. In loopy models, where there are cycles or feedback loops in the graphical model, we need to initialize messages to kick-start the message passing process. Messages are passed between connected nodes in the model to exchange information and update beliefs iteratively. Initializing messages provides a starting point for the iterative process and ensures that the model converges to a meaningful solution.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In variational inference procedures, we need to initialize marginals because variational methods aim to approximate the true posterior distribution with a simpler, tractable distribution. Initializing marginals involves providing initial estimates for the parameters of this approximating distribution. These initial estimates serve as a starting point for the optimization process, allowing the algorithm to iteratively refine the approximation until it converges to a close approximation of the true posterior distribution. ","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_inference_results = partially_pooled_inference(dataset)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (α, σ_α, σ_β, σ, FVC_est, μ_β, μ_α, β)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Checking-the-model","page":"Bayesian Linear Regression","title":"Checking the model","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/#Inspecting-the-learned-parameters","page":"Bayesian Linear Regression","title":"Inspecting the learned parameters","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# Convert to `Normal` since it supports easy plotting with `StatsPlots`\nlet \n    local μ_α = Normal(mean_std(partially_pooled_inference_results.posteriors[:μ_α])...)\n    local μ_β = Normal(mean_std(partially_pooled_inference_results.posteriors[:μ_β])...)\n    local α = map(d -> Normal(mean_std(d)...), partially_pooled_inference_results.posteriors[:α])\n    local β = map(d -> Normal(mean_std(d)...), partially_pooled_inference_results.posteriors[:β])\n    \n    local p1 = plot(μ_α, title = \"q(μ_α)\", fill = 0, fillalpha = 0.2, label = false)\n    local p2 = plot(μ_β, title = \"q(μ_β)\", fill = 0, fillalpha = 0.2, label = false)\n    \n    local p3 = plot(title = \"q(α)...\", legend = false)\n    local p4 = plot(title = \"q(β)...\", legend = false)\n    \n    foreach(d -> plot!(p3, d), α) # Add each individual `α` on plot `p3`\n    foreach(d -> plot!(p4, d), β) # Add each individual `β` on plot `p4`\n    \n    plot(p1, p2, p3, p4, size = (1200, 400), layout = @layout([ a b; c d ]))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Looks like our model learned personalized alphas and betas for each patient!","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Visualizing-FVC-decline-curves-for-some-patients","page":"Bayesian Linear Regression","title":"Visualizing FVC decline curves for some patients","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Now, let's visually inspect the FVC decline curves predicted by our model. We will complete the FVC table by predicting all the missing values. To do this, we need to create a table to accommodate the predictions.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function patientchart_bayesian(results, dataset, encoder, patient_id; kwargs...)\n    info            = patientinfo(dataset, patient_id)\n    patient_code_id = encoder[patient_id]\n\n    patient_α = results.posteriors[:α][patient_code_id]\n    patient_β = results.posteriors[:β][patient_code_id]\n\n    estimated_σ = inv(mean(results.posteriors[:σ]))\n    \n    predict_weeks = range(-12, 134)\n\n    predicted = map(predict_weeks) do week\n        pm = mean(patient_α) + mean(patient_β) * week\n        pv = var(patient_α) + var(patient_β) * week ^ 2 + estimated_σ\n        return pm, sqrt(pv)\n    end\n    \n    p = patientchart(dataset, patient_id; kwargs...)\n    \n    return plot!(p, predict_weeks, getindex.(predicted, 1), ribbon = getindex.(predicted, 2), color = :orange)\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"patientchart_bayesian (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p1 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00007637202177411956430\")\np2 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00009637202177434476278\")\np3 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00011637202177653955184\")\n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The results match our expectations perfectly! Let's highlight the observations:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The model successfully learned Bayesian Linear Regressions! The orange line representing the learned predicted FVC mean closely aligns with the red line representing the deterministic linear regression. More importantly, the model effectively predicts uncertainty, demonstrated by the light orange region surrounding the mean FVC line.\nThe model predicts higher uncertainty in cases where the data points are more dispersed, such as in the 1st and 3rd patients. In contrast, when data points are closely grouped together, as seen in the 2nd patient, the model predicts higher confidence, resulting in a narrower light orange region.\nAdditionally, across all patients, we observe that the uncertainty increases as we look further into the future. The light orange region widens as the number of weeks increases, reflecting the growth of uncertainty over time.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Computing-the-modified-Laplace-Log-Likelihood-and-RMSE","page":"Bayesian Linear Regression","title":"Computing the modified Laplace Log Likelihood and RMSE","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"As mentioned earlier, the competition evaluated models using a modified version of the Laplace Log Likelihood, which takes into account both the accuracy and certainty of each prediction—a valuable feature in medical applications.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"To compute the metric, we predicted both the FVC value and its associated confidence measure (standard deviation σ). The metric is given by the formula:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginequation\n    beginaligned\n        sigma_mathrmclipped = max(sigma 70) \n        delta = min(vert mathrmFVC_mathrmtrue - mathrmFVC_mathrmpredvert 1000) \n        mathrmmetric = -fracsqrt2deltasigma_mathrmclipped - mathrmln(sqrt2sigma_mathrmclipped) \n    endaligned\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"To prevent large errors from disproportionately penalizing results, errors were thresholded at 1000 ml. Additionally, confidence values were clipped at 70 ml to account for the approximate measurement uncertainty in FVC. The final score was determined by averaging the metric across all (Patient, Week) pairs. It is worth noting that metric values will be negative, and a higher score indicates better model performance.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function FVC_predict(results)\n    return broadcast(results.posteriors[:FVC_est], Ref(results.posteriors[:σ])) do f, s\n        return @call_rule NormalMeanPrecision(:out, Marginalisation) (m_μ = f, q_τ = s)\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"FVC_predict (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function compute_rmse(results, dataset)\n    FVC_predicted = FVC_predict(results)\n    return mean((dataset[!, \"FVC\"] .- mean.(FVC_predicted)) .^ 2) ^ (1/2)\nend\n\nfunction compute_laplace_log_likelihood(results, dataset)\n    FVC_predicted = FVC_predict(results)\n    sigma_c = std.(FVC_predicted)\n    sigma_c[sigma_c .< 70] .= 70\n    delta = abs.(mean.(FVC_predicted) .- dataset[!, \"FVC\"])\n    delta[delta .> 1000] .= 1000\n    return mean(-sqrt(2) .* delta ./ sigma_c .- log.(sqrt(2) .* sigma_c))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"compute_laplace_log_likelihood (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"println(\"RMSE: $(compute_rmse(partially_pooled_inference_results, dataset))\")\nprintln(\"Laplace Log Likelihood: $(compute_laplace_log_likelihood(partially_pooled_inference_results, dataset))\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"RMSE: 124.01306457993996\nLaplace Log Likelihood: -6.156795767593613","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"What do these numbers signify? They indicate that adopting this approach would lead to outperforming the majority of public solutions in the competition. In several seconds of inference!","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Interestingly, most public solutions rely on a standard deterministic Neural Network and attempt to model uncertainty through a quantile loss, adhering to a frequentist approach. The importance of uncertainty in single predictions is growing in the field of machine learning, becoming a crucial requirement. Especially when the consequences of an inaccurate prediction are significant, knowing the probability distribution of individual predictions becomes essential.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Add-layer-to-model-hierarchy:-Smoking-Status","page":"Bayesian Linear Regression","title":"Add layer to model hierarchy: Smoking Status","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We can enhance the model by incorporating the column \"SmokingStatus\" as a pooling level, where model parameters will be partially pooled within the groups \"Never smoked,\" \"Ex-smoker,\" and \"Currently smokes.\" To achieve this, we need to:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Encode the \"SmokingStatus\" column. Map the patient encoding to the corresponding \"SmokingStatus\" encodings. Refine and retrain the model with the additional hierarchical structure.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"combine(groupby(dataset, \"SmokingStatus\"), nrow)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"3×2 DataFrame\n Row │ SmokingStatus     nrow\n     │ String31          Int64\n─────┼─────────────────────────\n   1 │ Ex-smoker          1038\n   2 │ Never smoked        429\n   3 │ Currently smokes     82","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"smoking_id_mapping   = Dict(map(((code, smoking_status), ) -> smoking_status => code, enumerate(unique(dataset[!, \"SmokingStatus\"]))))\nsmoking_code_encoder = Dict(map(unique(values(patient_ids))) do patient_id\n    smoking_status = first(unique(patientinfo(dataset, patient_id)[!, \"SmokingStatus\"]))\n    return patient_code_encoder[patient_id] => smoking_id_mapping[smoking_status]\nend)\n\nsmoking_status_patient_mapping = map(id -> smoking_code_encoder[id], 1:length(unique(patient_ids)));","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function partially_pooled_with_smoking(patient_codes, smoking_status_patient_mapping, weeks, data)\n    μ_α_global ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of α (intercept)\n    μ_β_global ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of β (slope)\n    σ_α_global ~ Gamma(shape = 1.75, scale = 45.54) # Corresponds to half-normal with scale 100.0\n    σ_β_global ~ Gamma(shape = 1.75, scale = 1.36)  # Corresponds to half-normal with scale 3.0\n\n    n_codes = length(patient_codes) # Total number of data points\n    n_smoking_statuses = length(unique(smoking_status_patient_mapping)) # Number of different smoking patterns\n    n_patients = length(unique(patient_codes)) # Number of unique patients in the data\n\n    local μ_α_smoking_status  # Individual intercepts for smoking pattern\n    local μ_β_smoking_status  # Individual slopes for smoking pattern\n    \n    for i in 1:n_smoking_statuses\n        μ_α_smoking_status[i] ~ Normal(mean = μ_α_global, precision = σ_α_global)\n        μ_β_smoking_status[i] ~ Normal(mean = μ_β_global, precision = σ_β_global)\n    end\n    \n    local α # Individual intercepts for each patient\n    local β # Individual slopes for each patient\n\n    for i in 1:n_patients\n        α[i] ~ Normal(mean = μ_α_smoking_status[smoking_status_patient_mapping[i]], precision = σ_α_global)\n        β[i] ~ Normal(mean = μ_β_smoking_status[smoking_status_patient_mapping[i]], precision = σ_β_global)\n    end\n\n    σ ~ Gamma(shape = 1.75, scale = 45.54) # Corresponds to half-normal with scale 100.0\n\n    local FVC_est\n\n    for i in 1:n_codes\n        FVC_est[i] ~ α[patient_codes[i]] + β[patient_codes[i]] * weeks[i] # FVC estimation using patient-specific α and β\n        data[i] ~ Normal(mean = FVC_est[i], precision = σ)              # Likelihood of the observed FVC data\n    end\n    \nend\n\n@constraints function partially_pooled_with_smooking_constraints()\n    q(μ_α_global, σ_α_global, μ_β_global, σ_β_global) = q(μ_α_global)q(σ_α_global)q(μ_β_global)q(σ_β_global)\n    q(μ_α_smoking_status, μ_β_smoking_status, σ_α_global, σ_β_global) = q(μ_α_smoking_status)q(μ_β_smoking_status)q(σ_α_global)q(σ_β_global)\n    q(μ_α_global, σ_α_global, μ_β_global, σ_β_global, σ) = q(μ_α_global)q(σ_α_global)q(μ_β_global)q(σ_β_global)q(σ)\n    q(μ_α_global, σ_α_global, α) = q(μ_α_global, α)q(σ_α_global)\n    q(μ_β_global, σ_β_global, β) = q(μ_β_global, β)q(σ_β_global)\n    q(FVC_est, σ) = q(FVC_est)q(σ) \nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_with_smooking_constraints (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"function partially_pooled_with_smoking(dataset, smoking_status_patient_mapping)\n    patient_codes = values(dataset[!, \"PatientCode\"])\n    weeks = values(dataset[!, \"Weeks\"])\n    FVC_obs = values(dataset[!, \"FVC\"]);\n\n    init = @initialization begin \n        μ(α) = vague(NormalMeanVariance)\n        μ(β) = vague(NormalMeanVariance)\n        q(σ) = Gamma(1.75, 45.54)\n        q(σ_α_global) = Gamma(1.75, 45.54)\n        q(σ_β_global) = Gamma(1.75, 1.36)\n    end\n    \n    return infer(\n        model = partially_pooled_with_smoking(\n            patient_codes = patient_codes, \n            smoking_status_patient_mapping = smoking_status_patient_mapping, \n            weeks = weeks\n        ),\n        data = (data = FVC_obs, ),\n        options = (limit_stack_depth = 500, ),\n        constraints = partially_pooled_with_smooking_constraints(),\n        initialization = init,\n        returnvars = KeepLast(),\n        iterations = 100,\n    )\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_with_smoking (generic function with 3 methods)","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"partially_pooled_with_smoking_inference_results = partially_pooled_with_smoking(dataset, smoking_status_patient_mapping);","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Inspect-the-learned-parameters","page":"Bayesian Linear Regression","title":"Inspect the learned parameters","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# Convert to `Normal` since it supports easy plotting with `StatsPlots`\nlet \n    local μ_α = Normal(mean_std(partially_pooled_with_smoking_inference_results.posteriors[:μ_α_global])...)\n    local μ_β = Normal(mean_std(partially_pooled_with_smoking_inference_results.posteriors[:μ_β_global])...)\n    local αsmoking = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:μ_α_smoking_status])\n    local βsmoking = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:μ_β_smoking_status])\n    local α = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:α])\n    local β = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:β])\n    \n    local p1 = plot(μ_α, title = \"q(μ_α_global)\", fill = 0, fillalpha = 0.2, label = false)\n    local p2 = plot(μ_β, title = \"q(μ_β_global)\", fill = 0, fillalpha = 0.2, label = false)\n    \n    local p3 = plot(title = \"q(α)...\", legend = false)\n    local p4 = plot(title = \"q(β)...\", legend = false)\n    \n    foreach(d -> plot!(p3, d), α) # Add each individual `α` on plot `p3`\n    foreach(d -> plot!(p4, d), β) # Add each individual `β` on plot `p4`\n    \n    local p5 = plot(title = \"q(μ_α_smoking_status)...\", legend = false)\n    local p6 = plot(title = \"q(μ_β_smoking_status)...\", legend = false)\n    \n    foreach(d -> plot!(p5, d, fill = 0, fillalpha = 0.2), αsmoking) \n    foreach(d -> plot!(p6, d, fill = 0, fillalpha = 0.2), βsmoking)\n    \n    plot(p1, p2, p3, p4, p5, p6, size = (1200, 600), layout = @layout([ a b; c d; e f ]))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Interpret-smoking-status-model-parameters","page":"Bayesian Linear Regression","title":"Interpret smoking status model parameters","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"The model parameters for each smoking status reveal intriguing findings, particularly concerning the trend, μ_β_smoking_status. In the summary below, it is evident that the trend for current smokers has a positive mean, while the trend for ex-smokers and those who have never smoked is negative.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"smoking_id_mapping","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Dict{InlineStrings.String31, Int64} with 3 entries:\n  \"Currently smokes\" => 3\n  \"Ex-smoker\"        => 1\n  \"Never smoked\"     => 2","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"posteriors_μ_β_smoking_status = partially_pooled_with_smoking_inference_results.posteriors[:μ_β_smoking_status]\n\nprintln(\"Trend for\")\nforeach(pairs(smoking_id_mapping)) do (key, id)\n    println(\"  $key: $(mean(posteriors_μ_β_smoking_status[id]))\")\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Trend for\n  Currently smokes: 1.8147143381168995\n  Ex-smoker: -4.572743474510769\n  Never smoked: -4.447769588035918","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Let's look at these curves for individual patients to help interpret these model results.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# Never smoked\np1 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00007637202177411956430\") \n# Ex-smoker\np2 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00009637202177434476278\") \n# Currently smokes\np3 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00011637202177653955184\") \n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Review-patients-that-currently-smoke","page":"Bayesian Linear Regression","title":"Review patients that currently smoke","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"When plotting each patient with the smoking status \"Currently smokes,\" we observe different trends. Some patients show a clear positive trend, while others do not exhibit a clear trend or even have a negative trend. Compared to the unpooled trend lines, the trend lines with partial pooling are less prone to overfitting and display greater uncertainty in both slope and intercept.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Depending on the purpose of the model, we can proceed in different ways:","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"If our goal is to gain insights into how different attributes relate to a patient's FVC over time, we can stop here and understand that current smokers might experience an increase in FVC over time when monitored for Pulmonary Fibrosis. We may then formulate hypotheses to explore the reasons behind this observation and design new experiments for further testing.\nHowever, if our aim is to develop a model for generating predictions to treat patients, it becomes crucial to ensure that the model does not overfit and can be trusted with new patients. To achieve this, we could adjust model parameters to shrink the \"Currently smokes\" group's parameters closer to the global parameters, or even consider merging the group with \"Ex-smokers.\" Additionally, collecting more data for current smokers could help in ensuring the model's robustness and preventing overfitting.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"let \n    local plots = []\n\n    for (i, patient) in enumerate(unique(filter(:SmokingStatus => ==(\"Currently smokes\"), dataset)[!, \"Patient\"]))\n        push!(plots, patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, patient))\n    end\n\n    plot(plots..., size = (1200, 1200))\nend","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#Modified-Laplace-Log-Likelihood-and-RMSE-for-model-with-Smoking-Status-Level","page":"Bayesian Linear Regression","title":"Modified Laplace Log Likelihood and RMSE for model with Smoking Status Level","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We calculate the metrics for the updated model and compare to the original model.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"println(\"RMSE: $(compute_rmse(partially_pooled_with_smoking_inference_results, dataset))\")\nprintln(\"Laplace Log Likelihood: $(compute_laplace_log_likelihood(partially_pooled_with_smoking_inference_results, dataset))\")","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"RMSE: 124.81042940540623\nLaplace Log Likelihood: -6.165660148605184","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Both the Laplace Log Likelihood and RMSE indicate slightly worse performance for the smoking status model. Adding this hierarchy level as it is did not improve the model's performance significantly. However, we did discover some interesting results from the smoking status level that might warrant further investigation. Additionally, we could attempt to enhance model performance by adjusting priors or exploring different hierarchy levels, such as gender.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/#References","page":"Bayesian Linear Regression","title":"References","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"[1] Ghahramani, Z. Probabilistic machine learning and artificial intelligence. Nature 521, 452–459 (2015). https://doi.org/10.1038/nature14541","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"[2] Rainforth, Thomas William Gamlen. Automating Inference, Learning, and Design Using Probabilistic Programming. University of Oxford, 2017.","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/bayesian_linear_regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/bayesian_linear_regression/Project.toml`\n  [336ed68f] CSV v0.10.15\n  [a93c6f00] DataFrames v1.7.0\n  [38e38edf] GLM v1.9.0\n  [b964fa9f] LaTeXStrings v1.4.0\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [f3b207a7] StatsPlots v0.15.7\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#RTS-vs-BIFM-Smoothing","page":"Rts Vs Bifm Smoothing","title":"RTS vs BIFM Smoothing","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"___Credits to Martin de Quincey___","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"This notebook performs Kalman smoothing on a factor graph using message passing, based on the BIFM Kalman smoother. This notebook is based on:","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"F. Wadehn, “State Space Methods with Applications in Biomedical Signal Processing,” ETH Zurich, 2019. Accessed: Jun. 16, 2021. [Online]. Available: https://www.research-collection.ethz.ch/handle/20.500.11850/344762\nH. Loeliger, L. Bruderer, H. Malmberg, F. Wadehn, and N. Zalmai, “On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing,” in 2016 Information Theory and Applications Workshop (ITA), Jan. 2016, pp. 1–10. doi: 10.1109/ITA.2016.7888168.","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"We perform Kalman smoothing in the linear state space model, represented by:","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"beginaligned\n    Z_k+1 = A Z_k + B U_k \n    Y_k = C Z_k + W_k\nendaligned","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"with observations Y_k, latent states Z_k and inputs U_k. W_k is the observation noise. A in mathrmR^n times n, B in mathrmR^n times m and C in mathrmR^d times n are the transition matrices in the model. Here n, m and d denote the dimensionality of the latent, input and output dimension, respectively.","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"The corresponding probabilistic model can be represented as ","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"beginaligned\n        p(y z u)\n        = p(z_0) prod_k=1^N p(y_k mid z_k) p(z_kmid z_k-1 u_k-1) p(u_k-1) \n        = mathcalN(z_0 mid mu_z_0 Sigma_z_0) left( prod_k=1^N mathcalN(y_k mid C z_k Sigma_W) delta(z_k - (Az_k-1 + Bu_k-1)) mathcalN(u_k-1 mid mu_i_k-1 Sigma_u_k-1) right)\nendaligned","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Import-packages","page":"Rts Vs Bifm Smoothing","title":"Import packages","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"using RxInfer, Random, LinearAlgebra, BenchmarkTools, ProgressMeter, Plots, StableRNGs","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Data-generation","page":"Rts Vs Bifm Smoothing","title":"Data generation","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"function generate_parameters(dim_out::Int64, dim_in::Int64, dim_lat::Int64; seed::Int64 = 123)\n    \n    # define noise levels\n    input_noise  = 500.0\n    output_noise = 50.0\n\n    # create random generator for reproducibility\n    rng = MersenneTwister(seed)\n\n    # generate matrices, input statistics and noise matrices\n    A      = diagm(0.8 .* ones(dim_lat) .+ 0.2 * rand(rng, dim_lat))                                            # size (dim_lat x dim_lat)\n    B      = rand(dim_lat, dim_in)                                                                              # size (dim_lat x dim_in)\n    C      = rand(dim_out, dim_lat)                                                                             # size (dim_out x dim_lat)\n    μu     = rand(dim_in) .* collect(1:dim_in)                                                                  # size (dim_in x 1)\n    Σu     = input_noise  .* collect(Hermitian(randn(rng, dim_in, dim_in) + diagm(10 .+ 10*rand(dim_in))))      # size (dim_in x dim_in)\n    Σy     = output_noise .* collect(Hermitian(randn(rng, dim_out, dim_out) + diagm(10 .+ 10*rand(dim_out))))   # size (dim_out x dim_out)\n    Wu     = inv(Σu)\n    Wy     = inv(Σy)\n    \n    # return parameters\n    return A, B, C, μu, Σu, Σy, Wu, Wy\n\nend;","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"function generate_data(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Σu::Array{Float64,2}, Σy::Array{Float64,2}; seed::Int64 = 123)\n        \n    # create random data generator\n    rng = StableRNG(seed)\n    \n    # preallocate space for variables\n    z = Vector{Vector{Float64}}(undef, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n    u = rand(rng, MvNormal(μu, Σu), nr_samples)'\n    \n    # set initial value of latent states\n    z_prev = zeros(size(A,1))\n    \n    # generate data\n    for i in 1:nr_samples\n\n        # generate new latent state\n        z[i] = A * z_prev + B * u[i,:]\n\n        # generate new observation\n        y[i] = C * z[i] + rand(rng, MvNormal(zeros(dim_out), Σy))\n        \n        # generate new observation\n        z_prev .= z[i]\n        \n    end\n    \n    # return generated data\n    return z, y, u\n    \nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"# specify settings\nnr_samples = 200\ndim_out = 3\ndim_in = 3\ndim_lat = 25\n\n# generate parameters\nA, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(dim_out, dim_in, dim_lat);\n            \n# generate data\ndata_z, data_y, data_u = generate_data(nr_samples, A, B, C, μu, Σu, Σy);\n\n# visualise data\np = Plots.plot(xlabel = \"sample\", ylabel = \"observations\")\n# plot each dimension independently\nfor i in 1:dim_out\n    Plots.scatter!(p, getindex.(data_y, i), label = \"y_$i\", alpha = 0.5, ms = 2)\nend\np","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Model-specification","page":"Rts Vs Bifm Smoothing","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"@model function RTS_smoother(y, A, B, C, μu, Wu, Wy)\n    \n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    dim_out = size(C, 1)\n    \n    # set initial hidden state\n    z_prev ~ MvNormal(mean = zeros(dim_lat), precision = 1e-5*diagm(ones(dim_lat)))\n\n    # loop through observations\n    for i in eachindex(y)\n\n        # specify input as random variable\n        u[i] ~ MvNormal(mean = μu, precision = Wu)\n        \n        # specify updated hidden state\n        z[i] ~ A * z_prev + B * u[i]\n        \n        # specify observation\n        y[i] ~ MvNormal(mean = C * z[i], precision = Wy)\n        \n        # update last/previous hidden state\n        z_prev = z[i]\n\n    end\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"@model function BIFM_smoother(y, A, B, C, μu, Wu, Wy)\n\n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    \n    # set priors\n    z_prior ~ MvNormal(mean = zeros(dim_lat), precision = 1e-5*diagm(ones(dim_lat)))\n    z[1]  ~ BIFMHelper(z_prior)\n    \n    # loop through observations\n    for i in eachindex(y)\n\n        # specify input as random variable\n        u[i]   ~ MvNormal(mean = μu, precision = Wu)\n\n        # specify observation\n        yt[i]  ~ BIFM(u[i], z[i], new(z[i+1])) where { meta = BIFMMeta(A, B, C) }\n        y[i]   ~ MvNormal(mean = yt[i], precision = Wy)\n    end\n    \n    # set final value\n    z[end] ~ MvNormal(mean = zeros(dim_lat), precision = zeros(dim_lat, dim_lat))\nend\n\n@constraints function bifm_constraint()\n    q(z_prior,z) = q(z_prior)q(z)\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"bifm_constraint (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Probabilistic-inference","page":"Rts Vs Bifm Smoothing","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"function inference_RTS(data_y, A, B, C, μu, Wu, Wy)\n    \n    # In this task the inference is unstable and can diverge\n    meta = @meta begin \n        *() -> ReactiveMP.MatrixCorrectionTools.ClampSingularValues(tiny, Inf)\n    end\n    \n    result = infer(\n        model      = RTS_smoother(A = A, B = B, C = C, μu = μu, Wu = Wu, Wy = Wy),\n        data       = (y = data_y, ),\n        returnvars = (z = KeepLast(), u = KeepLast()),\n        meta = meta\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"inference_RTS (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"function inference_BIFM(data_y, A, B, C, μu, Wu, Wy)\n    result = infer(\n        model      = BIFM_smoother(A = A, B = B, C = C, μu = μu, Wu = Wu, Wy = Wy),\n        data       = (y = data_y, ),\n        constraints = bifm_constraint(),\n        returnvars = (z = KeepLast(), u = KeepLast())\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"inference_BIFM (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Experiments-for-200-observations","page":"Rts Vs Bifm Smoothing","title":"Experiments for 200 observations","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"z_BIFM, u_BIFM = inference_BIFM(data_y, A, B, C, μu, Wu, Wy)\nz_RTS, u_RTS = inference_RTS(data_y, A, B, C, μu, Wu, Wy);","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"ax1 = Plots.plot(title = \"RTS smoother\", xlabel = \"sample\", ylabel = \"latent state z\")\nax2 = Plots.plot(title = \"BIFM smoother\", xlabel = \"sample\", ylabel = \"latent state z\")\n\nmz_RTS = mean.(z_RTS)\nmz_BIFM = mean.(z_BIFM)\n\n# Do not plot all latent states, otherwise the output is just too cluttered\n# The main idea here is to check that both algorithms return the (approximately) same output\nfor i in 1:5\n    Plots.scatter!(ax1, getindex.(data_z, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)\n    Plots.plot!(ax1, getindex.(mz_RTS, i), label = nothing)\n    Plots.scatter!(ax2, getindex.(data_z, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)    \n    Plots.plot!(ax2, getindex.(mz_BIFM, i), label = nothing)\nend\n\nPlots.plot(ax1, ax2, layout = @layout([ a; b ]))","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"ax1 = Plots.plot(title = \"RTS smoother\", xlabel = \"sample\", ylabel = \"latent state u\")\nax2 = Plots.plot(title = \"BIFM smoother\", xlabel = \"sample\", ylabel = \"latent state u\")\n\nrdata_u = collect(eachrow(data_u))\nmu_RTS = mean.(u_RTS)\nmu_BIFM = mean.(u_BIFM)\n\n# Do not plot all latent states, otherwise the output is just too cluttered\n# The main idea here is to check that both algorithms return the (approximately) same output\nfor i in 1:1\n    Plots.scatter!(ax1, getindex.(rdata_u, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)\n    Plots.plot!(ax1, getindex.(mu_RTS, i), label = nothing)\n    Plots.scatter!(ax2, getindex.(rdata_u, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)    \n    Plots.plot!(ax2, getindex.(mu_BIFM, i), label = nothing)\nend\n\nPlots.plot(ax1, ax2, layout = @layout([ a; b ]))","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/#Benchmark","page":"Rts Vs Bifm Smoothing","title":"Benchmark","text":"","category":"section"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"# This example runs in our documentation pipeline, benchmark executes approximatelly in 20 minutes so we bypass it in the documentation\n# For those who are interested in exact benchmark numbers clone this example and set `run_benchmark = true`\nrun_benchmark = false\n\nif run_benchmark\n    trials_range = 30\n    trials_n = 500\n    trials_RTS  = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n    trials_BIFM = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n\n\n    @showprogress for k = 1 : trials_range\n\n        # generate parameters\n        local A, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(3, 3, k);\n                    \n        # generate data|\n        local data_z, data_y, data_u = generate_data(trials_n, A, B, C, μu, Σu, Σy);\n\n        # run inference\n        trials_RTS[k] = @benchmark inference_RTS($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n        trials_BIFM[k] = @benchmark inference_BIFM($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n\n    end\n\n    m_RTS = [median(trials_RTS[k].times) for k=1:trials_range] ./ 1e9\n    q1_RTS = [quantile(trials_RTS[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_RTS = [quantile(trials_RTS[k].times, 0.75) for k=1:trials_range] ./ 1e9\n    m_BIFM = [median(trials_BIFM[k].times) for k=1:trials_range] ./ 1e9\n    q1_BIFM = [quantile(trials_BIFM[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_BIFM = [quantile(trials_BIFM[k].times, 0.75) for k=1:trials_range] ./ 1e9;\n\n    p = Plots.plot(ylabel = \"duration [sec]\", xlabel = \"latent state dimension\", title = \"Benchmark\", yscale = :log)\n    p = Plots.plot!(p, m_RTS, ribbon = ((q1_RTS .- q3_RTS) ./ 2), color = \"blue\", label = \"mean (RTS)\")\n    p = Plots.plot!(p, 1:trials_range, m_BIFM, ribbon = ((q1_BIFM .- q3_BIFM) ./ 2), color = \"orange\", label = \"mean (BIFM)\")\n    Plots.savefig(p, \"rts_bifm_benchmark.png\")\n    p\nend","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/rts_vs_bifm_smoothing/","page":"Rts Vs Bifm Smoothing","title":"Rts Vs Bifm Smoothing","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/rts_vs_bifm_smoothing/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [91a5bcdd] Plots v1.40.9\n  [92933f4c] ProgressMeter v1.10.2\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<style>\n    :root {\n        --tag-bg-color: #f3f6f9;\n        --tag-text-color: #476582;\n        --category-text-color: #2c3e50;\n        --card-border-color: #e9ecef;\n        --card-bg-color: transparent;\n        --text-color: inherit;\n        --resources-bg-color: #f8f9fa;\n        --text-muted-color: #666;\n        --description-text-color: #476582;\n    }\n    \n    .theme--documenter-dark {\n        --tag-bg-color: #2d2d2d;\n        --tag-text-color: #9ecbff;\n        --category-text-color: #e6e6e6;\n        --card-border-color: #404040;\n        --card-bg-color: #1f1f1f;\n        --text-color: #e6e6e6;\n        --resources-bg-color: #1f1f1f;\n        --text-muted-color: #999;\n        --description-text-color: #9ecbff;\n    }\n</style>","category":"page"},{"location":"autogenerated/list_of_examples/#List-of-Examples","page":"List of Examples","title":"List of Examples","text":"","category":"section"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Welcome to our curated collection of RxInfer.jl examples! Here you'll find a comprehensive set of tutorials, demonstrations, and real-world applications that showcase the power and flexibility of RxInfer.jl.","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Each example comes with:","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"A detailed description of concepts covered\nRelevant tags for easy filtering\nComplete source code and explanations\nVisualizations and results analysis","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"note: Contributing\nThis gallery is community-driven and automatically generated from our repository. We welcome your contributions!Report a bug\nSubmit a pull request\nRead contribution guide\nRxInfer.jl respository","category":"page"},{"location":"autogenerated/list_of_examples/#External-Resources","page":"List of Examples","title":"External Resources","text":"","category":"section"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1.2em; border-radius: 8px; \n    background-color: var(--resources-bg-color, #f8f9fa); \n    border: 1px solid var(--card-border-color, #e9ecef);\">\n    <h4 style=\"margin: 0 0 1em 0; color: var(--category-text-color, #333);\">Community Tutorials & Guides</h4>\n    <ul style=\"margin: 0; padding-left: 1.2em;\">\n        <li style=\"margin-bottom: 0.8em;\">\n            <strong>Active Inference with RxInfer.jl</strong><br/>\n            <span style=\"color: var(--text-muted-color, #666);\">An in-depth exploration of Active Inference principles guided by \n            <a href=\"https://www.linkedin.com/in/kobusesterhuysen/\">Kobus Esterhuysen</a> at \n            <a href=\"https://learnableloop.com/#category=RxInfer\">Learnable Loop</a>.</span>\n        </li>\n        <li style=\"margin-bottom: 0.8em;\">\n            <strong>Video Tutorial Series</strong><br/>\n            <span style=\"color: var(--text-muted-color, #666);\">Comprehensive video tutorials covering RxInfer.jl's core concepts and applications by \n            <a href=\"https://www.youtube.com/@doggodotjl/search?query=RxInfer\">@doggotodjl</a>.</span>\n        </li>\n        <li style=\"margin-bottom: 0.8em;\">\n            <strong>Victor Flores blogpost</strong><br/>\n            <span style=\"color: var(--text-muted-color, #666);\">A collection of projects and examples with RxInfer (but not limited to) at \n            <a href=\"https://vflores-io.github.io/\">vflores-io</a>.</span>\n        </li>\n    </ul>\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<h2 style=\"margin-top: 2em; margin-bottom: 1em; color: var(--category-text-color, #2c3e50);\">\n    Basic Examples\n</h2>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: -0.5em 0 2em 0; color: var(--description-text-color, #476582);\">\n    Fundamental concepts and introductory tutorials. Start here if you're new to RxInfer.jl.\nThese examples cover basic probabilistic models, inference techniques, and data processing.\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Bayesian Binomial Regression","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            An introductory tutorial to Bayesian binomial regression with RxInfer. \n    Learn how to model binary outcomes using logistic regression with proper Bayesian inference.\n    The example demonstrates the use of Expectation Propagation (EP) algorithm and Polya-Gamma augmentation.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    multivariate\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    expectation propagation\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    polya-gamma\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Bayesian Linear Regression","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An extensive tutorial on Bayesian linear regression with RxInfer with a lot of examples, including multivariate and hierarchical linear regression.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    tutorial\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hierarchical model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    multivariate\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Bayesian Multinomial Regression","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n            An introductory tutorial to Bayesian multinomial regression with RxInfer. \n    Learn how to model categorical outcomes using multinomial regression with proper Bayesian inference.\n    The example demonstrates the use of Expectation Propagation (EP) algorithm and Polya-Gamma augmentation.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    multivariate\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    expectation propagation\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    polya-gamma\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Coin toss model (Beta-Bernoulli)","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of Bayesian inference in Beta-Bernoulli model with IID observations.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    conjugate model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    iid observations\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    beta bernoulli\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Feature Functions in Bayesian Regression","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of Bayesian inference in a parametric Gaussian regression model.\nBased on \"Probabilistic Numerics: Computation as Machine Learning\" by Hennig, Osborne and Kersting.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    probabilistic numerics\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basis functions\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    parametric\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"How to train your Hidden Markov Model","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of structured variational Bayesian inference in Hidden Markov Model with unknown transition and observational matrices.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hmm\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    structured inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    variational inference\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Kalman filtering and smoothing","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this demo, we are interested in Bayesian state estimation in different types of State-Space Models, including linear, nonlinear, and cases with missing observations\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    state space model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    kalman filter\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    missing data\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    nonlinear\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"POMDP Control with Reactive Inference","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example demonstrating how to perform control in Partially Observable Markov Decision Processes (POMDPs) using reactive message passing and variational inference.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    pomdp\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    control\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    structured inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    variational inference\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Predicting Bike Rental Demand","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An illustrative guide to implementing prediction mechanisms within RxInfer.jl, using bike rental demand forecasting as a contextual example.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    basic examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    prediction\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    time series\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    real data\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<h2 style=\"margin-top: 2em; margin-bottom: 1em; color: var(--category-text-color, #2c3e50);\">\n    Advanced Examples\n</h2>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: -0.5em 0 2em 0; color: var(--description-text-color, #476582);\">\n    More complex applications and advanced inference techniques. These examples demonstrate\nsophisticated models, performance optimization, and integration with other Julia packages.\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Active Inference Mountain car","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This notebooks covers RxInfer usage in the Active Inference setting for the simple mountain car problem.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    active inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    control\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    reinforcement learning\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Advanced Tutorial","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This notebook covers the fundamentals and advanced usage of the `RxInfer.jl` package.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    tutorial\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    fundamentals\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Assessing People's Skills","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        The demo is inspired by the example from Chapter 2 of Bishop's Model-Based Machine Learning book. We are going to perform an exact inference to assess the skills of a student given the results of the test.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    exact inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    educational\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    skill assessment\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Chance-Constrained Active Inference","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This notebook applies reactive message passing for active inference in the context of chance-constraints.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    active inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    constraints\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    control\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Conjugate-Computational Variational Message Passing (CVI)","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example provides an extensive tutorial for the non-conjugate message-passing based inference by exploiting the local CVI approximation.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    cvi\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    non conjugate\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    variational inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    tutorial\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Drone Dynamics","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows how to use RxInfer.jl automated inference to simulate drone dynamics.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Global Parameter Optimisation","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows how to use RxInfer.jl automated inference within other optimisation packages such as Optim.jl.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    optimization\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    parameter estimation\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    integration\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Solve GP regression by SDE","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this notebook, we solve a GP regression problem by using 'Stochastic Differential Equation' (SDE). This method is well described in the dissertation 'Stochastic differential equation methods for spatio-temporal Gaussian process regression.' by Arno Solin and 'Sequential Inference for Latent Temporal Gaussian Process Models' by Jouni Hartikainen.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    gaussian process\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    sde\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    regression\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    state space model\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Infinite Data Stream","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows RxInfer capabilities of running inference for infinite time-series data.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    streaming\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    online inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    time series\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Multi-agent Trajectory Planning","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example shows how to plan multi-agents' trajectories while avoiding obstacles and collisions between agents.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Nonlinear Sensor Fusion","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        Nonlinear object position identification using a sparse set of sensors\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    sensor fusion\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    nonlinear\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    sparse data\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Robotic Arm","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example explores how RxInfer.jl's automated inference can be applied to path planning for a robotic arm and demonstrates how probabilistic inference enables smooth and efficient motion planning. Ideal for those interested in robotics, Bayesian inference, and intelligent control systems.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    advanced examples\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    robotics\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    probabilistic inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    path planning\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    Bayesian methods\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<h2 style=\"margin-top: 2em; margin-bottom: 1em; color: var(--category-text-color, #2c3e50);\">\n    Problem Specific\n</h2>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: -0.5em 0 2em 0; color: var(--description-text-color, #476582);\">\n    Real-world applications and domain-specific models. These examples show how RxInfer.jl\ncan be applied to specific problems like time series analysis and signal processing.\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Autoregressive Models","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of Bayesian treatment of latent AR and ARMA models. Reference: [Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models](https://www.mdpi.com/1099-4300/23/6/683).\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    time series\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    arma\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    latent variables\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Gamma Mixture Model","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example implements one of the Gamma mixture experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/ .\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    mixture model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    gamma distribution\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    clustering\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Gaussian Mixture","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example implements variational Bayesian inference in univariate and multivariate Gaussian mixture models with mean-field assumption.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    mixture model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    gaussian\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    mean field\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    clustering\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Hierarchical Gaussian Filter","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of online inference procedure for Hierarchical Gaussian Filter with univariate noisy observations using Variational Message Passing algorithm. Reference: [Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter](https://ieeexplore.ieee.org/document/9173980).\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hierarchical model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    online inference\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    filtering\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Invertible neural networks: a tutorial","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of variational Bayesian Inference with invertible neural networks. Reference: Bart van Erp, Hybrid Inference with Invertible Neural Networks in Factor Graphs.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    neural networks\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    invertible networks\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    hybrid inference\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Litter Model","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        Using Bayesian Inference and RxInfer to estimate daily litter events (adapted from https://learnableloop.com/posts/LitterModel_PORT.html)\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    real data\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    time series\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    event modeling\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"ODE Parameter Estimation","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        An example of solving Lotka Volterra ODE with RxInfer.jl. Reference: [Lotka Volterra ODE](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations).\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    ode\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    differential equations\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Probit Model (EP)","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    expectation propagation\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    probit\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    state space model\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"RTS vs BIFM Smoothing","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        This example performs BIFM Kalman smoother on a factor graph using message passing and compares it with the RTS implementation.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    smoothing\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    kalman filter\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    comparison\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Simple Nonlinear Node","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this example we create a non-conjugate model and use a nonlinear link function between variables. We show how to extend the functionality of `RxInfer` and to create a custom factor node with arbitrary message passing update rules.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    nonlinear\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    custom node\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    message passing\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Structural Dynamics with Augmented Kalman Filter","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        In this example, we estimate system states and unknown input forces for a simple **structural dynamical system** using the Augmented Kalman Filter (AKF) (https://www.sciencedirect.com/science/article/abs/pii/S0888327011003931) in **RxInfer**.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    kalman filter\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    structural dynamics\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    state estimation\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"<div style=\"margin: 1.5em 0; padding: 1em 1.2em; border-radius: 8px; \n    border: 1px solid var(--card-border-color, #e9ecef);\n    background: var(--card-bg-color, transparent);\">\n    <h3 style=\"margin: 0 0 0.6em 0;\">","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"Universal Mixtures","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"    </h3>\n    <p style=\"margin: 0 0 1em 0; line-height: 1.6; color: var(--text-color, inherit);\">\n        Universal mixture modeling.\n\n    </p>\n    <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    problem specific\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    mixture model\n</span>\n <span style=\"display: inline-block; padding: 3px 7px; margin: 2px; \n    border-radius: 3px; font-size: 0.9em;\n    background: var(--tag-bg-color, #f3f6f9);\n    color: var(--tag-text-color, #476582);\">\n    universal approximation\n</span>\n\n</div>","category":"page"},{"location":"autogenerated/list_of_examples/","page":"List of Examples","title":"List of Examples","text":"note: Contributing\nThis gallery is community-driven and automatically generated from our repository. We welcome your contributions!Report a bug\nSubmit a pull request\nRead contribution guide\nRxInfer.jl respository","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"","category":"page"},{"location":"categories/problem_specific/litter_model/#Litter-Model","page":"Litter Model","title":"Litter Model","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Adapted from LearnableLoopAI","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"using RxInfer, Random, Distributions, Plots, LaTeXStrings, XLSX, DataFrames","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"In this project the client is responsible for the delittering of a mile-long beach walk-way in the Pacific Northwest in the USA. The density of foot traffic is roughly uniform along its length. Volunteers provide their services for cleaning up litter.","category":"page"},{"location":"categories/problem_specific/litter_model/#Symbols-Nomenclature-Notation-(KUF)","page":"Litter Model","title":"Symbols | Nomenclature |Notation (KUF)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"informed by Powell Universal Framework (PUF), Bert de Vries, AIF literature","category":"page"},{"location":"categories/problem_specific/litter_model/#Taxonomy-of-Machine-Learning","page":"Litter Model","title":"Taxonomy of Machine Learning","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#Supervised-Learning","page":"Litter Model","title":"Supervised Learning","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#State-Functions","page":"Litter Model","title":"State Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Provision (Acquisition)\nmathbfp_i = f_p(i)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Functions","page":"Litter Model","title":"Observation Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Response: mathbfr_i = f_r(brevemathbfs_i)\nObservation with Noise: mathbfy_i = mathbfbreves_t + mathbfv_t\nCovariate noise (optional)\nObservation noise: mathbfv_i = mathcalN(mathbfbrevem_i mathbfbreveSigma_V)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Sets","page":"Litter Model","title":"Observation Sets","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Without state noise: (mathbfbreves y)\nWith state noise: (mathbfz y)","category":"page"},{"location":"categories/problem_specific/litter_model/#Unsupervised-Learning","page":"Litter Model","title":"Unsupervised Learning","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#State-Functions-2","page":"Litter Model","title":"State Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Provision (Acquisition)\nmathbfp_i = f_p(i)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Functions-2","page":"Litter Model","title":"Observation Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Response: mathbfr_i = f_r(brevemathbfs_i)\nDirect Observation: mathbfy_t = mathbfbreves_t","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Sets-2","page":"Litter Model","title":"Observation Sets","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Unordered independent observations: y","category":"page"},{"location":"categories/problem_specific/litter_model/#Sequential/Series-Learning","page":"Litter Model","title":"Sequential/Series Learning","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#State-Functions-3","page":"Litter Model","title":"State Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Provision (Transition)\nBase transition: mathbfp_t = f_p(brevemathbfs_t-1)\nComplete state equation:   mathbfbreves_t = f_B(mathbfbreves_t-1)+ f_E(mathbfa_t) + mathbfw_dt + mathbfw_t\nComponents:\nAction: f_E(mathbfa_t) (optional)\nSystem noise: mathbfw_t = mathcalN(mathbfp_t mathbfbreveSigma_W)\nDisturbance/exogenous: mathbfw_dt (optional)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Functions-3","page":"Litter Model","title":"Observation Functions","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Response: mathbfr_t = f_r(brevemathbfs_t) = f_A(brevemathbfs_t) = brevemathbfA brevemathbfs_t\nObservation with Noise: mathbfy_t = f_A(mathbfbreves_t) + mathbfv_t\nObservation noise: mathbfv_t = mathcalN(mathbfr_t mathbfbreveSigma_V)","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-Sequence","page":"Litter Model","title":"Observation Sequence","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Ordered correlated observations y (time/spatial)","category":"page"},{"location":"categories/problem_specific/litter_model/#Overall-Structure","page":"Litter Model","title":"Overall Structure","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Experiment has one-to-many Batches","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"- Batch (into the page) has one-to-many Sequences\n\t- Sequence (down the page) has one-to-many Datapoints\n\t\t- Datapoint (into the page) has one-to-many Matrices\n\t\t\t- Matrix (down the page) has one-to-many Vectors\n\t\t\t\t- Vector (towards right) has one-to-many Components\n\t\t\t\t\t- Component/Element of type\n\t\t\t\t\t\t- Numerical [continuous/proportional]\n\t\t\t\t\t\t\t- int/real/float (continuous)\n\t\t\t\t\t\t- Categorical [non-continuous/non-formal]\n\t\t\t\t\t\t\t- AIF calls it 'discrete'\n\t\t\t\t\t\t\t- ordinal (ordered)\n\t\t\t\t\t\t\t- nominal (no order)\n\t\t\t\t\t\t- for computers, elements need to be numbers, so categoricals encoded as numbers too","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Most complex Datapoint handled is a multispectral image, i.e. 3D","category":"page"},{"location":"categories/problem_specific/litter_model/#True-vs-Inferred-variables:","page":"Litter Model","title":"True vs Inferred variables:","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"True variables associated with Generative Process genpr\ne.g. breves brevemathbfs brevetheta\nInferred variables associated with Generative Model agent\ne.g. s mathbfs theta","category":"page"},{"location":"categories/problem_specific/litter_model/#General","page":"Litter Model","title":"General","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Global code variables will be prefixed with an underscore '_'.","category":"page"},{"location":"categories/problem_specific/litter_model/#Active-Inference:-Bridging-Minds-and-Machines","page":"Litter Model","title":"Active Inference: Bridging Minds and Machines","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"In recent years, the landscape of machine learning has undergone a profound transformation with the emergence of active inference, a novel paradigm that draws inspiration from the principles of biological systems to inform intelligent decision-making processes. Unlike traditional approaches to machine learning, which often passively receive data and adjust internal parameters to optimize performance, active inference represents a dynamic and interactive framework where agents actively engage with their environment to gather information and make decisions in real-time.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"At its core, active inference is rooted in the notion of agents as embodied entities situated within their environments, constantly interacting with and influencing their surroundings. This perspective mirrors the fundamental processes observed in living organisms, where perception, action, and cognition are deeply intertwined to facilitate adaptive behavior. By leveraging this holistic view of intelligence, active inference offers a unified framework that seamlessly integrates perception, decision-making, and action, thereby enabling agents to navigate complex and uncertain environments more effectively.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"One of the defining features of active inference is its emphasis on the active acquisition of information. Rather than waiting passively for sensory inputs, agents proactively select actions that are expected to yield the most informative outcomes, thus guiding their interactions with the environment. This active exploration not only enables agents to reduce uncertainty and make more informed decisions but also allows them to actively shape their environments to better suit their goals and objectives.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Furthermore, active inference places a strong emphasis on the hierarchical organization of decision-making processes, recognizing that complex behaviors often emerge from the interaction of multiple levels of abstraction. At each level, agents engage in a continuous cycle of prediction, inference, and action, where higher-level representations guide lower-level processes while simultaneously being refined and updated based on incoming sensory information.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The applications of active inference span a wide range of domains, including robotics, autonomous systems, neuroscience, and cognitive science. In robotics, active inference offers a promising approach for developing robots that can adapt and learn in real-time, even in unpredictable and dynamic environments. In neuroscience and cognitive science, active inference provides a theoretical framework for understanding the computational principles underlying perception, action, and decision-making in biological systems.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"In conclusion, active inference represents a paradigm shift in machine learning, offering a principled and unified framework for understanding and implementing intelligent behavior in artificial systems. By drawing inspiration from the principles of biological systems, active inference holds the promise of revolutionizing our approach to building intelligent machines and understanding the nature of intelligence itself.","category":"page"},{"location":"categories/problem_specific/litter_model/#Business-Understanding","page":"Litter Model","title":"Business Understanding","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Although the current project covers a small part of the span of Active Inference, we would nevertheless like to execute it within this context.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The client is responsible for the delittering of a mile-long beach walk-way in the Pacific Northwest in the USA. The density of foot traffic is roughly uniform along its length. Volunteers provide their services for cleaning up litter. One of the key determinants of the client's planning is an estimation of the number of daily litter events along this walkway. The client does not want to over-engage his team of volunteers, nor does he want litter to become too noticeable.","category":"page"},{"location":"categories/problem_specific/litter_model/#Data-Understanding","page":"Litter Model","title":"Data Understanding","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The number of daily litter events will be modeled by a Poisson distribution with parameter theta. This parameter, usually denoted by lambda, represents both the mean as well as the variance of the Poisson distribution. The theta parameter will be learned or inferred by a model.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"For additional insight, we will simulate some litter event data.","category":"page"},{"location":"categories/problem_specific/litter_model/#Data-Preparation","page":"Litter Model","title":"Data Preparation","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"We will use simulated data to prepare the model. To apply the model we will use data gathered from observations along the walk-way. There is no need to perform additional data preparation.","category":"page"},{"location":"categories/problem_specific/litter_model/#Modeling","page":"Litter Model","title":"Modeling","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#Core-Elements","page":"Litter Model","title":"Core Elements","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"This section attempts to answer three important questions:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"What metrics are we going to track?\nWhat decisions do we intend to make?\nWhat are the sources of uncertainty?","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"For this problem, the only metric we are interested in is the daily number of litter events so that we can use Bayesian inference to estimate the mean of the Poisson distribution that that represents the littering events.","category":"page"},{"location":"categories/problem_specific/litter_model/#Environment-Model-(Generative-Process)","page":"Litter Model","title":"Environment Model (Generative Process)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The number of daily litter events will be given by $ n^{Daily} \\sim Pois(\\theta) $","category":"page"},{"location":"categories/problem_specific/litter_model/#State-variables","page":"Litter Model","title":"State variables","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"We do not have state variables. The only variable that needs to be inferred is theta, the mean (and variance) of the generative process, i.e. the Poisson distribution.","category":"page"},{"location":"categories/problem_specific/litter_model/#Decision-variables","page":"Litter Model","title":"Decision variables","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"There will be no decision variables for this project.","category":"page"},{"location":"categories/problem_specific/litter_model/#Exogenous-information-variables","page":"Litter Model","title":"Exogenous information variables","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"We assume that the volunteers that inspect the walk-way do not miscount litter events. Consequently we will not make provision for exogenous information variables.","category":"page"},{"location":"categories/problem_specific/litter_model/#Next-State-function","page":"Litter Model","title":"Next State function","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The provision function, f_p(), provides another state/datapoint, called the provision/pre-state. Because this is a combinatorial system, the provision function acquires the next state/datapoint making use of a simulation or a data set.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"mathbfp_i = f_p(i)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## provision function, provides another state/datapoint from simulation\nfunction fˢⁱᵐₚ(s; θ̆, 𝙼, 𝚅, 𝙲, rng)\n    dp = Vector{Vector{Vector{Float64}}}(undef, 𝙼)\n    for m in 1:𝙼 ## Matrices\n        dp[m] = Vector{Vector{Float64}}(undef, 𝚅)\n        for v in 1:𝚅 ## Vectors\n            dp[m][v] = Vector{Float64}(undef, 𝙲)\n            for c in 1:𝙲 ## Components\n               dp[m][v][c] = float(rand(rng, Poisson(θ̆)))\n            end\n        end\n    end\n    s̆ = dp\n    return s̆\nend\n\n_s = 1 ## s for sequence\n_θ̆ˢⁱᵐ = 15 ## lambda of Poisson distribution\n_rng = MersenneTwister(57)\n## _s̆ = fˢⁱᵐₚ(_s, θ̆=_θ̆ˢⁱᵐ, 𝙼=3, 𝚅=4, 𝙲=5, rng=_rng) ## color image with 3 colors, 4 rows, 5 cols of elements\n## _s̆ = fˢⁱᵐₚ(_s, θ̆=_θ̆ˢⁱᵐ, 𝙼=1, 𝚅=4, 𝙲=5, rng=_rng) ## b/w image with 4 rows, 5 cols of elements\n_s̆ = fˢⁱᵐₚ(_s, θ̆=_θ̆ˢⁱᵐ, 𝙼=1, 𝚅=1, 𝙲=5, rng=_rng) ## vector with 5 elements\n## _s̆ = fˢⁱᵐₚ(_s, θ̆=_θ̆ˢⁱᵐ, 𝙼=1, 𝚅=1, 𝙲=1, rng=_rng) ## vector with 1 element\n;","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## provision function, provides another state/datapoint from field\nfunction fᶠˡᵈₚ(s; 𝙼, 𝚅, 𝙲, df)\n    dp = Vector{Vector{Vector{Float64}}}(undef, 𝙼)\n    for m in 1:𝙼 ## Matrices\n        dp[m] = Vector{Vector{Float64}}(undef, 𝚅)\n        for v in 1:𝚅 ## Vectors\n            dp[m][v] = Vector{Float64}(undef, 𝙲)\n            for c in 1:𝙲 ## Components\n                dp[m][v][c] = df[s, :incidents]\n            end\n        end\n    end\n    s̆ = dp\n    return s̆\nend\n## _s = 1 ## s for sequence\n## dp = fᶠˡᵈₚ(_s, 𝙼=3, 𝚅=4, 𝙲=5, df=_fld_df) ## color image with 3 colors, 4 rows, 5 cols of elements\n## dp = fᶠˡᵈₚ(_s, 𝙼=1, 𝚅=4, 𝙲=5, df=_fld_df) ## b/w image with 4 rows, 5 cols of elements\n## dp = fᶠˡᵈₚ(_s, 𝙼=1, 𝚅=1, 𝙲=5, df=_fld_df) ## vector with 5 elements\n## dp = fᶠˡᵈₚ(_s, 𝙼=1, 𝚅=1, 𝙲=1, df=_fld_df) ## vector with 1 element","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"fᶠˡᵈₚ (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Because there is no noise to be combined with, the next state becomes","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"brevemathbfs_i = mathbfp_i","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The breve/bowl indicates that the parameters and variables are hidden and not observed.","category":"page"},{"location":"categories/problem_specific/litter_model/#Observation-function","page":"Litter Model","title":"Observation function","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The response function, f_r(), provides the response to the state/datapoint, called the response: mathbfr_i = f_r(brevemathbfs_i)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## response function, provides the response to a state/datapoint\nfunction fᵣ(s̆)\n    return s̆ ## no noise\nend\nfᵣ(_s̆);","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Because there is no noise to be combined with, the next observation becomes","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"mathbfy_i = mathbfbreves_i","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The breve/bowl indicates that the parameters and variables are hidden and not observed.","category":"page"},{"location":"categories/problem_specific/litter_model/#Implementation-of-the-Environment-Model-(Generative-Process)","page":"Litter Model","title":"Implementation of the Environment Model (Generative Process)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Let's simulate some data with IID observations from a Poisson distribution, that represents the litter incidents. We also assume that the mean incidents per day is 15:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## Data comes from either a simulation/lab (sim|lab) OR from the field (fld)\n## Data are handled either in batches (batch) OR online as individual points (point)\nfunction sim_data(rng, 𝚂, 𝙳, 𝙼, 𝚅, 𝙲, θ̆)\n    p = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    s̆ = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    r = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    y = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    for s in 1:𝚂 ## sequences\n        p[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        s̆[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        r[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        y[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        for d in 1:𝙳 ## datapoints\n            p[s][d] = fˢⁱᵐₚ(s; θ̆=θ̆, 𝙼=𝙼, 𝚅=𝚅, 𝙲=𝙲, rng=rng)\n            s̆[s][d] = p[s][d] ## no system noise\n            r[s][d] = fᵣ(s̆[s][d])\n            y[s][d] = r[s][d]\n        end\n    end\n    return y\nend;\n\nfunction fld_data(df, 𝚂, 𝙳, 𝙼, 𝚅, 𝙲)\n    p = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    s̆ = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    r = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    y = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, 𝚂)\n    for s in 1:𝚂 ## sequences\n        p[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        s̆[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        r[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        y[s] = Vector{Vector{Vector{Vector{Float64}}}}(undef, 𝙳)\n        for d in 1:𝙳 ## datapoints\n            p[s][d] = fᶠˡᵈₚ(s; 𝙼=𝙼, 𝚅=𝚅, 𝙲=𝙲, df=df)\n            s̆[s][d] = p[s][d] ## no system noise\n            r[s][d] = fᵣ(s̆[s][d])\n            y[s][d] = r[s][d]\n        end\n    end\n    return y\nend;","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## number of Batches in an experiment\n## _𝙱 = 1 ## not used yet\n\n## number of Sequences/examples in a batch\n_𝚂 = 365\n## _𝚂 = 3\n\n## number of Datapoints in a sequence\n_𝙳 = 1\n## _𝙳 = 2\n## _𝙳 = 3\n\n## number of Matrices in a datapoint\n_𝙼 = 1\n\n## number of Vectors in a matrix\n_𝚅 = 1\n\n## number of Components in a vector\n_𝙲 = 1\n\n_θ̆ˢⁱᵐ = 15 ## hidden lambda of Poisson distribution\n_rng = MersenneTwister(57);","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_yˢⁱᵐ = sim_data(_rng, _𝚂, _𝙳, _𝙼, _𝚅, _𝙲, _θ̆ˢⁱᵐ) ## simulated data\n_yˢⁱᵐ = first.(first.(first.(first.(_yˢⁱᵐ))));","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## methods(print)\n## print(_yˢⁱᵐ[1:2])\n\n## Customize the display width to control positioning or prevent wrapping\n## io = IOContext(stdout, :displaysize => (50, 40)) ## (rows, cols)\n## print(io, _yˢⁱᵐ[1:3])\n## print(io, _yˢⁱᵐ)\n\nprint(IOContext(stdout, :displaysize => (24, 5)), _yˢⁱᵐ[1:10]);","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"[8.0, 10.0, 14.0, 9.0, 15.0, 9.0, 12.0, 15.0, 19.0, 18.0]","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_rθ = range(0, _𝚂, length=1*_𝚂)\n_p = plot(title=\"Simulated Daily Litter Events\", xlabel=\"Day\")\n_p = plot!(_rθ, _yˢⁱᵐ, linetype=:steppre, label=\"# daily events\", c=1)\nplot(_p)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/litter_model/#Uncertainty-Model","page":"Litter Model","title":"Uncertainty Model","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#Agent-Model-(Generative-Model)","page":"Litter Model","title":"Agent Model (Generative Model)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"In this project, we are going to perform an exact inference for a litter model that can be represented as:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"beginaligned\np(theta) = mathrmGamma(theta mid alpha^Gamma theta^Gamma)\np(x_i mid theta) = mathrmPois(x_i mid theta)\nendaligned","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"where x_i in 0 1  is an observation induced by a Poisson likelihood while p(theta) is a Gamma prior distribution on the parameter of the Poisson distribution. We are interested in inferring the posterior distribution of theta.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The generative model is:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"$","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"\\begin{aligned} p(x:,\\theta)    &= p(x: \\mid \\theta) \\cdot p(\\theta) \\\n                 &= p(x{1:N} \\mid \\theta) \\cdot p(\\theta) \\\n                 &= \\prod{i=1}^N{p(xi \\mid \\theta)} \\cdot p(\\theta) \\\n                 &= \\prod{i=1}^N{\\mathrm{Pois}(x_i \\mid \\theta)} \\cdot \\Gamma(\\theta \\mid \\alpha^{\\Gamma}, \\theta^{\\Gamma}) \\end{aligned} $","category":"page"},{"location":"categories/problem_specific/litter_model/#Implementation-of-the-Agent-Model-(Generative-Model)","page":"Litter Model","title":"Implementation of the Agent Model (Generative Model)","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"We will use the RxInfer Julia package. RxInfer stands at the forefront of Bayesian inference tools within the Julia ecosystem, offering a powerful and versatile platform for probabilistic modeling and analysis. Built upon the robust foundation of the Julia programming language, RxInfer provides researchers, data scientists, and practitioners with a streamlined workflow for conducting Bayesian inference tasks with unprecedented speed and efficiency.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"At its core, RxInfer leverages cutting-edge techniques from the realm of reactive programming to enable dynamic and interactive model specification and estimation. This unique approach empowers users to define complex probabilistic models with ease, seamlessly integrating prior knowledge, data, and domain expertise into the modeling process.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"With RxInfer, conducting Bayesian inference tasks becomes a seamless and intuitive experience. The package offers a rich set of tools for performing parameter estimation, model comparison, and uncertainty quantification, all while leveraging the high-performance capabilities of Julia to deliver results in a fraction of the time required by traditional methods.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Whether tackling problems in machine learning, statistics, finance, or any other field where uncertainty reigns supreme, RxInfer equips users with the tools they need to extract meaningful insights from their data and make informed decisions with confidence.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"RxInfer represents a paradigm shift in the world of Bayesian inference, combining the expressive power of Julia with the flexibility of reactive programming to deliver a state-of-the-art toolkit for probabilistic modeling and analysis. With its focus on speed, simplicity, and scalability, RxInfer is poised to become an indispensable tool for researchers and practitioners seeking to harness the power of Bayesian methods in their work.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"To transfer the above factorized generative model to the RxInfer package, we need to include each of the factors:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"N\nKronecker-delta factors (for the N observations)\n1\nGamma factor (for the prior distribution)\nN\nPoisson factors (for the litter events)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## parameters for the prior distribution\n_αᴳᵃᵐ, _θᴳᵃᵐ = 350., .05;","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"## Litter model: Gamma-Poisson\n@model function litter_model(x, αᴳᵃᵐ, θᴳᵃᵐ)\n    ## prior on θ parameter of the model\n    θ ~ Gamma(shape=αᴳᵃᵐ, rate=θᴳᵃᵐ) ## 1 Gamma factor\n\n    ## assume daily number of litter incidents is a Poisson distribution\n    for i in eachindex(x)\n        x[i] ~ Poisson(θ) ## not θ̃; N Poisson factors\n    end\nend","category":"page"},{"location":"categories/problem_specific/litter_model/#Agent-(Policy)-Evaluation","page":"Litter Model","title":"Agent (Policy) Evaluation","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/#Evaluate-with-simulated-data","page":"Litter Model","title":"Evaluate with simulated data","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_result = infer(\n    model= litter_model(αᴳᵃᵐ= _αᴳᵃᵐ, θᴳᵃᵐ= _θᴳᵃᵐ), \n    data= (x= _yˢⁱᵐ, )\n)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_θˢⁱᵐ = _result.posteriors[:θ]","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"ExponentialFamily.GammaShapeRate{Float64}(a=5838.0, b=365.05)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_rθ = range(0, 20, length=500)\n_p = plot(title=\"Simulation results: Distribution of \"*L\"θ^{\\mathrm{sim}}=λ\")\nplot!(_rθ, (x) -> pdf(Gamma(_αᴳᵃᵐ, _θᴳᵃᵐ), x), fillalpha=0.3, fillrange=0, label=\"P(θ)\", c=1,)\nplot!(_rθ, (x) -> pdf(_θˢⁱᵐ, x), fillalpha=0.3, fillrange=0, label=\"P(θ|x)\", c=3)\nvline!([_θ̆ˢⁱᵐ], label=\"Hidden θ\", c=2)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/litter_model/#Evaluation","page":"Litter Model","title":"Evaluation","text":"","category":"section"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The following data comes from the inspections of the volunteers over a period of 12 months:","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_fld_df = DataFrame(XLSX.readtable(\"litter_incidents.xlsx\", \"Sheet1\"))\n_yᶠˡᵈ = fld_data(_fld_df, _𝚂, _𝙳, _𝙼, _𝚅, _𝙲) ## field data\n_yᶠˡᵈ = first.(first.(first.(first.(_yᶠˡᵈ))))\nprint(IOContext(stdout, :displaysize => (24, 30)), _yᶠˡᵈ[1:10]);","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"[5.0, 7.0, 6.0, 10.0, 8.0, 5.0, 7.0, 9.0, 13.0, 9.0]","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_rθ = range(0, _𝚂, length=1*_𝚂)\n_p = plot(title=\"Field Daily Litter Events\", xlabel=\"Day\")\n_p = plot!(_rθ, _yᶠˡᵈ, linetype=:steppre, label=\"# daily events\", c=1)\nplot(_p)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_result = infer(\n    model=litter_model(αᴳᵃᵐ= _αᴳᵃᵐ, θᴳᵃᵐ= _θᴳᵃᵐ), \n    data= (x= _yᶠˡᵈ, )\n)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_θᶠˡᵈ = _result.posteriors[:θ]","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"ExponentialFamily.GammaShapeRate{Float64}(a=3200.0, b=365.05)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"_rθ = range(0, 20, length=500)\n_p = plot(title=\"Field results: Distribution of \"*L\"θ^{\\mathrm{fld}}=λ\")\nplot!(_rθ, (x) -> pdf(Gamma(_αᴳᵃᵐ, _θᴳᵃᵐ), x), fillalpha=0.3, fillrange=0, label=\"P(θ)\", c=1,)\nplot!(_rθ, (x) -> pdf(_θᶠˡᵈ, x), fillalpha=0.3, fillrange=0, label=\"P(θ|x)\", c=3)","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"The actual generative process actually had a much lower mean daily litter events, around about 8 events per day. The client can work with this value during planning of how to use his volunteers in the field.","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/litter_model/","page":"Litter Model","title":"Litter Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/litter_model/Project.toml`\n  [a93c6f00] DataFrames v1.7.0\n  [31c24e10] Distributions v0.25.117\n  [b964fa9f] LaTeXStrings v1.4.0\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [fdbf4ff8] XLSX v0.10.4\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#ODE-Parameter-Estimation","page":"Ode Parameter Estimation","title":"ODE Parameter Estimation","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"In this notebook we will explore how we can solve and learn the parameters of an ODE simultaneously using RxInfer. To illustrate how we can can utilize RxInfer, we will take Lotka-Volterra differential equation as an example. We will explore three different alternatives to parameter estimation. The first alternative will demonstrate how we can use free energy to obtain point estimates. The second alternative will demonstrate how we can use a prior distribution on the parameters to obtain a posterior estimate for the unknown parameters of the ODE. The second alternative will do parameter learning in two stages. The first stage will obtain the initialization for the prior hyper-parameters and then use these initial values of the prior to obtain the posterior by message passing. The third alternative will use purely message passing. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"using RxInfer, Optim, LinearAlgebra, Plots, SeeToDee, StaticArrays, StableRNGs","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Introduction-to-Lotka-Volterra-Equations","page":"Ode Parameter Estimation","title":"Introduction to Lotka-Volterra Equations","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"The Lotka-Volterra equations, are a pair of first-order nonlinear differential equations frequently used to describe the dynamics of biological systems in which two species interact: one as a predator and the other as prey. The equations are defined as follows:","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Prey Population Dynamics:   fracdxdt = alpha x - beta xy","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Predator Population Dynamics:   fracdydt = -gamma y + delta xy","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"In this ODE, x is the population of the prey (e.g., rabbits), y is the population of the predator (e.g., foxes), alpha represents the maximum growth rate of the prey, beta is the rate of predation, gamma is the predator's per capita death rate and delta is the growth rate of the predator population based on the availability of prey.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function lotka_volterra(u, z, p, t)\n    α, β, δ, γ = p[SA[1,2,3,4]]\n    x, y = u[SA[1, 2]]\n    du1 = α * x - β * x * y\n    du2 = -δ * y + γ * x * y\n\n    return [du1, du2]\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#The-Runge-Kutta-4th-Order-(RK4)-Method","page":"Ode Parameter Estimation","title":"The Runge-Kutta 4th Order (RK4) Method","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"The Runge-Kutta 4th order method is one of the most widely used numerical techniques for solving ordinary differential equations (ODEs). For a system of the form:","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"fracdxdt = f(x t)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"where x can be a scalar or vector-valued function, RK4 provides a numerical approximation with local truncation error of order O(h^5) and global error of order O(h^4).","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Algorithm","page":"Ode Parameter Estimation","title":"Algorithm","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Given the current state x_n at time t_n, RK4 computes the state at t_n+1 = t_n + dt using four intermediate evaluations:","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"beginaligned\nk_1 = f(x_n t_n) \nk_2 = f(x_n + fracdt2k_1 t_n + fracdt2) \nk_3 = f(x_n + fracdt2k_2 t_n + fracdt2) \nk_4 = f(x_n + dtk_3 t_n + dt)\nendaligned","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"The solution is then advanced using a weighted average of these evaluations:","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"x_n+1 = x_n + fracdt6(k_1 + 2k_2 + 2k_3 + k_4)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"For this implementation, we will use the SeeToDee package to define the RK4 method. This is necessary to create a non-linear deterministic node for the RxInfer model. SeeToDee package requires the dynamics function to be defined as f(x, u, θ, t), where u is the control input. Since we don't have any control input, we will set u = 0.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"NOTE: There are many improved versions of solvers that can be more appropriate. For this simple problem though RK4 will be enough to convey the message. However, in practice for real world problems adaptive or implicit variants of solvers are preferred. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"dt = 0.1 # sample_interval\n\nfunction lotka_volterra_rk4(x, θ, t, dt)\n    lotka_volterra_dynamics = SeeToDee.Rk4(lotka_volterra, dt)\n    return lotka_volterra_dynamics(x, 0, θ, t)\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"lotka_volterra_rk4 (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Data-Generation","page":"Ode Parameter Estimation","title":"Data Generation","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Lotka Volterra data is generated using the RK4 method. The data is then corrupted with noise to simulate real-world observations. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"DISCLAIMER: Since Lotka-Volterra equations model prey and predator dynamics, adding a Gaussian noise is not realistic. Although adding other noise forms are possible it will complicate the inference process. Therefore, we will use Gaussian noise for instructive purposes. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function generate_data(θ; x = ones(2), t =0.0, dt = 0.001, n = 1000, v = 1, seed = 123)\n    rng = StableRNG(seed)\n    data = Vector{Vector{Float64}}(undef, n)\n    ts = Vector{Float64}(undef, n)\n    for i in 1:n\n        data[i] = lotka_volterra_rk4(x, θ, t, dt)\n        x = data[i]\n        t += dt\n        ts[i] = t\n    end\n    noisy_data = map(data) do d\n        noise = sqrt(v) * [randn(rng), randn(rng)]\n        d + noise\n    end\n    return data, noisy_data, ts\nend\n\nnoisev = 0.35\nn = 10000\ntrue_params = [1.0, 1.5, 3.0, 1.0]\ndata_long, noisy_data_long, ts_long = generate_data(true_params,dt = dt, n = n, v = noisev);\n\n## We create a smaller dataset for the global parameter optimization. Utilizing the entire dataset for the global optimization will take too much time. \nn_train = 100\ndata = data_long[1:n_train]\nnoisy_data = noisy_data_long[1:n_train]\nts = ts_long[1:n_train];","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Data-Visualization","page":"Ode Parameter Estimation","title":"Data Visualization","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"p = plot(layout=(2,1))\nplot!(subplot=1, ts, [d[1] for d in data], label=\"True x₁\", color=:blue)\nplot!(subplot=1, ts, [d[1] for d in noisy_data], seriestype=:scatter, label=\"Noisy x₁\", color=:blue, alpha=0.3, markersize=1.3)\nplot!(subplot=2, ts, [d[2] for d in data], label=\"True x₂\", color=:red)\nplot!(subplot=2, ts, [d[2] for d in noisy_data], seriestype=:scatter, label=\"Noisy x₂\", color=:red, alpha=0.3, markersize=1.3)\nxlabel!(\"Time\")\nylabel!(subplot=1, \"Prey Population\")\nylabel!(subplot=2, \"Predator Population\")","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#First-Alternative:-Global-Parameter-Optimization","page":"Ode Parameter Estimation","title":"First Alternative: Global Parameter Optimization","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"In the first alternative we will construct one time-segment of Lotka-Volterra equation. We will use lotka_volterra_rk4 function to create non-linear node. This function was defined earlier to numerically solve the Lotka-Volterra equations using the 4th order Runge-Kutta method. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"@model function lotka_volterra_model_without_prior(obs, mprev, Vprev, dt, t, θ)\n    xprev ~ MvNormalMeanCovariance(mprev, Vprev)\n    x     := lotka_volterra_rk4(xprev, θ, t, dt)\n    obs   ~ MvNormalMeanCovariance(x,  noisev * diageye(length(mprev)))\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Non-linear deterministic nodes require meta specification that will determine the type of message approximations to be used. In this case, we can use the Linearization method that will trigger an Extended Kalman Filter (EKF) type of approximation or the Unscented method that will trigger an Unscented Kalman Filter (UKF) type of approximation. Moreover, because we are using RxInfer in an online setting we need to specify how the mean and covariance of the Gaussian distribution will be updated. We do this by using the @autoupdates macro and initialize using the @initialization macro. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"delta_meta = @meta begin\n    lotka_volterra_rk4() ->  Linearization()\nend\n\nautoupdates_without_prior = @autoupdates begin\n    mprev, Vprev= mean_cov(q(x))\nend\n\n@initialization function initialize_without_prior(mx, Vx)\n    q(x) = MvNormalMeanCovariance(mx, Vx)\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Free-Energy-Computation","page":"Ode Parameter Estimation","title":"Free Energy Computation","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"We will now define the free energy function that will be minimized to infer the parameters of the model. Since the parameters of the model are not constrained to be positive, we will use the exp function to transform the parameters to the positive domain. We will set the free energy to true to keep track of the free energy values. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function compute_free_energy_without_prior(θ ; mx = ones(2), Vx = 1e-6 * diageye(2))\n    θ = exp.(θ)\n    result = infer(\n        model = lotka_volterra_model_without_prior(dt = dt, θ = θ),\n        data = (obs = noisy_data, t= ts),\n        initialization = initialize_without_prior(mx, Vx),\n        meta = delta_meta,\n        autoupdates = autoupdates_without_prior,\n        keephistory = length(noisy_data),\n        free_energy = true\n    )\n    return sum(result.free_energy_final_only_history)\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Now we are ready to perform the parameter inference by minimizing the free energy function. We will use the optimize function from the Optim package to perform the optimization. We will use the NelderMead method as the optimizer as it doesn't require gradient information and is faster.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"res_without_prior  = optimize(compute_free_energy_without_prior, zeros(4), NelderMead(), Optim.Options(show_trace = true, show_every = 300));","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Iter     Function value    √(Σ(yᵢ-ȳ)²)/n \n------   --------------    --------------\n     0     1.274436e+03     9.650180e+00\n * time: 8.606910705566406e-5","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"θ_minimizer_without_prior = exp.(res_without_prior.minimizer)\nprintln(\"\\nEstimated point mass valued parameters:\")\nfor (i, (name, val)) in enumerate(zip([\"α\", \"β\", \"γ\", \"δ\"], θ_minimizer_without_prior))\n    println(\" * $name: $(round(val, digits=3))\")\nend\n\nprintln(\"\\nActual parameters used to generate data:\")\nfor (i, (name, val)) in enumerate(zip([\"α\", \"β\", \"γ\", \"δ\"], true_params))\n    println(\" * $name: $(round(val, digits=3))\")\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Estimated point mass valued parameters:\n * α: 0.994\n * β: 1.491\n * γ: 3.054\n * δ: 0.997\n\nActual parameters used to generate data:\n * α: 1.0\n * β: 1.5\n * γ: 3.0\n * δ: 1.0","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Second-Alternative:-RxInfer-Model-with-Prior-on-the-Parameters","page":"Ode Parameter Estimation","title":"Second Alternative: RxInfer Model with Prior on the Parameters","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"We will now define the corresponding RxInfer model with the prior distribution on the parameters. For this, we will use the @model macro to create a time segment for the ODE using the deterministic ODE solver lotka_volterra_rk4 as a non-linear node in the RxInfer model. For the prior distribution of the parameters, we will use a multivariate Gaussian distribution with mean mθ and covariance Vθ that will be initialized using the initialize macro.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"@model function lotka_volterra_model(obs, mprev, Vprev, dt, t, mθ, Vθ)\n    θ     ~ MvNormalMeanCovariance(mθ, Vθ)\n    xprev ~ MvNormalMeanCovariance(mprev, Vprev)\n    x     := lotka_volterra_rk4(xprev, θ, t, dt)\n    obs   ~ MvNormalMeanCovariance(x,  noisev * diageye(length(mprev)))\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"autoupdates = @autoupdates begin\n    mprev, Vprev= mean_cov(q(x))\n    mθ, Vθ = mean_cov(q(θ))\nend\n\n@initialization function initialize(mx, Vx, mθ, Vθ)\n    q(x) = MvNormalMeanCovariance(mx, Vx)\n    q(θ) = MvNormalMeanCovariance(mθ, Vθ)\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Prior-Initialization-by-means-of-Free-Energy-Minimization","page":"Ode Parameter Estimation","title":"Prior Initialization by means of Free Energy Minimization","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"We will now define the free energy function that will be minimized to infer the initial hyper-parameters of the prior distribution. Since we have 4 parameters, we will initialize the mean of the prior distribution with 4 elements and the diagonal elements of the covariance matrix. Again, we will use the exp function to transform the parameters to the positive domain. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"function compute_free_energy(θ ; mx = ones(2), Vx = 1e-6 * diageye(2))\n    θ = exp.(θ)\n    mθ = θ[1:4]\n    Vθ = Diagonal(θ[5:end])\n    result = infer(\n        model = lotka_volterra_model(dt = dt,),\n        data = (obs = noisy_data, t = ts),\n        initialization = initialize(mx, Vx, mθ, Vθ),\n        meta = delta_meta,\n        autoupdates = autoupdates,\n        keephistory = length(noisy_data),\n        free_energy = true\n    )\n    return sum(result.free_energy_final_only_history)\nend;","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Parameter-Inference","page":"Ode Parameter Estimation","title":"Parameter Inference","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"We will now perform the parameter inference by minimizing the free energy function. We will use the optimize function from the Optim package to perform the optimization. We will use the NelderMead method as the optimizer as it doesn't require gradient information and is faster.","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"res = optimize(compute_free_energy, [zeros(4); 0.1ones(4)], NelderMead(), Optim.Options(show_trace = true, show_every = 300));","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Iter     Function value    √(Σ(yᵢ-ȳ)²)/n \n------   --------------    --------------\n     0     2.814831e+02     2.192538e-01\n * time: 8.082389831542969e-5\n   300     2.714613e+02     1.554900e-03\n * time: 4.6847429275512695\n   600     2.712411e+02     6.805549e-07\n * time: 9.450692892074585","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"θ_minimizer = exp.(res.minimizer)\nmθ_init = θ_minimizer[1:4]\nVθ_init = Diagonal(θ_minimizer[5:end])\n\nprintln(\"\\nEstimated initialization parameters for the prior distribution:\")\nfor (i, (name, val, var)) in enumerate(zip([\"α\", \"β\", \"γ\", \"δ\"], mθ_init, θ_minimizer[5:8]))\n    println(\" * $name: $(round(val, digits=3)) ± $(round(sqrt(var), digits=3))\")\nend\n\nprintln(\"\\nActual parameters used to generate data:\")\nfor (i, (name, val)) in enumerate(zip([\"α\", \"β\", \"γ\", \"δ\"], true_params))\n    println(\" * $name: $(round(val, digits=3))\")\nend","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Estimated initialization parameters for the prior distribution:\n * α: 2.279 ± 1.275\n * β: 1.91 ± 2.119\n * γ: 1.636 ± 2.228\n * δ: 0.962 ± 0.693\n\nActual parameters used to generate data:\n * α: 1.0\n * β: 1.5\n * γ: 3.0\n * δ: 1.0","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Having estimated the initial hyper-parameters of the prior distribution, we can now perform the parameter inference by online message passing. We will use the infer function to perform the inference. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"\nresult = infer(\n    model = lotka_volterra_model(dt = dt,),\n    data = (obs = noisy_data_long, t= ts_long),\n    initialization = initialize(ones(2), 1e-6 * diageye(2), mθ_init, Vθ_init),\n    meta = delta_meta,\n    autoupdates = autoupdates,\n    keephistory = length(noisy_data_long),\n    free_energy = true\n);","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"mθ_posterior = mean.(result.history[:θ])\nVθ_posterior = var.(result.history[:θ])\n\np = plot(layout=(4,1), size=(800,1000), legend=:right)\n\nparam_names = [\"α\", \"β\", \"γ\", \"δ\"]\n\nfor i in 1:4\n    means = [m[i] for m in mθ_posterior]\n    stds = [2sqrt(v[i]) for v in Vθ_posterior]\n    \n    plot!(p[i], means, ribbon=stds, label=\"Posterior\", subplot=i)\n    hline!(p[i], [true_params[i]], label=\"True value\", linestyle=:dash, color=:red, subplot=i)\n    \n    title!(p[i], param_names[i], subplot=i)\n    if i == 4 \n        xlabel!(p[i], \"Time step\", subplot=i)\n    end\nend\n\n# Place legend at top right for all subplots\nplot!(p, legend=:topright)\n\ndisplay(p)\nfinal_means = last(mθ_posterior)\nfinal_vars = last(Vθ_posterior)\nfinal_stds = sqrt.(final_vars)\n\n# Print results\nprintln(\"\\nFinal Parameter Estimates:\")\nfor (param, mean, std) in zip(param_names, final_means, final_stds)\n    println(\"$param: $mean ± $(std)\")\nend\n\n# Get final covariance matrix\nfinal_cov = cov(last(result.history[:θ]))\nprintln(\"\\nFinal Parameter Covariance Matrix:\")\ndisplay(final_cov)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Final Parameter Estimates:\nα: 0.9873125810391418 ± 0.02268319328262911\nβ: 1.4928568988039093 ± 0.026900704209026304\nγ: 3.032995698910327 ± 0.12553964480472204\nδ: 1.01924256539008 ± 0.033832412596848514\n\nFinal Parameter Covariance Matrix:\n4×4 Matrix{Float64}:\n 0.000514527   0.00038494    8.38569e-5   3.78402e-5\n 0.00038494    0.000723648  -8.1867e-5   -4.44924e-5\n 8.38569e-5   -8.1867e-5     0.0157602    0.00373872\n 3.78402e-5   -4.44924e-5    0.00373872   0.00114463","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"from = 1\nskip = 1        \nto = 500\n\n# Get state estimates and variances\nmx = mean.(result.history[:x])\nVx = var.(result.history[:x])\n\n# Plot state estimates with uncertainty bands\np1 = plot(ts_long[from:skip:to] , getindex.(mx, 1)[from:skip:to], ribbon=2*sqrt.(getindex.(Vx, 1)[from:skip:to]), \n          label=\"Prey estimate\", legend=:topright)\nscatter!(p1, ts_long[from:skip:to], getindex.(noisy_data_long, 1)[from:skip:to], label=\"Noisy prey observations\", alpha=0.5,ms=1)\nplot!(p1, ts_long[from:skip:to], getindex.(data_long, 1)[from:skip:to], label=\"True prey\", linestyle=:dash)\ntitle!(p1, \"Prey Population\")\n\np2 = plot(ts_long[from:skip:to], getindex.(mx, 2)[from:skip:to], ribbon=2*sqrt.(getindex.(Vx, 2)[from:skip:to]), \n          label=\"Predator estimate\", legend=:topright)\nscatter!(p2, ts_long[from:skip:to], getindex.(noisy_data_long, 2)[from:skip:to], label=\"Noisy predator observations\", alpha=0.5, ms=1)\nplot!(p2, ts_long[from:skip:to], getindex.(data_long, 2)[from:skip:to] , label=\"True predator\", linestyle=:dash)\ntitle!(p2, \"Predator Population\")\n\nplot(p1, p2, layout=(2,1), size=(1000,600))","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/#Third-Alternative:-RxInfer-Model-with-Exponential-Transformation-on-the-Parameters","page":"Ode Parameter Estimation","title":"Third Alternative: RxInfer Model with Exponential Transformation on the Parameters","text":"","category":"section"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"So far we have used the exp function to transform the parameters to the positive domain and computed free energy. This transformation was done outside of @model macro. In this approach, we will use the exp function to transform the parameters to the positive domain but within the @model macro. We will then use the Unscented method to approximate the non-linear deterministic node. This approach is more computationally efficient than the previous one, however it may suffer from accuracy issues as we may not have a good hyper-parameter initialization. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"NOTE: We can not use exp.() inside the @model macro as the model macro doesn't support broadcasting yet. So we need to define a function that will apply the exp function to the parameters. ","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"expf(θ) = exp.(θ) ## This function is used to apply the exp function to the parameters within the @model macro\n\n@model function lotka_volterra_model2(obs, mprev, Vprev, dt, t, mθ, Vθ)\n    θ     ~ MvNormalMeanCovariance(mθ, Vθ)\n    xprev ~ MvNormalMeanCovariance(mprev, Vprev)\n    θ_exp := expf(θ)\n    x     := lotka_volterra_rk4(xprev, θ_exp, t, dt)\n    obs   ~ MvNormalMeanCovariance(x,  noisev * diageye(length(mprev)))\nend\n\ndelta_meta2 = @meta begin\n    lotka_volterra_rk4() ->  Unscented()\n    expf() ->  Unscented()\nend\n\nautoupdates2 = @autoupdates begin\n    mprev, Vprev= mean_cov(q(x))\n    mθ, Vθ = mean_cov(q(θ))\nend\n\n@initialization function initialize2(mx, Vx, mθ, Vθ)\n    q(x) = MvNormalMeanCovariance(mx, Vx)\n    q(θ) = MvNormalMeanCovariance(mθ, Vθ)\nend\n\n\nresult2  = infer(\n    model = lotka_volterra_model2(dt = dt,),\n    data = (obs = noisy_data_long, t= ts_long),\n    initialization = initialize2(ones(2),  1e-6diageye(2), zeros(4), 0.1*diageye(4)),\n    meta = delta_meta2,\n    autoupdates = autoupdates2,\n    keephistory = length(noisy_data_long),\n    free_energy = true\n)","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"RxInferenceEngine:\n  Posteriors stream    | enabled for (θ_exp, θ, xprev, x)\n  Free Energy stream   | enabled\n  Posteriors history   | available for (θ_exp, θ, xprev, x)\n  Free Energy history  | available\n  Enabled events       | [  ]","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"mθ_exp =  mean.(result2.history[:θ_exp])\nVθ_exp = var.(result2.history[:θ_exp])\n\n# Plot the inferred parameters with uncertainty\np1 = plot(ts_long, getindex.(mθ_exp, 1), ribbon=2*sqrt.(getindex.(Vθ_exp, 1)), label=\"α\", legend=:topright)\nplot!(p1, ts_long, fill(true_params[1], length(ts_long)), label=\"True α\", linestyle=:dash)\ntitle!(p1, \"Parameter α\")\n\np2 = plot(ts_long, getindex.(mθ_exp, 2), ribbon=2*sqrt.(getindex.(Vθ_exp, 2)), label=\"β\", legend=:topright)\nplot!(p2, ts_long, fill(true_params[2], length(ts_long)), label=\"True β\", linestyle=:dash)\ntitle!(p2, \"Parameter β\")\n\np3 = plot(ts_long, getindex.(mθ_exp, 3), ribbon=2*sqrt.(getindex.(Vθ_exp, 3)), label=\"γ\", legend=:topright)\nplot!(p3, ts_long, fill(true_params[3], length(ts_long)), label=\"True γ\", linestyle=:dash)\ntitle!(p3, \"Parameter γ\")\n\np4 = plot(ts_long, getindex.(mθ_exp, 4), ribbon=2*sqrt.(getindex.(Vθ_exp, 4)), label=\"δ\", legend=:topright)\nplot!(p4, ts_long, fill(true_params[4], length(ts_long)), label=\"True δ\", linestyle=:dash)\ntitle!(p4, \"Parameter δ\")\n\nplot(p1, p2, p3, p4, layout=(4,1), size=(1000,800))","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"# Print final parameter estimates and covariance\nfinal_means = last(mθ_exp)\nfinal_vars = last(Vθ_exp)\nfinal_stds = sqrt.(final_vars)\n\n# Print results\nprintln(\"\\nFinal Parameter Estimates:\")\nfor (param, mean, std) in zip(param_names, final_means, final_stds)\n    println(\"$param: $mean ± $(std)\")\nend\n\nprintln(\"\\nFinal parameter covariance matrix:\")\ndisplay(cov(last(result2.history[:θ_exp])))","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Final Parameter Estimates:\nα: 1.1262630068329251 ± 0.025551178790336644\nβ: 1.559153042713237 ± 0.02790701016695101\nγ: 2.493178031020226 ± 0.11291883296479856\nδ: 0.8461062937663921 ± 0.030319628433966395\n\nFinal parameter covariance matrix:\n4×4 Matrix{Float64}:\n 0.000652863   0.000508431   8.64717e-5   3.83381e-5\n 0.000508431   0.000778801  -7.76106e-5  -5.9575e-5\n 8.64717e-5   -7.76106e-5    0.0127507    0.00295032\n 3.83381e-5   -5.9575e-5     0.00295032   0.00091928","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"\n# Get state estimates and variances\nmx = mean.(result2.history[:x])\nVx = var.(result2.history[:x])\n\n# Plot state estimates with uncertainty bands\np1 = plot(ts_long[from:skip:to] , getindex.(mx, 1)[from:skip:to], ribbon=2*sqrt.(getindex.(Vx, 1)[from:skip:to]), \n          label=\"Prey estimate\", legend=:topright)\nscatter!(p1, ts_long[from:skip:to], getindex.(noisy_data_long, 1)[from:skip:to], label=\"Noisy prey observations\", alpha=0.5,ms=1)\nplot!(p1, ts_long[from:skip:to], getindex.(data_long, 1)[from:skip:to], label=\"True prey\", linestyle=:dash)\ntitle!(p1, \"Prey Population\")\n\np2 = plot(ts_long[from:skip:to], getindex.(mx, 2)[from:skip:to], ribbon=2*sqrt.(getindex.(Vx, 2)[from:skip:to]), \n          label=\"Predator estimate\", legend=:topright)\nscatter!(p2, ts_long[from:skip:to], getindex.(noisy_data_long, 2)[from:skip:to], label=\"Noisy predator observations\", alpha=0.5, ms=1)\nplot!(p2, ts_long[from:skip:to], getindex.(data_long, 2)[from:skip:to] , label=\"True predator\", linestyle=:dash)\ntitle!(p2, \"Predator Population\")\n\nplot(p1, p2, layout=(2,1), size=(1000,600))","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/ode_parameter_estimation/","page":"Ode Parameter Estimation","title":"Ode Parameter Estimation","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/ode_parameter_estimation/Project.toml`\n  [429524aa] Optim v1.11.0\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n⌃ [1c904df7] SeeToDee v1.2.1\n  [860ef19b] StableRNGs v1.0.2\n  [90137ffa] StaticArrays v1.9.13\n  [37e2e46d] LinearAlgebra v1.11.0\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Invertible-neural-networks:-a-tutorial","page":"Invertible Neural Network Tutorial","title":"Invertible neural networks: a tutorial","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Table of contents","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Introduction\nModel specification\nModel compilation\nProbabilistic inference\nParameter estimation","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Introduction","page":"Invertible Neural Network Tutorial","title":"Introduction","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Load-required-packages","page":"Invertible Neural Network Tutorial","title":"Load required packages","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Before we can start, we need to import some packages:","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"using RxInfer\nusing Random\nusing StableRNGs\n\nusing ReactiveMP        # ReactiveMP is included in RxInfer, but we explicitly use some of its functionality\nusing LinearAlgebra     # only used for some matrix specifics\nusing Plots             # only used for visualisation\nusing Distributions     # only used for sampling from multivariate distributions\nusing Optim             # only used for parameter optimisation","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Model-specification","page":"Invertible Neural Network Tutorial","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Specifying an invertible neural network model is easy. The general recipe looks like follows: model = FlowModel(input_dim, (layer1(options), layer2(options), ...)). Here the first argument corresponds to the input dimension of the model and the second argument is a tuple of layers. An example model can be defined as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"model = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Alternatively, the input_dim can also be passed as an InputLayer layer as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"In the above AdditiveCouplingLayer layers the input bfx = x_1 x_2 ldots x_N is partitioned into chunks of unit length. These partitions are additively coupled to an output bfy = y_1 y_2 ldots y_N as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"beginaligned\n    y_1 = x_1 \n    y_2 = x_2 + f_1(x_1) \n    vdots \n    y_N = x_N + f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Importantly, this structure can easily be converted as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"beginaligned\n    x_1 = y_1 \n    x_2 = y_2 - f_1(x_1) \n    vdots \n    x_N = y_N - f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"f_n","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"is an arbitrarily complex function, here chosen to be a PlanarFlow, but this can be interchanged for any function or neural network. The permute keyword argument (which defaults to true) specifies whether the output of this layer should be randomly permuted or shuffled. This makes sure that the first element is also transformed in consecutive layers.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"A permutation layer can also be added by itself as a PermutationLayer layer with a custom permutation matrix if desired.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false),\n        PermutationLayer(PermutationMatrix(2)),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Model-compilation","page":"Invertible Neural Network Tutorial","title":"Model compilation","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"In the current models, the layers are setup to work with the passed input dimension. This means that the function f_n is repeated input_dim-1 times for each of the partitions. Furthermore the permutation layers are set up with proper permutation matrices. If we print the model we get","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"model","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"ReactiveMP.FlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayerEmpty{Tuple{R\neactiveMP.PlanarFlowEmpty{1}}}, ReactiveMP.PermutationLayer{Int64}, Reactiv\neMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}}}(2, (\nReactiveMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}\n(2, (ReactiveMP.PlanarFlowEmpty{1}(),), 1), ReactiveMP.PermutationLayer{Int\n64}(2, [0 1; 1 0]), ReactiveMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.\nPlanarFlowEmpty{1}}}(2, (ReactiveMP.PlanarFlowEmpty{1}(),), 1)))","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"The text below describes the terms above. Please note the distinction in typing and elements, i.e. FlowModel{types}(elements):","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"FlowModel - specifies that we are dealing with a flow model.\n3 - Number of layers.\nTuple{AdditiveCouplingLayerEmpty{...},PermutationLayer{Int64},AdditiveCouplingLayerEmpty{...}} - tuple of layer types.\nTuple{ReactiveMP.PlanarFlowEmpty{1},ReactiveMP.PlanarFlowEmpty{1}} - tuple of functions f_n.\nPermutationLayer{Int64}(2, [0 1; 1 0]) - permutation layer with input dimension 2 and permutation matrix [0 1; 1 0].","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"From inspection we can see that the AdditiveCouplingLayerEmpty and PlanarFlowEmpty objects are different than before. They are initialized for the correct dimension, but they do not have any parameters registered to them. This is by design to allow for separating the model specification from potential optimization procedures. Before we perform inference in this model, the parameters should be initialized. We can randomly initialize the parameters as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"compiled_model = compile(model)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"ReactiveMP.CompiledFlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayer{Tupl\ne{ReactiveMP.PlanarFlow{Float64, Float64}}}, ReactiveMP.PermutationLayer{In\nt64}, ReactiveMP.AdditiveCouplingLayer{Tuple{ReactiveMP.PlanarFlow{Float64,\n Float64}}}}}(2, (ReactiveMP.AdditiveCouplingLayer{Tuple{ReactiveMP.PlanarF\nlow{Float64, Float64}}}(2, (ReactiveMP.PlanarFlow{Float64, Float64}(0.90034\n71948047783, -0.7599184059109225, 0.0038441640370240463),), 1), ReactiveMP.\nPermutationLayer{Int64}(2, [0 1; 1 0]), ReactiveMP.AdditiveCouplingLayer{Tu\nple{ReactiveMP.PlanarFlow{Float64, Float64}}}(2, (ReactiveMP.PlanarFlow{Flo\nat64, Float64}(-0.39913131905279925, -0.6569490065153973, -0.08759672253233\n154),), 1)))","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Now we can see that random parameters have been assigned to the individual functions inside of our model. Alternatively if we would like to pass our own parameters, then this is also possible. You can easily find the required number of parameters using the nr_params(model) function.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"ReactiveMP.CompiledFlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayer{Tupl\ne{ReactiveMP.PlanarFlow{Float64, Float64}}}, ReactiveMP.PermutationLayer{In\nt64}, ReactiveMP.AdditiveCouplingLayer{Tuple{ReactiveMP.PlanarFlow{Float64,\n Float64}}}}}(2, (ReactiveMP.AdditiveCouplingLayer{Tuple{ReactiveMP.PlanarF\nlow{Float64, Float64}}}(2, (ReactiveMP.PlanarFlow{Float64, Float64}(0.72964\n12319250487, -0.9767336128037319, -0.4749869451771002),), 1), ReactiveMP.Pe\nrmutationLayer{Int64}(2, [0 1; 1 0]), ReactiveMP.AdditiveCouplingLayer{Tupl\ne{ReactiveMP.PlanarFlow{Float64, Float64}}}(2, (ReactiveMP.PlanarFlow{Float\n64, Float64}(0.3490911082645933, -0.8184067956921087, -1.4578214732352386),\n), 1)))","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Probabilistic-inference","page":"Invertible Neural Network Tutorial","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"We can perform inference in our compiled model through standard usage of RxInfer and its underlying ReactiveMP inference engine. Let's first generate some random 2D data which has been sampled from a standard normal distribution and is consecutively passed through an invertible neural network. Using the forward(model, data) function we can propagate data in the forward direction.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"function generate_data(nr_samples::Int64, model::CompiledFlowModel; seed = 123)\n\n    rng = StableRNG(seed)\n    \n    # specify latent sampling distribution\n    dist = MvNormal([1.5, 0.5], I)\n\n    # sample from the latent distribution\n    x = rand(rng, dist, nr_samples)\n\n    # transform data\n    y = zeros(Float64, size(x))\n    for k = 1:nr_samples\n        y[:,k] .= ReactiveMP.forward(model, x[:,k])\n    end\n\n    # return data\n    return y, x\n\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"# generate data\ny, x = generate_data(1000, compiled_model)\n\n# plot generated data\np1 = scatter(x[1,:], x[2,:], alpha=0.3, title=\"Original data\", size=(800,400))\np2 = scatter(y[1,:], y[2,:], alpha=0.3, title=\"Transformed data\", size=(800,400))\nplot(p1, p2, legend = false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"The probabilistic model for doing inference can be described as ","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"@model function invertible_neural_network(y)\n\n    # specify prior\n    z_μ ~ MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    z_Λ ~ Wishart(2.0, tiny*diagm(ones(2)))\n\n    # specify observations\n    for k in eachindex(y)\n\n        # specify latent state\n        x[k] ~ MvNormalMeanPrecision(z_μ, z_Λ)\n\n        # specify transformed latent value\n        y_lat[k] ~ Flow(x[k])\n\n        # specify observations\n        y[k] ~ MvNormalMeanCovariance(y_lat[k], tiny*diagm(ones(2)))\n\n    end\n\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Here the model is passed inside a meta data object of the flow node. Inference then resorts to","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"observations = [y[:,k] for k=1:size(y,2)]\n\nfmodel         = invertible_neural_network()\ndata           = (y = observations, )\ninitialization = @initialization begin \n    q(z_μ) = MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    q(z_Λ) = Wishart(2.0, tiny*diagm(ones(2)))\nend\nreturnvars     = (z_μ = KeepLast(), z_Λ = KeepLast(), x = KeepLast(), y_lat = KeepLast())\n\nconstraints = @constraints begin\n    q(z_μ, x, z_Λ) = q(z_μ)q(z_Λ)q(x)\nend\n\n@meta function fmeta(model)\n    compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))\n    Flow(y_lat, x) -> FlowMeta(compiled_model) # defaults to FlowMeta(compiled_model; approximation=Linearization()). \n                                               # other approximation methods can be e.g. FlowMeta(compiled_model; approximation=Unscented(input_dim))\nend\n\n# First execution is slow due to Julia's initial compilation \nresult = infer(\n    model          = fmodel, \n    data           = data,\n    constraints    = constraints,\n    meta           = fmeta(model),\n    initialization = initialization,\n    returnvars     = returnvars,\n    free_energy    = true,\n    iterations     = 10, \n    showprogress   = false\n)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Inference results:\n  Posteriors       | available for (z_μ, z_Λ, y_lat, x)\n  Free Energy:     | Real[29485.3, 23762.9, 23570.6, 23570.6, 23570.6, 2357\n0.6, 23570.6, 23570.6, 23570.6, 23570.6]","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"fe_flow = result.free_energy\nzμ_flow = result.posteriors[:z_μ]\nzΛ_flow = result.posteriors[:z_Λ]\nx_flow  = result.posteriors[:x]\ny_flow  = result.posteriors[:y_lat];","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"As we can see, the variational free energy decreases inside of our model.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"plot(1:10, fe_flow/size(y,2), xlabel=\"iteration\", ylabel=\"normalized variational free energy [nats/sample]\", legend=false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"If we plot a random noisy observation and its approximated transformed uncertainty we obtain:","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"# pick a random observation\nid = rand(StableRNG(321), 1:size(y,2))\nrand_observation = MvNormal(y[:,id], 5e-1*diagm(ones(2)))\nwarped_observation = MvNormal(ReactiveMP.backward(compiled_model, y[:,id]), ReactiveMP.inv_jacobian(compiled_model, y[:,id])*5e-1*diagm(ones(2))*ReactiveMP.inv_jacobian(compiled_model, y[:,id])');\n\np1 = scatter(x[1,:], x[2,:], alpha=0.1, title=\"Latent distribution\", size=(1200,500), label=\"generated data\")\ncontour!(-5:0.1:5, -5:0.1:5, (x, y) -> pdf(MvNormal([1.5, 0.5], I), [x, y]), c=:viridis, colorbar=false, linewidth=2)\nscatter!([mean(zμ_flow)[1]], [mean(zμ_flow)[2]], color=\"red\", markershape=:x, markersize=5, label=\"inferred mean\")\ncontour!(-5:0.01:5, -5:0.01:5, (x, y) -> pdf(warped_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, colorbar=false)\nscatter!([mean(warped_observation)[1]], [mean(warped_observation)[2]], color=\"red\", label=\"transformed noisy observation\")\np2 = scatter(y[1,:], y[2,:], alpha=0.1, label=\"generated data\")\nscatter!([ReactiveMP.forward(compiled_model, mean(zμ_flow))[1]], [ReactiveMP.forward(compiled_model, mean(zμ_flow))[2]], color=\"red\", marker=:x, label=\"inferred mean\")\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(MvNormal([1.5, 0.5], I), ReactiveMP.backward(compiled_model, [x, y])), c=:viridis, colorbar=false, linewidth=2)\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(rand_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, label=\"random noisy observation\", colorba=false)\nscatter!([mean(rand_observation)[1]], [mean(rand_observation)[2]], color=\"red\", label=\"random noisy observation\")\nplot(p1, p2, legend = true)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/#Parameter-estimation","page":"Invertible Neural Network Tutorial","title":"Parameter estimation","text":"","category":"section"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"The flow model is often used to learn unknown probabilistic mappings. Here we will demonstrate it as follows for a binary classification task with the following data:","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n\n    # sample weights\n    w = rand(rng, nr_samples, 2)\n\n    # sample appraisal\n    y = zeros(Float64, nr_samples)\n    for k = 1:nr_samples\n        y[k] = 1.0*(w[k,1] > 0.5)*(w[k,2] < 0.5)\n    end\n\n    # return data\n    return y, w\n\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"data_y, data_x = generate_data(50);\nscatter(data_x[:,1], data_x[:,2], marker_z=data_y, xlabel=\"w1\", ylabel=\"w2\", colorbar=false, legend=false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"We will then specify a possible model as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"# specify flow model\nmodel = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()), # defaults to AdditiveCouplingLayer(PlanarFlow(); permute=true)\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"The corresponding probabilistic model for the binary classification task can be created as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"@model function invertible_neural_network_classifier(x, y)\n\n    # specify observations\n    for k in eachindex(x)\n\n        # specify latent state\n        x_lat[k] ~ MvNormalMeanPrecision(x[k], 1e3*diagm(ones(2)))\n\n        # specify transformed latent value\n        y_lat1[k] ~ Flow(x_lat[k])\n        y_lat2[k] ~ dot(y_lat1[k], [1, 1])\n\n        # specify observations\n        y[k] ~ Probit(y_lat2[k]) # default: where { pipeline = RequireMessage(in = NormalMeanPrecision(0, 1.0)) }\n\n    end\n\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"fcmodel       = invertible_neural_network_classifier()\ndata          = (y = data_y, x = [data_x[k,:] for k=1:size(data_x,1)], )\n\n@meta function fmeta(model, params)\n    compiled_model = compile(model, params)\n    Flow(y_lat1, x_lat) -> FlowMeta(compiled_model)\nend","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"fmeta (generic function with 2 methods)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Here we see that the compilation occurs inside of our probabilistic model. As a result we can pass parameters (and a model) to this function which we wish to opmize for some criterium, such as the variational free energy. Inference can be described as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"For the optimization procedure, we will simplify our inference loop, such that it only accepts parameters as an argument (which is wishes to optimize) and outputs a performance metric.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"function f(params)\n    Random.seed!(123) # Flow uses random permutation matrices, which is not good for the optimisation procedure\n    result = infer(\n        model                   = fcmodel, \n        data                    = data,\n        meta                    = fmeta(model, params),\n        free_energy             = true,\n        free_energy_diagnostics = nothing, # Free Energy can be set to NaN due to optimization procedure\n        iterations              = 10, \n        showprogress            = false\n    );\n    \n    result.free_energy[end]\nend;","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Optimization can be performed using the Optim package. Alternatively, other (custom) optimizers can be implemented, such as:","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(store_trace = true, show_trace = true, show_every = 50), autodiff=:forward)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"uses finitediff and is slower/less accurate.","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"or","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"# create gradient function\ng = (x) -> ForwardDiff.gradient(f, x);\n\n# specify initial params\nparams = randn(nr_params(model))\n\n# create custom optimizer (here Adam)\noptimizer = Adam(params; λ=1e-1)\n\n# allocate space for gradient\n∇ = zeros(nr_params(model))\n\n# perform optimization\nfor it = 1:10000\n\n    # backward pass\n    ∇ .= ForwardDiff.gradient(f, optimizer.x)\n\n    # gradient update\n    ReactiveMP.update!(optimizer, ∇)\n\nend\n","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(f_tol = 1e-3, store_trace = true, show_trace = true, show_every = 100), autodiff=:forward)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Iter     Function value   Gradient norm \n     0     5.888958e+02     8.943663e+02\n * time: 0.027489900588989258\n   100     1.059823e+01     4.118858e+00\n * time: 13.424142837524414\n * Status: success\n\n * Candidate solution\n    Final objective value:     9.902797e+00\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.23e-03 ≰ 0.0e+00\n    |x - x'|/|x'|          = 5.83e-04 ≰ 0.0e+00\n    |f(x) - f(x')|         = 9.64e-03 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 9.73e-04 ≤ 1.0e-03\n    |g(x)|                 = 2.21e+00 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   16  (vs limit Inf)\n    Iterations:    116\n    f(x) calls:    312\n    ∇f(x) calls:   312","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"optimization results are then given as","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"params = Optim.minimizer(res)\ninferred_model = compile(model, params)\ntrans_data_x_1 = hcat(map((x) -> ReactiveMP.forward(inferred_model, x), [data_x[k,:] for k=1:size(data_x,1)])...)'\ntrans_data_x_2 = map((x) -> dot([1, 1], x), [trans_data_x_1[k,:] for k=1:size(data_x,1)])\ntrans_data_x_2_split = [trans_data_x_2[data_y .== 1.0], trans_data_x_2[data_y .== 0.0]]\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, size=(1200,400), c=:viridis, colorbar=false, title=\"original data\")\np2 = scatter(trans_data_x_1[:,1], trans_data_x_1[:,2], marker_z = data_y, c=:viridis, size=(1200,400), colorbar=false, title=\"|> warp\")\np3 = histogram(trans_data_x_2_split; stacked=true, bins=50, size=(1200,400), title=\"|> dot\")\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"using StatsFuns: normcdf\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, title=\"original labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np2 = scatter(data_x[:,1], data_x[:,2], marker_z = normcdf.(trans_data_x_2), title=\"predicted labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np3 = contour(0:0.01:1, 0:0.01:1, (x, y) -> normcdf(dot([1,1], ReactiveMP.forward(inferred_model, [x,y]))), title=\"Classification map\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/invertible_neural_network_tutorial/","page":"Invertible Neural Network Tutorial","title":"Invertible Neural Network Tutorial","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/invertible_neural_network_tutorial/Project.toml`\n  [31c24e10] Distributions v0.25.117\n  [429524aa] Optim v1.11.0\n  [91a5bcdd] Plots v1.40.9\n  [a194aa59] ReactiveMP v5.2.1\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [4c63d2b9] StatsFuns v1.3.2\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Advanced-Tutorial","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"using RxInfer, Plots","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This notebook covers the fundamentals and advanced usage of the RxInfer.jl package.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#General-model-specification-syntax","page":"Advanced Tutorial","title":"General model specification syntax","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We use the @model macro from the GraphPPL.jl package to create a probabilistic model p(s y) and we also specify extra constraints on the variational family of distributions mathcalQ, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of GraphPPL.jl.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `@model` macro accepts a regular Julia function\n@model function test_model1(s_mean, s_precision, y)\n    \n    # the `tilde` operator creates a functional dependency\n    # between variables in our model and can be read as \n    # `sampled from` or `is modeled by`\n    s ~ Normal(mean = s_mean, precision = s_precision)\n    y ~ Normal(mean = s, precision = 1.0)\n    \n    # It is possible to return something from the model specification (including variables and nodes)\n    return \"Hello world\"\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @model macro creates a function with the same name and with the same set of input arguments as the original function (test_model1(s_mean, s_precision, y) in this example). The arguments are however converted to the keyword arguments. The @model macro does not support positional arguments.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use control flow statements such as if or for blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the @model block. As an example consider the following (valid!) model:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model2(y)\n    \n    if length(y) <= 1\n        error(\"The `length` of `y` argument must be greater than one.\")\n    end\n    \n    s[1] ~ Normal(mean = 0.0, precision = 0.1)\n    y[1] ~ Normal(mean = s[1], precision = 1.0)\n    \n    for i in eachindex(y)\n        s[i] ~ Normal(mean = s[i - 1], precision = 1.0)\n        y[i] ~ Normal(mean = s[i], precision = 1.0)\n    end\n    \nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use complex expressions inside the functional dependency expressions","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y ~ Normal(mean = 2.0 * (s + 1.0), precision = 1.0)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `~` creates random variables automatically\ns ~ Normal(mean = 0.0, precision1.0)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Probabilistic-inference-in-RxInfer.jl","page":"Advanced Tutorial","title":"Probabilistic inference in RxInfer.jl","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl uses the Rocket.jl package API for inference routines. Rocket.jl is a reactive programming extension for Julia that is higly inspired by RxJS and similar libraries from the Rx ecosystem. It consists of observables, actors, subscriptions and operators. For more information and rigorous examples see Rocket.jl github page.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Observables","page":"Advanced Tutorial","title":"Observables","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Observables are lazy push-based collections and they deliver their values over time.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Timer that emits a new value every second and has an initial one second delay \nobservable = timer(300, 300)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerObservable(300, 300)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"actor = (value) -> println(value)\nsubscription1 = subscribe!(observable, actor)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We always need to unsubscribe from some observables\nunsubscribe!(subscription1)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We can modify our observables\nmodified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ProxyObservable(Int64, MapProxy(Int64))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscription2 = subscribe!(modified, (value) -> println(value))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(subscription2)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Coin-Toss-Model","page":"Advanced Tutorial","title":"Coin Toss Model","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model(y)\n    # We endow θ parameter of our model with some prior\n    θ  ~ Beta(2.0, 7.0)\n    # We assume that the outcome of each coin flip \n    # is modeled by a Bernoulli distribution\n    y .~ Bernoulli(θ)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can call the infer function to run inference in such model:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"p = 0.75 # Bias of a coin\n\ndataset = float.(rand(Bernoulli(p), 500));\n\nresult = infer(\n    model = coin_toss_model(),\n    data  = (y = dataset, )\n)\n\nprintln(\"Inferred bias is \", mean(result.posteriors[:θ]), \" with standard deviation is \", std(result.posteriors[:θ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inferred bias is 0.7426326129666012 with standard deviation is 0.0193588108\n89841","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can see that the inferred bias is quite close to the actual value we used in the dataset generation with a low standard deviation.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Reactive-Online-Inference","page":"Advanced Tutorial","title":"Reactive Online Inference","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function online_coin_toss_model(θ_a, θ_b, y)\n    θ ~ Beta(θ_a, θ_b)\n    y ~ Bernoulli(θ)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"autoupdates = @autoupdates begin \n    θ_a, θ_b = params(q(θ))\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@autoupdates begin\n    (θ_a, θ_b) = params(q(θ))\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"init = @initialization begin\n    q(θ) = vague(Beta)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Initial state: \n  q(θ) = Distributions.Beta{Float64}(α=1.0, β=1.0)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"rxresult = infer(\n    model = online_coin_toss_model(),\n    data  = (y = dataset, ),\n    autoupdates = autoupdates,\n    historyvars = (θ = KeepLast(), ),\n    keephistory = length(dataset),\n    initialization = init,\n    autostart = true\n);","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"animation = @animate for i in 1:length(dataset)\n    plot(mean.(rxresult.history[:θ][1:i]), ribbon = std.(rxresult.history[:θ][1:i]), title = \"Online coin bias inference\", label = \"Inferred bias\", legend = :bottomright)\n    hline!([ p ], label = \"Real bias\", size = (600, 200))\nend\n\ngif(animation, \"online-coin-bias-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we used static dataset and the history field of the reactive inference result, but the rxinference function also supports any real-time reactive stream and can run indefinitely.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, RxInfer is not limited to only the sum-product algorithm but it also supports variational message passing with Constrained Bethe Free Energy Minimisation.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Variational-inference","page":"Advanced Tutorial","title":"Variational inference","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"On a very high-level, RxInfer is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions q in mathcalQ. Often this involves assuming some factorization over q. ","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6(y)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we want to specify extra constraints for q_a for Bethe factorisation:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)\nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl package exports @constraints macro to simplify factorisation and form constraints specification. Read more about @constraints macro in the corresponding documentation section, here we show a simple example of the same factorisation constraints specification, but with @constraints macro:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints6 = @constraints begin\n     q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints: \n  q(μ, τ) = q(μ)q(τ)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"init = @initialization begin\n    q(μ) = vague(NormalMeanPrecision)\n    q(τ) = vague(GammaShapeRate)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Initial state: \n  q(μ) = ExponentialFamily.NormalMeanPrecision{Float64}(μ=0.0, w=1.0e-12)\n  q(τ) = ExponentialFamily.GammaShapeRate{Float64}(a=1.0, b=1.0e-12)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Inference","page":"Advanced Tutorial","title":"Inference","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To run inference in this model we again need to create a synthetic dataset and call the infer function.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);\nresult = infer(\n    model          = test_model6(),\n    data           = (y = dataset, ),\n    constraints    = constraints6, \n    initialization = init,\n    returnvars     = (μ = KeepLast(), τ = KeepLast()),\n    iterations     = 10,\n    free_energy    = true,\n    showprogress   = true\n)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inference results:\n  Posteriors       | available for (μ, τ)\n  Free Energy:     | Real[14763.3, 3275.66, 681.567, 643.297, 643.297, 643.\n297, 643.297, 643.297, 643.297, 643.297]","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -2.992325863538303, std = 0.014447588865611649","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 4.790803350263971, std = 0.2140373087634853","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Form-constraints","page":"Advanced Tutorial","title":"Form constraints","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to support form constraints, the @constraints macro supports additional type specifications for posterior marginals.  For example, here how we can perform the EM algorithm with PointMass form constraint.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7(y)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"As in the previous example we can use @constraints macro to achieve the same goal with a nicer syntax:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints7 = @constraints begin \n    q(μ) :: PointMassFormConstraint()\n    \n    q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints: \n  q(μ, τ) = q(μ)q(τ)\n  q(μ) :: PointMassFormConstraint()","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);\nresult = infer(\n    model          = test_model7(),\n    data           = (y = dataset, ),\n    constraints    = constraints7, \n    initialization = init,\n    returnvars     = (μ = KeepLast(), τ = KeepLast()),\n    iterations     = 10,\n    free_energy    = true,\n    showprogress   = true\n)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inference results:\n  Posteriors       | available for (μ, τ)\n  Free Energy:     | Real[14766.5, 2046.7, 627.527, 627.527, 627.527, 627.5\n27, 627.527, 627.527, 627.527, 627.527]","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -3.0005693815504886, std = 0.0","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 4.916267991416006, std = 0.21964265554434997","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Meta-data-specification","page":"Advanced Tutorial","title":"Meta data specification","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During model specification some functional dependencies may accept an optional meta object in the where { ... } clause. The purpose of the meta object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The meta object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n# the autoregressive process and its order. In addition it specifies that the message computation rules should\n# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n# by cost of possible numerical instabilities during an inference procedure\ns[i] ~ AR(s[i - 1], θ, γ) where { meta = ARMeta(Multivariate, order, ARsafe()) }\n...\ns[i] ~ AR(s[i - 1], θ, γ) where { meta = ARMeta(Univariate, order, ARunsafe()) }","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Another example with GaussianControlledVariance, or simply GCV [see Hierarchical Gaussian Filter], node:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\nxt ~ GCV(xt_min, zt, real_k, real_w) where { meta = GCVMetadata(GaussHermiteCubature(21)) }","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#GraphPPL.jl-@meta-macro","page":"Advanced Tutorial","title":"GraphPPL.jl @meta macro","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Users can use @meta macro from the GraphPPL.jl package to achieve the same goal. Read more about @meta macro in the corresponding documentation section. Here is a simple example of the same meta specification:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@meta begin \n     AR(s, θ, γ) -> ARMeta(Multivariate, 5, ARsafe())\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Meta: \n  ReactiveMP.AR(s, θ, γ) -> ReactiveMP.ARMeta{Distributions.Multivariate, R\neactiveMP.ARsafe}(5, ReactiveMP.ARsafe())","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Creating-custom-nodes-and-message-computation-rules","page":"Advanced Tutorial","title":"Creating custom nodes and message computation rules","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/#Custom-nodes","page":"Advanced Tutorial","title":"Custom nodes","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To create a custom functional form and to make it available during model specification the ReactiveMP inference engine exports the @node macro:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n\n# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\nstruct NormalMeanVariance end \n\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# It is also possible to use function objects as a node functional form\nfunction dot end\n\n# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n# out = dot(x, a)\n@node typeof(dot) Deterministic [ out, x, a ]","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"After that it is possible to use the newly created node during model specification:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n    ...\n    y ~ dot(x, a)\n    ...\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Custom-messages-computation-rules","page":"Advanced Tutorial","title":"Custom messages computation rules","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl exports the @rule macro to create custom message computation rules. For example let us create a simple + node to be available for usage in the model specification usage. We refer to A Factor Graph Approach to Signal Modelling , System Identification and Filtering [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the + node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for + node is the following:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu_z = mu_x + mu_y\nV_z = V_x + V_y\nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To specify this in RxInfer.jl we use the @node and @rule macros:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@node typeof(+) Deterministic  [ z, x, y ]\n\n@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n    x_mean, x_var = mean_var(m_x)\n    y_mean, y_var = mean_var(m_y)\n    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example, for the @rule macro, we specify a type of our functional form: typeof(+). Next, we specify an edge we are going to compute an outbound message for. Marginalisation indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(z) = int q(z x y) mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: ) (Image: )","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu(z) = int f(x y z)mu(x)mu(y)mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nnu(z) = exp int log f(x y z)q(x)q(y)mathrmdxmathrmdy \nendaligned","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @rule macro supports both cases with special prefixes during rule specification:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"m_ prefix corresponds to the incoming message on a specific edge\nq_ prefix corresponds to the posterior marginal of a specific edge","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Sum-Product rule with m_ messages used:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n    m_out_mean, m_out_cov = mean_cov(m_out)\n    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Variational rule with Mean-Field assumption with q_ posteriors used:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n    return NormalMeanPrecision(mean(q_out), mean(q_τ))\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl also supports structured rules. It is possible to obtain joint marginal over a set of edges:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n    m, V = mean_cov(q_out_μ)\n    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n    α = convert(typeof(θ), 1.5)\n    return Gamma(α, θ)\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"NOTE: In the @rule specification the messages or marginals arguments must be in order with interfaces specification from @node macro:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference backend expects arguments in `@rule` macro to be in the same order\n@node NormalMeanPrecision Stochastic [ out, μ, τ ]","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Any rule always has access to the meta information with hidden the meta::Any variable:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n    ...\n    println(meta)\n    ...\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to dispatch on a specific type of a meta object:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n    ...\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"or","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n    ...\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/#Customizing-messages-computational-pipeline","page":"Advanced Tutorial","title":"Customizing messages computational pipeline","text":"","category":"section"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In certain situations it might be convenient to customize the default message computational pipeline. RxInfer.jl supports the pipeline keyword in the where { ... } clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Logs all outbound messages\ny[i] ~ Normal(mean = x[i], precision = 1.0) where { pipeline = LoggerPipelineStage() }\n# In principle, it is possible to approximate outbound messages with Laplace Approximation (this is not an implemented feature, but a concept)\ny[i] ~ Normal(mean = x[i], precision = 1.0) where { pipeline = LaplaceApproximation() }","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let us return to the coin toss model, but this time we want to print flowing messages:","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model_log(y)\n    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n    end\nend","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = float.(rand(Bernoulli(p), 5));\nresult = infer(\n    model = coin_toss_model_log(),\n    data  = (y = dataset, )\n)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"[θ][Distributions.Beta][out]: DeferredMessage([ use `as_message` to compute\n the message ])\n[y[1]][Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to c\nompute the message ])\n[y[2]][Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to c\nompute the message ])\n[y[3]][Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to c\nompute the message ])\n[y[4]][Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to c\nompute the message ])\n[y[5]][Distributions.Bernoulli][p]: DeferredMessage([ use `as_message` to c\nompute the message ])\nInference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/advanced_tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/advanced_tutorial/Project.toml`\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#Active-Inference-Mountain-car","page":"Active Inference Mountain Car","title":"Active Inference Mountain car","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"using RxInfer, Plots","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"A group of friends is going to a camping site that is located on the biggest mountain in the Netherlands. They use an electric car for the trip. When they are almost there, the car's battery is almost empty and is therefore limiting the engine force. Unfortunately, they are in the middle of a valley and don't have enough power to reach the camping site. Night is falling and they still need to reach the top of the mountain. As rescuers, let us develop an Active Inference (AI) agent that can get them up the hill with the limited engine power.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#The-environmental-process-of-the-mountain","page":"Active Inference Mountain Car","title":"The environmental process of the mountain","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Firstly, we specify the environmental process according to Ueltzhoeffer (2017) \"Deep active inference\". This process shows how the environment evolves after interacting with the agent.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Particularly, let's denote z_t = (phi_t dotphi_t) as the environmental state depending on the position phi_t and velocity dotphi_t of the car; a_t as the action of the environment on the car. Then the evolution of the state is described as follows  ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"beginaligned \ndotphi_t = dotphi_t-1 + F_g(phi_t-1) + F_f(dotphi_t-1) + F_a(a_t)\nphi_t = phi_t-1 + dotphi_t \nendaligned","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"where F_g(phi_t-1) is the gravitational force of the hill landscape that depends on the car's position","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"F_g(phi) = begincases\n        -005(2phi + 1)    mathrmif  phi  0 \n        -005 left(1 + 5phi^2)^-frac12 + phi^2 (1 + 5phi^2)^-frac32 + frac116phi^4 right   mathrmotherwise\nendcases","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"F_f(dotphi)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"is the friction on the car defined through the car's velocity F_f(dotphi)  = -01  dotphi and F_a(a) is the engine force F_a(a) = 004 tanh(a) Since the car is on low battery, we use the tanh(cdot) function to limit the engine force to the interval [-0.04, 0.04].","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"In the cell below, the create_physics function defines forces F_g F_f F_a; and the create_world function defines the environmental process of the mountain.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"import HypergeometricFunctions: _₂F₁\n\nfunction create_physics(; engine_force_limit = 0.04, friction_coefficient = 0.1)\n    # Engine force as function of action\n    Fa = (a::Real) -> engine_force_limit * tanh(a) \n\n    # Friction force as function of velocity\n    Ff = (y_dot::Real) -> -friction_coefficient * y_dot \n    \n    # Gravitational force (horizontal component) as function of position\n    Fg = (y::Real) -> begin\n        if y < 0\n            0.05*(-2*y - 1)\n        else\n            0.05*(-(1 + 5*y^2)^(-0.5) - (y^2)*(1 + 5*y^2)^(-3/2) - (y^4)/16)\n        end\n    end\n    \n    # The height of the landscape as a function of the horizontal coordinate\n    height = (x::Float64) -> begin\n        if x < 0\n            h = x^2 + x\n        else\n            h = x * _₂F₁(0.5,0.5,1.5, -5*x^2) + x^3 * _₂F₁(1.5, 1.5, 2.5, -5*x^2) / 3 + x^5 / 80\n        end\n        return 0.05*h\n    end\n\n    return (Fa, Ff, Fg,height)\nend;\n\nfunction create_world(; Fg, Ff, Fa, initial_position = -0.5, initial_velocity = 0.0)\n\n    y_t_min = initial_position\n    y_dot_t_min = initial_velocity\n    \n    y_t = y_t_min\n    y_dot_t = y_dot_t_min\n    \n    execute = (a_t::Float64) -> begin\n        # Compute next state\n        y_dot_t = y_dot_t_min + Fg(y_t_min) + Ff(y_dot_t_min) + Fa(a_t)\n        y_t = y_t_min + y_dot_t\n    \n        # Reset state for next step\n        y_t_min = y_t\n        y_dot_t_min = y_dot_t\n    end\n    \n    observe = () -> begin \n        return [y_t, y_dot_t]\n    end\n        \n    return (execute, observe)\nend","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"create_world (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Let's visualize the mountain landscape and the situation of the car. ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"engine_force_limit   = 0.04\nfriction_coefficient = 0.1\n\nFa, Ff, Fg, height = create_physics(\n    engine_force_limit = engine_force_limit,\n    friction_coefficient = friction_coefficient\n);\ninitial_position = -0.5\ninitial_velocity = 0.0\n\nx_target = [0.5, 0.0] \n\nvalley_x = range(-2, 2, length=400)\nvalley_y = [ height(xs) for xs in valley_x ]\nplot(valley_x, valley_y, title = \"Mountain valley\", label = \"Landscape\", color = \"black\")\nscatter!([ initial_position ], [ height(initial_position) ], label=\"initial car position\")   \nscatter!([x_target[1]], [height(x_target[1])], label=\"camping site\")","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#Naive-approach","page":"Active Inference Mountain Car","title":"Naive approach","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Well, let's see how our friends were struggling with the low-battery car when they tried to get it to the camping site before we come to help. They basically used the brute-force method, i.e. just pushing the gas pedal for full power.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"N_naive  = 100 # Total simulation time\npi_naive = 100.0 * ones(N_naive) # Naive policy for right full-power only\n\n# Let there be a world\n(execute_naive, observe_naive) = create_world(; \n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n);\n\ny_naive = Vector{Vector{Float64}}(undef, N_naive)\nfor t = 1:N_naive\n    execute_naive(pi_naive[t]) # Execute environmental process\n    y_naive[t] = observe_naive() # Observe external states\nend\n\nanimation_naive = @animate for i in 1:N_naive\n    plot(valley_x, valley_y, title = \"Naive policy\", label = \"Landscape\", color = \"black\", size = (800, 400))\n    scatter!([y_naive[i][1]], [height(y_naive[i][1])], label=\"car\")\n    scatter!([x_target[1]], [height(x_target[1])], label=\"goal\")   \nend\n\n# The animation is saved and displayed as markdown picture for the automatic HTML generation\ngif(animation_naive, \"ai-mountain-car-naive.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"They failed as expected since the car doesn't have enough power. This helps to understand that the brute-force approach is not the most efficient one in this case and hopefully a bit of swinging is necessary to achieve the goal.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#Active-inference-approach","page":"Active Inference Mountain Car","title":"Active inference approach","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Now let's help them solve the problem with an active inference approach. Particularly, we create an agent that predicts the future car position as well as the best possible actions in a probabilistic manner.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"We start by specifying a probabilistic model for the agent that describes the agent's internal beliefs over the external dynamics of the environment. The generative model is defined as follows","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"beginaligned\np_t(xsu) propto p(s_t-1) prod_k=t^t+T p(x_k mid s_k)  p(s_k mid s_k-1u_k)  p(u_k)  p(x_k) nonumber\nendaligned","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"where the factors are defined as","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(x_k) = mathcalN(x_k mid x_goalV_goal)  quad (mathrmtarget)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(s_k mid s_k-1u_k) = mathcalN(s_k mid tildeg(s_k-1)+h(u_k)gamma^-1)  quad (mathrmstate  transition)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(x_k mid s_k) = mathcalN(x_k mid s_ktheta) quad (mathrmobservation)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(u_k) = mathcalN(u_k mid m_uV_u) quad (mathrmcontrol)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"p(s_t-1) = mathcalN(s_t-1 mid m_t-1V_t-1) quad (mathrmprevious  state)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"where ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"x\ndenotes observations of the agent after interacting with the environment; \ns_t = (s_tdots_t)\nis the state of the car embodying its position and velocity; \nu_t\ndenotes the control state of the agent; \nh(cdot)\nis the tanh(cdot) function modeling engine control; \ntildeg(cdot)\nexecutes a linear approximation of equations (1) and (2): ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"beginaligned \ndots_t = dots_t-1 + F_g(s_t-1) + F_f(dots_t-1)\ns_t = s_t-1 + dots_t\nendaligned","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"In the cell below, the @model macro and the meta blocks are used to define the probabilistic model and the approximation methods for the nonlinear state-transition functions, respectively. In addition, the beliefs over the future states (up to T steps ahead) of the agent is included.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"@model function mountain_car(m_u, V_u, m_x, V_x, m_s_t_min, V_s_t_min, T, Fg, Fa, Ff, engine_force_limit)\n    \n    # Transition function modeling transition due to gravity and friction\n    g = (s_t_min::AbstractVector) -> begin \n        s_t = similar(s_t_min) # Next state\n        s_t[2] = s_t_min[2] + Fg(s_t_min[1]) + Ff(s_t_min[2]) # Update velocity\n        s_t[1] = s_t_min[1] + s_t[2] # Update position\n        return s_t\n    end\n    \n    # Function for modeling engine control\n    h = (u::AbstractVector) -> [0.0, Fa(u[1])] \n    \n    # Inverse engine force, from change in state to corresponding engine force\n    h_inv = (delta_s_dot::AbstractVector) -> [atanh(clamp(delta_s_dot[2], -engine_force_limit+1e-3, engine_force_limit-1e-3)/engine_force_limit)] \n    \n    # Internal model perameters\n    Gamma = 1e4*diageye(2) # Transition precision\n    Theta = 1e-4*diageye(2) # Observation variance\n\n    s_t_min ~ MvNormal(mean = m_s_t_min, cov = V_s_t_min)\n    s_k_min = s_t_min\n\n    local s\n    \n    for k in 1:T\n        u[k] ~ MvNormal(mean = m_u[k], cov = V_u[k])\n        u_h_k[k] ~ h(u[k]) where { meta = DeltaMeta(method = Linearization(), inverse = h_inv) }\n        s_g_k[k] ~ g(s_k_min) where { meta = DeltaMeta(method = Linearization()) }\n        u_s_sum[k] ~ s_g_k[k] + u_h_k[k]\n        s[k] ~ MvNormal(mean = u_s_sum[k], precision = Gamma)\n        x[k] ~ MvNormal(mean = s[k], cov = Theta)\n        x[k] ~ MvNormal(mean = m_x[k], cov = V_x[k]) # goal\n        s_k_min = s[k]\n    end\n    \n    return (s, )\nend","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"After specifying the generative model, let's create an Active Inference(AI) agent for the car.  Technically, the agent goes through three phases: Act-Execute-Observe, Infer and Slide.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Act-Execute-Observe:   In this phase, the agent performs an action onto the environment at time t and gets T observations in exchange. These observations are basically the prediction of the agent on how the environment evolves over the next T time step. \nInfer:  After receiving observations, the agent starts updating its internal probabilistic model by doing inference. Particularly, it finds the posterior distributions over the state s_t and control u_t, i.e. p(s_tmid x_t) and p(u_tmid x_t).\nSlide:  After updating its internal belief, the agent moves to the next time step and uses the inferred action u_t in the previous time step to interact with the environment.  ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"In the cell below, we create the agent through the create_agent function, which includes compute, act, slide and future functions:","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"The act function selects the next action based on the inferred policy. On the other hand, the future function predicts the next T positions based on the current action. These two function implement the Act-Execute-Observe phase.\nThe compute function infers the policy (which is a set of actions for the next T time steps) and the agent's state using the agent internal model. This function implements the Infer phase. We call it compute to avoid the clash with the infer function of RxInfer.jl.\nThe slide function implements the Slide phase, which moves the agent internal model to the next time step.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"# We are going to use some private functionality from ReactiveMP, \n# in the future we should expose a proper API for this\nimport RxInfer.ReactiveMP: getrecent, messageout\n\nfunction create_agent(;T = 20, Fg, Fa, Ff, engine_force_limit, x_target, initial_position, initial_velocity)\n    Epsilon = fill(huge, 1, 1)                # Control prior variance\n    m_u = Vector{Float64}[ [ 0.0] for k=1:T ] # Set control priors\n    V_u = Matrix{Float64}[ Epsilon for k=1:T ]\n\n    Sigma    = 1e-4*diageye(2) # Goal prior variance\n    m_x      = [zeros(2) for k=1:T]\n    V_x      = [huge*diageye(2) for k=1:T]\n    V_x[end] = Sigma # Set prior to reach goal at t=T\n\n    # Set initial brain state prior\n    m_s_t_min = [initial_position, initial_velocity] \n    V_s_t_min = tiny * diageye(2)\n    \n    # Set current inference results\n    result = nothing\n\n    # The `infer` function is the heart of the agent\n    # It calls the `RxInfer.inference` function to perform Bayesian inference by message passing\n    compute = (upsilon_t::Float64, y_hat_t::Vector{Float64}) -> begin\n        m_u[1] = [ upsilon_t ] # Register action with the generative model\n        V_u[1] = fill(tiny, 1, 1) # Clamp control prior to performed action\n\n        m_x[1] = y_hat_t # Register observation with the generative model\n        V_x[1] = tiny*diageye(2) # Clamp goal prior to observation\n\n        data = Dict(:m_u       => m_u, \n                    :V_u       => V_u, \n                    :m_x       => m_x, \n                    :V_x       => V_x,\n                    :m_s_t_min => m_s_t_min,\n                    :V_s_t_min => V_s_t_min)\n        \n        model  = mountain_car(T = T, Fg = Fg, Fa = Fa, Ff = Ff, engine_force_limit = engine_force_limit) \n        result = infer(model = model, data = data)\n    end\n    \n    # The `act` function returns the inferred best possible action\n    act = () -> begin\n        if result !== nothing\n            return mode(result.posteriors[:u][2])[1]\n        else\n            return 0.0 # Without inference result we return some 'random' action\n        end\n    end\n    \n    # The `future` function returns the inferred future states\n    future = () -> begin \n        if result !== nothing \n            return getindex.(mode.(result.posteriors[:s]), 1)\n        else\n            return zeros(T)\n        end\n    end\n\n    # The `slide` function modifies the `(m_s_t_min, V_s_t_min)` for the next step\n    # and shifts (or slides) the array of future goals `(m_x, V_x)` and inferred actions `(m_u, V_u)`\n    slide = () -> begin\n\n        model  = RxInfer.getmodel(result.model)\n        (s, )  = RxInfer.getreturnval(model)\n        varref = RxInfer.getvarref(model, s) \n        var    = RxInfer.getvariable(varref)\n        \n        slide_msg_idx = 3 # This index is model dependend\n        (m_s_t_min, V_s_t_min) = mean_cov(getrecent(messageout(var[2], slide_msg_idx)))\n\n        m_u = circshift(m_u, -1)\n        m_u[end] = [0.0]\n        V_u = circshift(V_u, -1)\n        V_u[end] = Epsilon\n\n        m_x = circshift(m_x, -1)\n        m_x[end] = x_target\n        V_x = circshift(V_x, -1)\n        V_x[end] = Sigma\n    end\n\n    return (compute, act, slide, future)    \nend","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"create_agent (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Now it's time to see if we can help our friends arrive at the camping site by midnight?","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"(execute_ai, observe_ai) = create_world(\n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n) # Let there be a world\n\nT_ai = 50\n\n(compute_ai, act_ai, slide_ai, future_ai) = create_agent(; # Let there be an agent\n    T  = T_ai, \n    Fa = Fa,\n    Fg = Fg, \n    Ff = Ff, \n    engine_force_limit = engine_force_limit,\n    x_target = x_target,\n    initial_position = initial_position,\n    initial_velocity = initial_velocity\n) \n\nN_ai = 100\n\n# Step through experimental protocol\nagent_a = Vector{Float64}(undef, N_ai) # Actions\nagent_f = Vector{Vector{Float64}}(undef, N_ai) # Predicted future\nagent_x = Vector{Vector{Float64}}(undef, N_ai) # Observations\n\nfor t=1:N_ai\n    agent_a[t] = act_ai()               # Invoke an action from the agent\n    agent_f[t] = future_ai()            # Fetch the predicted future states\n    execute_ai(agent_a[t])              # The action influences hidden external states\n    agent_x[t] = observe_ai()           # Observe the current environmental outcome (update p)\n    compute_ai(agent_a[t], agent_x[t]) # Infer beliefs from current model state (update q)\n    slide_ai()                          # Prepare for next iteration\nend\n\nanimation_ai = @animate for i in 1:N_ai\n    # pls - plot landscape\n    pls = plot(valley_x, valley_y, title = \"Active inference results\", label = \"Landscape\", color = \"black\")\n    pls = scatter!(pls, [agent_x[i][1]], [height(agent_x[i][1])], label=\"car\")\n    pls = scatter!(pls, [x_target[1]], [height(x_target[1])], label=\"goal\")   \n    pls = scatter!(pls, agent_f[i], height.(agent_f[i]), label = \"Predicted future\", alpha = map(i -> 0.5 / i, 1:T_ai))\n    \n    # pef - plot engine force\n    pef = plot(Fa.(agent_a[1:i]), title = \"Engine force (agents actions)\", xlim = (0, N_ai), ylim = (-0.05, 0.05))\n    \n    plot(pls, pef, size = (800, 400))\nend\n    \n# The animation is saved and displayed as markdown picture for the automatic HTML generation\ngif(animation_ai, \"ai-mountain-car-ai.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Voila! The car now is able to reach the camping site with a smart strategy.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"The left figure shows the agent reached its goal by swinging and the right one shows the corresponding engine force. As we can see, at the beginning the agent tried to reach the goal directly (with full engine force) but after some trials it realized that's not possible. Since the agent looks ahead for 50 time steps, it has enough time to explore other policies, helping it learn to move back to get more momentum to reach the goal.","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Now our friends can enjoy their trip at the camping site!. ","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/#Reference","page":"Active Inference Mountain Car","title":"Reference","text":"","category":"section"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"We refer reader to the Thijs van de Laar (2019) \"Simulating active inference processes by message passing\" original paper with more in-depth overview and explanation of the active inference agent implementation by message passing. The original environment/task description is from Ueltzhoeffer (2017) \"Deep active inference\".","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/active_inference_mountain_car/","page":"Active Inference Mountain Car","title":"Active Inference Mountain Car","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/active_inference_mountain_car/Project.toml`\n  [34004b35] HypergeometricFunctions v0.3.27\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Predicting-Bike-Rental-Demand","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Important Note: This notebook primarily aims to show how to manage missing data and generate predictions using a model. It does not serve as a comprehensive guide to building the most advanced model for this dataset or showcase the best practices in feature engineering. To keep explanations clear, we will use simplified assumptions and straightforward models. For applications in the real world, you should employ more sophisticated approaches and feature engineering methods. However, we will present a more complex model towards the end of this notebook for you to explore, albeit with less detailed guidance.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using RxInfer","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Preamble:-Enabling-Predictions","page":"Predicting Bike Rental Demand","title":"Preamble: Enabling Predictions","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"RxInfer.jl facilitates predictions in two primary ways. ","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Implicit Prediction: By adding missing instances directly into the data, which are then treated as regular observations during inference.\nExplicit Prediction: By defining a separate data variable in the model. This approach doesn't necessitate passing missing instances as the data variable but does require specifying the predictvar argument in the inference function.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Example","page":"Predicting Bike Rental Demand","title":"Example","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Consider the following model:","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"@model function example_model(y)\n\n    h ~ NormalMeanPrecision(0, 1.0)\n    x ~ NormalMeanPrecision(h, 1.0)\n    y ~ NormalMeanPrecision(x, 10.0)\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Implicit Prediction\nresult = infer(model = example_model(), data = (y = missing,))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (h, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Explicit Prediction\nresult = infer(model = example_model(), predictvars = (y = KeepLast(),))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (h, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Both approaches yield the same results, but the choice depends on personal preferences and the model's structure. In scenarios with a clear distinction between observed and predicted variables, the explicit method is preferable. However, our subsequent example will not differentiate between observations and predictions, as it utilizes a state space representation.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using CSV, DataFrames, Plots","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Objective","page":"Predicting Bike Rental Demand","title":"Objective","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This example aims to simultaneously learn the dynamics of the feature space and predict hourly bike rental demand through reactive message passing, a signature approach of RxInfer.jl.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Dataset-Source","page":"Predicting Bike Rental Demand","title":"Dataset Source","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Data for this example study is sourced from the Kaggle Bike Count Prediction Dataset. For the purpose of this example, the original dataset from Kaggle has been adapted by removing categorical variables such as season, holiday, and working day. Additionally we take only 500 entries. This modification allows us to focus on modeling the continuous variables without additional complexities of handling categorical data. Nevertheless, this extension is feasible within RxInfer.jl.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Load the data\ndf = CSV.read(\"modified_bicycle.csv\", DataFrame)\ndf[1:10, :]","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"10×6 DataFrame\n Row │ datetime            temp     atemp    humidity  windspeed  count\n     │ String31            Float64  Float64  Float64   Float64    Int64\n─────┼──────────────────────────────────────────────────────────────────\n   1 │ 2011-01-01 0:00:00     9.84   14.395      81.0     0.0        16\n   2 │ 2011-01-01 1:00:00     9.02   13.635      80.0     0.0        40\n   3 │ 2011-01-01 2:00:00     9.02   13.635      80.0     0.0        32\n   4 │ 2011-01-01 3:00:00     9.84   14.395      75.0     0.0        13\n   5 │ 2011-01-01 4:00:00     9.84   14.395      75.0     0.0         1\n   6 │ 2011-01-01 5:00:00     9.84   12.88       75.0     6.0032      1\n   7 │ 2011-01-01 6:00:00     9.02   13.635      80.0     0.0         2\n   8 │ 2011-01-01 7:00:00     8.2    12.88       86.0     0.0         3\n   9 │ 2011-01-01 8:00:00     9.84   14.395      75.0     0.0         8\n  10 │ 2011-01-01 9:00:00    13.12   17.425      76.0     0.0        14","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# we reserve few samples for prediction\nn_future = 24\n\n# `x` is a sequence of observed features\nX = Union{Vector{Float64}, Missing}[[row[i] for i in 2:(ncol(df))-1] for row in eachrow(df)][1:end-n_future]\n\n# `y` is a sequence of \"count\" bicycles\ny = Union{Float64, Missing}[df[:, \"count\"]...][1:end-n_future]\n\nstate_dim = length(X[1]); # dimensionality of feature space","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Generative-Model-with-Priors","page":"Predicting Bike Rental Demand","title":"Generative Model with Priors","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"We present a generative model that delineates the latent dynamics of feature evolution, represented by mathbfh_t, and their link to the bike rental counts, mathbfy_t.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Equations-and-Priors","page":"Predicting Bike Rental Demand","title":"Equations and Priors","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Feature Dynamics with Prior:\nPrior: mathbfa sim mathcalN(mathbf0 mathbfI)\nDynamics: mathbfh_t sim mathcalN(mathbfA h_t-1 mathbfQ)\nmathbfA\nis the transition matrix, reshaped from the prior vector mathbfa, and mathbfQ represents process noise.\nNoisy Observations:\nmathbfx_t sim mathcalN(mathbfh_t mathbfP)\nRepresents the observed noisy state of the features.\nCount Prediction with Prior:\nPrior: boldsymboltheta sim mathcalN(mathbf0 mathbfI)\nPrediction: y_t sim mathcalN(textsoftplus(boldsymboltheta^topmathbfh_t) sigma^2)\nModels the bike rental count as influenced by a non-linear transformation of the hidden state.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Interpretation","page":"Predicting Bike Rental Demand","title":"Interpretation","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This framework aims to simultaneously infer the transition matrix mathbfA and the regression parameters boldsymboltheta, providing a comprehensive view of the feature space dynamics and the count prediction.\nBy employing Gaussian priors on both mathbfa and boldsymboltheta, we incorporate beliefs about their distributions.\nThe inference process aims to discover these underlying dynamics, enabling predictions of both features mathbfx_t and counts y_t.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# # We augument the dataset with missing entries for 24 hours ahead\nappend!(X, repeat([missing], n_future))\nappend!(y, repeat([missing], n_future));","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Function to perform the state transition in the model.\n# It reshapes vector `a` into a matrix and multiplies it with vector `x` to simulate the transition.\nfunction transition(a, x)\n    nm, n = length(a), length(x)\n    m = nm ÷ n  # Calculate the number of rows for reshaping 'a' into a matrix\n    A = reshape(a, (m, n))  \n    return A * x\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"transition (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# The dotsoftplus function combines a dot product and softplus transformation.\n# While useful for ensuring positivity, it may not be the optimal choice for all scenarios,\n# especially if the data suggests other forms of relationships or distributions.\nimport StatsFuns: softplus\ndotsoftplus(a, x) = softplus(dot(a, x))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"dotsoftplus (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# model definction\n@model function bicycle_ssm(x, y, h0, θ0, a0, Q, s)\n\n    a ~ a0\n    θ ~ θ0\n    h_prior ~ h0\n\n    h_prev = h_prior\n    for i in eachindex(y)\n        \n        h[i] ~ MvNormal(μ=transition(a, h_prev), Σ=Q)\n        x[i] ~ MvNormal(μ=h[i], Σ=diageye(state_dim))\n        y[i] ~ Normal(μ=dotsoftplus(θ, h[i]), σ²=s)\n        h_prev = h[i]\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# In this example, we opt for a basic Linearization approach for the transition and dotsoftplus functions.\n# However, alternative methods like Unscented or CVI approximations can also be considered.\nbicycle_ssm_meta = @meta begin \n    transition() -> Linearization()\n    dotsoftplus() -> Linearization()\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Meta: \n  transition() -> ReactiveMP.Linearization()\n  dotsoftplus() -> ReactiveMP.Linearization()","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# prior_h: Based on first observation, assuming initial state is similar with equal variance.\nprior_h = MvNormalMeanCovariance(X[1], diageye(state_dim))\n# prior_θ, prior_a: No initial bias, parameters independent with equal uncertainty.\nprior_θ = MvNormalMeanCovariance(zeros(state_dim), diageye(state_dim))\nprior_a = MvNormalMeanCovariance(zeros(state_dim^2), diageye(state_dim^2));","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Note that, in contrast with other examples, we wrap the data y in an UnfactorizedData struct. This is to indicate to the inference engine that we want to infer a joint posterior distribution over the missing values in y and the latent variables. More information on this can be found in the documentation on variational constraints.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# the deterministic relationsships (transition) and (dotsoftplus) will induce loops in the graph representation of our model, this necessiates the initialization of the messages\nimessages = @initialization begin\n    μ(h) = prior_h\n    μ(a) = prior_a\n    μ(θ) = prior_θ\nend\n# Assumptions about the model parameters:\n# Q: Process noise based on observed features' variance, assuming process variability reflects observed features variability.\n# s: Observation noise based on observed data variance, directly estimating variance in the data, important for predictions\nbicycle_model = bicycle_ssm(h0=prior_h, θ0=prior_θ, a0=prior_a, Q=var(filter(!ismissing, X)).*diageye(state_dim), s=var(filter(!ismissing, y)))\n\nresult = infer(\n    model = bicycle_model,\n    data  = (x=X, y=UnfactorizedData(y)), \n    options = (limit_stack_depth = 500, ), \n    returnvars = KeepLast(),\n    predictvars = KeepLast(),\n    initialization = imessages,\n    meta = bicycle_ssm_meta,\n    iterations = 20,\n    showprogress=true,\n)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (a, h, h_prior, θ)\n  Predictions      | available for (y, x)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# For a sake of this example, we extract only predictions\nmean_y, cov_y = mean.(result.predictions[:y]), cov.(result.predictions[:y])\nmean_x, cov_x = mean.(result.predictions[:x]), var.(result.predictions[:x])\n\nmean_x1, cov_x1 = getindex.(mean_x, 1), getindex.(cov_x, 1)\nmean_x2, cov_x2 = getindex.(mean_x, 2), getindex.(cov_x, 2)\nmean_x3, cov_x3 = getindex.(mean_x, 3), getindex.(cov_x, 3)\nmean_x4, cov_x4 = getindex.(mean_x, 4), getindex.(cov_x, 4);","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"slice = (300, length(y))\ndata = df[:, \"count\"][length(y)-n_future:length(y)]\n\np = scatter(y, \n            color=:darkblue, \n            markerstrokewidth=0,\n            label=\"Observed Count\", \n            alpha=0.6)\n\n# Plotting the mean prediction with variance ribbon\nplot!(mean_y, ribbon=sqrt.(cov_y), \n      color=:orange, \n      fillalpha=0.3,\n      label=\"Predicted Mean ± Std Dev\")\n\n# Adding a vertical line to indicate the start of the future prediction\nvline!([length(y)-n_future], \n       label=\"Prediction Start\", \n       linestyle=:dash, \n       linecolor=:green)\n\n# Future (unobserved) data\nplot!(length(y)-n_future:length(y), data, label=\"Future Count\")\n\n# Adjusting the limits\nxlims!(slice)\n\n# Enhancing the plot with titles and labels\ntitle!(\"Bike Rental Demand Prediction\")\nxlabel!(\"Time\")\nylabel!(\"Bike Count\")\n\n# Adjust the legend\nlegend=:topright\n\n# Show the final plot\ndisplay(p)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using Plots\n\n# Define a color palette\npalette = cgrad(:viridis)\n\n# Plot the hidden states with observations\np1 = plot(mean_x1, ribbon=sqrt.(cov_x1), color=palette[1], label=\"Hidden State 1\", legend=:topleft)\nplot!(df[!, :temp], color=:grey, label=\"Temperature\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Temperature vs Hidden State 1\")\n\np2 = plot(mean_x2, ribbon=sqrt.(cov_x2), color=palette[2], label=\"Hidden State 2\", legend=:topleft)\nplot!(df[!, :atemp], color=:grey, label=\"Feels-Like Temp\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Feels-Like Temp vs Hidden State 2\")\n\np3 = plot(mean_x3, ribbon=sqrt.(cov_x3), color=palette[3], label=\"Hidden State 3\", legend=:topleft)\nplot!(df[!, :humidity], color=:grey, label=\"Humidity\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Humidity vs Hidden State 3\")\n\np4 = plot(mean_x4, ribbon=sqrt.(cov_x4), color=palette[4], label=\"Hidden State 4\", legend=:topleft)\nplot!(df[!, :windspeed], color=:grey, label=\"Windspeed\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Windspeed vs Hidden State 4\")\n\nfor p in [p1, p2, p3, p4]\n    xlims!(p, first(slice), last(slice))\nend\n\nplot(p1, p2, p3, p4, layout=(2, 2), size=(800, 400))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/#Improving-the-model","page":"Predicting Bike Rental Demand","title":"Improving the model","text":"","category":"section"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"While our current model's predictions may not closely match real-world data, it's important to recognize that certain assumptions and simplifications were made that might have affected the results. The model is essentially a theoretical framework, highlighting the ability to simultaneously deduce states, parameters, and predictions, with an emphasis on the analysis's predictive aspect.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"To enhance the model and refine its predictions, we can employ variational message passing. This method enables us to eliminate loops within the model by substituting initial messages with initial marginal distributions. This is achieved by utilizing ContinuousTransition (also referred to as CTransition) and SoftDot (aka softdot) nodes. These nodes facilitate the variational approximation of the transition matrix and the dot product, respectively.","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"transformation = a -> reshape(a, state_dim, state_dim)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"#31 (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# model definction\n@model function bicycle_ssm_advanced(x, y, h0, θ0, a0, P0, γ0)\n\n    a ~ a0\n    θ ~ θ0\n    h_prior ~ h0\n    P ~ P0\n    γ ~ γ0\n\n    h_prev = h_prior\n    for i in eachindex(y)\n        \n        h[i] ~ CTransition(h_prev, a, P)\n        x[i]  ~ MvNormal(μ=h[i], Λ=diageye(state_dim))\n        _y[i] ~ softdot(θ, h[i], γ)\n        y[i] ~ Normal(μ=softplus(_y[i]), γ=1e4)\n        h_prev = h[i]\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"bicycle_ssm_advanced_meta = @meta begin \n    softplus() -> Linearization()\n    CTransition() -> CTMeta(transformation)\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Meta: \n  softplus() -> ReactiveMP.Linearization()\n  ReactiveMP.ContinuousTransition() -> ReactiveMP.ContinuousTransitionMeta{\nMain.anonymous.var\"#31#32\"}(Main.anonymous.var\"#31#32\"())","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"bicycle_ssm_advanced_constraints = @constraints begin\n    q(h_prior, h, a, P, γ, _y, y, θ) = q(h_prior, h)q(a)q(P)q(γ)q(_y, y)q(θ)\nend","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Constraints: \n  q(h_prior, h, a, P, γ, _y, y, θ) = q(h_prior, h)q(a)q(P)q(γ)q(_y, y)q(θ)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"prior_P = ExponentialFamily.WishartFast(state_dim+2, inv.(var(filter(!ismissing, X))) .* diageye(state_dim))\nprior_a = MvNormalMeanPrecision(ones(state_dim^2), diageye(state_dim^2));\n\nprior_γ = GammaShapeRate(1.0, var(filter(!ismissing, y)))\nprior_h = MvNormalMeanPrecision(X[1], diageye(state_dim))\nprior_θ = MvNormalMeanPrecision(ones(state_dim), diageye(state_dim))\n\nimarginals = @initialization begin \n    q(h) = prior_h\n    q(a) = prior_a\n    q(P) = prior_P\n    q(γ) = prior_γ\n    q(θ) = prior_θ\nend\n\nbicycle_model_advanced = bicycle_ssm_advanced(h0=prior_h, θ0=prior_θ, a0=prior_a, P0=prior_P, γ0=prior_γ)\n\nresult_advanced = infer(\n    model = bicycle_model_advanced,\n    data  = (x=X, y=y), \n    options = (limit_stack_depth = 500, ), \n    returnvars = KeepLast(),\n    predictvars = KeepLast(),\n    initialization = imarginals,\n    constraints = bicycle_ssm_advanced_constraints,\n    meta = bicycle_ssm_advanced_meta,\n    iterations = 10,\n    showprogress=true,\n)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (a, P, _y, γ, h, h_prior, θ)\n  Predictions      | available for (y, x)","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# For a sake of this example, we extract only predictions\nmean_y, cov_y = mean.(result_advanced.predictions[:y]), cov.(result_advanced.predictions[:y])","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"([16.000001019201765, 39.9999999771452, 32.00000029690829, 13.0000012305664\n04, 1.000001735201384, 1.0000019345942357, 2.0000015515352927, 3.0000014149\n59412, 8.000001680637501, 14.000001845050349  …  30.828422951410317, 30.492\n908849047463, 30.18361227547192, 29.894986174196585, 29.622442386947597, 29\n.36222177841353, 29.1112910142145, 28.867262253805237, 28.628327961907303, \n28.393168163897517], [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.000\n1, 0.0001, 0.0001, 0.0001  …  0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.000\n1, 0.0001, 0.0001, 0.0001, 0.0001])","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"slice = (300, length(y))\ndata = df[:, \"count\"][length(y)-n_future:length(y)]\n\npa = scatter(y, \n            color=:darkblue, \n            markerstrokewidth=0,\n            label=\"Observed Count\", \n            alpha=0.6)\n\n# Plotting the mean prediction with variance ribbon\nplot!(mean_y, ribbon=sqrt.(cov_y), \n      color=:orange, \n      fillalpha=0.3,\n      label=\"Predicted Mean ± Std Dev\")\n\n# Adding a vertical line to indicate the start of the future prediction\nvline!([length(y)-n_future], \n       label=\"Prediction Start\", \n       linestyle=:dash, \n       linecolor=:green)\n\n# Future (unobserved) data\nplot!(length(y)-n_future:length(y), data, label=\"Future Count\")\n\n# Adjusting the limits\nxlims!(slice)\n\n# Enhancing the plot with titles and labels\ntitle!(\"Advanced model\")\nxlabel!(\"Time\")\nylabel!(\"Bike Count\")\n\n# Adjust the legend\nlegend=:topright\n\n# Show the final plot\nplot(pa, p, size=(800, 400))","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/predicting_bike_rental_demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/predicting_bike_rental_demand/Project.toml`\n  [336ed68f] CSV v0.10.15\n  [a93c6f00] DataFrames v1.7.0\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [4c63d2b9] StatsFuns v1.3.2\n","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Infinite-Data-Stream","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"This example shows the capabilities of RxInfer to perform Bayesian inference on real-time signals. As usual, first, we start with importing necessary packages:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"using RxInfer, Plots, Random, StableRNGs","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"For demonstration purposes we will create a synthetic environment that has a hidden underlying signal, which we cannot observer directly. Instead, we will observe a noised realisation of this hidden signal:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"mutable struct Environment\n    rng                   :: AbstractRNG\n    current_state         :: Float64\n    observation_precision :: Float64\n    history               :: Vector{Float64}\n    observations          :: Vector{Float64}\n    \n    Environment(current_state, observation_precision; seed = 123) = begin \n         return new(StableRNG(seed), current_state, observation_precision, [], [])\n    end\nend\n\nfunction getnext!(environment::Environment)\n    environment.current_state = environment.current_state + 1.0\n    nextstate  = 10sin(0.1 * environment.current_state)\n    observation = rand(NormalMeanPrecision(nextstate, environment.observation_precision))\n    push!(environment.history, nextstate)\n    push!(environment.observations, observation)\n    return observation\nend\n\nfunction gethistory(environment::Environment)\n    return environment.history\nend\n\nfunction getobservations(environment::Environment)\n    return environment.observations\nend","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"getobservations (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Model-specification","page":"Infinite Data Stream","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"We assume that we don't know the shape of our signal in advance. So we try to fit a simple gaussian random walk with unknown observation noise:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"@model function kalman_filter(x_prev_mean, x_prev_var, τ_shape, τ_rate, y)\n    x_prev ~ Normal(mean = x_prev_mean, variance = x_prev_var)\n    τ ~ Gamma(shape = τ_shape, rate = τ_rate)\n\n    # Random walk with fixed precision\n    x_current ~ Normal(mean = x_prev, precision = 1.0)\n    y ~ Normal(mean = x_current, precision = τ)\n    \nend\n\n# We assume the following factorisation between variables \n# in the variational distribution\n@constraints function filter_constraints()\n    q(x_prev, x_current, τ) = q(x_prev, x_current)q(τ)\nend","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"filter_constraints (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Prepare-environment","page":"Infinite Data Stream","title":"Prepare environment","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"initial_state         = 0.0\nobservation_precision = 0.1","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"0.1","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"After we have created the environment we can observe how our signal behaves:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"testenvironment = Environment(initial_state, observation_precision);\n\nanimation = @animate for i in 1:100\n    getnext!(testenvironment)\n    \n    history = gethistory(testenvironment)\n    observations = getobservations(testenvironment)\n    \n    p = plot(size = (1000, 300))\n    \n    p = plot!(p, 1:i, history[1:i], label = \"Hidden signal\")\n    p = scatter!(p, 1:i, observations[1:i], ms = 4, alpha = 0.7, label = \"Observation\")\nend\n\ngif(animation, \"infinite-data-stream.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Filtering-on-static-dataset","page":"Infinite Data Stream","title":"Filtering on static dataset","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"RxInfer is flexible and allows for running inference both on real-time and static datasets. In the next section we show how to perform the filtering procedure on a static dataset. We also will verify our inference procedure by checking on the Bethe Free Energy values:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"n                  = 300\nstatic_environment = Environment(initial_state, observation_precision);\n\nfor i in 1:n\n    getnext!(static_environment)\nend\n\nstatic_history      = gethistory(static_environment)\nstatic_observations = getobservations(static_environment);\nstatic_datastream   = from(static_observations) |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_static(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    init = @initialization begin\n        q(x_current) = NormalMeanVariance(0.0, 1e3) \n        q(τ) = GammaShapeRate(1.0, 1.0)\n    end\n\n    engine = infer(\n        model          = kalman_filter(),\n        constraints    = filter_constraints(),\n        datastream     = datastream,\n        autoupdates    = autoupdates,\n        returnvars     = (:x_current, ),\n        keephistory    = 10_000,\n        historyvars    = (x_current = KeepLast(), τ = KeepLast()),\n        initialization = init,\n        iterations     = 10,\n        free_energy    = true,\n        autostart      = true,\n    )\n    \n    return engine\nend","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_static (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"result = run_static(static_environment, static_datastream);","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"static_inference = @animate for i in 1:n\n    estimated = result.history[:x_current]\n    p = plot(1:i, mean.(estimated[1:i]), ribbon = var.(estimated[1:n]), label = \"Estimation\")\n    p = plot!(static_history[1:i], label = \"Real states\")    \n    p = scatter!(static_observations[1:i], ms = 2, label = \"Observations\")\n    p = plot(p, size = (1000, 300), legend = :bottomright)\nend\n\ngif(static_inference, \"infinite-data-stream-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"plot(result.free_energy_history, label = \"Bethe Free Energy (averaged)\")","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/#Filtering-on-realtime-dataset","page":"Infinite Data Stream","title":"Filtering on realtime dataset","text":"","category":"section"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"Next lets create a \"real\" infinite stream. We use timer() observable from Rocket.jlto emulate real-world scenario. In our example we are going to generate a new data point every ~41ms (24 data points per second). For demonstration purposes we force stop after n data points, but there is no principled limitation to run inference indefinite:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_and_plot(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    posteriors = []\n    \n    plotfn = (q_current) -> begin \n        IJulia.clear_output(true)\n        \n        push!(posteriors, q_current)\n\n        p = plot(mean.(posteriors), ribbon = var.(posteriors), label = \"Estimation\")\n        p = plot!(gethistory(environment), label = \"Real states\")    \n        p = scatter!(getobservations(environment), ms = 2, label = \"Observations\")\n        p = plot(p, size = (1000, 300), legend = :bottomright)\n\n        display(p)\n    end\n    \n    init = @initialization begin\n        q(x_current) = NormalMeanVariance(0.0, 1e3)\n        q(τ) = GammaShapeRate(1.0, 1.0)\n    end\n\n    engine = infer(\n        model         = kalman_filter(),\n        constraints   = filter_constraints(),\n        datastream    = datastream,\n        autoupdates   = autoupdates,\n        returnvars    = (:x_current, ),\n        initialization = init,\n        iterations    = 10,\n        autostart     = false,\n    )\n    \n    qsubscription = subscribe!(engine.posteriors[:x_current], plotfn)\n    \n    RxInfer.start(engine)\n    \n    return engine\nend","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_and_plot (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"# This example runs in our documentation pipeline, which does not support \"real-time\" execution context\n# We skip this code if run not in Jupyter notebook (see below an example with gif)\nengine = nothing \nif isdefined(Main, :IJulia)\n    timegen      = 41 # 41 ms\n    environment  = Environment(initial_state, observation_precision);\n    observations = timer(timegen, timegen) |> map(Float64, (_) -> getnext!(environment)) |> take(n) # `take!` automatically stops after `n` observations\n    datastream   = observations |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));\n    engine = run_and_plot(environment, datastream)\nend;","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"The plot above is fully interactive and we can stop and unsubscribe from our datastream before it ends:","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"if !isnothing(engine) && isdefined(Main, :IJulia)\n    RxInfer.stop(engine)\n    IJulia.clear_output(true)\nend;","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/infinite_data_stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/infinite_data_stream/Project.toml`\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Autoregressive-Models","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Ever wondered how financial analysts predict tomorrow's stock prices, how meteorologists forecast next week's weather, or how engineers anticipate system failures before they happen? Welcome to the fascinating world of autoregressive models – the mathematical engines that power predictions when the future depends on the past.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In this hands-on example, we'll dive into the elegant framework of Bayesian autoregressive modeling using RxInfer.jl, a powerful probabilistic programming library that makes complex inference tasks surprisingly accessible. Unlike traditional approaches that give you a single prediction, our Bayesian approach provides complete predictive distributions, capturing the uncertainty that's inherent in any real-world forecast.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"You'll discover how to:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Create and understand AR models through a Bayesian lens with RxInfer.jl and @model macro\nGenerate synthetic data to test your inference algorithms\nPerform automated variational Bayesian inference with RxInfer.jl\nMake probabilistic predictions with quantified uncertainty\nApply these techniques to real-world stock price data\nAs a bonus we implement a simple version of ARMA models","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Whether you're predicting financial markets, analyzing sensor readings, modeling climate patterns, or exploring any time-dependent phenomenon, the techniques you'll learn here provide a robust foundation for sophisticated time series analysis using autoregressive models.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#The-Mathematics-Behind-Autoregressive-Models","page":"Autoregressive Models","title":"The Mathematics Behind Autoregressive Models","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"At their core, autoregressive (AR) models capture a fundamental principle: the future depends on the past. But how do we translate this intuition into mathematical precision? Let's build the framework together.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Imagine we're tracking a variable over time - stock prices, temperature readings, or any quantity that evolves sequentially. In an AR model, we express the current value as a function of its previous values, plus some random noise. This elegantly captures both deterministic patterns and inherent uncertainty.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In our Bayesian formulation, we model this process as:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\np(gamma) = Gamma(gammaa b)\np(mathbftheta) = mathcalN(mathbfthetamathbfmu Sigma)\np(x_tmathbfx_t-1t-k) = mathcalN(x_tmathbftheta^Tmathbfx_t-1t-k gamma^-1)\np(y_tx_t) = mathcalN(y_tx_t tau^-1)\nendaligned","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Here's what this means in plain language:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"x_t\nrepresents our system's true state at time t\nmathbfx_t-1t-k\ncaptures the sequence of k previous states\nmathbftheta\nholds the \"memory coefficients\" - how much each past state influences the present\ngamma\ncontrols the randomness in state transitions (higher values mean less randomness)\ny_t\nis what we actually observe, which includes some measurement noise controlled by tau","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"It's worth noting that this particular formulation is a latent autoregressive model, where the AR process (x_t) is hidden behind the likelihood function. In classical AR models, the states are directly observed without this additional observation layer. This latent structure gives us more flexibility in modeling real-world phenomena where measurements contain noise or where the underlying process isn't directly observable.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The beauty of this formulation is that it handles both the \"signal\" (predictable patterns) and the \"noise\" (random fluctuations) in a principled way.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For readers interested in the deeper theoretical foundations, we recommend Albert Podusenko's excellent work on Message Passing-Based Inference for Time-Varying Autoregressive Models.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Now, let's translate this mathematical framework into code and see it in action!","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"using RxInfer, Distributions, LinearAlgebra, Plots, StableRNGs, DataFrames, CSV, Dates","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Starting-Simple:-From-Synthetic-to-Real-World-Data","page":"Autoregressive Models","title":"Starting Simple: From Synthetic to Real-World Data","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In our code implementation, we begin by generating synthetic data using predefined sets of coefficients for autoregressive models with orders 1, 2, and 5.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Starting with synthetic data offers several advantages:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Ground Truth: We know the exact coefficients that generated the data, making it possible to evaluate how well our inference algorithms recover these parameters.\nControl: We can test our models under different noise levels, sample sizes, and process specifications without the complexity of real-world data.\nLearning Progression: By beginning with synthetic examples, we can build intuition about how AR models behave before tackling the messier challenges of real data.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Later in the example, we'll transition to real-world stock price data, where the true generative process is unknown.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# The following coefficients correspond to stable poles\ncoefs_ar_1 = [-0.27002517200218096]\ncoefs_ar_2 = [0.4511170798064709, -0.05740081602446657]\ncoefs_ar_5 = [0.10699399235785655, -0.5237303489793305, 0.3068897071844715, -0.17232255282458891, 0.13323964347539288];","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The coefficients we've selected aren't arbitrary - they're carefully chosen to ensure stability in our autoregressive processes. In signal processing and time series analysis, stable poles refer to coefficients that keep the AR process from exploding or diverging over time. Mathematically, this means that the roots of the AR characteristic polynomial must lie inside the unit circle in the complex plane. For example, in a first-order AR model where x_t = theta x_t-1 + varepsilon_t, we need theta  1 to ensure stability. For higher-order models, the constraints become more complex, but the principle remains the same: without stability, our models would produce unrealistic, explosive behavior.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function generate_synthetic_dataset(; n, θ, γ = 1.0, τ = 1.0, rng = StableRNG(42), states1 = randn(rng, length(θ)))\n    order = length(θ)\n\n    # Convert precision parameters to standard deviation\n    τ_std = sqrt(inv(τ))\n\n    # Initialize states and observations\n    states       = Vector{Vector{Float64}}(undef, n + 3order)\n    observations = Vector{Float64}(undef, n + 3order)\n\n    # `NormalMeanPrecision` is exported by `RxInfer.jl`\n    # and is a part of `ExponentialFamily.jl`\n    states[1]       = states1\n    observations[1] = rand(rng, NormalMeanPrecision(states[1][1], γ))\n    \n    for i in 2:(n + 3order)\n        previous_state  = states[i - 1]\n        transition      = dot(θ, previous_state)\n        next_x          = rand(rng, NormalMeanPrecision(transition, τ))\n        states[i]       = vcat(next_x, previous_state[1:end-1])\n        observations[i] = rand(rng, NormalMeanPrecision(next_x, γ))\n    end\n    \n    return states[1+3order:end], observations[1+3order:end]\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"generate_synthetic_dataset (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can now generate several synthetic datasets and plot them to see how they look like:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function plot_synthetic_dataset(; dataset, title)\n    states, observations = dataset\n    p = plot(first.(states), label = \"Hidden states\", title = title)\n    p = scatter!(p, observations, label = \"Observations\")\n    return p\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot_synthetic_dataset (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"dataset_1 = generate_synthetic_dataset(n = 100, θ = coefs_ar_1)\ndataset_2 = generate_synthetic_dataset(n = 100, θ = coefs_ar_2)\ndataset_5 = generate_synthetic_dataset(n = 100, θ = coefs_ar_5)\n\np1 = plot_synthetic_dataset(dataset = dataset_1, title = \"AR(1)\")\np2 = plot_synthetic_dataset(dataset = dataset_2, title = \"AR(2)\")\np3 = plot_synthetic_dataset(dataset = dataset_5, title = \"AR(5)\")\n\nplot(p1, p2, p3, layout = @layout([ a b ; c ]))","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Model-Specification:-Translating-Theory-to-Code","page":"Autoregressive Models","title":"Model Specification: Translating Theory to Code","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"With our synthetic data ready, we now tackle the critical step of encoding our autoregressive model as a probabilistic program in RxInfer. This translation from mathematical notation to executable code is where the power of probabilistic programming truly shines.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@model function lar_multivariate(y, order, γ)\n    # `c` is a unit vector of size `order` with first element equal to 1\n    c = ReactiveMP.ar_unit(Multivariate, order)\n    \n    τ  ~ Gamma(α = 1.0, β = 1.0)\n    θ  ~ MvNormal(mean = zeros(order), precision = diageye(order))\n    x0 ~ MvNormal(mean = zeros(order), precision = diageye(order))\n    \n    x_prev = x0\n    \n    for i in eachindex(y)\n \n        x[i] ~ AR(x_prev, θ, τ) \n        y[i] ~ Normal(mean = dot(c, x[i]), precision = γ)\n        \n        x_prev = x[i]\n    end\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Constraints-specification","page":"Autoregressive Models","title":"Constraints specification","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Bayesian inference for complex models often requires approximations. Our code uses variational inference with a specific factorization constraint:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@constraints function ar_constraints() \n    q(x0, x, θ, τ) = q(x0, x)q(θ)q(τ)\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_constraints (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"This constraint defines how we'll approximate the joint posterior:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We factorize it into three independent components\nStates (x0, x) remain jointly distributed, preserving temporal dependencies\nModel parameters (θ, τ) are separated from states and each other\nEach component can be updated independently during inference","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"This factorization balances statistical accuracy with computational efficiency and allows RxInfer to apply efficient message-passing algorithms while maintaining the most important dependencies in the model.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Meta-specification","page":"Autoregressive Models","title":"Meta specification","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The @meta block in RxInfer provides essential configuration information to specific nodes in your probabilistic model. In short, it tells RxInfer how to customize inference algorithms, what approximation methods to use, and allows to specify extra computational parameters for custom complex factor nodes. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@meta function ar_meta(order)\n    AR() -> ARMeta(Multivariate, order, ARsafe())\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_meta (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For autoregressive models, this block tells RxInfer:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Which type of AR process to use (Univariate/Multivariate)\nThe order of the process (how many past values influence the current one)\nAny stability constraints to apply during inference","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For more information about specific arguments refer to the AR node documentation. For more information on meta blocks, see the RxInfer.jl documentation.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Initialization-specification","page":"Autoregressive Models","title":"Initialization specification","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The @initialization block in RxInfer specifies the initial marginal distributions for the model parameters. This is crucial for the convergence of inference algorithms, especially for complex models like autoregressive processes.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For our autoregressive models, we initialize:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@initialization function ar_init(order)\n    q(τ) = GammaShapeRate(1.0, 1.0)\n    q(θ) = MvNormalMeanPrecision(zeros(order), diageye(order))\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_init (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Inference","page":"Autoregressive Models","title":"Inference","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"With our model defined, constraints established, and meta configurations in place, we're now at the exciting moment of truth - running Bayesian inference on our latent autoregressive model!","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"real_θ = coefs_ar_5\nreal_τ = 0.5\nreal_γ = 2.0\n\norder = length(real_θ)\nn     = 500 \n\nstates, observations = generate_synthetic_dataset(n = n, θ = real_θ, τ = real_τ, γ = real_γ)\n\nresult = infer(\n    model          = lar_multivariate(order = order, γ = real_γ), \n    data           = (y = observations, ),\n    constraints    = ar_constraints(),\n    meta           = ar_meta(order),\n    initialization = ar_init(order),\n    options        = (limit_stack_depth = 500, ),\n    returnvars     = (x = KeepLast(), τ = KeepEach(), θ = KeepEach()),\n    free_energy    = true,\n    iterations     = 20\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Inference results:\n  Posteriors       | available for (τ, θ, x)\n  Free Energy:     | Real[1575.77, 1261.1, 1056.63, 1011.25, 1001.28, 997.4\n91, 995.92, 995.375, 995.066, 994.961, 994.913, 994.833, 994.899, 994.864, \n994.834, 994.894, 994.872, 994.88, 994.897, 994.913]","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Now that our inference procedure has completed, we've obtained posterior distributions for all our model parameters and latent states. The AR coefficients, precision parameters, and hidden state sequence have all been inferred from the data, with complete uncertainty quantification.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"mean(result.posteriors[:θ][end])","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"5-element Vector{Float64}:\n  0.06182696232009309\n -0.5172296604873058\n  0.18038487975628942\n -0.1591736495117487\n  0.0966030389097711","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"cov(result.posteriors[:θ][end])","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"5×5 Matrix{Float64}:\n  0.0019919    -9.0483e-5     0.00100443  -0.000263633   0.000309501\n -9.0483e-5     0.00194875   -9.51917e-5   0.000858899  -0.000261436\n  0.00100443   -9.51917e-5    0.00241975  -9.69051e-5    0.00100208\n -0.000263633   0.000858899  -9.69051e-5   0.00195085   -9.23067e-5\n  0.000309501  -0.000261436   0.00100208  -9.23067e-5    0.00199453","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"real_θ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"5-element Vector{Float64}:\n  0.10699399235785655\n -0.5237303489793305\n  0.3068897071844715\n -0.17232255282458891\n  0.13323964347539288","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"But numbers alone don't tell the full story. Let's visualize these results to better understand what our model has captured. By plotting the inferred latent states against our observations, we can see how well our model has filtered out the noise to reveal the underlying process dynamics.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"posterior_states       = result.posteriors[:x]\nposterior_τ            = result.posteriors[:τ]\n\np1 = plot(first.(states), label=\"Hidden state\")\np1 = scatter!(p1, observations, label=\"Observations\")\np1 = plot!(p1, first.(mean.(posterior_states)), ribbon = 3first.(std.(posterior_states)), label=\"Inferred states (+-3σ)\", legend = :bottomright)\np1 = lens!(p1, [20, 40], [-2, 2], inset = (1, bbox(0.2, 0.0, 0.4, 0.4)))\n\np2 = plot(mean.(posterior_τ), ribbon = 3std.(posterior_τ), label = \"Inferred τ (+-3σ)\", legend = :topright)\np2 = plot!([ real_τ ], seriestype = :hline, label = \"Real τ\")\n\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"When performing variational inference in RxInfer, the Bethe Free Energy (BFE) graph is a crucial diagnostic tool that reveals the convergence properties of our inference algorithm.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The Bethe Free Energy represents the objective function being minimized during variational inference. On the graph:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The vertical axis shows the BFE value (lower is better)\nThe horizontal axis shows iteration number\nThe downward slope indicates the algorithm is improving its approximation\nA plateau signals convergence - the point where additional iterations yield minimal improvement","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(result.free_energy, label = \"Bethe Free Energy\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For autoregressive models specifically, the BFE graph helps us:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Confirm convergence: Ensuring our parameter and state estimates are reliable\nDetect inference challenges: Slow convergence may indicate model misspecification\nCompare models: Different AR orders or constraints can be compared by their final BFE values","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"A sharply decreasing curve that quickly plateaus suggests efficient, successful inference In contrast, a slowly decreasing or unstable curve might indicate challenges with our model specification or data characteristics.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"It is also interesting to plot the convergence of our AR coefficients:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"posterior_coefficients = result.posteriors[:θ]\n\npθ = []\ncθ = Plots.palette(:tab10)\n\nθms = mean.(posterior_coefficients)\nθvs = 3std.(posterior_coefficients)\n\nfor i in 1:length(first(θms))\n    push!(pθ, plot(getindex.(θms, i), ribbon = getindex.(θvs, i), label = \"Estimated θ[$i]\", color = cθ[i]))\nend\n\nfor i in 1:length(real_θ)\n    plot!(pθ[i], [ real_θ[i] ], seriestype = :hline, label = \"Real θ[$i]\", color = cθ[i], linewidth = 2)\nend\n\nplot(pθ..., size = (800, 300), legend = :bottomright)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The inference process has successfully recovered the key parameters of our autoregressive model with good precision. Looking at the plots, we can see well-formed posterior distributions for both our AR coefficients (θ) and precision parameters (τ). The visualization reveals not just point estimates, but complete distributions that capture the remaining uncertainty in each parameter. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Prediction-with-Autoregressive-Models","page":"Autoregressive Models","title":"Prediction with Autoregressive Models","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In the next section, we'll put our inferred model to the ultimate test: prediction. Having captured the underlying dynamics of our time series through Bayesian inference, we can now use these posterior distributions to forecast future values with quantified uncertainty. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"It's worth emphasizing that in our latent autoregressive model, we're predicting the hidden state process rather than directly observed values. This is a more challenging task than prediction in classical AR models where states are directly observed. We must account for both the uncertainty in the AR dynamics and the additional uncertainty introduced by the observation model.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"RxInfer makes this process remarkably straightforward, allowing us to propagate our beliefs about model parameters and states forward in time. The resulting predictive distributions will show not just what we expect to happen next, but how confident we should be in those expectations.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Example-with-Sinusoidal-Autoregressive-Pattern","page":"Autoregressive Models","title":"Example with Sinusoidal Autoregressive Pattern","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For this demonstration, we'll use a sinusoidal-like signal to test our predictive capabilities. This choice is particularly meaningful since sinusoidal patterns can be perfectly generated by second-order autoregressive processes with specific coefficients.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function generate_sinusoidal_coefficients(; f) \n    a1 = 2cos(2pi*f)\n    a2 = -1\n    return [a1, a2]\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"generate_sinusoidal_coefficients (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The signal then would look something like:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Generate coefficients\npredictions_coefficients = generate_sinusoidal_coefficients(f = 0.03)\n\n# Generate dataset\npredictions_dataset = generate_synthetic_dataset(n = 350, θ = predictions_coefficients, τ = 1.0, γ = 0.01)\n\n# Plot dataset\nplot_synthetic_dataset(dataset = predictions_dataset, title = \"Sinusoidal AR(2)\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can use the same model as before to automatically infer the coefficients of the sinusoidal pattern and predict the future values in the following way:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"number_of_predictions = 100\n\npredictions_states, predictions_observations = predictions_dataset\n\n# We inject `missing` values to the observations to simulate \n# the future values that we want to predict\npredictions_observations_with_predictions = vcat(\n    predictions_observations,\n    [ missing for _ in 1:number_of_predictions ]\n)\n\n# It is better to use `UnfactorizedData` for prediction\npredictions_result = infer(\n    model          = lar_multivariate(order = 2, γ = 0.01), \n    data           = (y = UnfactorizedData(predictions_observations_with_predictions), ),\n    constraints    = ar_constraints(),\n    meta           = ar_meta(2),\n    initialization = ar_init(2),\n    options        = (limit_stack_depth = 500, ),\n    returnvars     = (x = KeepLast(), τ = KeepEach(), θ = KeepEach()),\n    free_energy    = false,\n    iterations     = 20\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Inference results:\n  Posteriors       | available for (τ, θ, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Note: In the current version of RxInfer, the free_energy option is not supported for prediction. Thus we explicitly set it to false. However, we already verified that the inference procedure converges to the correct coefficients with the previous example.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Prediction-results","page":"Autoregressive Models","title":"Prediction results","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We first can check if the inference procedure has converged to the correct coefficients:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Extract the inferred coefficients (mean of posterior)\ninferred_coefficients = predictions_result.posteriors[:θ][end]\n\nprintln(\"True coefficients: \", predictions_coefficients)\nprintln(\"Inferred coefficients (mean value): \", mean(inferred_coefficients))\n\nμ_true = predictions_coefficients\nμ_inferred = mean(inferred_coefficients)\n\n# Create grid of points\nx = range(μ_true[1]-0.025, μ_true[1]+0.025, length=100)\ny = range(μ_true[2]-0.025, μ_true[2]+0.025, length=100)\n\n# Create contour plot\ncoefficients_plot = contour(x, y, (x, y) -> pdf(inferred_coefficients, [x, y]), \n    fill=true, \n    title=\"True vs Inferred AR Coefficients\",\n    xlabel=\"θ₁\",\n    ylabel=\"θ₂\",\n    levels = 14, \n    color=:turbo,\n    colorbar = false\n)\n\n# Add point for true coefficients\nscatter!(coefficients_plot, [μ_true[1]], [μ_true[2]], \n    label=\"True coefficients\",\n    marker=:star,\n    markersize=20,\n    color=:red\n)\n\n# Add point for inferred mean\nscatter!(coefficients_plot, [μ_inferred[1]], [μ_inferred[2]], \n    label=\"Inferred mean\",\n    markersize=8,\n    color=:white\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"True coefficients: [1.9645745014573774, -1.0]\nInferred coefficients (mean value): [1.9607030557929608, -0.997248085033154\n8]","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Now let's visualize our prediction results to see how well our model captures the underlying temporal patterns. By plotting the predicted values against the actual test data, we can immediately assess the quality of our forecasts. Pay special attention to the confidence intervals (shaded regions) surrounding our predictions – these represent our model's uncertainty with propagated uncertainty from the inferred coefficients. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"predictions_posterior_states = predictions_result.predictions[:y][end]\n\npredictions_posterior_states_mean = mean.(predictions_posterior_states)\npredictions_posterior_states_std = std.(predictions_posterior_states)\n\npred_p = scatter(predictions_observations, label=\"Observations\", ms=2)\npred_p = plot!(pred_p, predictions_posterior_states_mean, ribbon=3predictions_posterior_states_std, label=\"Predictions\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Wide intervals suggest high uncertainty, while narrow ones indicate confidence in specific outcomes. When forecasting with AR models, several limitations deserve attention. First, AR models inherently assume that future patterns will resemble past ones - a tenuous assumption during regime changes or external shocks. Second, uncertainty compounds rapidly with prediction horizon; while one-step-ahead forecasts may appear precise, multi-step predictions quickly develop wide confidence intervals that reflect the model's decreasing predictive power.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"To put it in the comparison, we could also use the inferred parameters to predict the future values using the inferred coefficients and the precision parameter. This approach however, will not yield the uncertainty estimates that we get from the inference procedure.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function predict_manual(; number_of_predictions, coefficients, precision, first_state, rng = StableRNG(42))\n    states = [ first_state ]\n    for i in 1:(number_of_predictions + 1)\n        next_x     = rand(rng, NormalMeanPrecision(dot(coefficients, states[end]), precision))\n        next_state = vcat(next_x, states[end][1:end-1])\n        push!(states, next_state)\n    end\n    return states[2:end]\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"predict_manual (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"predicted_manually = predict_manual(; \n    number_of_predictions = number_of_predictions, \n    coefficients = predictions_coefficients, \n    precision = 0.1, \n    first_state = predictions_states[end]\n)\n\nplot(1:length(predictions_states), first.(predictions_states), label = \"Real state\")\nscatter!(1:length(predictions_observations), first.(predictions_observations), label = \"Observations\", ms = 2)\nplot!((length(predictions_observations)+1):length(predictions_observations) + number_of_predictions + 1, first.(predicted_manually), label = \"Predictions manually\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Let's plot both predictions together to see the difference:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"pred_p_manual = scatter(predictions_observations, label=\"Observations\", ms=2)\npred_p_manual = plot!(pred_p_manual, predictions_posterior_states_mean, ribbon=3predictions_posterior_states_std, label=\"Predictions\")\npred_p_manual = plot!(pred_p_manual, (length(predictions_observations)+1):length(predictions_observations) + number_of_predictions + 1, first.(predicted_manually), label = \"Predictions manual\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can see that manual prediction calculations, while computationally simpler, lack the crucial uncertainty quantification that we get from a proper Bayesian inference procedure. Additionally, the predictive power of an AR process directly relates to its order N - higher orders can capture more complex temporal dependencies and longer memory effects, but at the cost of potential overfitting. An AR(2) process can only predict based on the immediate previous observation, creating simple exponential trends, while an AR(5) can detect and forecast more intricate patterns like seasonal oscillations or cyclical behaviors. However, this improved predictive power comes with diminishing returns as N increases, requiring careful model selection to balance complexity against generalization ability for optimal forecasting performance.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Stock-Prices-Dataset","page":"Autoregressive Models","title":"Stock Prices Dataset","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Stock prices make for a challenging but instructive test case. They're notoriously difficult to predict, but often exhibit both short-term momentum (AR components) and characteristic responses to market shocks (MA components). We will use American Airlines stock data downloaded from Kaggle","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"x_df = CSV.read(\"aal_stock.csv\", DataFrame)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"1259×7 DataFrame\n  Row │ date        open     high     low      close    volume    Name\n      │ Date        Float64  Float64  Float64  Float64  Int64     String3\n──────┼───────────────────────────────────────────────────────────────────\n    1 │ 2013-02-08    15.07    15.12   14.63     14.75   8407500  AAL\n    2 │ 2013-02-11    14.89    15.01   14.26     14.46   8882000  AAL\n    3 │ 2013-02-12    14.45    14.51   14.1      14.27   8126000  AAL\n    4 │ 2013-02-13    14.3     14.94   14.25     14.66  10259500  AAL\n    5 │ 2013-02-14    14.94    14.96   13.16     13.99  31879900  AAL\n    6 │ 2013-02-15    13.93    14.61   13.93     14.5   15628000  AAL\n    7 │ 2013-02-19    14.33    14.56   14.08     14.26  11354400  AAL\n    8 │ 2013-02-20    14.17    14.26   13.15     13.33  14725200  AAL\n  ⋮   │     ⋮          ⋮        ⋮        ⋮        ⋮        ⋮         ⋮\n 1253 │ 2018-01-30    52.45    53.05   52.36     52.59   4741808  AAL\n 1254 │ 2018-01-31    53.08    54.71   53.0      54.32   5962937  AAL\n 1255 │ 2018-02-01    54.0     54.64   53.59     53.88   3623078  AAL\n 1256 │ 2018-02-02    53.49    53.99   52.03     52.1    5109361  AAL\n 1257 │ 2018-02-05    51.99    52.39   49.75     49.76   6878284  AAL\n 1258 │ 2018-02-06    49.32    51.5    48.79     51.18   6782480  AAL\n 1259 │ 2018-02-07    50.91    51.98   50.89     51.4    4845831  AAL\n                                                         1244 rows omitted","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# We will use \"close\" column\nx_data = filter(!ismissing, x_df[:, 5])\n\n# Plot data\nplot(x_data, xlabel=\"Day\", ylabel=\"Price\", label=\"Close\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Preparing-the-dataset-for-inference-and-prediction","page":"Autoregressive Models","title":"Preparing the dataset for inference and prediction","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"To validate the inference and prediction results we will also split our dataset into two parts \"observed\" and \"to_predict\", which commonly also reffered as to \"train\" and \"test\" sets.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"observed_size = length(x_data) - 50\n\n# Observed part\nx_observed    = Float64.(x_data[1:observed_size])\n\n# We need to predict this part\nx_to_predict   = Float64.(x_data[observed_size+1:end])\n\nx_observed_length   = length(x_observed)\nx_to_predict_length = length(x_to_predict)\n\nplot(1:x_observed_length, x_observed, label = \"Observed signal\")\nplot!((x_observed_length + 1):(x_observed_length + x_to_predict_length), x_to_predict, label = \"To predict\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can use the same model as before for the stock prices dataset. Let's however put the model to the ultimate test and use AR(50) to predict the future values. ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"stock_observations_with_predictions = vcat(\n    x_observed,\n    [ missing for _ in 1:length(x_to_predict) ]\n)\n\nstock_predictions_result = infer(\n    model          = lar_multivariate(order = 50, γ = 1.0), \n    data           = (y = UnfactorizedData(stock_observations_with_predictions), ),\n    constraints    = ar_constraints(),\n    meta           = ar_meta(50),\n    initialization = ar_init(50),\n    options        = (limit_stack_depth = 500, ),\n    returnvars     = (x = KeepLast(), τ = KeepEach(), θ = KeepEach()),\n    free_energy    = false,\n    iterations     = 20\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Inference results:\n  Posteriors       | available for (τ, θ, x)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(1:x_observed_length, x_observed, label = \"Observed signal\")\nplot!((x_observed_length + 1):(x_observed_length + x_to_predict_length), x_to_predict, label = \"To predict\")\n\nstock_predictions = stock_predictions_result.predictions[:y][end]\n\nplot!(mean.(stock_predictions), ribbon = std.(stock_predictions), label = \"Predictions\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can also plot it against the hidden states in the model and using only the first component of the hidden state:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(1:x_observed_length, x_observed, label = \"Observed signal\")\nplot!((x_observed_length + 1):(x_observed_length + x_to_predict_length), x_to_predict, label = \"To predict\")\n\nstock_hidden_states = stock_predictions_result.posteriors[:x]\n\nplot!(first.(mean.(stock_hidden_states)), ribbon = first.(std.(stock_hidden_states)), label = \"x[1]\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Autoregressive-Moving-Average-Model","page":"Autoregressive Models","title":"Autoregressive Moving Average Model","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"As a final touch, we will implement a fully Bayesian version of ARMA model. Autoregressive Moving Average (ARMA) models represent a powerful synthesis of two fundamental time series components: the autoregressive (AR) part, which captures how current values depend on past observations, and the moving average (MA) part, which models the persistence of random shocks. This combination makes ARMA models particularly well-suited for financial data like stock prices, where both momentum effects (AR) and reaction to news or market shocks (MA) influence price movements. In this example, we'll see how Bayesian inference with RxInfer can reveal these underlying dynamics while quantifying our uncertainty every step of the way.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Mathematical-Formulation-of-ARMA-Model","page":"Autoregressive Models","title":"Mathematical Formulation of ARMA Model","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Bayesian ARMA model can be effectively implemeted in RxInfer.jl. For theoretical details on Varitional Inference for ARMA model, we refer the reader to the following paper.  The Bayesian ARMA model can be written as follows:","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\ne_t sim mathcalN(0 gamma^-1) \ntheta sim mathcalN(mathbf0 mathbfI) \neta sim mathcalN(mathbf0 mathbfI) \nmathbfh_0 sim mathcalNleft(beginbmatrix\ne_-1 \ne_-2\nendbmatrix mathbfIright) \nmathbfh_t = mathbfSmathbfh_t-1 + mathbfc e_t-1 \nmathbfx_t = boldsymboltheta^topmathbfx_t-1 + boldsymboleta^topmathbfh_t + e_t \nendaligned","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"where shift matrix mathbfS is defined as","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\nmathbfS = beginpmatrix\n0  0 \n1  0 \nendpmatrix\nendaligned","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function shift(dim)\n    S = Matrix{Float64}(I, dim, dim)\n    for i in dim:-1:2\n        S[i,:] = S[i-1, :]\n    end\n    S[1, :] = zeros(dim)\n    return S\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"shift (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"shift(2)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"2×2 Matrix{Float64}:\n 0.0  0.0\n 1.0  0.0","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"and unit vector mathbfc: ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\nmathbfc=1 0\nendaligned","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"when MA order is 2. In this way, mathbfh_t containing errors e_t can be viewed as hidden state.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Intractabilities-in-ARMA-model","page":"Autoregressive Models","title":"Intractabilities in ARMA model","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In short, the Bayesian ARMA model has two intractabilities: ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"induced by the multiplication of two Gaussian RVs, i.e., boldsymboleta^topmathbfh_t, \ninduced by errors e_t that prevents analytical update of precision parameter gamma (this can be easily seen when constructing the Factor Graph, i.e. there is a loop). ","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Both problems can be easily resolved in RxInfer.jl, by creating a hybrid inference algorithm based on Loopy Variational Message Passing.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#ARMA-model-specification","page":"Autoregressive Models","title":"ARMA model specification","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"The model specification is the trickiest part of this implementation. Note how we need to carefully define the relationship between observed values, latent states, and error terms. The ARMA model's loops create inference challenges that wouldn't exist in simpler models - this is why we need to specify proper initialization and factorization constraints to avoid convergence problems. For the @meta we will simply reuse the previously defined ar_meta function.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@model function ARMA(x, x_prev, priors, p_order, q_order)\n    \n    # arguments\n    c = zeros(q_order); c[1] = 1.0;\n    S = shift(q_order); # MA\n\n    # set priors\n    γ    ~ priors[:γ]\n    η    ~ priors[:η]\n    θ    ~ priors[:θ]\n    τ    ~ priors[:τ]\n    \n    h[1] ~ priors[:h]\n    z[1] ~ AR(h[1], η, τ)\n    e[1] ~ Normal(mean = 0.0, precision = γ)\n    x[1] ~ dot(c, z[1]) + dot(θ, x_prev[1]) + e[1]\n\n    for t in 1:length(x)-1\n        h[t+1] ~ S * h[t] + c * e[t]\n        z[t+1] ~ AR(h[t+1], η, τ)\n        e[t+1] ~ Normal(mean = 0.0, precision = γ)\n        x[t+1] ~ dot(c, z[t+1]) + dot(θ, x_prev[t+1]) + e[t+1]\n    end\nend\n\n@constraints function arma_constraints()\n    q(z, h, η, τ, γ,e) = q(z, h)q(η)q(τ)q(γ)q(e)\nend\n\n@initialization function arma_initialization(priors) \n    q(h)   = priors[:h]\n    μ(h)   = priors[:h]\n    q(γ)   = priors[:γ]\n    q(τ)   = priors[:τ]\n    q(η)   = priors[:η]\n    q(θ)   = priors[:θ]\nend","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"arma_initialization (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"p_order = 10 # AR\nq_order = 4  # MA","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"4","category":"page"},{"location":"categories/problem_specific/autoregressive_models/#Inference-with-ARMA-model","page":"Autoregressive Models","title":"Inference with ARMA model","text":"","category":"section"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Now, everything should be ready for the infer call from RxInfer on the stock prices dataset defined earlier.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"priors  = (\n    h = MvNormalMeanPrecision(zeros(q_order), diageye(q_order)),\n    γ = GammaShapeRate(1e4, 1.0),\n    τ = GammaShapeRate(1e2, 1.0),\n    η = MvNormalMeanPrecision(ones(q_order), diageye(q_order)),\n    θ = MvNormalMeanPrecision(zeros(p_order), diageye(p_order))\n)\n\narma_x_data = Float64.(x_data[p_order+1:end])[1:observed_size]\narma_x_prev_data = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][1:observed_size]\n\nresult = infer(\n    model = ARMA(priors=priors, p_order = p_order, q_order = q_order), \n    data  = (x = arma_x_data, x_prev = arma_x_prev_data),\n    initialization = arma_initialization(priors),\n    constraints    = arma_constraints(),\n    meta           = ar_meta(q_order),\n    returnvars     = KeepLast(),\n    iterations     = 20,\n    options        = (limit_stack_depth = 400, ),\n)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Inference results:\n  Posteriors       | available for (γ, e, τ, h, z, θ, η)","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(mean.(result.posteriors[:e]), ribbon = var.(result.posteriors[:e][end]), label = \"eₜ\")","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"What we've seen in this example is more than just a stock price forecast - it's a demonstration of how modern probabilistic programming with RxInfer enables sophisticated time series modeling with relatively concise code. The same techniques can be applied across domains from economics to engineering, wherever systems exhibit both memory effects and response to external shocks. And most importantly, the Bayesian approach gives us a principled way to quantify uncertainty in our predictions - essential for robust decision-making in any domain.","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/autoregressive_models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/autoregressive_models/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [a93c6f00] DataFrames v1.7.0\n  [31c24e10] Distributions v0.25.117\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/#Simple-Nonlinear-Node","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"section"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"using RxInfer, Random, StableRNGs","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Here is an example of creating custom node with nonlinear function approximation with samplelist.","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/#Custom-node-creation","page":"Simple Nonlinear Node","title":"Custom node creation","text":"","category":"section"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"struct NonlinearNode end # Dummy structure just to make Julia happy\n\nstruct NonlinearMeta{R, F}\n    rng      :: R\n    fn       :: F   # Nonlinear function, we assume 1 float input - 1 float output\n    nsamples :: Int # Number of samples used in approximation\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@node NonlinearNode Deterministic [ out, in ]","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"We need to define two Sum-product message computation rules for our new custom node","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Rule for outbound message on out edge given inbound message on in edge\nRule for outbound message on in edge given inbound message on out edge\nBoth rules accept optional meta object","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Rule for outbound message on `out` edge given inbound message on `in` edge\n@rule NonlinearNode(:out, Marginalisation) (m_in::NormalMeanVariance, meta::NonlinearMeta) = begin \n    samples = rand(meta.rng, m_in, meta.nsamples)\n    return SampleList(map(meta.fn, samples))\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Rule for outbound message on `in` edge given inbound message on `out` edge\n@rule NonlinearNode(:in, Marginalisation) (m_out::Gamma, meta::NonlinearMeta) = begin     \n    return ContinuousUnivariateLogPdf((x) -> logpdf(m_out, meta.fn(x)))\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/#Model-specification","page":"Simple Nonlinear Node","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"After we have defined our custom node with custom rules we may proceed with a model specification:","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"beginaligned\np(theta) = mathcalN(thetamu_theta sigma_theta)\np(m) = mathcalN(thetamu_m sigma_m)\np(w) = f(theta)\np(y_im w) = mathcalN(y_im w)\nendaligned","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Given this IID model, we aim to estimate the precision of a Gaussian distribution. We pass a random variable theta through a non-linear transformation f to make it positive and suitable for a precision parameter of a Gaussian distribution. We, later on, will estimate the posterior of theta. ","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@model function nonlinear_estimation(y, θ_μ, m_μ, θ_σ, m_σ)\n    \n    # define a distribution for the two variables\n    θ ~ Normal(mean = θ_μ, variance = θ_σ)\n    m ~ Normal(mean = m_μ, variance = m_σ)\n\n    # define a nonlinear node\n    w ~ NonlinearNode(θ)\n\n    # We consider the outcome to be normally distributed\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = m, precision = w)\n    end\n    \nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@constraints function nconstsraints(nsamples)\n    q(θ) :: SampleListFormConstraint(nsamples, LeftProposal())\n    q(w) :: SampleListFormConstraint(nsamples, RightProposal())\n    \n    q(θ, w, m) = q(θ)q(m)q(w)\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nconstsraints (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@meta function nmeta(fn, nsamples)\n    NonlinearNode(θ, w) -> NonlinearMeta(StableRNG(123), fn, nsamples)\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nmeta (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@initialization function ninit()\n    q(m) = vague(NormalMeanPrecision)\n    q(w) = vague(Gamma)\nend","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"ninit (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Here we generate some data","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nonlinear_fn(x) = abs(exp(x) * sin(x))","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nonlinear_fn (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"seed = 123\nrng  = StableRNG(seed)\n\nniters   = 15 # Number of VMP iterations\nnsamples = 5_000 # Number of samples in approximation\n\nn = 500 # Number of IID samples\nμ = -10.0\nθ = -1.0\nw = nonlinear_fn(θ)\n\ndata = rand(rng, NormalMeanPrecision(μ, w), n);","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"now that synthetic data/constriants/model is defined, lets infer:","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"result = infer(\n    model = nonlinear_estimation(θ_μ = 0.0, m_μ = 0.0, θ_σ=100.0, m_σ=1.0),\n    meta =  nmeta(nonlinear_fn, nsamples),\n    constraints = nconstsraints(nsamples),\n    data = (y = data, ), \n    initialization = ninit(),\n    returnvars = (θ = KeepLast(), ),\n    iterations = niters,  \n    showprogress = true\n)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"we can check the posterior now","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"θposterior = result.posteriors[:θ]","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"SampleList(Univariate, 5000)","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Let's us visualise the results","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"using Plots, StatsPlots\n\nestimated = Normal(mean_std(θposterior)...)\n\nplot(estimated, title=\"Posterior for θ\", label = \"Estimated\", legend = :bottomright, fill = true, fillopacity = 0.2, xlim = (-3, 3), ylim = (0, 2))\nvline!([ θ ], label = \"Real value of θ\")","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/simple_nonlinear_node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/simple_nonlinear_node/Project.toml`\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [f3b207a7] StatsPlots v0.15.7\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#Bayesian-Binomial-Regression","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"This notebook is an introductory tutorial to Bayesian binomial regression with RxInfer.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"using RxInfer, ReactiveMP, Random, Plots, StableRNGs, LinearAlgebra, StatsPlots, LaTeXStrings","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#Likelihood-Specification","page":"Bayesian Binomial Regression","title":"Likelihood Specification","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"For observations y_i with predictors mathbfx_i, Binomial regression models the number of successes y_i as a function of the predictors mathbfx_i and the regression coefficients boldsymbolbeta","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"beginequation\ny_i sim textBinomial(n_i p_i)\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"where:","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"y_i","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"is the number of successes, n_i is the number of trials, p_i is the probability of success. The probability p_i is linked to the predictors through the logistic function:","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"beginequation\np_i = frac11 + e^-mathbfx_i^Tboldsymbolbeta\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#Prior-Distributions","page":"Bayesian Binomial Regression","title":"Prior Distributions","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"We specify priors for the regression coefficients:","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"beginequation\nboldsymbolbeta sim mathcalN_xi(boldsymbolxi boldsymbolLambda)\nendequation","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"as a Normal distribution in precision-weighted mean form.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#Model-Specification","page":"Bayesian Binomial Regression","title":"Model Specification","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"The likelihood and the prior distributions form the probabilistic model","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"p(y x beta n) = p(beta) prod_i=1^N p(y_i mid x_i beta n_i)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"where the goal is to infer the posterior distributions p(beta mid y x n). Due to logistic link function, the posterior distribution is not conjugate to the prior distribution. This means that we need to use a more complex inference algorithm to infer the posterior distribution. Before dwelling into the details of the inference algorithm, let's first generate some synthetic data to work with.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"function generate_synthetic_binomial_data(\n    n_samples::Int,\n    true_beta::Vector{Float64};\n    seed::Int=42\n)\n    n_features = length(true_beta)\n    rng = StableRNG(seed)\n    \n    X = randn(rng, n_samples, n_features)\n    \n    n_trials = rand(rng, 5:20, n_samples)\n    \n    logits = X * true_beta\n    probs = 1 ./ (1 .+ exp.(-logits))\n    \n    y = [rand(rng, Binomial(n_trials[i], probs[i])) for i in 1:n_samples]\n    \n    return X, y, n_trials, probs\nend\n\n\nn_samples = 10000\ntrue_beta =  [-3.0 , 2.6]\n\nX, y, n_trials,probs = generate_synthetic_binomial_data(n_samples, true_beta);\nX = [collect(row) for row in eachrow(X)];","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"We generate X as the design matrix and y as the number of successes and n_trials as the number of trials. Next task is to define the graphical model. RxInfer provides a BinomialPolya factor node that is a combination of a Binomial distribution and a PolyaGamma distribution introduced in [1]. The BinomialPolya factor node is used to model the likelihood of the binomial distribution. ","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Due to non-conjugacy of the likelihood and the prior distribution, we need to use a more complex inference algorithm. RxInfer provides an Expectation Propagation (EP) [2] algorithm to infer the posterior distribution. Due to EP's approximation, we need to specify an inbound message for the regression coefficients while using the BinomialPolya factor node. This feature is implemented in the dependencies keyword argument during the creation of the BinomialPolya factor node. ReactiveMP.jl provides a RequireMessageFunctionalDependencies type that is used to specify the inbound message for the regression coefficients β. Refer to the ReactiveMP.jl documentation for more information.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"@model function binomial_model(prior_xi, prior_precision, n_trials, X, y) \n    β ~ MvNormalWeightedMeanPrecision(prior_xi, prior_precision)\n    for i in eachindex(y)\n        y[i] ~ BinomialPolya(X[i], n_trials[i], β) where {\n            dependencies = RequireMessageFunctionalDependencies(β = MvNormalWeightedMeanPrecision(prior_xi, prior_precision))\n        }\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"This example uses the precision-weighted mean parametrization (MvNormalWeightedMeanPrecision) of the Gaussian distribution for efficiency reasons. While this is less conventional than the standard mean-covariance form, the example would work equally well with any parametrization. The choice of parametrization mainly affects computational efficiency and numerical stability, not the underlying model or results.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Having specified the model, we can now utilize the infer function to infer the posterior distribution.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"n_features = length(true_beta)\nresults = infer(\n    model = binomial_model(prior_xi = zeros(n_features), prior_precision = diageye(n_features),),\n    data = (X=X, y=y,n_trials=n_trials),\n    iterations = 30,\n    free_energy = true,\n    showprogress = true,\n    options = (\n        limit_stack_depth = 100, # to prevent stack-overflow errors\n    )\n)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Inference results:\n  Posteriors       | available for (β)\n  Free Energy:     | Real[21992.9, 16235.8, 13785.0, 12519.7, 11800.1, 1136\n6.2, 11094.1, 10918.7, 10803.3, 10726.3  …  10561.6, 10560.6, 10559.9, 1055\n9.4, 10559.0, 10558.8, 10558.6, 10558.5, 10558.4, 10558.3]","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"We can now plot the free energy to see if the inference algorithm is converging.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"plot(results.free_energy,fontfamily = \"Computer Modern\", label=\"Free Energy\", xlabel=\"Iteration\", ylabel=\"Free Energy\", title=\"Free Energy Convergence\")","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Free energy is converging to a stable value, indicating that the inference algorithm is converging. Let's visualize the posterior distribution and how it compares to the true parameters.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"m = mean(results.posteriors[:β][end])\nΣ = cov(results.posteriors[:β][end])\n\np = plot(xlims=(-3.1,-2.9), ylims=(2.5,2.7),fontfamily = \"Computer Modern\")\ncovellipse!(m, Σ, n_std=1, aspect_ratio=1, label=\"1σ Contours\", color=:green, fillalpha=0.2)\ncovellipse!(m, Σ, n_std=3, aspect_ratio=1, label=\"3σ Contours\", color=:blue, fillalpha=0.2)\nscatter!([m[1]], [m[2]], label=\"Mean estimate\", color=:blue)\nscatter!([true_beta[1]], [true_beta[2]], label=\"True Parameters\")","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"We can perform prediction by augmenting the data with missing values. For that, we can create a new vector y_with_missing that contains missing values for the last 2000 samples.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"y_with_missing = Vector{Union{Missing, Int}}(missing, n_samples)\nfor i in 1:n_samples\n    if i > 8000\n        y_with_missing[i] = missing\n    else\n        y_with_missing[i] = y[i]\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"results_with_missing = infer(\n    model = binomial_model(prior_xi = zeros(n_features), prior_precision = diageye(n_features),),\n    data = (X=X, y=y_with_missing,n_trials=n_trials),\n    iterations = 30,\n    showprogress = true,\n    options = (\n        limit_stack_depth = 100, # to prevent stack-overflow errors\n    )\n)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Inference results:\n  Posteriors       | available for (β)\n  Predictions      | available for (y)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"probs_prediction = map(d -> d.p,results_with_missing.predictions[:y][end][8000:end])\nerr = probs_prediction .- probs[8000:end]\nmse = mean(err.^2)\nprintln(\"Mean squared error: \", mse)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Mean squared error: 3.541846183800687e-6","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"function bin_predictions(true_probs, pred_probs; n_bins=20)\n    bins = range(0, 1, length=n_bins+1)\n    bin_means = Float64[]\n    bin_stds = Float64[]\n    bin_centers = Float64[]\n    \n    for i in 1:n_bins\n        mask = (true_probs .>= bins[i]) .& (true_probs .< bins[i+1])\n        if any(mask)\n            push!(bin_means, mean(pred_probs[mask]))\n            push!(bin_stds, std(pred_probs[mask]))\n            push!(bin_centers, (bins[i] + bins[i+1])/2)\n        end\n    end\n    return bin_centers, bin_means, bin_stds\nend\n\n# Create the plot\nbin_centers, bin_means, bin_stds = bin_predictions(probs[8000:end], probs_prediction)\n\np = plot(\n    xlabel = \"True Probability\",\n    ylabel = \"Predicted Probability\",\n    title = \"Prediction Performance\",\n    aspect_ratio = 1,\n    legend = :bottomright,\n    grid = true,\n    fontfamily = \"Computer Modern\",\n    dpi = 300\n)\n\n# Add perfect prediction line\nplot!([0, 1], [0, 1], \n    label = \"Perfect Prediction\", \n    color = :black, \n    linestyle = :dash,\n    linewidth = 2\n)\n\n# Add scatter plot with reduced opacity and size\nscatter!(\n    probs[8000:end], \n    probs_prediction,\n    label = \"Individual Predictions\",\n    alpha = 0.1,  # Reduced opacity\n    color = :blue,\n    markersize = 1,\n    markerstrokewidth = 0\n)\n\n# Add binned means with error bars\nscatter!(\n    bin_centers,\n    bin_means,\n    yerror = bin_stds,\n    label = \"Binned Mean ± SD\",\n    color = :red,\n    markersize = 4\n)\n\nannotate!(\n    0.05, \n    0.95, \n    text(\"MSE = $(round(mse, digits=8))\", 8, :left, :top)\n)\n\n# Customize axes\nplot!(\n    xlims = (0,1),\n    ylims = (0,1),\n    xticks = 0:0.2:1,\n    yticks = 0:0.2:1\n)","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/#References","page":"Bayesian Binomial Regression","title":"References","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"[1] Polson, N. G., Scott, J. G., & Windle, J. (2013). Bayesian inference for logistic models using Polya-Gamma latent variables. Journal of the American Statistical Association, 108(1), 136-146.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"[2] Minka, T. (2001). Expectation Propagation for approximate Bayesian inference. Uncertainty in Artificial Intelligence, 2, 362-369.","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/bayesian_binomial_regression/","page":"Bayesian Binomial Regression","title":"Bayesian Binomial Regression","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/bayesian_binomial_regression/Project.toml`\n  [b964fa9f] LaTeXStrings v1.4.0\n  [91a5bcdd] Plots v1.40.9\n  [a194aa59] ReactiveMP v5.2.1\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [f3b207a7] StatsPlots v0.15.7\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Gaussian-Mixture","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"This notebook illustrates how to use the NormalMixture node in RxInfer.jl for both univariate and multivariate observations.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Load-packages","page":"Gaussian Mixture","title":"Load packages","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"using RxInfer, Plots, Random, LinearAlgebra, StableRNGs, LaTeXStrings","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Univariate-Gaussian-Mixture-Model","page":"Gaussian Mixture","title":"Univariate Gaussian Mixture Model","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Consider the data set of length N observed below.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"function generate_univariate_data(nr_samples; rng = MersenneTwister(123))\n\n    # data generating parameters\n    class        = [1/3, 2/3]\n    mean1, mean2 = -10, 10\n    precision    = 1.777\n\n    # generate data\n    z = rand(rng, Categorical(class), nr_samples)\n    y = zeros(nr_samples)\n    for k in 1:nr_samples\n        y[k] = rand(rng, Normal(z[k] == 1 ? mean1 : mean2, 1/sqrt(precision)))\n    end\n\n    return y\n\nend;","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"data_univariate = generate_univariate_data(100)\nhistogram(data_univariate, bins=50, label=\"data\", normed=true)\nxlims!(minimum(data_univariate), maximum(data_univariate))\nylims!(0, Inf)\nylabel!(\"relative occurrence [%]\")\nxlabel!(\"y\")","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Model-specification","page":"Gaussian Mixture","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The goal here is to create a model for the data set above. In this case a Gaussian mixture model with K components seems to suite the situation well. We specify the factorized model as  p(y z s m w) = prod_n=1^N bigg(p(y_n mid m w z_n) p(z_n mid s) bigg)prod_k=1^K bigg(p(m_k) p(w_k) bigg) p(s) where the individual terms are specified as beginaligned     p(s)                    = mathrmBeta(s mid alpha_s beta_s) \n    p(m_k)                = mathcalN(m_k mid mu_k sigma_k^2)          p(w_k)                = Gamma(w_k mid alpha_k beta_k) \n    p(z_n mid s)           = mathrmBer(z_n mid s) \n    p(y_n mid m w z_n)   = prod_k=1^K mathcalNleft(y_n mid m_k w_kright)^z_nk endaligned","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The set of observations y = y_1 y_2 ldots y_N is modeled by a mixture of Gaussian distributions, parameterized by means m = m_1 m_2 ldots m_K and precisions w =  w_1 w_2 ldots w_K, where k denotes the component index. This component is selected per observation by the indicator variable z_n, which is a one-of-K encoded vector satisfying sum_k=1^K z_nk = 1 and z_nk in 0 1 forall k. We put a hyperprior on these variables, termed s, which represents the relative occurrence of the different realizations of z_n.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Here we implement the following model with uninformative values for the hyperparameters as","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"@model function univariate_gaussian_mixture_model(y)\n    \n    s ~ Beta(1.0, 1.0)\n\n    m[1] ~ Normal(mean = -2.0, variance = 1e3)\n    w[1] ~ Gamma(shape = 0.01, rate = 0.01)\n\n    m[2] ~ Normal(mean = 2.0, variance = 1e3)\n    w[2] ~ Gamma(shape = 0.01, rate = 0.01)\n\n    for i in eachindex(y)\n        z[i] ~ Bernoulli(s)\n        y[i] ~ NormalMixture(switch = z[i], m = m, p = w)\n    end\n    \nend","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Probabilistic-inference","page":"Gaussian Mixture","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"In order to fit the model to the data, we are interested in computing the posterior distribution p(z s m w mid y) However, computation of this term is intractable. Therefore, it is approximated by a naive mean-field approximation, specified as  p(z s m w mid y) approx prod_n=1^N q(z_n) prod_k=1^K bigg(q(m_k) q(w_k)bigg) q(s) with the functional forms beginaligned     q(s)   = mathrmBeta(s mid hatalpha_s hatbeta_s) \n    q(m_k) = mathcalN(m_k mid hatmu_k hatsigma^2_k) \n    q(w_k) = Gamma (w_k mid hatalpha_k hatbeta_k) \n    q(z_n) = mathrmBer(z_n mid hatp_n) endaligned In order to get the inference procedure started, these marginal distribution need to be initialized.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"n_iterations = 10\n\ninit = @initialization begin\n    q(s) = vague(Beta)\n    q(m) = [NormalMeanVariance(-2.0, 1e3), NormalMeanVariance(2.0, 1e3)]\n    q(w) = [vague(GammaShapeRate), vague(GammaShapeRate)]\nend\n\nresults_univariate = infer(\n    model = univariate_gaussian_mixture_model(), \n    constraints = MeanField(),\n    data  = (y = data_univariate,), \n    initialization = init, \n    iterations  = n_iterations, \n    free_energy = true\n)","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Inference results:\n  Posteriors       | available for (m, w, s, z)\n  Free Energy:     | Real[262.985, 206.795, 183.824, 140.402, 138.188, 138.\n184, 138.184, 138.184, 138.184, 138.184]","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Results","page":"Gaussian Mixture","title":"Results","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Below the inference results can be seen as a function of the iterations","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"m1 = [results_univariate.posteriors[:m][i][1] for i in 1:n_iterations]\nm2 = [results_univariate.posteriors[:m][i][2] for i in 1:n_iterations]\nw1 = [results_univariate.posteriors[:w][i][1] for i in 1:n_iterations]\nw2 = [results_univariate.posteriors[:w][i][2] for i in 1:n_iterations];","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"mp = plot(mean.(m1), ribbon = std.(m1) .|> sqrt, label = L\"posterior $m_1$\")\nmp = plot!(mean.(m2), ribbon = std.(m2) .|> sqrt, label = L\"posterior $m_2$\")\nmp = plot!(mp, [ -10 ], seriestype = :hline, label = L\"true $m_1$\")\nmp = plot!(mp, [ 10 ], seriestype = :hline, label = L\"true $m_2$\")\n\nwp = plot(mean.(w1), ribbon = std.(w1) .|> sqrt, label = L\"posterior $w_1$\", legend = :bottomright, ylim = (-1, 3))\nwp = plot!(wp, mean.(w2), ribbon = std.(w2) .|> sqrt, label = L\"posterior $w_2$\")\nwp = plot!(wp, [ 1.777 ], seriestype = :hline, label = L\"true $w_1$\")\nwp = plot!(wp, [ 1.777 ], seriestype = :hline, label = L\"true $w_2$\")\n\nswp = plot(mean.(results_univariate.posteriors[:s]), ribbon = std.(results_univariate.posteriors[:s]) .|> sqrt, label = L\"posterior $s$\")\nswp = plot!(swp, [ 2/3 ], seriestype = :hline, label = L\"true $s$\")\n\nfep = plot(results_univariate.free_energy, label = \"Free Energy\", legend = :topright)\n\nplot(mp, wp, swp, fep, layout = @layout([ a b; c d ]), size = (800, 400))\nxlabel!(\"iteration\")","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Multivariate-Gaussian-Mixture-Model","page":"Gaussian Mixture","title":"Multivariate Gaussian Mixture Model","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The above example can also be extended to the multivariate case. Consider the data set below","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"function generate_multivariate_data(nr_samples; rng = MersenneTwister(123))\n\n    L         = 50.0\n    nr_mixtures = 6\n\n    probvec = normalize!(ones(nr_mixtures), 1)\n\n    switch = Categorical(probvec)\n\n    gaussians = map(1:nr_mixtures) do index\n        angle      = 2π / nr_mixtures * (index - 1)\n        basis_v    = L * [ 1.0, 0.0 ]\n        R          = [ cos(angle) -sin(angle); sin(angle) cos(angle) ]\n        mean       = R * basis_v \n        covariance = Matrix(Hermitian(R * [ 10.0 0.0; 0.0 20.0 ] * transpose(R)))\n        return MvNormal(mean, covariance)\n    end\n\n    z = rand(rng, switch, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n\n    for n in 1:nr_samples\n        y[n] = rand(rng, gaussians[z[n]])\n    end\n\n    return y\n\nend;","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"data_multivariate = generate_multivariate_data(500)\n\nsdim(n) = (a) -> map(d -> d[n], a) # helper function\nscatter(data_multivariate |> sdim(1), data_multivariate |> sdim(2), ms = 2, alpha = 0.4, size = (600, 400), legend=false)\nxlabel!(L\"y_1\")\nylabel!(L\"y_2\")","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Model-specification-2","page":"Gaussian Mixture","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The goal here is to create a model for the data set above. In this case a Gaussian mixture model with K components seems to suite the situation well. We specify the factorized model as  p(y z s m w) = prod_n=1^N bigg(p(y_n mid m W z_n) p(z_n mid s) bigg)prod_k=1^K bigg(p(m_k) p(W_k) bigg) p(s) where the individual terms are specified as beginaligned     p(s)                    = mathrmDir(s mid alpha_s) \n    p(m_k)                = mathcalN(m_k mid mu_k Sigma_k)          p(W_k)                = mathcalW(W_k mid V_k nu_k) \n    p(z_n mid s)           = mathrmCat(z_n mid s) \n    p(y_n mid m W z_n)   = prod_k=1^K mathcalNleft(y_n mid m_k W_kright)^z_nk endaligned","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The set of observations y = y_1 y_2 ldots y_N is modeled by a mixture of Gaussian distributions, parameterized by means m = m_1 m_2 ldots m_K and precisions W =  W_1 W_2 ldots W_K, where k denotes the component index. This component is selected per observation by the indicator variable z_n, which is a one-of-K encoded vector satisfying sum_k=1^K z_nk = 1 and z_nk in 0 1 forall k. We put a hyperprior on these variables, termed s, which represents the relative occurrence of the different realizations of z_n.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"@model function multivariate_gaussian_mixture_model(nr_mixtures, priors, y)\n    local m\n    local w\n\n    for k in 1:nr_mixtures        \n        m[k] ~ priors[k]\n        w[k] ~ Wishart(3, 1e2*diagm(ones(2)))\n    end\n    \n    s ~ Dirichlet(ones(nr_mixtures))\n    \n    for n in eachindex(y)\n        z[n] ~ Categorical(s) \n        y[n] ~ NormalMixture(switch = z[n], m = m, p = w)\n    end\n    \nend","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Probabilistic-inference-2","page":"Gaussian Mixture","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"In order to fit the model to the data, we are interested in computing the posterior distribution p(z s m W mid y) However, computation of this term is intractable. Therefore, it is approximated by a naive mean-field approximation, specified as  p(z s m W mid y) approx prod_n=1^N q(z_n) prod_k=1^K bigg(q(m_k) q(W_k)bigg) q(s) with the functional forms beginaligned     q(s)   = mathrmDir(s mid hatalpha_s) \n    q(m_k) = mathcalN(m_k mid hatmu_k hatSigma_k) \n    q(w_k) = mathcalW(W_k mid hatV_k hatnu_k) \n    q(z_n) = mathrmCat(z_n mid hatp_n) endaligned In order to get the inference procedure started, these marginal distribution need to be initialized.","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"rng = MersenneTwister(121)\npriors = [MvNormal([cos(k*2π/6), sin(k*2π/6)], diagm(1e2 * ones(2))) for k in 1:6]\ninit = @initialization begin\n    q(s) = vague(Dirichlet, 6)\n    q(m) = priors\n    q(w) = Wishart(3, diagm(1e2 * ones(2)))\nend\n\nresults_multivariate = infer(\n    model = multivariate_gaussian_mixture_model(\n        nr_mixtures = 6, \n        priors = priors,\n    ), \n    data  = (y = data_multivariate,), \n    constraints   = MeanField(),\n    initialization = init, \n    iterations  = 50, \n    free_energy = true\n)","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Inference results:\n  Posteriors       | available for (w, m, s, z)\n  Free Energy:     | Real[3927.95, 3884.37, 3884.37, 3884.37, 3884.37, 3884\n.37, 3884.37, 3884.37, 3884.37, 3884.37  …  3884.37, 3884.37, 3884.37, 3884\n.37, 3884.37, 3884.37, 3884.37, 3884.37, 3884.37, 3884.37]","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/#Results-2","page":"Gaussian Mixture","title":"Results","text":"","category":"section"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Below the inference results can be seen","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"p_data = scatter(data_multivariate |> sdim(1), data_multivariate |> sdim(2), ms = 2, alpha = 0.4, legend=false, title=\"Data\", xlims=(-75, 75), ylims=(-75, 75))\np_result = plot(xlims = (-75, 75), ylims = (-75, 75), title=\"Inference result\", legend=false, colorbar = false)\nfor (e_m, e_w) in zip(results_multivariate.posteriors[:m][end], results_multivariate.posteriors[:w][end])\n    gaussian = MvNormal(mean(e_m), Matrix(Hermitian(mean(inv, e_w))))\n    global p_result = contour!(p_result, range(-75, 75, step = 0.25), range(-75, 75, step = 0.25), (x, y) -> pdf(gaussian, [ x, y ]), title=\"Inference result\", legend=false, levels = 7, colorbar = false)\nend\np_fe = plot(results_multivariate.free_energy, label = \"Free Energy\")\n\nplot(p_data, p_result, p_fe, layout = @layout([ a b; c ]))","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/gaussian_mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/gaussian_mixture/Project.toml`\n  [b964fa9f] LaTeXStrings v1.4.0\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/#Hierarchical-Gaussian-Filter","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"section"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this demo the goal is to perform approximate variational Bayesian Inference for Univariate Hierarchical Gaussian Filter (HGF).","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Simple HGF model can be defined as:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  x^(j)_k  sim  mathcalN(x^(j)_k - 1 f_k(x^(j - 1)_k)) \n  y_k  sim  mathcalN(x^(j)_k tau_k)\nendaligned","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"where j is an index of layer in hierarchy, k is a time step and f_k is a variance activation function. RxInfer.jl export Gaussian Controlled Variance (GCV) node with f_k = exp(kappa x + omega) variance activation function. By default the node uses Gauss-Hermite cubature with a prespecified number of approximation points in the cubature. In this demo we also show how we can change the hyperparameters in different approximation methods (iin this case Gauss-Hermite cubature) with the help of metadata structures. Here how our model will look like with the GCV node:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  z_k  sim  mathcalN(z_k - 1 mathcaltau_z) \n  x_k  sim  mathcalN(x_k - 1 exp(kappa z_k + omega)) \n  y_k  sim  mathcalN(x_k mathcaltau_y)\nendaligned","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this experiment we will create a single time step of the graph and perform variational message passing filtering alrogithm to estimate hidden states of the system. For a more rigorous introduction to Hierarchical Gaussian Filter we refer to Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter paper.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"For simplicity we will consider tau_z, tau_y, kappa and omega known and fixed, but there are no principled limitations to make them random variables too.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"using RxInfer, BenchmarkTools, Random, Plots, StableRNGs","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function generate_data(rng, n, k, w, zv, yv)\n    z_prev = 0.0\n    x_prev = 0.0\n\n    z = Vector{Float64}(undef, n)\n    v = Vector{Float64}(undef, n)\n    x = Vector{Float64}(undef, n)\n    y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        z[i] = rand(rng, Normal(z_prev, sqrt(zv)))\n        v[i] = exp(k * z[i] + w)\n        x[i] = rand(rng, Normal(x_prev, sqrt(v[i])))\n        y[i] = rand(rng, Normal(x[i], sqrt(yv)))\n\n        z_prev = z[i]\n        x_prev = x[i]\n    end \n    \n    return z, x, y\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Seed for reproducibility\nseed = 42\n\nrng = StableRNG(seed)\n\n# Parameters of HGF process\nreal_k = 1.0\nreal_w = 0.0\nz_variance = abs2(0.2)\ny_variance = abs2(0.1)\n\n# Number of observations\nn = 300\n\nz, x, y = generate_data(rng, n, real_k, real_w, z_variance, y_variance);","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    scatter!(px, 1:n, y, label = \"y_i\", color = :red, ms = 2, alpha = 0.2)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/#Online-learning-(Filtering)","page":"Hierarchical Gaussian Filter","title":"Online learning (Filtering)","text":"","category":"section"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To create a model we use the @model macro:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# We create a single-time step of corresponding state-space process to\n# perform online learning (filtering)\n@model function hgf(y, κ, ω, z_variance, y_variance, z_prev_mean, z_prev_var, x_prev_mean, x_prev_var)\n\n    z_prev ~ Normal(mean = z_prev_mean, variance = z_prev_var)\n    x_prev ~ Normal(mean = x_prev_mean, variance = x_prev_var)\n\n    # Higher layer is modelled as a random walk \n    z_next ~ Normal(mean = z_prev, variance = z_variance)\n    \n    # Lower layer is modelled with `GCV` node\n    x_next ~ GCV(x_prev, z_next, κ, ω)\n    \n    # Noisy observations \n    y ~ Normal(mean = x_next, variance = y_variance)\nend\n\n@constraints function hgfconstraints() \n    # Mean-field factorization constraints\n    q(x_next, x_prev, z_next) = q(x_next)q(x_prev)q(z_next)\nend\n\n@meta function hgfmeta()\n    # Lets use 31 approximation points in the Gauss Hermite cubature approximation method\n    GCV() -> GCVMetadata(GaussHermiteCubature(31)) \nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"hgfmeta (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"The code below uses the infer function from RxInfer to generate the message passing algorithm given the model and constraints specification.  We also specify the @autoupdates in order to set new priors for the next observation based on posteriors.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function run_inference(data, real_k, real_w, z_variance, y_variance)\n\n    autoupdates   = @autoupdates begin\n        # The posterior becomes the prior for the next time step\n        z_prev_mean, z_prev_var = mean_var(q(z_next))\n        x_prev_mean, x_prev_var = mean_var(q(x_next))\n    end\n\n    init = @initialization begin\n        q(x_next) = NormalMeanVariance(0.0, 5.0)\n        q(z_next) = NormalMeanVariance(0.0, 5.0)\n    end\n\n    return infer(\n        model          = hgf(κ = real_k, ω = real_w, z_variance = z_variance, y_variance = y_variance),\n        constraints    = hgfconstraints(),\n        meta           = hgfmeta(),\n        data           = (y = data, ),\n        autoupdates    = autoupdates,\n        keephistory    = length(data),\n        historyvars    = (\n            x_next = KeepLast(),\n            z_next = KeepLast()\n        ),\n        initialization = init,\n        iterations     = 5,\n        free_energy    = true,\n    )\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"run_inference (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Everything is ready to run the algorithm. We used the online version of the algorithm, thus we need to fetch the history of the posterior estimation instead of the actual posteriors.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"result = run_inference(y, real_k, real_w, z_variance, y_variance);\n\nmz = result.history[:z_next];\nmx = result.history[:x_next];","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz), ribbon = std.(mz), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx), ribbon = std.(mx), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(result.free_energy_history, label = \"Bethe Free Energy\")","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see BetheFreeEnergy converges nicely to a stable point. ","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/#Offline-learning-(Smoothing)","page":"Hierarchical Gaussian Filter","title":"Offline learning (Smoothing)","text":"","category":"section"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Aside from online learning, we can also perform offline learning (smoothing) with the HGF model to learn the parameters in case we have collected all the data. In this offline setting, we treat the parameters kappa and omega as random variables and place a prior over them. These parameters will be updated along with latent states during the inference. First, let's define the HGF model for offline learning","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"#Model for offline learning (smoothing)\n\n@model function hgf_smoothing(y, z_variance, y_variance)\n    # Initial states \n    z_prev ~ Normal(mean = 0., variance = 5.0)\n    x_prev ~ Normal(mean = 0., variance = 5.0)\n\n    # Priors on κ and ω\n    κ ~ Normal(mean = 1.5, variance = 1.0)\n    ω ~ Normal(mean = 0.0, variance = 0.05)\n\n    for i in eachindex(y)\n        # Higher layer \n        z[i] ~ Normal(mean = z_prev, variance = z_variance)\n\n        # Lower layer \n        x[i] ~ GCV(x_prev, z[i], κ, ω)\n\n        # Noisy observations \n        y[i] ~ Normal(mean = x[i], variance = y_variance)\n\n        # Update last/previous hidden states\n        z_prev = z[i]\n        x_prev = x[i]\n    end\nend\n\n@constraints function hgfconstraints_smoothing() \n    #Structured mean-field factorization constraints\n    q(x_prev,x, z,κ,ω) = q(x_prev,x)q(z)q(κ)q(ω)\nend\n\n@meta function hgfmeta_smoothing()\n    # Lets use 31 approximation points in the Gauss Hermite cubature approximation method\n    GCV() -> GCVMetadata(GaussHermiteCubature(31)) \nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"hgfmeta_smoothing (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Similar to the filtering case, we use the infer function from RxInfer to implement inference. ","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function run_inference_smoothing(data, z_variance, y_variance)\n    @initialization function hgf_init_smoothing()\n        q(x) = NormalMeanVariance(0.0,5.0)\n        q(z) = NormalMeanVariance(0.0,5.0)\n        q(κ) = NormalMeanVariance(1.5,1.0)\n        q(ω) = NormalMeanVariance(0.0,0.05)\n    end\n\n    #Let's do inference with 20 iterations \n    return infer(\n        model = hgf_smoothing(z_variance = z_variance, y_variance = y_variance,),\n        data = (y = data,),\n        meta = hgfmeta_smoothing(),\n        constraints = hgfconstraints_smoothing(),\n        initialization = hgf_init_smoothing(),\n        iterations = 20,\n        options = (limit_stack_depth = 100, ), \n        returnvars = (x = KeepLast(), z = KeepLast(),ω=KeepLast(),κ=KeepLast(),),\n        free_energy = true \n    )\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"run_inference_smoothing (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Now we can get the result.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"result_smoothing = run_inference_smoothing(y, z_variance, y_variance);\nmz_smoothing = result_smoothing.posteriors[:z];\nmx_smoothing = result_smoothing.posteriors[:x];","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz_smoothing), ribbon = std.(mz_smoothing), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx_smoothing), ribbon = std.(mx_smoothing), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles to the real hidden states and appears \"smoother\" compared to the filtering case. We may be also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(result_smoothing.free_energy, label = \"Bethe Free Energy\")","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Finally, we can also extract the marginals q(kappa) and q(omega) to get the appropximation of these parameters.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"q_κ = result_smoothing.posteriors[:κ]\nq_ω = result_smoothing.posteriors[:ω]\n\nprintln(\"Approximate value of κ: \", mean(q_κ))\nprintln(\"Approximate value of ω: \", mean(q_ω))","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Approximate value of κ: 0.7543386579898821\nApproximate value of ω: -0.18164892866626792","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Let's visualize their marginal distributions.","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"range_w = range(-1,0.5,length = 1000)\nrange_k = range(0,2,length = 1000)\nlet \n    pw = plot(title = \"Marginal q(w)\")\n    pk = plot(title = \"Marginal q(k)\")\n    \n    plot!(pw, range_w, (x) -> pdf(q_ω, x), fillalpha=0.3, fillrange = 0, label=\"Posterior q(w)\", c=3, legend_position=(0.1,0.95),legendfontsize=9)\n    vline!([real_w], label=\"Real w\")\n    xlabel!(\"w\")\n    \n    \n    plot!(pk, range_k, (x) -> pdf(q_κ, x), fillalpha=0.3, fillrange = 0, label=\"Posterior q(k)\", c=3, legend_position=(0.1,0.95),legendfontsize=9)\n    vline!([real_k], label=\"Real k\")\n    xlabel!(\"k\")\n    \n    plot(pk, pw, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see, both the marginals q(kappa) and q(omega) are not quite off from the true values. Specifically, the means of q(kappa) and q(omega) are approximately 075 and -018, respectively, which are quite close to their true values. ","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/hierarchical_gaussian_filter/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Structural-Dynamics-with-Augmented-Kalman-Filter","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics with Augmented Kalman Filter","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This example demonstrates state and input force estimation for structural dynamical systems with Augmented Kalman Filter (AKF) implemented in RxInfer.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"NOTE: This example was originally featured in this blog post. Check it out for additional insights! The notebook has been prepared by Víctor Flores and adapted by Dmitry Bagaev.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#State-and-Input-Estimation","page":"Structural Dynamics With Augmented Kalman Filter","title":"State and Input Estimation","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"State-space models are fundamental tools in control theory and signal processing that allow us to analyze complex dynamical systems by breaking them down into first-order differential equations. They are particularly important for structural dynamics problems because they can capture both the internal states (like position and velocity) and external influences (like forces) in a unified mathematical framework. A typical state-space model formulation might look like this:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"xk+1 sim mathcalN(A xk + B pk Q)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"yk sim mathcalN(G xk + J pk R)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"xk\nrepresents the system states at time-step k\npk\nrepresents the unknown input forces at time-step k\nyk\nrepresents our noisy measurements at time-step k\nA\nis the state transition matrix that describes how the system evolves from one time step to the next\nB\nis the input matrix that maps the external forces to their effects on the states\nQ\nis the process noise covariance matrix that captures uncertainties in the system dynamics\nR\nis the measurement noise covariance matrix that represents uncertainties in sensor measurements","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#4-floor-shear-building-model","page":"Structural Dynamics With Augmented Kalman Filter","title":"4-floor shear building model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"For this example, we consider a simplified 4-floor shear building model with 4 degrees of freedom (DOF). This system is depicted below:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this example, the dynamics of a structural system are governed by its mass (M), stiffness (K), and damping (C) matrices, leading to the equation of motion:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"M ddotx(t) + C dotx(t) + K x(t) = p(t)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where x(t) represents the displacements at each degree of freedom, and p(t) is the external force applied to the system.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This model captures the essential dynamics of a multi-story structure while remaining computationally manageable. The system matrices are defined as follows:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"M\nis the diagonal mass matrix representing the lumped masses at each floor,  \nK\nis the stiffness matrix representing inter-floor lateral stiffness, and  \nC\nis the proportional damping matrix reflecting energy dissipation.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Lets begin the experiment! To start, we import the necessary packages.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"using LinearAlgebra, Statistics, Random, Plots","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"To keep our analysis organized, we'll use a custom StructuralModelData data structure. This structure serves as a central repository for model parameters, simulation settings, system matrices, results, and outputs.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# define a data structure for the structural model environment\nstruct StructuralModelData\n    t::Union{Nothing, Any}\n    ndof::Union{Nothing, Int64}\n    nf::Union{Nothing, Int64}\n    N_data::Union{Nothing, Int64}\n    y_meas::Union{Nothing, Vector{Vector{Float64}}}\n    A_aug::Union{Nothing, Matrix{Float64}}\n    G_aug::Union{Nothing, Matrix{Float64}}\n    G_aug_fullfield::Union{Nothing, Matrix{Float64}}\n    Q_akf::Union{Nothing, Matrix{Float64}}\n    R::Union{Nothing, LinearAlgebra.Diagonal{Float64, Vector{Float64}}}\n    x_real::Union{Nothing, Matrix{Float64}}\n    y_real::Union{Nothing, Matrix{Float64}}\n    p_real::Union{Nothing, Matrix{Float64}}\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We also define a structure for the system matrices.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# define the structural system matrices\nstruct StructuralMatrices\n    M::Union{Nothing, Matrix{Float64}}\n    K::Union{Nothing, Matrix{Float64}}\n    C::Union{Nothing, Matrix{Float64}}\nend\n\n\nM = I(4)\n\n\nK = [\n    2  -1   0    0;\n   -1   2  -1    0;\n    0  -1   2   -1;\n    0   0  -1    1\n] * 1e3\n\nC = [\n    2   -1    0    0;\n   -1    2   -1    0;\n    0   -1    2   -1;\n    0    0   -1    1\n]\n\nStructuralModel = StructuralMatrices(M, K, C);","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Constructing-the-State-Space-Model","page":"Structural Dynamics With Augmented Kalman Filter","title":"Constructing the State-Space Model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We convert the structural system into its discrete-time state-space form for numerical simulation. Starting from the equation of motion:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"M ddotx(t) + C dotx(t) + K x(t) = F(t)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"we introduce the state variable:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"z(t) = beginbmatrix x(t)  dotx(t) endbmatrix","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"which allows us to express the system as:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"dotz(t) = A_textc z(t) + B_textc p(t)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"A_textc = beginbmatrix 0  I  -(M^-1 K)  -(M^-1 C) endbmatrix\nB_textc = beginbmatrix 0  M^-1 S_p endbmatrix\nS_p\nis the input selection matrix that determines where the external forces p(t) are applied.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"To perform simulations, the system is discretized using a time step Delta t as:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"zk+1 = A zk + B pk","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"A = e^A_textc Delta t\nis the state transition matrix.\nB = (A - I) A_textc^-1 B_textc\nis the input matrix, obtained by integrating the continuous-time system.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This state-space representation forms the basis for propagating the system states during simulation.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to construct the state space model\nfunction construct_ssm(StructuralModel,dt, ndof, nf)\n    # unpack the structural model\n    M = StructuralModel.M\n    K = StructuralModel.K\n    C = StructuralModel.C\n    \n    \n    Sp = zeros(ndof, nf)\n    Sp[4, 1] = 1\n\n    Z = zeros(ndof, ndof)\n    Id = I(ndof)\n\n    A_continuous = [Z Id;\n                    -(M \\ K) -(M \\ C)]\n    B_continuous = [Z; Id \\ M] * Sp\n\n    A = exp(dt * A_continuous)\n    B = (A - I(2*ndof)) * A_continuous \\ B_continuous\n\n    return A, B, Sp\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"construct_ssm (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Generating-Input-Forces","page":"Structural Dynamics With Augmented Kalman Filter","title":"Generating Input Forces","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"External forces pk acting on the system are modeled as Gaussian white noise:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"pk sim mathcalN(mu sigma^2)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where mu is the mean and sigma controls the intensity of the force.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this example, the inputs are generated independently at each time step k and across input channels to simulate random excitations, such as wind or seismic forces.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to generate random input noise\nfunction generate_input(N_data::Int, nf::Int; input_mu::Float64, input_std::Float64)\n    Random.seed!(42)\n    p_real = input_mu .+ randn(N_data, nf) .* input_std\n    return p_real\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"generate_input (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Observation-Model","page":"Structural Dynamics With Augmented Kalman Filter","title":"Observation Model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"System responses, such as accelerations, are often measured at specific locations using sensors. The measurements are simulated using the equation:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"yk = G xk + J pk + vk","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"G\nmaps the system states xk to measured outputs.\nJ\nmaps the input forces pk to the measurements.\nvk sim mathcalN(0 sigma_y^2 I)\nis Gaussian noise representing sensor inaccuracies.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The noise variance sigma_y^2 is chosen as a fraction of the true system response variance for realism.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this example, accelerations are measured at selected degrees of freedom (e.g., nodes 1 and 4).","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to generate the measurements and noise\nfunction generate_measurements(ndof, na, nv, nd, N_data, x_real, y_real, p_real, StructuralModel, Sp)\n    # unpack the structural model\n    M = StructuralModel.M\n    K = StructuralModel.K\n    C = StructuralModel.C\n    \n    Sa = zeros(na, ndof)            # selection matrix\n    Sa[1, 1] = 1                    # acceleration at node 1\n    Sa[2, 4] = 1                    # acceleration at node 4\n    G = Sa * [-(M \\ K) -(M \\ C)] \n    J = Sa * (I \\ M) * Sp\n\n    ry = Statistics.var(y_real[2*ndof+1, :], ) * (0.1^2)        # simulate noise as 1% RMS of the noise-free acceleration response\n\n    nm = na + nv + nd\n\n    R = I(nm) .* ry\n\n    y_meas = zeros(nm, N_data)\n    y_noise = sqrt(ry) .* randn(nm, N_data)\n\n    # reconstruct the measurements\n    y_meas = Vector{Vector{Float64}}(undef, N_data)\n    for i in 1:N_data\n        y_meas[i] = G * x_real[:, i] + J * p_real[i, :] + y_noise[:, i]\n    end\n\n    return y_meas, G, J, R\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"generate_measurements (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Simulating-the-Structural-Response","page":"Structural Dynamics With Augmented Kalman Filter","title":"Simulating the Structural Response","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The structural response under applied forces is governed by the state-space equations:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"beginaligned\nxk+1  = A xk + B pk \nyk    = G_textfull xk + J_textfull pk\nendaligned","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where xk are the system states, pk are the input forces, and yk are the full-field responses, i.e., the response at every degree of freedom in our structure.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The function below returns:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"True States: x_textreal, propagated using $ A $ and $ B $.\nFull-Field Responses: y_textreal, incorporating both states and inputs.\nInput Forces: p_textreal, generated as stochastic excitations.\nResponse Matrices: G_textfull (state-to-response) and J_textfull (input-to-response).","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"These outputs simulate the physical behavior of the system and serve as the basis for inference. We keep the matrices because they will be used later when analyzing our results.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to simulate the structural response\nfunction simulate_response(A, B, StructuralModel, Sp, nf, ndof, N_data)\n    # unpack the structural model\n    M = StructuralModel.M\n    K = StructuralModel.K\n    C = StructuralModel.C\n    \n    p_real = generate_input(N_data, nf, input_mu = 0.0, input_std = 0.05)\n\n    Z = zeros(ndof, ndof)\n    Id = I(ndof)\n    \n    G_full = [\n            Id Z;\n            Z Id;\n            -(M \\ K) -(M \\ C)\n            ]\n\n    J_full = [\n        Z;\n        Z;\n        Id \\ M\n    ] * Sp\n    \n    # preallocate matrices\n    x_real = zeros(2 * ndof, N_data)\n    y_real = zeros(3 * ndof, N_data)\n\n    for i in 2:N_data\n        x_real[:, i] = A * x_real[:, i-1] + B * p_real[i-1, :]\n        y_real[:, i] = G_full * x_real[:, i-1] + J_full * p_real[i-1, :]\n    end\n\n    return x_real, y_real, p_real, G_full, J_full\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"simulate_response (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Augmented-State-Space-Model","page":"Structural Dynamics With Augmented Kalman Filter","title":"Augmented State-Space Model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In structural health monitoring, external input forces pk acting on a structure, such as environmental loads or unknown excitations, are often not directly measurable. To estimate both the system states xk and these unknown input forces, we augment the state vector as follows:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"tildexk = \nbeginbmatrix\nxk \npk\nendbmatrix","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This approach allows us to simultaneously infer the internal system states (e.g., displacements and velocities) and the unknown inputs using available measurements.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The augmented system dynamics are then expressed as:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"beginaligned\ntildexk+1  = A_textaug tildexk + wk \nyk  = G_textaug tildexk + vk\nendaligned","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"where:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"A_textaug\n: Augmented state transition matrix.  \nG_textaug\n: Augmented measurement matrix.  \nQ_textakf\n: Augmented process noise covariance, capturing uncertainties in both states and inputs.  \nwk\n, vk: Process and measurement noise.  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Full-Field-vs.-Measurement-Space","page":"Structural Dynamics With Augmented Kalman Filter","title":"Full-Field vs. Measurement Space","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"To avoid confusion, we define two augmented measurement matrices:  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"G_textaug\n: Projects the augmented state vector tildexk to the observed sensor measurements (e.g., accelerations at specific nodes).  \nG^*\n: The augmented full-field measurement matrix, which projects the augmented state vector to the full-field system response. This includes all degrees of freedom (displacements, velocities, and accelerations).  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The distinction is critical:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"G_textaug\nis used directly in the smoother to estimate states and inputs from limited measurements.  \nG^*\nis used later to reconstruct the full response field for visualization and validation.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"For clarity, we will refer to the augmented full-field matrix as G^* throughout the rest of this example, whereas, in the code, this will be the G_aug_fullfield object.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Noise-Covariances","page":"Structural Dynamics With Augmented Kalman Filter","title":"Noise Covariances","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this step, the process and measurement noise covariances are assumed to be known or pre-calibrated. For example:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The input force uncertainty Q_p is set to reflect significant variability.  \nState noise covariance Q_x is chosen to reflect uncertainty in the model.  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The augmented noise covariance matrix Q_textakf combines these quantities:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Q_textakf =\nbeginaligned\nbeginbmatrix\nQ_x  0 \n0  Q_p\nendbmatrix\nendaligned","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# function to construct the augmented model\nfunction construct_augmented_model(A, B, G, J, G_full, J_full, nf, ndof)\n    Z_aug = zeros(nf, 2*ndof)\n    A_aug = [\n        A B;\n        Z_aug I(nf)\n        ]\n    G_aug = [G J]\n\n    G_aug_fullfield = [G_full J_full]                               # full-field augmented matrix\n\n    Qp_aug = I(nf) * 1e-2                                           # assumed known or pre-callibrated\n    \n    Qx_aug = zeros(2*ndof, 2*ndof)\n    Qx_aug[(ndof+1):end, (ndof+1):end] = I(ndof) * 1e-1             # assumed known or pre-callibrated\n\n    Q_akf = [\n        Qx_aug Z_aug';\n        Z_aug Qp_aug\n    ]\n\n    return A_aug, G_aug, Q_akf, G_aug_fullfield\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"construct_augmented_model (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Finally, we combine all the key steps into a single workflow to generate the system dynamics, responses, measurements, and the augmented state-space model.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The results are stored in a StructuralModelData object for convenient access:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"function get_structural_model(StructuralModel, simulation_time, dt)\n\n    # intialize\n    ndof = size(StructuralModel.M)[1]                               # number of degrees of freedom\n    nf = 1                                                          # number of inputs\n    na, nv, nd = 2, 0, 0                                            # number of oberved accelerations, velocities, and displacements\n    N_data = Int(simulation_time / dt) + 1\n    t = range(0, stop=simulation_time, length=N_data)\n\n    # construct state-space model from structural matrices\n    A, B, Sp = construct_ssm(StructuralModel, dt, ndof, nf)\n\n    # Generate input and simulate response\n    x_real, y_real, p_real, G_full, J_full = simulate_response(A, B, StructuralModel, Sp, nf, ndof, N_data)\n\n    # Generate measurements\n    y_meas, G, J, R = generate_measurements(ndof, na, nv, nd, N_data, x_real, y_real, p_real, StructuralModel, Sp)\n\n    # Construct augmented model\n    A_aug, G_aug, Q_akf, G_aug_fullfield = construct_augmented_model(A, B, G, J, G_full, J_full, nf, ndof)\n\n    return StructuralModelData(t, ndof, nf, N_data, y_meas, A_aug, G_aug, G_aug_fullfield, Q_akf, R, x_real, y_real, p_real)\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"get_structural_model (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We define the simulation time and time step, then run the workflow to generate the structural model:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"simulation_time = 5.0\ndt = 0.001\n\nmodel_data = get_structural_model(StructuralModel, simulation_time, dt);","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#State-and-Input-Estimation-with-RxInfer","page":"Structural Dynamics With Augmented Kalman Filter","title":"State and Input Estimation with RxInfer","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In this section, we use RxInfer to estimate the system states and unknown input forces from the simulated noisy measurements using the Augmented State Space Model discussed.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"using RxInfer","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Defining-the-AKF-Smoother-Model","page":"Structural Dynamics With Augmented Kalman Filter","title":"Defining the AKF Smoother Model","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Here, we define our Augmented Kalman Filter (AKF) smoother using RxInfer. This probabilistic model estimates the system states and unknown input forces based on the measurements.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"State Prior: We start with a prior belief about the initial state, x0.  \nState Transition: At each time step, the system state evolves based on the transition matrix A and process noise covariance Q: xk sim mathcalN(A xk-1 Q)\nMeasurements: The observations (sensor data) are modeled as noisy measurements of the states: yk sim mathcalN(G xk R) where G maps the states to the measurements, and R is the measurement noise covariance.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"@model function smoother_model(y, x0, A, G, Q, R)\n\n    x_prior ~ x0\n    x_prev = x_prior  # initialize previous state with x_prior\n\n    for i in 1:length(y)\n        x[i] ~ MvNormal(mean = A * x_prev, cov = Q)\n        y[i] ~ MvNormal(mean = G * x[i], cov = R)\n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Running-the-AKF-Smoother","page":"Structural Dynamics With Augmented Kalman Filter","title":"Running the AKF Smoother","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Now that we have our system set up, it's time to estimate the system states and unknown input forces using RxInfer. We'll run the Augmented Kalman Filter (AKF) smoother to make sense of the noisy measurements.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Here’s the game plan:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Unpack the Data:   We grab everything we need from the model_data object – time, matrices, measurements, and noise covariances.\nSet the Initial State:   We start with a prior belief about the first state, assuming it's zero with some process noise:   x_0 sim mathcalN(0 Q_textakf)\nRun the Smoother:   We define a helper function to keep things tidy. This function calls RxInfer’s infer method, which does the heavy lifting for us.\nExtract and Reconstruct:  \nRxInfer gives us state marginals, which are the posterior estimates of the states.  \nUsing a helper function, we reconstruct the full-field responses (displacements, velocities, and accelerations).  \nWe also extract the estimated input forces, which are part of the augmented state.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"That’s it! With just a few lines of code, RxInfer takes care of the math behind the scenes and delivers smooth, reliable estimates of what’s happening inside the system.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# RxInfer returns the result in its own structure. \n# Here we wrap the results in a different struct for the example's convenience\nstruct InferenceResults\n    state_marginals\n    y_full_means\n    y_full_stds\n    p_means\n    p_stds\nend\n\nfunction run_smoother(model_data)\n    # unpack the model data\n    t               = model_data.t;\n    N_data          = model_data.N_data\n    A_aug           = model_data.A_aug;\n    G_aug           = model_data.G_aug;\n    G_aug_fullfield = model_data.G_aug_fullfield;\n    Q_akf           = model_data.Q_akf;\n    R               = model_data.R;\n    y_meas          = model_data.y_meas;\n    \n    # initialize the state - required when doing smoothing\n    x0 = MvNormalMeanCovariance(zeros(size(A_aug, 1)), Q_akf);\n\n    # define the smoother engine\n    function smoother_engine(y_meas, A, G, Q, R)\n        # run the akf smoother\n        result_smoother = infer(\n            model   = smoother_model(x0 = x0, A = A, G = G, Q = Q, R = R),\n            data    = (y = y_meas,),\n            options = (limit_stack_depth = 500, ) # This setting is required for large models\n        )\n\n        # return posteriors as this inference task returns the results as posteriors\n        # because inference is done over the full graph\n        return result_smoother.posteriors[:x]\n    end\n\n    # get the marginals of x\n    state_marginals = smoother_engine(y_meas, A_aug, G_aug, Q_akf, R)\n    \n    # reconstructing the full-field response:\n    # use helper function to reconstruct the full-field response\n    y_full_means, y_full_stds = reconstruct_full_field(state_marginals, G_aug_fullfield, N_data)\n\n    # extract the estimated input (input modeled as an augmentation state)\n    p_results_means = getindex.(mean.(state_marginals), length(state_marginals[1]))\n    p_results_stds = getindex.(std.(state_marginals), length(state_marginals[1]))\n    \n    return InferenceResults(state_marginals, y_full_means, y_full_stds, p_results_means, p_results_stds)\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"run_smoother (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/#Mapping-States-to-Full-Field-Responses","page":"Structural Dynamics With Augmented Kalman Filter","title":"Mapping States to Full-Field Responses","text":"","category":"section"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In the run_smoother function, we used a helper function to map the state estimates from the AKF smoother back to the full-field responses (e.g., displacements, velocities, and accelerations).  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Why is this important?","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"While the smoother estimates the system states, we often care about physical quantities like accelerations or displacements across the entire structure.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Using the augmented full-field matrix G^*, we compute:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Response means from state means:   mu_yi = G^* mu_xi  \nResponse uncertainties from state covariances:   sigma_yi = sqrttextdiag(G^* Sigma_xi G^*^top)  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"This gives us both the expected responses and their uncertainties at each time step.  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"In other words, this function connects the smoother’s internal state estimates to meaningful, physical quantities, making it easy to visualize the system’s behavior.  ","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# helper function to reconstruct the full field response from the state posteriors\nfunction reconstruct_full_field(\n    x_marginals,\n    G_aug_fullfield,\n    N_data::Int\n)\n    \n    # preallocate the full field response\n    y_means = Vector{Vector{Float64}}(undef, N_data)        # vector of vectors\n    y_stds = Vector{Vector{Float64}}(undef, N_data)\n\n    # reconstruct the full-field response using G_aug_fullfield\n    for i in 1:N_data\n        # extract the mean and covariance of the state posterior\n        state_mean = mean(x_marginals[i])       # each index is a vector\n        state_cov = cov(x_marginals[i])\n\n        # project mean and covariance onto the full-field response space\n        y_means[i] = G_aug_fullfield * state_mean\n        y_stds[i] = sqrt.(diag(G_aug_fullfield * state_cov * G_aug_fullfield'))\n    end\n\n    return y_means, y_stds\n\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"reconstruct_full_field (generic function with 1 method)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We now run the AKF smoother using the structural model data to estimate the system states, reconstruct the full-field responses, and extract the input forces along with their uncertainties.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Let’s fire up that RxInfer!","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# run the smoother\nsmoother_results = run_smoother(model_data);","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"We first write a helper function and then plot the true states, full-field response, input, their estimates, and the associated uncertainty:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# helper function\nfunction plot_with_uncertainty(\n    t,\n    true_values,\n    estimated_means,\n    estimated_uncertainties,\n    ylabel_text,\n    title_text,\n    label_suffix=\"\";\n    plot_size = (700,300),\n   \n)\n    # plot true values\n    plt = plot(\n        t,\n        true_values,\n        label=\"true ($label_suffix)\",\n        lw=2,\n        color=:blue,\n        size=plot_size,\n        left_margin = 5Plots.mm,\n        top_margin = 5Plots.mm,  \n        bottom_margin = 5Plots.mm  \n    )\n\n    # plot estimated values with uncertainty ribbon\n    plot!(\n        plt,\n        t,\n        estimated_means,\n        ribbon=estimated_uncertainties,\n        fillalpha=0.3,\n        label=\"estimated ($label_suffix)\",\n        lw=2,\n        color=:orange,\n        linestyle=:dash\n    )\n\n    # add labels and title\n    xlabel!(\"time (s)\")\n    ylabel!(ylabel_text)\n    title!(title_text)\n    \n    return plt\nend","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"plot_with_uncertainty (generic function with 2 methods)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"# select some DOFs to plot\nndof = size(StructuralModel.M)[1]\n\ndisplay_state_dof    = 4                # dof 1:4 displacements, dof 5:8 velocities\ndisplay_response_dof = 2*ndof + 1       # dof 1:4 displacements, dof 5:8 velocities, dof 9:12 accelerations\ndisplay_input_dof    = 1                # the only one really\n\n# plot the states\nstate_plot = plot_with_uncertainty(\n    model_data.t,\n    model_data.x_real[display_state_dof, :],\n    getindex.(mean.(smoother_results.state_marginals), display_state_dof),\n    getindex.(std.(smoother_results.state_marginals), display_state_dof),\n    \"state value\",\n    \"state estimate (dof $(display_state_dof))\",\n    \"state dof $(display_state_dof)\"\n);\n\n# plot the responses\nresponse_plot = plot_with_uncertainty(\n    model_data.t,\n    model_data.y_real[display_response_dof, :],\n    getindex.(smoother_results.y_full_means, display_response_dof),\n    getindex.(smoother_results.y_full_stds, display_response_dof),\n    \"response value\",\n    \"reconstructed response (dof $(display_response_dof))\",\n    \"response dof $(display_response_dof)\"\n);\n\n# plot the inputs\ninput_plot = plot_with_uncertainty(\n    model_data.t,\n    model_data.p_real[:, display_input_dof],\n    smoother_results.p_means,\n    smoother_results.p_stds,\n    \"force value\",\n    \"input estimate (applied at dof $(display_input_dof))\",\n    \"input force $(display_input_dof)\"\n);\n\ndisplay(state_plot)\ndisplay(response_plot)\ndisplay(input_plot)","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"(Image: ) (Image: ) (Image: )","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Let's quickly go over these results now:","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"State estimation: The true state and the estimated state show excellent agreement, demonstrating the accuracy of the smoother model implemented via RxInfer. The uncertainty bounds around the estimated states are noticeable, especially early in the domain. This reflects the natural uncertainty in state estimation since only accelerations are observed, whereas displacements and velocities are inferred through integration.\nReconstructed response: the real response and the reconstructed response align well across the domain, confirming that the filter captures the dynamics quite nicely. The uncertainty bounds here are narrower, showing that the confidence improves as the filter incorporates observations of these quantities of interest (i.e. accelerations).\nInput force reconstruction: The input force and its reconstructed counterpart show significant high frequency variations with very narrow uncertainty bounds. This is expected because accelerations, being the directly observed quantities, are estimated with higher confidence. Plus, we gave ourselves a small advantage by using a well-calibrated prior on this quantity of interest (Q_p).","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"The results demonstrate how well the smoother model, implemented with RxInfer, performs in capturing the system dynamics and reconstructing hidden states and inputs. Notably, setting up the probabilistic model was straightforward and intuitive—much easier than dealing with the rest of the structural modeling! This highlights the power of RxInfer for quickly building and solving complex inference problems while keeping the implementation clean and efficient.","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"With just a few lines of code, we were able to estimate states, reconstruct responses, and confidently quantify uncertainties—a win for both accuracy and usability. 🚀","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/","page":"Structural Dynamics With Augmented Kalman Filter","title":"Structural Dynamics With Augmented Kalman Filter","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/structural_dynamics_with_augmented_kalman_filter/Project.toml`\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [10745b16] Statistics v1.11.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"page"},{"location":"categories/problem_specific/universal_mixtures/#Universal-Mixtures","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"section"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"using RxInfer, Distributions, Random, Plots","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane are having a coin toss competition. Before they start, they both have the feeling that something is not right. The coin is unbalanced and favors one side over the other. However, both John and Jane do not know which side is being favored. John thinks that the coin favors heads and Jane thinks tails. Coincidentally, both John and Jane have a strong mathematics background and are aware of the appropriate likelihood function for this experiment","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y_i mid theta) = mathrmBer(y_i mid theta)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"where y_i in 01 are the outcomes of the coin tosses, i.e. heads or tails, and where theta is the coin parameter. They express their gut feeling about the fairness of the coin in terms of a prior distribution over the coin parameter theta, which represents the probability of the coin landing on heads. Based on their gut feeling and the support of thetain01 they come up with the prior beliefs:","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textJohn) = mathrmBeta(theta mid 7 2)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textJane) = mathrmBeta(theta mid 2 7)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior beliefs\")\nplot!(rθ, (x) -> pdf(Beta(7.0, 2.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) John\", c=1)\nplot!(rθ, (x) -> pdf(Beta(2.0, 7.0), x), fillalpha=0.3, fillrange = 0, label=\"p(θ) Jane\", c=3,)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane really want to clear the odds and decide to perform a lengthy experiment. They toss the unbalanced coin N = 10 times, because their favorite TV show is cancelled anyway and therefore they have plenty of time. ","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"true_coin = Bernoulli(0.25)\nnr_throws = 10\ndataset = Int.(rand(MersenneTwister(42), true_coin, nr_throws))\nnr_heads, nr_tails = sum(dataset), nr_throws-sum(dataset)\nprintln(\"experimental outcome: \\n - heads: \", nr_heads, \"\\n - tails: \", nr_tails);","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"experimental outcome: \n - heads: 5\n - tails: 5","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"For computing the posterior beliefs p(theta mid y) about the parameter theta, they will perform probabilistic inference in the model based on Bayes' rule. Luckily everything is tractable and therefore they can resort to exact inference. They decide to outsource these tedious computations using RxInfer.jl and specify the following models:","category":"page"},{"location":"categories/problem_specific/universal_mixtures/#John's-model:","page":"Universal Mixtures","title":"John's model:","text":"","category":"section"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta mid textJohn) = p(theta mid textJohn) prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_john(y)\n\n    # specify John's prior model over θ\n    θ ~ Beta(7.0, 2.0)\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"categories/problem_specific/universal_mixtures/#Jane's-model:","page":"Universal Mixtures","title":"Jane's model:","text":"","category":"section"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta mid textJane) = p(theta mid textJane) prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_jane(y)\n\n    # specify Jane's prior model over θ\n    θ ~ Beta(2.0, 7.0)\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Now it is time to figure out whose prior belief was the best and who was actually right. They perform probabilistic inference automatically and compute the Bethe free energy to compare eachothers models. For acyclic models, the Bethe free energy mathrmF_B bounds the model evidence p(y) as ","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"mathrmF_Bpq geq - ln p(y)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_john = infer(\n    model = beta_model_john(), \n    data  = (y = dataset, ),\n    free_energy = true,\n)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[8.28853]","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_jane = infer(\n    model = beta_model_jane(), \n    data  = (y = dataset, ),\n    free_energy = true\n)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[8.28853]","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"From these results, they agree that Jane her gut feeling was right all along, as her Bethe free energy is lower and therefore her model evidence is higher. Nonetheless, after the 10 throws, they now have a better idea about the underlying theta parameter. They formulate this through the posterior distributions p(theta mid y textJohn) and p(theta mid y textJane):","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior beliefs\")\nplot!(rθ, (x) -> pdf(result_john.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) John\", c=1)\nplot!(rθ, (x) -> pdf(result_jane.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"p(θ|y) Jane\", c=3,)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"What John and Jane did not know, was that Mary, their neighbour, was overhearing their conversation. She was also curious, but could not see the coin. She did not really know how to formulate a prior distribution over theta, so instead she combined both John and Jane their prior beliefs. She had the feeling that John his assessment was more correct, as he was often going to the casino. As a result, she mixed the prior beliefs of John and Jane with proportions 0.7 and 0.3, respectively. Her model for the environment is specified as","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta c mid textMary) = p(c mid textMary)  p(theta mid textJohn)^c p(theta mid textJane)^1-c prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"where c describes the probability of John being correct as ","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(c mid textMary) = mathrmBer(c mid 07)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"The predictive distribution p(theta mid textMary) for theta (similar to the prior beliefs of John and Jane) she obtained from the marginalisation over c as","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textMary) = sum_cin01 p(cmidtextMary) p(theta mid textJohn)^c p(theta mid textJane)^1-c = 07 cdot p(theta mid textJohn) + 03 cdot p(theta mid textJane)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior belief\")\nplot!(rθ, (x) -> pdf(MixtureDistribution([Beta(2.0, 7.0), Beta(7.0, 2.0)], [ 0.3, 0.7 ]), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) Mary\", c=1)\nplot!(rθ, (x) -> 0.7*pdf(Beta(7.0, 2.0), x), c=3, label=\"\")\nplot!(rθ, (x) -> 0.3*pdf(Beta(2.0, 7.0), x), c=3, label=\"\")","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"She was also interested in the results and used the new Mixture node and addons in ReactiveMP.jl. She specified her model as follows and performed inference in this model:","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_mary(y)\n\n    # specify John's and Jane's prior models over θ\n    θ_jane ~ Beta(2.0, 7.0)\n    θ_john ~ Beta(7.0, 2.0)\n\n    # specify initial guess as to who is right\n    john_is_right ~ Bernoulli(0.7) \n\n    # specify mixture prior Distribution\n    θ ~ Mixture(switch = john_is_right, inputs = [θ_jane, θ_john])\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"This Mixture node updates the belief over c on the performance of the individual models of both John and Jane using so-called scale factors, as introduced in Nguyen et al.. The specific update rules for this node can be found here.","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_mary = infer(\n    model = beta_model_mary(), \n    data  = (y = dataset, ),\n    returnvars = (θ = KeepLast(), θ_john = KeepLast(), θ_jane = KeepLast(), john_is_right = KeepLast()),\n    addons = AddonLogScale(),\n    postprocess = UnpackMarginalPostprocess(),\n)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (john_is_right, θ_john, θ, θ_jane)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Mary was happy, with her mixture prior, she beat John in terms of performance. However, it was not the best decision to think that John was right. In fact, after the experiment there was only a minor possibility remaining that John was right. Her posterior distribution over theta also changed, and as expected the estimate from Jane was more prominent.","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior belief\")\nplot!(rθ, (x) -> pdf(result_mary.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) Mary\", c=1)\nplot!(rθ, (x) -> result_mary.posteriors[:θ].weights[1] * pdf(component(result_mary.posteriors[:θ], 1), x), label=\"\", c=3)\nplot!(rθ, (x) -> result_mary.posteriors[:θ].weights[2] * pdf(component(result_mary.posteriors[:θ], 2), x), label=\"\", c=3)","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/universal_mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/universal_mixtures/Project.toml`\n  [31c24e10] Distributions v0.25.117\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Feature-Functions-in-Bayesian-Regression","page":"Feature Functions In Bayesian Regression","title":"Feature Functions in Bayesian Regression","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This notebook demonstrates how we can use probabilistic methods to learn and predict continuous functions from noisy data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This example is inspired by Chapter 4.1 Regression from the excellent book Probabilistic Numerics by Phillip Hennig, Michael A. Osborn, and Hans P. Kersting. We'll take their theoretical foundations and bring them to life with practical code examples.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The code and narrative in this notebook is written by Dmitry Bagaev (GitHub, LinkedIn). While some explanations draw from the book's content, we'll focus on building intuition through interactive examples and visualizations.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"By the end of this notebook, you'll understand:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The power of linear regression with basis functions\nHow to handle uncertainty in your predictions\nPractical implementation using Julia and RxInfer.jl","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"We start by importing all required packages for this example, the primary of which is of course RxInfer!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"using RxInfer, StableRNGs, LinearAlgebra, Plots, DataFrames","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Gaussian distributions (multivariate) assign probability density to vectors of real numbers - think of them as sophisticated probability maps for multiple variables at once. In numerical applications, we often encounter real-valued functions f  mathbbX rightarrow R over some input domain mathbbX (imagine predicting house prices based on features like size and location).","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"A interesting way to use the Gaussian inference framework is to assume that f can be written as a weighted sum over a finite number F of feature functions phi_i  mathbbX rightarrow mathbbR_i=1F (much like how a house price might be a weighted combination of its features, e.g. size, number of floors, number of rooms, etc..):","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nf(x) = sum_i=1^F phi_i(x)omega_i = Phi^T_x omega  mathrmwhere  omega in mathbbR^F\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"As discussed in the Probabilistic Numerics book, uncertainty is a fundamental aspect of numerical computations. When we perform regression, we are essentially solving an inverse problem - trying to infer the underlying function from noisy observations. This inherently involves uncertainty for several reasons:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Our observations usually contain noise and measurement errors\nWe have a finite number of samples, leaving gaps in our knowledge\nThe true function may be more complex than our model can capture","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"By modeling uncertainty explicitly through a Gaussian distribution over the weights omega, we can:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Quantify our confidence in predictions\nMake more robust decisions by accounting for uncertainty\nDetect when we're extrapolating beyond our data\nPropagate uncertainty on the next step in our Machine Learning pipeline","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Mathematically, we express this uncertainty as:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\np(omega) = mathcalN(omega vert mu Sigma)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Where mu represents our best estimate of the weights and Sigma captures our uncertainty about them.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Dataset:-Noisy-Observations-in-the-Real-World","page":"Feature Functions In Bayesian Regression","title":"Dataset: Noisy Observations in the Real World","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"In real-world scenarios, we rarely have access to perfect measurements. Instead, we collect observations Y = y_1 cdots  y_N  in mathbbR that are corrupted by Gaussian noise - a common and mathematically convenient way to model measurement uncertainty. These noisy samples of our target function f are taken at specific input locations X, with the noise characterized by a covariance matrix Lambda  mathbbR^NN. This setup mirrors many practical applications, from sensor measurements to experimental data collection.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's assume we have collected noisy measurenets Y at locations X:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"N = 40\nΛ = I\nX = range(-8, 8, length=N)\n\nrng = StableRNG(42)\n\n# Arbitrary non-linear function, which is hidden\nf(x) = -((-x / 3)^3 - (-x / 2)^2 + x + 10) \n\nY = rand(rng, MvNormalMeanCovariance(f.(X), Λ))\n\n# Can be loaded from a file or a database\ndf = DataFrame(X = X, Y = Y)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"40×2 DataFrame\n Row │ X         Y\n     │ Float64   Float64\n─────┼────────────────────\n   1 │ -8.0      -5.63321\n   2 │ -7.58974  -3.75472\n   3 │ -7.17949  -2.26681\n   4 │ -6.76923  -1.95387\n   5 │ -6.35897  -2.92934\n   6 │ -5.94872  -2.31714\n   7 │ -5.53846  -4.10432\n   8 │ -5.12821  -4.08565\n  ⋮  │    ⋮         ⋮\n  34 │  5.53846  -1.5234\n  35 │  5.94872   0.98319\n  36 │  6.35897   3.86051\n  37 │  6.76923   4.48039\n  38 │  7.17949   8.71697\n  39 │  7.58974  12.703\n  40 │  8.0      19.0635\n           25 rows omitted","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Train-and-Test-Dataset-Configurations","page":"Feature Functions In Bayesian Regression","title":"Train & Test Dataset Configurations","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"To thoroughly evaluate our model's performance and robustness, we'll create three distinct train-test splits of our data. This approach helps us understand how well our model generalizes to different regions of the input space and whether it can effectively capture the underlying patterns regardless of which portions of the data it learns from.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"We'll explore the following configurations:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Forward Split: Uses the first half for training and second half for testing, evaluating the model's ability to extrapolate to higher x-values\nReverse Split: Uses the first half for testing and second half for training, testing extrapolation to lower x-values\nInterleaved Split: Uses first and last quarters for training and middle portion for testing, assessing interpolation capabilities","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"These diverse splits will help reveal any biases in our model and ensure it performs consistently across different regions of the input space. They also allow us to evaluate both interpolation (predicting within the training range) and extrapolation (predicting outside the training range) capabilities.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"# Split data into train/test sets\n# Forward split - first half train, second half test\ndataset_1 = let mid = N ÷ 2\n    (\n        y_train = Y[1:mid], x_train = X[1:mid],\n        y_test = Y[mid+1:end], x_test = X[mid+1:end]\n    )\nend\n\n# Reverse split - first half test, second half train  \ndataset_2 = let mid = N ÷ 2\n    (\n        y_test = Y[1:mid], x_test = X[1:mid],\n        y_train = Y[mid+1:end], x_train = X[mid+1:end]\n    )\nend\n\n# Interleaved split - first/last quarters train, middle half test\ndataset_3 = let q1 = N ÷ 4, q3 = 3N ÷ 4\n    (\n        y_train = [Y[1:q1]..., Y[q3+1:end]...],\n        x_train = [X[1:q1]..., X[q3+1:end]...],\n        y_test = Y[q1+1:q3],\n        x_test = X[q1+1:q3]\n    )\nend\n\ndatasets = [dataset_1, dataset_2, dataset_3]\n\n# Create visualization for each dataset split\nps = map(enumerate(datasets)) do (i, dataset)\n    p = plot(\n        xlim = (-10, 10), \n        ylim = (-30, 30),\n        title = \"Dataset $i\",\n        xlabel = \"x\",\n        ylabel = \"y\"\n    )\n    scatter!(p, \n        dataset[:x_train], dataset[:y_train],\n        yerror = Λ,\n        label = \"Train dataset\",\n        color = :blue,\n        markersize = 4\n    )\n    scatter!(p,\n        dataset[:x_test], dataset[:y_test], \n        yerror = Λ,\n        label = \"Test dataset\",\n        color = :red,\n        markersize = 4\n    )\n    return p\nend\n\nplot(ps..., size = (1200, 400), layout = @layout([a b c]))","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The datasets above provide nonlinear data with independent and identically distributed (i.i.d.) Gaussian observation noise, where we set the noise covariance Λ = I (identity matrix).","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Bayesian-Inference-with-RxInfer","page":"Feature Functions In Bayesian Regression","title":"Bayesian Inference with RxInfer","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Our exciting challenge is to uncover the probability distribution over the parameter vector omega, given our basis functions phi and observed data points (XY). To tackle this, we'll harness the power of probabilistic programming by constructing an elegant generative model using RxInfer's @model macro. The beauty of this approach lies in its simplicity - we can express our entire model in just a few lines of code:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"@model function parametric_regression(ϕs, x, y, μ, Σ, Λ)\n    # Prior distribution over parameters ω\n    ω ~ MvNormal(mean = μ, covariance = Σ)\n    \n    # Design matrix Φₓ where each element is ϕᵢ(xⱼ)\n    Φₓ = [ϕ(xᵢ) for xᵢ in x, ϕ in ϕs]\n    \n    # Likelihood of observations y given parameters ω\n    y ~ MvNormal(mean = Φₓ * ω, covariance = Λ)\nend","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's break down the key components of our probabilistic model:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"phimathrms\ncontains our basis functions phi_i - these are the building blocks of our model\nx\nholds the input locations X where we've made observations. Think of these as the points along the x-axis where we've collected data, like timestamps or spatial coordinates.\ny\ncontains our noisy measurements at each location in X.\nmu\ndefines our prior beliefs about the average values of the parameters omega. Setting mu = 0 indicates we believe the parameters are centered around zero before seeing any data.\nSigma\nencodes our uncertainty about omega before seeing data. A larger Sigma means we're more uncertain, while smaller values indicate stronger prior beliefs.\nLambda\nrepresents the noise in our observations. For example, Lambda = 01I suggests our measurements have small, independent Gaussian noise, while larger values indicate noisier data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"To put this model to work, we'll use RxInfer's powerful infer function. Here's how:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"function infer_ω(; ϕs, x, y)\n    # Create probabilistic model, \n    # RxInfer will construct the graph of this model auutomatically\n    model = parametric_regression(\n        ϕs = ϕs, \n        μ  = zeros(length(ϕs)),\n        Σ  = I,\n        Λ  = I,\n        x  = x\n    )\n\n    # Let RxInfer do all the math for you\n    result = infer(\n        model = model, \n        data  = (y = y,)\n    )\n\n    # Return posterior over ω\n    return result.posteriors[:ω]\nend","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"infer_ω (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#How-to-choose-basis-functions?","page":"Feature Functions In Bayesian Regression","title":"How to choose basis functions?","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Just like how choosing between pizza toppings can make or break your dinner, the choice of basis functions phi_i  can dramatically impact our results! Think of basis functions as the building blocks of our mathematical LEGO set -  pick the wrong pieces and your model might end up looking more like abstract art than a useful predictor.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Why does this matter? Because these functions are the \"vocabulary\" our model uses to describe the patterns in our data. Choose a too-simple vocabulary and your model will sound like a caveman (\"data go up, data go down\"). Choose one that's  too complex and it might start speaking mathematical gibberish!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's embark on a thrilling journey through different datasets with various basis function choices. We'll create a  handy function that will:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Take our basis functions for a test drive 🚗\nRun inference on multiple datasets defined above\nCreate beautiful plots that would make any statistician swoon","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(RxInfer makes it so easy to perform Bayesian inference so I have more time to make beautiful plots!)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"function plot_inference_results_for(; ϕs, datasets, title = \"\", rng = StableRNG(42))\n    # Create main plot showing basis functions\n    p1 = plot(\n        title = \"Basis functions: $(title)\", \n        xlabel = \"x\",\n        ylabel = \"y\",\n        xlim = (-5, 5), \n        ylim = (-10, 10),\n        legend = :outertopleft,\n        grid = true,\n        fontfamily = \"Computer Modern\"\n    )\n\n    # Plot basis functions in gray\n    plot_ϕ!(p1, ϕs, color = :gray, alpha = 0.5, \n            labels = [\"ϕ$i\" for _ in 1:1, i in 1:length(ϕs)])\n    \n    # Add examples with random ω values\n    plot_ϕ!(p1, ϕs, randn(rng, length(ϕs), 3), \n            linewidth = 2)\n\n    # Create subplot for each dataset\n    ps = map(enumerate(datasets)) do (i, dataset)\n        p2 = plot(\n            title = \"Dataset #$(i): $(title)\",\n            xlabel = \"x\",\n            ylabel = \"y\", \n            xlim = (-10, 10),\n            ylim = (-25, 25),\n            grid = true,\n            fontfamily = \"Computer Modern\"\n        )\n\n        # Infer posterior over ω\n        ωs = infer_ω(\n            ϕs = ϕs, \n            x = dataset[:x_train], \n            y = dataset[:y_train]\n        )\n\n        # Plot posterior mean\n        plot_ϕ!(p2, ϕs, mean(ωs),\n                linewidth = 3,\n                color = :green,\n                labels = \"Posterior mean\")\n\n        # Plot posterior samples\n        plot_ϕ!(p2, ϕs, rand(ωs, 15),\n                linewidth = 1,\n                color = :gray,\n                alpha = 0.4,\n                labels = nothing)\n\n        # Add data points\n        scatter!(p2, dataset[:x_train], dataset[:y_train],\n                yerror = Λ,\n                label = \"Training data\",\n                color = :royalblue,\n                markersize = 4)\n        scatter!(p2, dataset[:x_test], dataset[:y_test],\n                yerror = Λ,\n                label = \"Test data\", \n                color = :crimson,\n                markersize = 4)\n\n        return p2\n    end\n\n    # Combine all plots\n    plot(p1, ps..., \n         size = (1000, 800),\n         margin = 5Plots.mm,\n         layout = (2,2))\nend\n\n# Helper function to plot basis functions\nfunction plot_ϕ!(p, ϕs; rl = -10, rr = 10, kwargs...)\n    xs = range(rl, rr, length = 200)\n    ys = [ϕ(x) for x in xs, ϕ in ϕs]\n    plot!(p, xs, ys; kwargs...)\nend\n\n# Helper function to plot function with given weights\nfunction plot_ϕ!(p, ϕs, ωs; rl = -10, rr = 10, kwargs...)\n    xs = range(rl, rr, length = 200)\n    ys = [ϕ(x) for x in xs, ϕ in ϕs]\n    yr = ys * ωs\n    labels = [\"Sample $i\" for _ in 1:1, i in 1:size(ωs,2)]\n    plot!(p, xs, yr, labels = labels; kwargs...)\nend","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_ϕ! (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Phew! We've finally escaped the plotting purgatory - you know it's bad when the visualization code is longer than the actual inference code! But fear not, dear reader, for we're about to dive into the juicy stuff. Grab your statistical popcorn, because the real fun is about to begin!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Polynomials:-The-Building-Blocks-of-Function-Approximation","page":"Feature Functions In Bayesian Regression","title":"Polynomials: The Building Blocks of Function Approximation","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's start our exploration with one of the most fundamental and elegant choices for basis functions: polynomials. These simple yet powerful functions form the backbone of many approximation techniques in mathematics and machine learning.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"For our polynomial basis functions phi_i, we'll use the classic form:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = x^i\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where i represents the degree of each polynomial term. This gives us a sequence of increasingly complex functions: constant (x^0 = 1), linear (x^1), quadratic (x^2), cubic (x^3), and so on. When combined with appropriate weights omega, these basis functions can approximate a wide variety of smooth functions - a result famously known as the Weierstrass approximation theorem.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's witness the magic of RxInfer as it efficiently infers the posterior distribution over the weights omega using these polynomial basis functions. The beauty of this approach lies in how it automatically determines the contribution of each polynomial term to fit our data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"polynomials\",\n    datasets = datasets,\n    ϕs       = [ (x) -> x ^ i for i in 0:5 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's break down what we're seeing in these fascinating plots! The first plot (in gray) reveals our polynomial basis functions in their raw form - from constant to quintic terms. Overlaid on these are some example functions generated by combining these basis functions with random weights ω, giving us a glimpse of the expressive power of polynomial approximation.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The subsequent plots demonstrate how our model performs inference on different datasets. Notice how the posterior distribution (shown by the shaded region) adapts to capture the uncertainty in different regions of the input space. It's particularly interesting to observe how the model's predictions change when faced with different training and test sets - a beautiful illustration of how the learning process is influenced by the data we feed it.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"While polynomials have served us well here, they're just one tool in our mathematical toolbox. Ready to explore some alternative basis functions that might capture different aspects of our target function? Let's dive into some exciting alternatives!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Trigonometric-Functions:-Catch-Some-Waves","page":"Feature Functions In Bayesian Regression","title":"Trigonometric Functions: Catch Some Waves","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"While polynomials are great (and we love them dearly), sometimes life isn't just about going up and down in straight-ish lines. Sometimes, we need to embrace our inner surfer and catch some waves! Enter trigonometric functions - the mathematical world's answer to the question \"What if everything just went round and round?\"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Trigonometric functions, particularly sin and cos, have been the backbone of mathematical analysis since ancient times. From describing planetary motions to analyzing sound waves, these periodic functions have a special place in the mathematician's heart. Their ability to represent cyclic patterns makes them particularly powerful for approximating periodic phenomena - something our polynomial friends from earlier might struggle with (imagine a polynomial trying to do the wave at a sports event - awkward!).","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"For our basis functions, we'll use scaled versions of sine and cosine:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = mathrmsin(fracxi) \nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = mathrmcos(fracxi)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where i acts as a frequency scaling factor. As i increases, our waves become more stretched out, giving us different frequencies to work with. Think of it as having an orchestra where each instrument plays the same tune but at different tempos!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's start by riding the sine wave alone (no cosine jealousy please!) for i = 15. Will these wavy functions give our polynomial predecessors a run for their money? Let's find out!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"trigonometric sin\",\n    datasets = datasets,\n    ϕs       = [ (x) -> sin(x / i) for i in 1:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Now let's examine the results using cosine basis functions","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"trigonometric cos\",\n    datasets = datasets,\n    ϕs       = [ (x) -> cos(x / i) for i in 1:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"And for our grand finale, let's combine both sin and cos - because two waves are better than one! (Just don't tell that to particle-wave duality...)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"trigonometric sin & cos\",\n    datasets = datasets,\n    ϕs       = [\n        [ (x) -> sin(x / i) for i in 1:4 ]...,\n        [ (x) -> cos(x / i) for i in 1:4 ]...,\n    ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Incredible! RxInfer proved to be quite the adaptable fellow - it handled these different basis functions without missing a beat. The results speak for themselves: our sine and cosine tag team performed remarkably well for this example. I guess you could say they really found their wavelength!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Comparing-Model-Performance-via-Log-Evidence","page":"Feature Functions In Bayesian Regression","title":"Comparing Model Performance via Log-Evidence","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Now that we've explored different basis functions, let's quantitatively evaluate their performance using Free Energy, also known as negative log-evidence or negative Evidence Lower BOund (ELBO). RxInfer can compute Free Energy values when requested, which serve as a principled way to compare different models.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Free Energy has several important properties:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"It acts as a proxy for negative log model evidence P(y|model)\nLower values indicate better model fit, balancing complexity and data fit\nIt automatically implements Occam's Razor by penalizing overly complex models","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"For example, if we have:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Model A: Free Energy = 100\nModel B: Free Energy = 50","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Then Model B provides a better explanation of the data, as exp(-50) > exp(-100).","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's analyze the Free Energy values for our polynomial and trigonometric basis functions to determine which model class provides the best explanation of our data. We'll check:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Pure sine basis functions\nPure cosine basis functions  \nCombined sine and cosine basis functions","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This will help us quantitatively validate our earlier visual assessments.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"# Combine the function definition with the usage\nfunction infer_ω_but_return_free_energy(; ϕs, x, y)\n    result = infer(\n        model = parametric_regression(\n            ϕs = ϕs, \n            μ  = zeros(length(ϕs)),\n            Σ  = I,\n            Λ  = I,\n            x  = x\n        ), \n        data  = (y = y,),\n        free_energy = true\n    )\n    return first(result.free_energy)\nend\n\ndfs = map(enumerate(datasets)) do (i, dataset)\n    # Generate basis functions\n    sin_bases = [(x) -> sin(x / i) for i in 1:8]\n    cos_bases = [(x) -> cos(x / i) for i in 1:8]\n    combined_bases = [\n        [(x) -> sin(x / i) for i in 1:4]...,\n        [(x) -> cos(x / i) for i in 1:4]...\n    ]\n\n    # Calculate free energy for each basis\n    energies = [\n        infer_ω_but_return_free_energy(ϕs=sin_bases, x=dataset[:x_train], y=dataset[:y_train]),\n        infer_ω_but_return_free_energy(ϕs=cos_bases, x=dataset[:x_train], y=dataset[:y_train]),\n        infer_ω_but_return_free_energy(ϕs=combined_bases, x=dataset[:x_train], y=dataset[:y_train])\n    ]\n\n    # Create DataFrame row\n    DataFrame(\n        dataset = fill(i, 3),\n        fns = [:sin, :cos, :sin_cos],\n        free_energy = energies\n    )\nend\n\nvcat(dfs...)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"9×3 DataFrame\n Row │ dataset  fns      free_energy\n     │ Int64    Symbol   Float64\n─────┼────────────────────────────────\n   1 │       1  sin       74.4889\n   2 │       1  cos       47.0473\n   3 │       1  sin_cos   95.6676\n   4 │       2  sin      366.386\n   5 │       2  cos      349.413\n   6 │       2  sin_cos    5.96904e10\n   7 │       3  sin      104.042\n   8 │       3  cos      163.047\n   9 │       3  sin_cos   84.0258","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The results demonstrate that the choice of basis functions plays a significant role, as evidenced by the varying values of the Free Energy function. For dataset 1, cosine-based basis functions perform better than both sine-based and combined sine-cosine basis functions. Meanwhile, for dataset 3, the combination of sine and cosine basis functions yields superior results.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"However, why limit ourselves to polynomials and trigonometric functions? Let's explore other possibilities!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Switch-Functions:-A-Binary-Approach-to-Basis-Functions","page":"Feature Functions In Bayesian Regression","title":"Switch Functions: A Binary Approach to Basis Functions","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's explore an intriguing and perhaps unconventional choice for basis functions: the switch functions. These functions, despite their simplicity, can be remarkably effective in certain scenarios.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"A switch function essentially divides the input space into two regions, outputting either +1 or -1 based on which side of a threshold the input falls. Mathematically, we define it as:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = mathrmsign(x - i)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where i serves as the threshold point. The function returns +1 when x  i and -1 when x  i. This creates a sharp \"switch\" at x = i, hence the name.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"What makes these functions particularly interesting is their ability to capture discontinuities and sharp transitions in the data. By combining multiple switch functions with different threshold points, we can approximate complex patterns through a series of binary decisions.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's see how these switch functions perform on our datasets!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"switches\",\n    datasets = datasets,\n    ϕs       = [ (x) -> sign(x - i) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Step-Functions:-A-Binary-Leap-Forward","page":"Feature Functions In Bayesian Regression","title":"Step Functions: A Binary Leap Forward","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's explore another fascinating class of basis functions: step functions. Also known as Heaviside functions, these elegant mathematical constructs make a dramatic jump from 0 to 1 at a specific threshold point.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Mathematically, we define our step basis functions as:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = mathbbI(x - i  0)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where mathbbI is the indicator function that equals 1 when its argument is true and 0 otherwise. Unlike the switch functions we saw earlier, step functions provide a unidirectional transition, making them particularly useful for modeling data with distinct regimes or threshold effects.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"steps\",\n    datasets = datasets,\n    ϕs       = [ (x) -> ifelse(x - i > 0, 1.0, 0.0) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Linear-Basis-Functions:-A-Classic-Twist","page":"Feature Functions In Bayesian Regression","title":"Linear Basis Functions: A Classic Twist","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Here's an intriguing proposition: what if we used linear functions as our basis functions in linear regression? While it might sound redundant at first, this approach offers a fascinating perspective. By centering linear functions at different points, we create a rich set of features that can capture both local and global trends in our data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The basis functions take the form:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = vert x - i vert\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"where each function measures the absolute distance from a reference point i. This creates a V-shaped function centered at each point, allowing us to model both increasing and decreasing trends with remarkable flexibility.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"linears\",\n    datasets = datasets,\n    ϕs       = [ (x) -> abs(x - i) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Absolute-Exponential-Functions:-Elegantly-Decaying-Distance","page":"Feature Functions In Bayesian Regression","title":"Absolute Exponential Functions: Elegantly Decaying Distance","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's venture into the realm of absolute exponential functions, a fascinating class of basis functions that elegantly capture the notion of distance-based influence. These functions, also known as Laplace kernels in some contexts, decay exponentially with the absolute distance from their center points.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The mathematical formulation reveals their elegant simplicity:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = e^-vert x - i vert\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This expression creates a peaked function that reaches its maximum of 1 at x = i and smoothly decays in both directions, providing a natural way to model localized influences that diminish with distance.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"abs exps\",\n    datasets = datasets,\n    ϕs       = [ (x) -> exp(-abs(x - i)) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Squared-Exponential-Functions:-The-Gaussian-Bell-Curves","page":"Feature Functions In Bayesian Regression","title":"Squared Exponential Functions: The Gaussian Bell Curves","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's explore another one of the most elegant and widely-used basis functions in machine learning - the squared exponential, also known as the Gaussian or radial basis function. These functions create perfect bell curves that smoothly decay in all directions from their centers.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The mathematical form reveals their graceful symmetry:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = e^-(x - i)^2\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"These functions have remarkable properties - they're infinitely differentiable and create ultra-smooth interpolations between points. Their rapid decay also provides natural localization, making them excellent choices for capturing both local and global patterns in data.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"sqrt exps\",\n    datasets = datasets,\n    ϕs       = [ (x) -> exp(-(x - i) ^ 2) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Sigmoid-Functions:-The-Neural-Network's-Activation","page":"Feature Functions In Bayesian Regression","title":"Sigmoid Functions: The Neural Network's Activation","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The sigmoid function, a cornerstone of neural network architectures, offers another fascinating basis for our exploration. This S-shaped curve elegantly transitions between two asymptotic values, creating a smooth, differentiable \"step\" that's invaluable in modeling transitions and decision boundaries.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The mathematical elegance of the sigmoid reveals itself in its formula:","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"beginalign\nphi_i(x) = frac11 + e^-3(x - 1)\nendalign","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"This function's graceful transition from 0 to 1 makes it particularly well-suited for capturing threshold phenomena and modeling probability-like quantities. Its bounded nature also provides natural regularization, preventing the explosive growth that can plague polynomial bases.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"plot_inference_results_for(\n    title    = \"sigmoids\",\n    datasets = datasets,\n    ϕs       = [ (x) -> 1 / (1 + exp(-3 * (x - i))) for i in -8:8 ], \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#The-Power-of-Combination:-Using-Different-Classes-of-Basis-Functions-Together","page":"Feature Functions In Bayesian Regression","title":"The Power of Combination: Using Different Classes of Basis Functions Together","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"What if we could harness the unique strengths of different basis functions we've explored? By combining polynomials, trigonometric functions, squared exponentials, and sigmoids, we can create an incredibly flexible and expressive basis that captures both global trends and local patterns. The polynomials can handle overall growth patterns, trigonometric functions can capture periodic behavior, squared exponentials can provide smooth local interpolation, and sigmoids can model sharp transitions. This combined approach leverages the best of each basis function family, potentially leading to more robust and accurate predictions. And here how easy it is to do so!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"# Combine all basis functions we've explored into one powerful basis\ncombined_basis = vcat(\n    # Polynomials (from first example)\n    [ (x) -> x ^ i for i in 0:5 ],\n    \n    # Trigonometric functions (from second example) \n    [ (x) -> sin(i*x) for i in 1:3 ],\n    [ (x) -> cos(i*x) for i in 1:3 ],\n    \n    # Squared exponentials (from seventh example)\n    [ (x) -> exp(-(x - i)^2) for i in -8:8 ],\n    \n    # Sigmoids (from eighth example)\n    [ (x) -> 1 / (1 + exp(-3 * (x - i))) for i in -8:8 ]\n)\n\nplot_inference_results_for(\n    title    = \"combined\",\n    datasets = datasets,\n    ϕs       = combined_basis, \n)","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Now that we've combined these different basis functions, it's interesting to explore how this powerful ensemble performs on our complete dataset. By visualizing the posterior distribution over functions induced by this combined basis, we can see how it leverages the unique characteristics of each basis type - the global trends captured by polynomials, the periodic patterns from trigonometric functions, the local smoothness from squared exponentials, and the sharp transitions enabled by sigmoids. Let's plot the results to see this rich expressiveness in action.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"combined_basis_ωs_all_data = infer_ω(ϕs = combined_basis, x = X, y = Y)\n\n# Left plot - local region\np1 = plot(\n    title = \"Local region\",\n    xlabel = \"x\",\n    ylabel = \"y\",\n    xlim = (-10, 10),\n    ylim = (-20, 20),\n    grid = true\n)\n\n# Plot posterior mean\nplot_ϕ!(p1, combined_basis, mean(combined_basis_ωs_all_data),\n    rl = -10,\n    rr = 10,\n    linewidth = 3,\n    color     = :green,\n    labels    = \"Posterior mean\"\n)\n\n# Plot posterior samples (in gray)\nplot_ϕ!(p1, combined_basis, rand(combined_basis_ωs_all_data, 50),\n    rl = -10,\n    rr = 10,\n    linewidth = 1,\n    color     = :gray,\n    alpha     = 0.4,\n    labels    = nothing\n)\n\n# Plot data points\nscatter!(p1, X, Y,\n    yerror     = Λ,\n    label      = \"Data\",\n    color      = :royalblue,\n    markersize = 4\n)\n\n# Right plot - bigger region\np2 = plot(\n    title = \"Extended region\",\n    xlabel = \"x\",\n    ylabel = \"y\",\n    xlim = (-30, 30),\n    ylim = (-75, 75),\n    grid = true\n)\n\n# Plot posterior mean\nplot_ϕ!(p2, combined_basis, mean(combined_basis_ωs_all_data),\n    rl = -30,\n    rr = 30,\n    linewidth = 3,\n    color     = :green,\n    labels    = \"Posterior mean\"\n)\n\n# Plot posterior samples (in gray)\nplot_ϕ!(p2, combined_basis, rand(combined_basis_ωs_all_data, 50),\n    rl = -30,\n    rr = 30,\n    linewidth = 1,\n    color     = :gray,\n    alpha     = 0.4,\n    labels    = nothing\n)\n\n# Plot data points\nscatter!(p2, X, Y,\n    label      = \"Data\", \n    color      = :royalblue,\n    markersize = 2\n)\n\np = plot(p1, p2, layout=(1,2), size=(1000,400), fontfamily = \"Computer Modern\")","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"The plot above beautifully demonstrates the expressive power of combining multiple basis functions. The posterior mean (shown in green) captures both the global trend and local variations in the data with remarkable accuracy. The gray lines, representing samples from the posterior distribution, illustrate the model's uncertainty - tighter in regions with more data points and wider in sparse regions. This combined basis approach leverages the strengths of each basis type: polynomials handle the overall trend, trigonometric functions capture periodic components, and localized basis functions manage fine details. The result is a flexible and robust model that adapts well to the complex patterns in our dataset.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/#Performance:-The-Need-for-Speed!","page":"Feature Functions In Bayesian Regression","title":"Performance: The Need for Speed! 🏎️","text":"","category":"section"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Alright, we've been having a blast playing with different basis functions, and RxInfer has been crunching those posterior calculations faster than you can say \"Bayesian inference\". But just how zippy is it really? Let's put our mathematical hot rod through its paces with our trusty polynomial basis functions and see what kind of speed records we can break! 🏁","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"using BenchmarkTools","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"In Julia, benchmarking is made easy with the BenchmarkTools package. The @benchmark macro runs the given expression multiple times to get statistically meaningful results. It provides detailed statistics about execution time, memory allocations, and garbage collection overhead. The output shows minimum, maximum, median and mean execution times, along with a nice histogram visualization of the timing distribution.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"@benchmark infer_ω(ϕs = $([ (x) -> x ^ i for i in 0:5 ]), x = $(datasets[1][:x_train]), y = $(datasets[1][:y_train]))","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n Range (min … max):  185.386 μs …  1.102 ms  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     222.271 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   228.912 μs ± 30.132 μs  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n      ▁       ▃▆███▇▅▁                                          \n  ▁▂▆██▆▅▄▃▄▅██████████▆▅▆▅▅▅▄▄▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁ ▃\n  185 μs          Histogram: frequency by time          317 μs <\n\n Memory estimate: 86.23 KiB, allocs estimate: 1499.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Let's benchmark inference on a larger dataset with 10,000 datapoints to test scalability.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"N_benchmark = 10_000\nX_benchmark = range(-8, 8, length=N_benchmark)\nY_benchmark = rand(rng, MvNormalMeanCovariance(f.(X_benchmark), Λ));\n\n@benchmark infer_ω(ϕs = $([ (x) -> x ^ i for i in 0:5 ]), x = $(X_benchmark), y = $(Y_benchmark))","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"BenchmarkTools.Trial: 9 samples with 1 evaluation per sample.\n Range (min … max):  609.232 ms … 627.891 ms  ┊ GC (min … max): 0.00% … 0.0\n0%\n Time  (median):     615.494 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   616.916 ms ±   6.692 ms  ┊ GC (mean ± σ):  0.00% ± 0.0\n0%\n\n  ▁▁        ▁      ▁  █                     ▁        ▁        ▁  \n  ██▁▁▁▁▁▁▁▁█▁▁▁▁▁▁█▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁█ ▁\n  609 ms           Histogram: frequency by time          628 ms <\n\n Memory estimate: 1.15 MiB, allocs estimate: 1503.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"And that's a wrap! From exploring different basis functions (polynomials, trigonometric functions, and even those fancy sigmoids) to performing lightning-fast Bayesian inference, we've seen how RxInfer handles parametric Gaussian regression with style. The benchmarks don't lie - processing 10,000 datapoints in a blast while keeping memory usage lean? That's not just fast, that's \"blink and you'll miss it\" fast! ","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Throughout this notebook, we've gone from basic data generation to sophisticated model inference, all while keeping things both mathematically rigorous and computationally efficient. Whether you're a Bayesian enthusiast or just someone who appreciates elegant mathematical machinery, this journey through parametric Gaussian regression shows that probabilistic programming doesn't have to be slow or memory-hungry.","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Thanks again to the authors of \"Probabilistic Numerics: Computation as Machine Learning\" for providing the theoretical foundations and inspiration for this notebook!","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/feature_functions_in_bayesian_regression/","page":"Feature Functions In Bayesian Regression","title":"Feature Functions In Bayesian Regression","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/feature_functions_in_bayesian_regression/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [a93c6f00] DataFrames v1.7.0\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Chance-Constrained-Active-Inference","page":"Chance Constraints","title":"Chance-Constrained Active Inference","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"This notebook applies reactive message passing for active inference in the context of chance-constraints. The implementation is based on (van de Laar et al., 2021, \"Chance-constrained active inference\") and discussion with John Boik.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"We consider a 1-D agent that tries to elevate itself above ground level. Instead of a goal prior, we impose a chance constraint on future states, such that the agent prefers to avoid the ground with a preset probability (chance) level. ","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"using Plots, Distributions, StatsFuns, RxInfer","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Chance-Constraint-Node-Definition","page":"Chance Constraints","title":"Chance-Constraint Node Definition","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"A chance-constraint is meant to constraint a marginal distribution to abide by certain properties. In this case, a (posterior) probability distribution should not \"overflow\" a given region by more than a certain probability mass. This constraint then affects adjacent beliefs and ultimately the controls to (hopefully) account for the imposed constraint.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"In order to enforce this constraint on a marginal distribution, an auxiliary chance-constraint node is included in the graphical model. This node then sends messages that enforce the marginal to abide by the preset conditions. In other words, the (chance) constraint on the (posterior) marginal, is converted to a prior constraint on the generative model that sends an adaptive message. We start by defining this chance-constraint node and its message.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"struct ChanceConstraint end  \n\n# Node definition with safe region limits (lo, hi), overflow chance epsilon and tolerance atol\n@node ChanceConstraint Stochastic [out, lo, hi, epsilon, atol]","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"# Function to compute normalizing constant and central moments of a truncated Gaussian distribution\nfunction truncatedGaussianMoments(m::Float64, V::Float64, a::Float64, b::Float64)\n    V = clamp(V, tiny, huge)\n    StdG = Distributions.Normal(m, sqrt(V))\n    TrG = Distributions.Truncated(StdG, a, b)\n    \n    Z = Distributions.cdf(StdG, b) - Distributions.cdf(StdG, a)  # safe mass for standard Gaussian\n    \n    if Z < tiny\n        # Invalid region; return undefined mean and variance of truncated distribution\n        Z    = 0.0\n        m_tr = 0.0\n        V_tr = 0.0\n    else\n        m_tr = Distributions.mean(TrG)\n        V_tr = Distributions.var(TrG)\n    end\n    \n    return (Z, m_tr, V_tr)\nend;","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"@rule ChanceConstraint(:out, Marginalisation) (\n    m_out::UnivariateNormalDistributionsFamily, # Require inbound message\n    q_lo::PointMass, \n    q_hi::PointMass, \n    q_epsilon::PointMass, \n    q_atol::PointMass) = begin \n\n    # Extract parameters\n    lo = mean(q_lo)\n    hi = mean(q_hi)\n    epsilon = mean(q_epsilon)\n    atol = mean(q_atol)\n    \n    (m_bw, V_bw) = mean_var(m_out)\n    (xi_bw, W_bw) = (m_bw, 1. /V_bw)  # check division by  zero\n    (m_tilde, V_tilde) = (m_bw, V_bw)\n    \n    # Compute statistics (and normalizing constant) of q in safe region G\n    # Phi_G is called the \"safe mass\" \n    (Phi_G, m_G, V_G) = truncatedGaussianMoments(m_bw, V_bw, lo, hi)\n\n    xi_fw = xi_bw\n    W_fw  = W_bw\n    if epsilon <= 1.0 - Phi_G # If constraint is active\n        # Initialize statistics of uncorrected belief\n        m_tilde = m_bw\n        V_tilde = V_bw\n        for i = 1:100 # Iterate at most this many times\n            (Phi_lG, m_lG, V_lG) = truncatedGaussianMoments(m_tilde, V_tilde, -Inf, lo) # Statistics for q in region left of G\n            (Phi_rG, m_rG, V_rG) = truncatedGaussianMoments(m_tilde, V_tilde, hi, Inf) # Statistics for q in region right of G\n\n            # Compute moments of non-G region as a mixture of left and right truncations\n            Phi_nG = Phi_lG + Phi_rG\n            m_nG = Phi_lG / Phi_nG * m_lG + Phi_rG / Phi_nG * m_rG\n            V_nG = Phi_lG / Phi_nG * (V_lG + m_lG^2) + Phi_rG/Phi_nG * (V_rG + m_rG^2) - m_nG^2\n\n            # Compute moments of corrected belief as a mixture of G and non-G regions\n            m_tilde = (1.0 - epsilon) * m_G + epsilon * m_nG\n            V_tilde = (1.0 - epsilon) * (V_G + m_G^2) + epsilon * (V_nG + m_nG^2) - m_tilde^2\n            # Re-compute statistics (and normalizing constant) of corrected belief\n            (Phi_G, m_G, V_G) = truncatedGaussianMoments(m_tilde, V_tilde, lo, hi)\n            if (1.0 - Phi_G) < (1.0 + atol)*epsilon\n                break # Break the loop if the belief is sufficiently corrected\n            end\n        end\n        \n        # Convert moments of corrected belief to canonical form\n        W_tilde = inv(V_tilde)\n        xi_tilde = W_tilde * m_tilde\n\n        # Compute canonical parameters of forward message\n        xi_fw = xi_tilde - xi_bw\n        W_fw  = W_tilde - W_bw\n    end\n\n    return NormalWeightedMeanPrecision(xi_fw, W_fw)\nend","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Definition-of-the-Environment","page":"Chance Constraints","title":"Definition of the Environment","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"We consider an environment where the agent has an elevation level, and where the agent directly controls its vertical velocity. After some time, an unexpected and sudden gust of wind tries to push the agent to the ground.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"wind(t::Int64) = -0.1*(60 <= t < 100) # Time-dependent wind profile\n\nfunction initializeWorld()\n    x_0 = 0.0 # Initial elevation\n    \n    x_t_last = x_0\n    function execute(t::Int64, a_t::Float64)\n        x_t = x_t_last + a_t + wind(t) # Update elevation\n    \n        x_t_last = x_t # Reset state\n                \n        return x_t\n    end\n\n    x_t = x_0 # Predefine outcome variable\n    observe() = x_t # State is fully observed\n\n    return (execute, observe)\nend;","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Generative-Model-for-Regulator","page":"Chance Constraints","title":"Generative Model for Regulator","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"We consider a fully observed Markov decision process, where the agent directly observes the true state (elevation) of the world. In this case we only need to define a chance-constrained generative model of future states. Inference for controls on this model then derives our controller.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"# m_u ::Vector{Float64}, ,   Control prior means\n# v_u = datavar(Float64, T)  Control prior variances\n# x_t ::Float64              Fully observed state\n\n@model function regulator_model(T, m_u, v_u, x_t, lo, hi, epsilon, atol)\n    \n    # Loop over horizon\n    x_k_last = x_t\n    for k = 1:T\n        u[k] ~ NormalMeanVariance(m_u[k], v_u[k]) # Control prior\n        x[k] ~ x_k_last + u[k] # Transition model\n        x[k] ~ ChanceConstraint(lo, hi, epsilon, atol) where { # Simultaneous constraint on state\n            dependencies = RequireMessageFunctionalDependencies(out = NormalWeightedMeanPrecision(0, 0.01))} # Predefine inbound message to break circular dependency\n        x_k_last = x[k]\n    end\n \nend","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Reactive-Agent-Definition","page":"Chance Constraints","title":"Reactive Agent Definition","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"function initializeAgent()\n    # Set control prior statistics\n    m_u = zeros(T)\n    v_u = lambda^(-1)*ones(T)\n    \n    function compute(x_t::Float64)\n        model_t = regulator_model(;T=T, lo=lo, hi=hi, epsilon=epsilon, atol=atol)\n        data_t = (m_u = m_u, v_u = v_u, x_t = x_t)\n\n        result = infer(\n            model = model_t,\n            data = data_t,\n            iterations = n_its)\n\n        # Extract policy from inference results\n        pol = mode.(result.posteriors[:u][end])\n\n        return pol\n    end\n\n    pol = zeros(T) # Predefine policy variable\n    act() = pol[1]\n\n    return (compute, act)\nend;","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Action-Perception-Cycle","page":"Chance Constraints","title":"Action-Perception Cycle","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"Next we define and execute the action-perception cycle. Because the state is fully observed, these is no slide (estimator) step in the cycle. ","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"# Simulation parameters\nN = 160 # Total simulation time\nT = 1 # Lookahead time horizon\nlambda = 1.0 # Control prior precision\nlo = 1.0 # Chance region lower bound\nhi = Inf # Chance region upper bound\nepsilon = 0.01 # Allowed chance violation\natol = 0.01 # Convergence tolerance for chance constraints\nn_its = 10;  # Number of inference iterations","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"(execute, observe) = initializeWorld() # Let there be a world\n(compute, act) = initializeAgent() # Let there be an agent\n\na = Vector{Float64}(undef, N) # Actions\nx = Vector{Float64}(undef, N) # States\nfor t = 1:N\n    a[t] = act()\n           execute(t, a[t])\n    x[t] = observe()\n           compute(x[t])\nend","category":"page"},{"location":"categories/advanced_examples/chance_constraints/#Results","page":"Chance Constraints","title":"Results","text":"","category":"section"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"Results show that the agent does not allow the wind to push it all the way to the ground.","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"p1 = plot(1:N, wind.(1:N), color=\"blue\", label=\"Wind\", ylabel=\"Velocity\", lw=2)\nplot!(p1, 1:N, a, color=\"red\", label=\"Control\", lw=2)\np2 = plot(1:N, x, color=\"black\", lw=2, label=\"Agent\", ylabel=\"Elevation\")\nplot(p1, p2, layout=(2,1))","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/chance_constraints/","page":"Chance Constraints","title":"Chance Constraints","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/chance_constraints/Project.toml`\n  [31c24e10] Distributions v0.25.117\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [4c63d2b9] StatsFuns v1.3.2\n","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/#How-to-train-your-Hidden-Markov-Model","page":"Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"","category":"section"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"In this example, we'll be tracking a Roomba as it moves throughout a 3-bedroom apartment consisting of a bathroom, a master bedroom, and a living room. It's important to keep track of your AI's, so we want to make sure we can keep tabs on it whenever we leave the apartment. ","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"First, in order to track the Roomba's movements using RxInfer, we need to come up with a model. Since we have a discrete set of rooms in the apartment, we can use a categorical distribution to represent the Roomba's position. There are three  rooms in the apartment, meaning we need three states in our categorical distribution. At time t, let's call the estimate of the Roomba's position s_t.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"However, we also know that some rooms are more accessible than others, meaning the Roomba is more likely to move between these rooms - for example, it's rare to have a door directly between the bathroom and the master bedroom. We can encode this information using a transition matrix, which we will call A.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Our Roomba is equipped with a small camera that tracks the surface it is moving over. We will use this camera to obtain our observations since we know that there is a carpet in the living room, tiles in the bathroom, and hardwood floors in the master bedroom. However, this method is not foolproof, and sometimes the Roomba will make mistakes and mistake the hardwood floor for tiles or the carpet for hardwood. Don't be too hard on the little guy, it's just a Roomba after all.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"At time t, we will call our observations x_t and encode the mapping from the Roomba's position to the observations in a matrix we call B. B also encodes the likelihood that the Roomba will make a mistake and get the wrong observation. This leaves us with the following model specification:","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"beginaligned\n    s_t  sim mathcalCat(A s_t-1)\n    x_t  sim mathcalCat(B s_t)\nendaligned","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"This type of discrete state space model is known as a Hidden Markov Model or HMM for short. Our goal is to learn the matrices A and B so we can use them to track the whereabouts of our little cleaning agent.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"using RxInfer, Random, BenchmarkTools, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"In order to generate data to mimic the observations of the Roomba, we need to specify two things: the actual transition probabilities between the states (i.e., how likely is the Roomba to move from one room to another), and the observation distribution (i.e., what type of texture will the Roomba encounter in each room). We can then use these specifications to generate observations from our hidden Markov model (HMM).","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"To generate our observation data, we'll follow these steps:","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Assume an initial state for the Roomba. For example, we can start the Roomba in the bedroom.\nDetermine where the Roomba went next by drawing from a Categorical distribution with the transition probabilities between the different rooms.\nDetermine the observation encountered in this room by drawing from a Categorical distribution with the corresponding observation probabilities.\nRepeat steps 2-3 for as many samples as we want.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"The following code implements this process and generates our observation data:","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"\"\"\"\n    rand_vec(rng, distribution::Categorical)\n\nThis function returns a one-hot encoding of a random sample from a categorical distribution. The sample is drawn with the `rng` random number generator.\n\"\"\"\nfunction rand_vec(rng, distribution::Categorical) \n    k = ncategories(distribution)\n    s = zeros(k)\n    drawn_category = rand(rng, distribution)\n    s[drawn_category] = 1.0\n    return s\nend\n\nfunction generate_data(n_samples; seed = 42)\n    \n    rng = MersenneTwister(seed)\n    \n    # Transition probabilities \n    state_transition_matrix = [0.9 0.0 0.1;\n                                                        0.0 0.9 0.1; \n                                                        0.05 0.05 0.9] \n    # Observation noise\n    observation_distribution_matrix = [0.9 0.05 0.05;\n                                                                         0.05 0.9 0.05;\n                                                                         0.05 0.05 0.9] \n    # Initial state\n    s_initial = [1.0, 0.0, 0.0] \n    \n    states = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the states\n    observations = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the observations\n    \n    s_prev = s_initial\n    \n    for t = 1:n_samples\n        s_probvec = state_transition_matrix * s_prev\n        states[t] = rand_vec(rng, Categorical(s_probvec ./ sum(s_probvec)))\n        obs_probvec = observation_distribution_matrix * states[t]\n        observations[t] = rand_vec(rng, Categorical(obs_probvec ./ sum(obs_probvec)))\n        s_prev = states[t]\n    end\n    \n    return observations, states\nend","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"We will generate 100 data points to simulate 100 ticks of the Roomba moving through the apartment. x_data will contain the Roomba's measurements of the floor it's currently on, and s_data will contain information on the room the Roomba was actually in.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"# Test data\nN = 100\nx_data, s_data = generate_data(N);\n\nscatter(argmax.(s_data), leg=false, xlabel=\"Time\",yticks= ([1,2,3],[\"Bedroom\",\"Living room\",\"Bathroom\"]))","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Now it is time to build our model. As mentioned earlier, we will use Categorical distributions for the states and observations. To learn the A and B matrices we can use DirichletCollection priors. For the A-matrix, since we have no apriori idea how the roomba is actually going to move we will assume that it moves randomly. We can represent this by filling our DirichletCollection prior on A with ones. Remember that this will get updated once we start learning, so it's fine if our initial guess is not quite accurate. As for the observations, we have good reason to trust our Roomba's measurements. To represent this, we will add large values to the diagonal of our prior on B. However, we also acknowledge that the Roomba is not infallible, so we will add some noise on the off-diagonal entries.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Since we will use Variational Inference, we also have to specify inference constraints. We will use a structured variational approximation to the true posterior distribution, where we decouple the variational posterior over the states (q(s_0, s)) from the posteriors over the transition matrices (q(A) and q(B)). This dependency decoupling in the approximate posterior distribution ensures that inference is tractable. Let's build the model!","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"# Model specification\n@model function hidden_markov_model(x)\n    \n    A ~ DirichletCollection(ones(3,3))\n    B ~ DirichletCollection([ 10.0 1.0 1.0; \n                                            1.0 10.0 1.0; \n                                            1.0 1.0 10.0 ])\n    \n    s_0 ~ Categorical(fill(1.0 / 3.0, 3))\n    \n    s_prev = s_0\n    \n    for t in eachindex(x)\n        s[t] ~ DiscreteTransition(s_prev, A) \n        x[t] ~ DiscreteTransition(s[t], B)\n        s_prev = s[t]\n    end\n    \nend\n\n# Constraints specification\n@constraints function hidden_markov_model_constraints()\n    q(s_0, s, A, B) = q(s_0, s)q(A)q(B)\nend","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"hidden_markov_model_constraints (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Now it's time to perform inference and find out where the Roomba went in our absence. Did it remember to clean the bathroom?","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"We'll be using Variational Inference to perform inference, which means we need to set some initial marginals as a starting point. RxInfer makes this easy with the vague function, which provides an uninformative guess. If you have better ideas, you can try a different initial guess and see what happens.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Since we're only interested in the final result - the best guess about the Roomba's position - we'll only keep the last results. Let's start the inference process!","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"imarginals = @initialization begin\n    q(A) = vague(DirichletCollection, (3, 3))\n    q(B) = vague(DirichletCollection, (3, 3)) \n    q(s) = vague(Categorical, 3)\nend\n\nireturnvars = (\n    A = KeepLast(),\n    B = KeepLast(),\n    s = KeepLast()\n)\n\nresult = infer(\n    model         = hidden_markov_model(), \n    data          = (x = x_data,),\n    constraints   = hidden_markov_model_constraints(),\n    initialization = imarginals, \n    returnvars    = ireturnvars, \n    iterations    = 20, \n    free_energy   = true\n);","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"That was fast! Let's take a look at our results. If we're successful, we should have a good idea about the actual layout of the apartment (a good posterior marginal over A) and about the uncertainty in the roombas observations (A good posterior over B). Let's see if it worked","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"println(\"Posterior Marginal for A:\")\nmean(result.posteriors[:A])","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Posterior Marginal for A:\n3×3 Matrix{Float64}:\n 0.920411   0.0198239  0.0543899\n 0.0268058  0.959672   0.108366\n 0.0527835  0.0205038  0.837244","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"println(\"Posterior Marginal for B:\")\nmean(result.posteriors[:B])","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Posterior Marginal for B:\n3×3 Matrix{Float64}:\n 0.934529   0.0482411  0.0365341\n 0.0433901  0.887064   0.036787\n 0.0220808  0.0646948  0.926679","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Finally, we can check if we were successful in keeping tabs on our Roomba's whereabouts. We can also check if our model has converged by looking at the Free Energy. ","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"p1 = scatter(argmax.(s_data), \n                        title=\"Inference results\", \n                        label = \"Real\", \n                        ms = 6, \n                        legend=:right,\n                        xlabel=\"Time\" ,\n                        yticks= ([1,2,3],[\"Bedroom\",\"Living room\",\"Bathroom\"]),\n                        size=(900,550)\n                        )\n\np1 = scatter!(p1, argmax.(ReactiveMP.probvec.(result.posteriors[:s])),\n                        label = \"Inferred\",\n                        ms = 3\n                        )\n\np2 = plot(result.free_energy, \n                    label=\"Free energy\",\n                    xlabel=\"Iteration Number\"\n                    )\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Neat! Now you know how to track a Roomba if you ever need to. You also learned how to fit a Hidden Markov Model using RxInfer in the process.","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/hidden_markov_model/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [31c24e10] Distributions v0.25.117\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/#Nonlinear-Sensor-Fusion","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"section"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"using RxInfer, Random, LinearAlgebra, Distributions, Plots, StatsPlots, Optimisers\nusing DataFrames, DelimitedFiles, StableRNGs","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In a secret ongoing mission to Mars, NASA has deployed its custom lunar roving vehicle, called WALL-E, to explore the area and to discover hidden minerals. During one of the solar storm, WALL-E's GPS unit got damaged, preventing it from accurately locating itself. The engineers at NASA were devastated as they developed the project over the past couple of years and spend most of their funding on it. Without being able to locate WALL-E, they were unable to complete their mission.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"A smart group of engineers came up with a solution to locate WALL-E. They decided to repurpose 3 nearby satelites as beacons for WALL-E, allowing it to detect its relative location to these beacons. However, these satelites were old and therefore WALL-E was only able to obtain noisy estimates of its distance to these beacons. These distances were communicated back to earth, where the engineers tried to figure our WALL-E's location. Luckily they knew the locations of these satelites and together with the noisy estimates of the distance to WALL-E they can infer the exact location of the moving WALL-E.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"To illustrate these noisy measurements, the engineers decided to plot them:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# fetch measurements\nbeacon_locations = readdlm(\"data/beacons.txt\")\ndistances = readdlm(\"data/distances.txt\")\nposition = readdlm(\"data/position.txt\")\nnr_observations = size(distances, 1);","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual location of WALL-E\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\")\n\n# plot noisy distance measurements\np2 = plot(distances, legend=:topleft, linewidth=3, label=[\"distance to beacon 1\" \"distance to beacon 2\" \"distance to beacon 3\"])\nxlabel!(\"time [sec]\"), ylabel!(\"distance [m]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In order to track the location of WALL-E based on the noisy distance measurements to the beacon, the engineers developed a probabilistic model for the movements for WALL-E and the distance measurements that followed from this. The engineers assumed that the position of WALL-E at time t, denoted by z_t, follows a 2-dimensional normal random walk:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(z_t mid z_t - 1) = mathcalN(z_t mid z_t-1mathrmI_2)\nendaligned","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"where mathrmI_2 denotes the 2-dimensional identity matrix. From the current position of WALL-E, we specify our noisy distance measurements y_t as a noisy set of the distances between WALL-E and the beacons, specified by s_i:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(y_t mid z_t)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixmathrmI_3 right  right)\nendaligned","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers are smart enough to automate the probabilistic inference procedure using RxInfer.jl. They specify the probabilistic model as:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# function to compute distance to beacons\nfunction compute_distances(z)    \n    distance1 = norm(z - beacon_locations[1,:])\n    distance2 = norm(z - beacon_locations[2,:])\n    distance3 = norm(z - beacon_locations[3,:])\n    distances = [distance1, distance2, distance3]\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model(y, W, R)\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:length(y)\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], W)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Due to non-linearity, exact probabilistic inference is intractable in this model. Therefore we resort to Conjugate-Computational Variational Inference (CVI) following the paper Probabilistic programming with stochastic variational message passing. This requires setting the @meta macro in RxInfer.jl.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Please note that we permit improper messages within the CVI procedure in this example by providing Val(false) to CVI constructor:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"    compute_distances(z) -> CVI(..., Val(false), ...)","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"This move may lead to numerical instabilities in other scenarios, however dissallowing improper messages in this case can lead to a biased estimates of posterior distribution. So, as a rule of thumb, you should try the default setting, and if it fails to find an unbiased result, enable improper messages.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_model_meta(nr_samples, nr_iterations, rng)\n    compute_distances(z) -> CVI(rng, nr_samples, nr_iterations, Optimisers.Descent(0.1), ForwardDiffGrad(), 1, Val(false), false)\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"NOTE: You can try out different meta for approximating the nonlinearity, e.g.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_linear_meta()\n    compute_distances(z) -> Linearization()\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_unscented_meta()\n    compute_distances(z) -> Unscented()\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"init = @initialization begin \n    μ(z) = MvNormalMeanPrecision(ones(2), 0.1 * diageye(2))\nend","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Initial state: \n  μ(z) = MvNormalMeanPrecision(\nμ: [1.0, 1.0]\nΛ: [0.1 0.0; 0.0 0.1]\n)","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_fast = infer(\n    model = random_walk_model(W = diageye(2), R = diageye(3)),\n    meta = random_walk_model_meta(1, 3, StableRNG(42)), # or random_walk_unscented_meta()\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initialization = init,\n);","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_accuracy = infer(\n    model = random_walk_model(W = diageye(2), R = diageye(3)),\n    meta = random_walk_model_meta(1000, 100, StableRNG(42)),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initialization = init,\n);","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"After running this fast inference procedure, the engineers plot the results and evaluate the performance:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_fast.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Fast (1 sample, 3 iterations)\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot beacon and actual and estimated location of WALL-E (accurate inference)\np2 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_accuracy.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Accurate (1000 samples, 100 iterations)\"); p2.series_list[end][:label] = \"estimated location ±2σ\"\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers were very happy with the solution, as it meant that the Mars mission could continue. However, they noted that the estimates began to deviate after WALL-E moved further away from the beacons. They deemed this was likely due to the noise in the distance measurements. Therefore, the engineers decided to adapt the model, such that they would also infer the process and observation noise precision matrices, Q and R respectively. They did this by adding Wishart priors to those matrices:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(Q) = mathcalW(Q mid 3 mathrmI_2) \n  p(R) = mathcalW(R mid 4 mathrmI_3) \n  p(z_t mid z_t - 1 Q) = mathcalN(z_t mid z_t-1 Q^-1)\n  p(y_t mid z_t R)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixR^-1 right  right)\nendaligned","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model_wishart(y)\n    # set priors on precision matrices\n    Q ~ Wishart(3, diageye(2))\n    R ~ Wishart(4, diageye(3))\n\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:length(y)\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], Q)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"meta = @meta begin \n    compute_distances(z) -> CVI(StableRNG(42), 1000, 100, Optimisers.Descent(0.01), ForwardDiffGrad(), 1, Val(false), false)\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Because of the added complexity with the Wishart distributions, the engineers simplify the problem by employing a structured mean-field factorization:","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"constraints = @constraints begin\n    q(z, Q, R) = q(z)q(Q)q(R)\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"init = @initialization begin \n    μ(z) = MvNormalMeanPrecision(zeros(2), 0.01 * diageye(2))\n    q(R) = Wishart(4, diageye(3))\n    q(Q) = Wishart(3, diageye(2))\nend;","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers run the inference procedure again and decide to track the inference performance using the Bethe free energy.","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_wishart = infer(\n    model = random_walk_model_wishart(),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = constraints,\n    meta = meta,\n    initialization = init,\n);","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"They plot the new estimates and the performance over time, and luckily WALL-E is found!","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_wishart.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot bethe free energy performance\np2 = plot(results_wishart.free_energy[2:end], label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/nonlinear_sensor_fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/nonlinear_sensor_fusion/Project.toml`\n  [a93c6f00] DataFrames v1.7.0\n  [8bb1440f] DelimitedFiles v1.9.1\n  [31c24e10] Distributions v0.25.117\n⌃ [3bd65402] Optimisers v0.3.4\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [f3b207a7] StatsPlots v0.15.7\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\nInfo Packages marked with ⌃ have new versions available and may be upgradable.\n","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Kalman-filtering-and-smoothing","page":"Kalman Filtering And Smoothing","title":"Kalman filtering and smoothing","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"In the following set of examples the goal is to estimate hidden states of a Dynamical process where all hidden states are Gaussians.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"We start our journey with a simple multivariate Linear Gaussian State Space Model (LGSSM), which can be solved analytically.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"We then solve an identification problem which does not have an analytical solution.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Utimately, we show how RxInfer.jl can deal with missing observations.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Gaussian-Linear-Dynamical-System","page":"Kalman Filtering And Smoothing","title":"Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"LGSSM can be described with the following equations:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"beginaligned\n p(x_ix_i - 1)  = mathcalN(x_iA * x_i - 1 mathcalP)\n p(y_ix_i)  = mathcalN(y_iB * x_i mathcalQ)\nendaligned","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"where x_i are hidden states, y_i are noisy observations, A, B are state transition and observational matrices, mathcalP and mathcalQ are state transition noise and observation noise covariance matrices. For a more rigorous introduction to Linear Gaussian Dynamical systems we refer to Simo Sarkka, Bayesian Filtering and Smoothing book.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"function generate_data(rng, A, B, Q, P)\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(B * x[i], P))\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"# Seed for reproducibility\nseed = 1234\n\nrng = MersenneTwister(1234)\n\n# We will model 2-dimensional observations with rotation matrix `A`\n# To avoid clutter we also assume that matrices `A`, `B`, `P` and `Q`\n# are known and fixed for all time-steps\nθ = π / 35\nA = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\nB = diageye(2)\nQ = diageye(2)\nP = 25.0 .* diageye(2)\n\n# Number of observations\nn = 300;","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"x, y = generate_data(rng, A, B, Q, P);","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations, which are represented as dots.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = scatter!(px, getindex.(y, 1), label = false, markersize = 2, color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\npx = scatter!(px, getindex.(y, 2), label = false, markersize = 2, color = :green)\n\nplot(px)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@model function rotate_ssm(y, x0, A, B, Q, P)\n    x_prior ~ x0\n    x_prev = x_prior\n    \n    for i in 1:length(y)\n        x[i] ~ MvNormalMeanCovariance(A * x_prev, Q)\n        y[i] ~ MvNormalMeanCovariance(B * x[i], P)\n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"To run inference we also specify prior for out first hidden state:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"x0 = MvNormalMeanCovariance(zeros(2), 100.0 * diageye(2));","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"# For large number of observations you need to use limit_stack_depth = 100 option during model creation, e.g. \n# infer(..., options = (limit_stack_depth = 500, ))`\nresult = infer(\n    model = rotate_ssm(x0=x0, A=A, B=B, Q=Q, P=P), \n    data = (y = y,),\n    free_energy = true\n);\n\nxmarginals  = result.posteriors[:x]\nlogevidence = -result.free_energy; # given the analytical solution, free energy will be equal to the negative log evidence","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\n\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :teal)\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :violet)\n\nplot(px)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the value for minus log evidence:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"logevidence","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"1-element Vector{Float64}:\n -1891.6471934594765","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#System-Identification-Problem","page":"Kalman Filtering And Smoothing","title":"System Identification Problem","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"In this example we are going to attempt to run Bayesian inference and decouple two random-walk signals, which were combined into a single single through some deterministic function f. We do not have access to the real values of these signals, but only to their combination. First, we create the generate_data function that accepts f as an argument:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"using RxInfer, Distributions, StableRNGs, Plots","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"function generate_data(f, n; seed = 123, x_i_min = -20.0, w_i_min = 20.0, noise = 20.0, real_x_τ = 0.1, real_w_τ = 1.0)\n\n    rng = StableRNG(seed)\n\n    real_x = Vector{Float64}(undef, n)\n    real_w = Vector{Float64}(undef, n)\n    real_y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        real_x[i] = rand(rng, Normal(x_i_min, sqrt(1.0 / real_x_τ)))\n        real_w[i] = rand(rng, Normal(w_i_min, sqrt(1.0 / real_w_τ)))\n        real_y[i] = rand(rng, Normal(f(real_x[i], real_w[i]), sqrt(noise)))\n\n        x_i_min = real_x[i]\n        w_i_min = real_w[i]\n    end\n    \n    return real_x, real_w, real_y\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"generate_data (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"The function returns the real signals real_x and  real_w for later comparison (we are not going to use them during inference) and their combined version real_y (we are going to use it as our observations during the inference). We also assume that real_y is corrupted with some measurement noise.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Combination-1:-y-x-w","page":"Kalman Filtering And Smoothing","title":"Combination 1: y = x + w","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"In our first example, we are going to use a simple addition (+) as the function f. In general, it is impossible to decouple the signals x and w without strong priors, but we can try and see how good an inference can be. The + operation on two random variables also has a special meaning in the probabilistic inference, namely the convolution of pdf's of the two random variables, and RxInfer treats it specially with many precomputed analytical rules, which may make the inference task easier. First, let us create a test dataset:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"n = 250\nreal_x, real_w, real_y = generate_data(+, n);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, real_x, label = \"x\")\npl = plot!(pl, real_w, label = \"w\")\n\npr = plot(title = \"Combined y = x + w\")\npr = scatter!(pr, real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"To run inference, we need to create a probabilistic model: our beliefs about how our data could have been generated. For this we can use the @model macro from RxInfer.jl:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@model function identification_problem(f, y, m_x_0, τ_x_0, a_x, b_x, m_w_0, τ_w_0, a_w, b_w, a_y, b_y)\n    \n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x_i_min = x0\n    w_i_min = w0\n\n    local x\n    local w\n    local s\n    \n    for i in 1:length(y)\n        x[i] ~ Normal(mean = x_i_min, precision = τ_x)\n        w[i] ~ Normal(mean = w_i_min, precision = τ_w)\n        s[i] := f(x[i], w[i])\n        y[i] ~ Normal(mean = s[i], precision = τ_y)\n        \n        x_i_min = x[i]\n        w_i_min = w[i]\n    end\n    \nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"RxInfer runs Bayesian inference as a variational optimisation procedure between the real solution and its variational proxy q. In our model specification we assumed noise components to be unknown, thus, we need to enforce a structured mean-field assumption for the variational family of distributions q. This inevitably reduces the accuracy of the result, but makes the task easier and allows for fast and analytical message passing-based variational inference:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"constraints = @constraints begin \n    q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y)\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Constraints: \n  q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"The next step is to assign priors, initialise needed messages and marginals and call the inference function:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"m_x_0, τ_x_0 = -20.0, 1.0\nm_w_0, τ_w_0 = 20.0, 1.0\n\n# We set relatively strong priors for random walk noise components\n# and sort of vague prior for the noise of the observations\na_x, b_x = 0.01, 0.01var(real_x)\na_w, b_w = 0.01, 0.01var(real_w)\na_y, b_y = 1.0, 1.0\n\n# We set relatively strong priors for messages\nxinit = map(r -> NormalMeanPrecision(r, τ_x_0), reverse(range(-60, -20, length = n)))\nwinit = map(r -> NormalMeanPrecision(r, τ_w_0), range(20, 60, length = n))\n\n\ninit = @initialization begin\n    μ(x) = xinit\n    μ(w) = winit\n    q(τ_x) = GammaShapeRate(a_x, b_x)\n    q(τ_w) = GammaShapeRate(a_w, b_w)\n    q(τ_y) = GammaShapeRate(a_y, b_y)\nend\n\nresult = infer(\n    model = identification_problem(f=+, m_x_0=m_x_0, τ_x_0=τ_x_0, a_x=a_x, b_x=b_x, m_w_0=m_w_0, τ_w_0=τ_w_0, a_w=a_w, b_w=b_w, a_y=a_y, b_y=b_y),\n    data  = (y = real_y,), \n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initialization = init,\n    iterations = 50\n)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Inference results:\n  Posteriors       | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Let's examine our inference results:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"τ_x_marginals = result.posteriors[:τ_x]\nτ_w_marginals = result.posteriors[:τ_w]\nτ_y_marginals = result.posteriors[:τ_y]\n\nsmarginals = result.posteriors[:s]\nxmarginals = result.posteriors[:x]\nwmarginals = result.posteriors[:w];","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(xmarginals[end]), ribbon = var.(xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(wmarginals[end]), ribbon = var.(wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(smarginals[end]), ribbon = std.(smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"The inference results are not so bad, even though RxInfer missed the correct values of the signals between 100 and 150.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Combination-2:-y-min(x,-w)","page":"Kalman Filtering And Smoothing","title":"Combination 2: y = min(x, w)","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"In this example we use a slightly more complex function, for which RxInfer does not have precomputed analytical message update rules. We are going to attempt to run Bayesian inference with min as a combination function. Note, however, that directly using min may cause problems for the built-in approximation methods as it has zero partial derviates with respect to all but one of the variables. We generate data with the min function directly however we model it with a somewhat smoothed version:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"# Smoothed version of `min` without zero-ed derivatives\nfunction smooth_min(x, y)    \n    if x < y\n        return x + 1e-4 * y\n    else\n        return y + 1e-4 * x\n    end\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"smooth_min (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"RxInfer supports arbitrary nonlinear functions, but it requires an explicit approximation method specification. That can be achieved with the built-in @meta macro:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"min_meta = @meta begin \n    # In this example we are going to use a simple `Linearization` method\n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Meta: \n  smooth_min() -> ReactiveMP.Linearization()","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"n = 200\nmin_real_x, min_real_w, min_real_y = generate_data(min, n, seed = 1, x_i_min = 0.0, w_i_min = 0.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, min_real_x, label = \"x\")\npl = plot!(pl, min_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, min_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"min_m_x_0, min_τ_x_0 = -1.0, 1.0\nmin_m_w_0, min_τ_w_0 = 1.0, 1.0\n\nmin_a_x, min_b_x = 1.0, 1.0\nmin_a_w, min_b_w = 1.0, 1.0\nmin_a_y, min_b_y = 1.0, 1.0\n\ninit = @initialization begin\n   μ(x) = NormalMeanPrecision(min_m_x_0, min_τ_x_0) \n   μ(w) = NormalMeanPrecision(min_m_w_0, min_τ_w_0)\n   q(τ_x) = GammaShapeRate(min_a_x, min_b_x) \n   q(τ_w) = GammaShapeRate(min_a_w, min_b_w)\n   q(τ_y) = GammaShapeRate(min_a_y, min_b_y)\nend\n\n\nmin_result = infer(\n    model = identification_problem(f=smooth_min, m_x_0=min_m_x_0, τ_x_0=min_τ_x_0, a_x=min_a_x, b_x=min_b_x, m_w_0=min_m_w_0, τ_w_0=min_τ_w_0, a_w=min_a_w, b_w=min_b_w, a_y=min_a_y, b_y=min_b_y),\n    data  = (y = min_real_y,), \n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initialization = init,\n    meta = min_meta,\n    iterations = 50\n)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Inference results:\n  Posteriors       | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"min_τ_x_marginals = min_result.posteriors[:τ_x]\nmin_τ_w_marginals = min_result.posteriors[:τ_w]\nmin_τ_y_marginals = min_result.posteriors[:τ_y]\n\nmin_smarginals = min_result.posteriors[:s]\nmin_xmarginals = min_result.posteriors[:x]\nmin_wmarginals = min_result.posteriors[:w]\n\npx1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, min_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(min_xmarginals[end]), ribbon = var.(min_xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, min_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(min_wmarginals[end]), ribbon = var.(min_wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, min_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(min_smarginals[end]), ribbon = std.(min_smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"As we can see inference with the min function is significantly harder. Even though the combined signal has been inferred with high precision the underlying x and w signals are barely inferred. This may be expected, since the min function essentially destroy the information about one of the signals, thus, making it impossible to decouple two seemingly identical random walk signals. The only one inferred signal is the one which is lower and we have no inference information about the signal which is above. It might be possible to infer the states, however, with more informative priors and structural information about two different signals (e.g. if these are not random walks). ","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Online-(filtering)-identification:-y-min(x,-w)","page":"Kalman Filtering And Smoothing","title":"Online (filtering) identification: y = min(x, w)","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Another way to approach to this problem is to use online (filtering) inference procedure from RxInfer, but for that we also need to modify our model specification a bit:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@model function rx_identification(f, m_x_0, τ_x_0, m_w_0, τ_w_0, a_x, b_x, a_y, b_y, a_w, b_w, y)\n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x ~ Normal(mean = x0, precision = τ_x)\n    w ~ Normal(mean = w0, precision = τ_w)\n\n    s := f(x, w)\n    y ~ Normal(mean = s, precision = τ_y)\n    \nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"We impose structured mean-field assumption for this model as well:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"rx_constraints = @constraints begin \n    q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ_y)\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Constraints: \n  q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ_y\n)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Online inference in the RxInfer supports the @autoupdates specification, which tells inference procedure how to update priors based on new computed posteriors:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"autoupdates = @autoupdates begin \n    m_x_0, τ_x_0 = mean_precision(q(x))\n    m_w_0, τ_w_0 = mean_precision(q(w))\n    a_x = shape(q(τ_x)) \n    b_x = rate(q(τ_x))\n    a_y = shape(q(τ_y))\n    b_y = rate(q(τ_y))\n    a_w = shape(q(τ_w)) \n    b_w = rate(q(τ_w))\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@autoupdates begin\n    (m_x_0, τ_x_0) = mean_precision(q(x))\n    (m_w_0, τ_w_0) = mean_precision(q(w))\n    a_x = shape(q(τ_x))\n    b_x = rate(q(τ_x))\n    a_y = shape(q(τ_y))\n    b_y = rate(q(τ_y))\n    a_w = shape(q(τ_w))\n    b_w = rate(q(τ_w))\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"As previously we need to define the @meta structure that specifies the approximation method for the nonlinear function smooth_min (f in the model specification):","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"rx_meta = @meta begin \n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Meta: \n  smooth_min() -> ReactiveMP.Linearization()","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Next step is to generate our dataset and to run the actual inference procedure! For that we use the infer function with autoupdates keyword:","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"n = 300\nrx_real_x, rx_real_w, rx_real_y = generate_data(min, n, seed = 1, x_i_min = 1.0, w_i_min = -1.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, rx_real_x, label = \"x\")\npl = plot!(pl, rx_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, rx_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"init = @initialization begin\n    q(w)= NormalMeanVariance(-2.0, 1.0) \n    q(x) = NormalMeanVariance(2.0, 1.0) \n    q(τ_x) = GammaShapeRate(1.0, 1.0) \n    q(τ_w) = GammaShapeRate(1.0, 1.0) \n    q(τ_y) = GammaShapeRate(1.0, 20.0)\nend\n\nengine = infer(\n    model         = rx_identification(f=smooth_min),\n    constraints   = rx_constraints,\n    data          = (y = rx_real_y,),\n    autoupdates   = autoupdates,\n    meta          = rx_meta,\n    returnvars    = (:x, :w, :τ_x, :τ_w, :τ_y, :s),\n    keephistory   = 1000,\n    historyvars   =  KeepLast(),\n    initialization = init,\n    iterations    = 10,\n    free_energy = true, \n    free_energy_diagnostics = nothing,\n    autostart     = true,\n)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"RxInferenceEngine:\n  Posteriors stream    | enabled for (w, s, τ_x, τ_w, τ_y, x)\n  Free Energy stream   | enabled\n  Posteriors history   | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)\n  Free Energy history  | available\n  Enabled events       | [  ]","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"rx_smarginals = engine.history[:s]\nrx_xmarginals = engine.history[:x]\nrx_wmarginals = engine.history[:w];","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, rx_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(rx_xmarginals), ribbon = var.(rx_xmarginals), label = \"Estimated X\")\n\npx1 = plot!(px1, rx_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(rx_wmarginals), ribbon = var.(rx_wmarginals), label = \"Estimated W\")\n\npx2 = scatter!(px2, rx_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(rx_smarginals), ribbon = std.(rx_smarginals), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"The results are quite similar to the smoothing case and, as we can see, one of the random walk is again in the \"disabled\" state, does not infer anything and simply increases its variance (which is expected for the random walk).","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/#Handling-Missing-Data","page":"Kalman Filtering And Smoothing","title":"Handling Missing Data","text":"","category":"section"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"An interesting case in filtering and smoothing problems is the processing of missing data. It can happen that sometimes your reading devices failt to acquire the data leading to missing observation.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Let us assume that the following model generates the data","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"beginaligned\n    x_t sim mathcalNleft(x_t-1 10right) \n    y_t sim mathcalNleft(x_t P right) \nendaligned","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Suppose that our measurement device fails to acquire data from time to time.  In this case, instead of scalar observation haty_t in mathrmR we sometimes will catch missing observations.","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"using RxInfer, Plots","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"@model function smoothing(x0, y)\n    \n    P ~ Gamma(shape = 0.001, scale = 0.001)\n    x_prior ~ Normal(mean = mean(x0), var = var(x0)) \n\n    local x\n    x_prev = x_prior\n\n    for i in 1:length(y)\n        x[i] ~ Normal(mean = x_prev, precision = 1.0)\n        y[i] ~ Normal(mean = x[i], precision = P)\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"P = 1.0\nn = 250\n\nreal_signal     = map(e -> sin(0.05 * e), collect(1:n))\nnoisy_data      = real_signal + rand(Normal(0.0, sqrt(P)), n);\nmissing_indices = 100:125\nmissing_data    = similar(noisy_data, Union{Float64, Missing}, )\n\ncopyto!(missing_data, noisy_data)\n\nfor index in missing_indices\n    missing_data[index] = missing\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"constraints = @constraints begin\n    q(x_prior, x, y, P) = q(x_prior, x)q(P)q(y)\nend","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Constraints: \n  q(x_prior, x, y, P) = q(x_prior, x)q(P)q(y)","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"x0_prior = NormalMeanVariance(0.0, 1000.0)\ninitm = @initialization begin\n    q(P) = Gamma(0.001, 0.001)\nend\n\nresult = infer(\n    model = smoothing(x0=x0_prior), \n    data  = (y = missing_data,), \n    constraints = constraints,\n    initialization = initm, \n    returnvars = (x = KeepLast(),),\n    iterations = 20\n);","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"plot(real_signal, label = \"Noisy signal\", legend = :bottomright)\nscatter!(missing_indices, real_signal[missing_indices], ms = 2, opacity = 0.75, label = \"Missing region\")\nplot!(mean.(result.posteriors[:x]), ribbon = var.(result.posteriors[:x]), label = \"Estimated hidden state\")","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/kalman_filtering_and_smoothing/","page":"Kalman Filtering And Smoothing","title":"Kalman Filtering And Smoothing","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/kalman_filtering_and_smoothing/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [31c24e10] Distributions v0.25.117\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Assessing-People’s-Skills","page":"Assessing People Skills","title":"Assessing People’s Skills","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"The goal of this demo is to demonstrate the use of the @node and @rule macros, which allow the user to define custom factor nodes and associated update rules respectively. We will introduce these macros in the context of a root cause analysis on a student's test results. This demo is inspired by Chapter 2 of \"Model-Based Machine Learning\" by Winn et al.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Problem-Statement","page":"Assessing People Skills","title":"Problem Statement","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We consider a student who takes a test that consists of three questions. Answering each question correctly requires a combination of skill and attitude. More precisely, has the student studied for the test, and have they partied the night before?","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We model the result for question i as a continuous variable r_iin01, and skill/attitude as a binary variable s_i in 0 1, where s_1 represents whether the student has partied, and s_2 and s_3 represent whether the student has studied the chapters for the corresponding questions.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We assume the following logic:","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"If the student is alert (has not partied), then they will score on the first question;\nIf the student is alert or has studied chapter two, then they will score on question two;\nIf the student can answer question two and has studied chapter three, then they will score on question three.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Generative-Model-Definition","page":"Assessing People Skills","title":"Generative Model Definition","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"To model the probability for correct answers, we assume a latent state variable t_i in 01. The dependencies between the variables can then be modeled by the following Bayesian network:","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"(s_1)   (s_2)   (s_3)\n  |       |       |\n  v       v       v\n(t_1)-->(t_2)-->(t_3)\n  |       |       |\n  v       v       v\n(r_1)   (r_2)   (r_3)","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"As prior beliefs, we assume that a student is equally likely to study/party or not: s_i sim Ber(05) for all i. Next, we model the domain logic as beginaligned   t_1 = s_1\n  t_2 = t_1  s_2\n  t_3 = t_2  s_3 endaligned For the scoring results we might not have a specific forward model in mind. However, we can define a backward mapping, from continuous results to discrete latent variables, as  t_i sim Ber(s_i) for all i.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Custom-Nodes-and-Rules","page":"Assessing People Skills","title":"Custom Nodes and Rules","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"The backward mapping from results to latents is quite specific to our application. Moreover, it does not define a proper generative forward model. In order to still define a full generative model for our application, we can define a custom Score node and define an update rule that implements the backward mapping from scores to latents as a message.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"In RxInfer, the @node macro defines a factor node. This macro accepts the new node type, an indicator for a stochastic or deterministic relationship, and a list of interfaces.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"using RxInfer, Random\n\n# Create Score node\nstruct Score end\n\n@node Score Stochastic [out, in]","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We can now define the backward mapping as a sum-product message through the @rule macro. This macro accepts the node type, the (outbound) interface on which the message is sent, any relevant constraints, and the message/distribution types on the remaining (inbound) interfaces.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"# Adding update rule for the Score node\n@rule Score(:in, Marginalisation) (q_out::PointMass,) = begin\n    return Bernoulli(mean(q_out))\nend","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Generative-Model-Specification","page":"Assessing People Skills","title":"Generative Model Specification","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"We can now build the full generative model.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"# GraphPPL.jl exports the `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function skill_model(r)\n\n    local s\n    # Priors\n    for i in eachindex(r)\n        s[i] ~ Bernoulli(0.5)\n    end\n\n    # Domain logic\n    t[1] ~ ¬s[1]\n    t[2] ~ t[1] || s[2]\n    t[3] ~ t[2] && s[3]\n\n    # Results\n    for i in eachindex(r)\n        r[i] ~ Score(t[i])\n    end\nend","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Inference-Specification","page":"Assessing People Skills","title":"Inference Specification","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"Let us assume that a student scored very low on all questions and set up and execute an inference algorithm.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"test_results = [0.1, 0.1, 0.1]\ninference_result = infer(\n    model=skill_model(),\n    data=(r=test_results,)\n)","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"Inference results:\n  Posteriors       | available for (s, t)","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/#Results","page":"Assessing People Skills","title":"Results","text":"","category":"section"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"# Inspect the results\nmap(params, inference_result.posteriors[:s])","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"3-element Vector{Tuple{Float64}}:\n (0.9872448979591837,)\n (0.06377551020408162,)\n (0.4719387755102041,)","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"These results suggest that this particular student was very likely out on the town last night.","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/assessing_people_skills/","page":"Assessing People Skills","title":"Assessing People Skills","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/assessing_people_skills/Project.toml`\n  [86711068] RxInfer v4.2.0\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Drone-Dynamics","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Note: These examples demonstrate the use of RxInfer for motion planning. The animations show the inferred trajectories from probabilistic inference, rather than simulated executions. For more realistic simulations, especially in the 3D drone example, the model would need to be extended with a reactive environment that responds to the drone's actions during plan execution. If you're interested in collaborating on a more realistic implementation, please open a discussion and let's work on it together!","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"using RxInfer, LinearAlgebra","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Defining-structures","page":"Drone Dynamics","title":"Defining structures","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    Environment(; gravitational_constant::Float64 = 9.81)\n\nThis structure contains the properties of the environment.\n\"\"\"\nBase.@kwdef struct Environment\n    gravitational_constant::Float64 = 9.81\nend\nget_gravity(env::Environment) = env.gravitational_constant","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_gravity (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    Drone(mass, inertia, radius, force_limit)\n\nThis structure contains the properties of the drone.\n\"\"\"\nBase.@kwdef struct Drone\n    mass::Float64\n    inertia::Float64\n    radius::Float64\n    force_limit::Float64\nend\nget_mass(drone::Drone) = drone.mass\nget_properties(drone::Drone) = (drone.mass, drone.inertia, drone.radius, drone.force_limit)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_properties (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    State(x, y, vx, vy, 𝜃, 𝜔)\n\nThis structure contains the state of the drone. It contains the position, velocity, and orientation of the drone.\n\"\"\"\nstruct State\n    x::Float64\n    y::Float64\n    vx::Float64\n    vy::Float64\n    𝜃::Float64\n    𝜔::Float64\nend\nget_state(state::State) = (state.x, state.y, state.vx, state.vy, state.𝜃, state.𝜔)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_state (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Model-specification","page":"Drone Dynamics","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    state_transition(state, actions, drone, environment, dt)\n\nThis function computes the next state of the drone given the current state, the actions, the drone properties and the environment properties.\n\"\"\"\nfunction state_transition(state, actions, drone::Drone, environment::Environment, dt)\n\n    # extract drone properties\n    m, I, r, limit  = get_properties(drone)\n\n    # extract environment properties\n    g = get_gravity(environment)\n\n    # extract feasible actions\n    Fl, Fr   = clamp.(actions, 0, limit)\n        \n    # extract state properties\n    x, y, vx, vy, θ, ω = state\n\n    # compute forces and torques\n    Fg = m * g\n    Fy = (Fl + Fr) * cos(θ) - Fg\n    Fx = (Fl + Fr) * sin(θ)\n    𝜏  = (Fl - Fr) * r\n\n    # compute movements\n    ax = Fx / m\n    ay = Fy / m\n    vx_new = vx + ax * dt\n#     vy_new = vx + ay * dt # old version\n    vy_new = vy + ay * dt   # new version\n    x_new  = x + vx * dt + ax * dt^2 / 2\n    y_new  = y + vy * dt + ay * dt^2 / 2\n        \n    # compute rotations\n    α = 𝜏 / I\n    ω_new = ω + α * dt\n    θ_new = θ + ω * dt + α * dt^2 / 2\n\t\n    return [x_new, y_new, vx_new, vy_new, θ_new, ω_new]\n\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Main.anonymous.state_transition","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"@model function drone_model(drone, environment, initial_state, goal, horizon, dt)\t\n\n\t# extract environment properties\n\tg = get_gravity(environment)\n\n\t# extract drone properties\n\tm = get_mass(drone)\n\n\t# initial state prior\n\ts[1] ~ MvNormal(mean = initial_state, covariance = 1e-5 * I)\n\n\tfor i in 1:horizon\n\n\t\t# prior on actions (mean compensates for gravity)\n\t\tu[i] ~ MvNormal(μ = [m * g / 2, m * g / 2], Σ = diageye(2))\n\n\t\t# state transition\n\t\ts[i + 1] ~ MvNormal(\n            μ = state_transition(s[i], u[i], drone, environment, dt), \n\t\t\tΣ = 1e-10 * I\n\t\t)\n\tend\n\t\n\ts[end] ~ MvNormal(mean = goal, covariance = 1e-5 * diageye(6))\n\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Probabilistic-inference","page":"Drone Dynamics","title":"Probabilistic inference","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"@meta function drone_meta()\n\n\t# approximate the state transition function using the Unscented transform\n\tstate_transition() -> Unscented()\n\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"drone_meta (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function move_to_target(drone::Drone, env::Environment, start::State, target, horizon, dt)\n\n    results = infer(\n        model = drone_model(\n            drone = drone, \n            environment = env,\n            horizon = horizon,\n            dt = dt\n        ),\n        data  = (\n            initial_state = collect(get_state(start)), \n            goal = [target[1], target[2], 0, 0, 0, 0],\n        ),\n        meta  = drone_meta(),\n        returnvars = (s = KeepLast(), u = KeepLast())\n    )\n\n    return results\n\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"move_to_target (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"drone = Drone(\n    mass = 1,\n    inertia = 1,\n    radius = 0.2,\n    force_limit = 15.0\n)\n\nenv = Environment()\n\nstart = State(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\ntarget = [-0.8, 0.6]\n\nresults = move_to_target(drone, env, start, target, 40, 0.05)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Inference results:\n  Posteriors       | available for (s, u)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#Plotting","page":"Drone Dynamics","title":"Plotting","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"using Plots","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function plot_drone!(p, drone::Drone, state::State; color = :black)\n    x, y, x_a, y_a, θ, ω = get_state(state)\n    _, _, radius, _ = get_properties(drone)\n    dx = radius * cos(θ)\n    dy = radius * sin(θ)\n\n    drone_position = [ x ], [ y ]\n    drone_engines  = [ x - dx, x + dx ], [ y + dy, y - dy ]\n    drone_coordinates = [ x - dx, x, x + dx ], [ y + dy, y, y - dy ]\n\n    rotation_matrix = [ cos(-θ) -sin(-θ); sin(-θ) cos(-θ) ]\n    engine_shape = [ -1 0 1; 1 -1 1 ]\n    drone_shape  = [ -2 -2 2 2 ; -1 1 1 -1 ]\n    \n    engine_shape = rotation_matrix * engine_shape\n    drone_shape  = rotation_matrix * drone_shape\n    engine_marker = Shape(engine_shape[1, :], engine_shape[2, :])\n    drone_marker  = Shape(drone_shape[1, :], drone_shape[2, :])\n    \n    scatter!(p, drone_position[1], drone_position[2]; color = color, label = false, marker = drone_marker)\n    scatter!(p, drone_engines[1], drone_engines[2]; color = color, label = false, marker = engine_marker, ms = 10)\n    plot!(p, drone_coordinates; color = color, label = false)\n\n    return p\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"plot_drone! (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function animate_drone(drone::Drone, target, results::InferenceResult)\n\n    states = hcat(map(p -> mean(p), results.posteriors[:s])...)\n    \n\n    animation = @animate for k in 1:size(states,2)\n\n        # plot target\n        p = scatter([target[1]], [target[2]], label = \"target\"; color = :red)\n\n        # plot drone\n        plot_drone!(p, drone, State(states[:, k]...))\n\n        xlims!(-1.5, 1.5)\n        ylims!(-1.5, 1.5)\n    \n    end\n\n    gif(animation, \"drone.gif\", show_msg = false)\n\n    nothing\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"animate_drone (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"animate_drone(drone, target, results)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"let \n    inferred_angle_mean = map(p -> mean(p)[5], results.posteriors[:s])\n    inferred_angle_std  = map(p -> std(p)[5],  results.posteriors[:s])\n    plot(inferred_angle_mean; ribbon = inferred_angle_std, fillalpha = 0.2, label = \"inferred angle\", size=(600,300))\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"let \n    inferred_forces_mean = hcat(map(p -> mean(p), results.posteriors[:u])...)'\n    inferred_forces_std  = hcat(map(p -> sqrt.(var(p)),  results.posteriors[:u])...)'\n    plot(inferred_forces_mean[:,1]; ribbon = inferred_forces_std[:,1], fillalpha = 0.2, label = \"Fl\", size=(600,300))\n    plot!(inferred_forces_mean[:,2]; ribbon = inferred_forces_std[:,2], fillalpha = 0.2, label = \"Fr\", size=(600,300))\n    \n    hline!([get_mass(drone) * get_gravity(env) / 2], label = \"Fg/2\")\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/#3D-Drone-Extension","page":"Drone Dynamics","title":"3D Drone Extension","text":"","category":"section"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"This section extends our 2D drone model into three-dimensional space, allowing for full spatial navigation. The model includes:","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"6 degrees of freedom (position and orientation)\nFour-motor configuration\nBasic aerodynamic forces","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Note: This implementation is a simplified model intended for educational purposes. While it captures the fundamental dynamics of a quadcopter, it omits advanced aerodynamic effects and motor dynamics for clarity.","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"# Extended Drone structure for 4 motors\nBase.@kwdef struct Drone3D\n    mass::Float64\n    inertia::Matrix{Float64}  # 3x3 inertia matrix\n    radius::Float64\n    arm_length::Float64\n    force_limit::Float64\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Main.anonymous.Drone3D","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function get_properties(drone::Drone3D)\n    return (\n        drone.mass,\n        drone.inertia,\n        drone.radius,\n        drone.arm_length,\n        drone.force_limit\n    )\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_properties (generic function with 2 methods)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"# Extended State for 3D\nstruct State3D\n    x::Float64   # position\n    y::Float64\n    z::Float64\n    vx::Float64  # velocity\n    vy::Float64\n    vz::Float64\n    ϕ::Float64   # roll\n    θ::Float64   # pitch\n    ψ::Float64   # yaw\n    ωx::Float64  # angular velocity\n    ωy::Float64\n    ωz::Float64\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function get_state(state::State3D)\n    return (\n        state.x, state.y, state.z,\n        state.vx, state.vy, state.vz,\n        state.ϕ, state.θ, state.ψ,\n        state.ωx, state.ωy, state.ωz\n    )\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"get_state (generic function with 2 methods)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    rotation_matrix(ψ, θ, ϕ)\n\nCreate a 3D rotation matrix from yaw (ψ), pitch (θ), and roll (ϕ) angles.\n\"\"\"\nfunction rotation_matrix(ψ, θ, ϕ)\n    # Rotation matrices for each axis\n    Rz = [cos(ψ) -sin(ψ) 0;\n          sin(ψ)  cos(ψ) 0;\n          0       0      1]\n    \n    Ry = [cos(θ)  0  sin(θ);\n          0       1  0;\n         -sin(θ)  0  cos(θ)]\n    \n    Rx = [1  0       0;\n          0  cos(ϕ) -sin(ϕ);\n          0  sin(ϕ)  cos(ϕ)]\n    \n    # Combined rotation matrix (ZYX order)\n    return Rz * Ry * Rx\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Main.anonymous.rotation_matrix","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"\"\"\"\n    state_transition_3d(state, actions, drone, environment, dt)\n\nCompute the next state of the 3D drone given current state and four motor forces.\n\"\"\"\nfunction state_transition_3d(state, actions, drone::Drone3D, environment::Environment, dt)\n    # Extract properties\n    m, I, r, L, limit = get_properties(drone)\n    g = get_gravity(environment)\n    \n    # Clamp motor forces\n    F1, F2, F3, F4 = clamp.(actions, 0, limit)\n    \n    # Extract state\n    x, y, z, vx, vy, vz, ϕ, θ, ψ, ωx, ωy, ωz = state\n    \n    # Current rotation matrix\n    R = rotation_matrix(ψ, θ, ϕ)\n    \n    # Total thrust force in body frame\n    F_total = sum([F1, F2, F3, F4])\n    \n    # Compute torques\n    τx = L * (F2 - F4)  # roll torque\n    τy = L * (F1 - F3)   # pitch torque\n    τz = (F1 + F3 - F2 - F4) * r  # yaw torque\n    \n    # Forces in world frame\n    F_world = R * [0, 0, F_total]\n    \n    # Accelerations\n    ax = F_world[1] / m\n    ay = F_world[2] / m\n    az = F_world[3] / m - g\n    \n    # Angular accelerations\n    α = I \\ ([τx, τy, τz] - cross([ωx, ωy, ωz], I * [ωx, ωy, ωz]))\n    \n    # Update velocities\n    vx_new = vx + ax * dt\n    vy_new = vy + ay * dt\n    vz_new = vz + az * dt\n    \n    # Update positions\n    x_new = x + vx * dt + ax * dt^2 / 2\n    y_new = y + vy * dt + ay * dt^2 / 2\n    z_new = z + vz * dt + az * dt^2 / 2\n    \n    # Update angular velocities\n    ωx_new = ωx + α[1] * dt\n    ωy_new = ωy + α[2] * dt\n    ωz_new = ωz + α[3] * dt\n    \n    # Update angles\n    ϕ_new = ϕ + ωx * dt + α[1] * dt^2 / 2\n    θ_new = θ + ωy * dt + α[2] * dt^2 / 2\n    ψ_new = ψ + ωz * dt + α[3] * dt^2 / 2\n    \n    return [\n        x_new, y_new, z_new,\n        vx_new, vy_new, vz_new,\n        ϕ_new, θ_new, ψ_new,\n        ωx_new, ωy_new, ωz_new\n    ]\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Main.anonymous.state_transition_3d","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"@model function drone_model_3d(drone, environment, initial_state, goal, horizon, dt)\n    # Extract properties\n    g = get_gravity(environment)\n    m = drone.mass\n    \n    # Initial state prior\n    s[1] ~ MvNormal(mean = initial_state, covariance = 1e-5 * I)\n    \n    for i in 1:horizon\n        # Prior on motor actions (mean compensates for gravity)\n        hover_force = m * g / 4\n        u[i] ~ MvNormal(μ = [hover_force, hover_force, hover_force, hover_force], Σ = diageye(4))\n        \n        # State transition\n        s[i + 1] ~ MvNormal(\n            μ = state_transition_3d(s[i], u[i], drone, environment, dt),\n            Σ = 1e-10 * I\n        )\n    end\n    \n    s[end] ~ MvNormal(mean = goal, covariance = 1e-5 * diageye(12))\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"@meta function drone_meta_3d()\n    state_transition_3d() -> Unscented()\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"drone_meta_3d (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function move_to_target_3d(drone::Drone3D, env::Environment, start::State3D, target, horizon, dt)\n    results = infer(\n        model = drone_model_3d(\n            drone = drone,\n            environment = env,\n            horizon = horizon,\n            dt = dt\n        ),\n        data = (\n            initial_state = collect(get_state(start)),\n            goal = [target[1], target[2], target[3], 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        ),\n        meta = drone_meta_3d(),\n        returnvars = (s = KeepLast(), u = KeepLast())\n    )\n    \n    return results\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"move_to_target_3d (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function move_through_waypoints(drone::Drone3D, env::Environment, start::State3D, waypoints, steps_per_segment=40, dt=0.05)\n    current_state = start\n    all_results = []\n    all_states = []\n    \n    # Move through each waypoint\n    for (i, target) in enumerate(waypoints)\n        println(\"Moving to waypoint $i: $target\")\n        \n        # Get results for this segment\n        results = move_to_target_3d(drone, env, current_state, target, steps_per_segment, dt)\n        push!(all_results, results)\n        \n        # Extract final state for next segment\n        final_states = hcat(map(p -> mean(p), results.posteriors[:s])...)\n        final_state = State3D(final_states[:, end]...)\n        push!(all_states, final_states)\n        \n        # Update current state\n        current_state = final_state\n    end\n    \n    # Combine all states for animation\n    combined_states = hcat(all_states...)\n    \n    return combined_states, waypoints\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"move_through_waypoints (generic function with 3 methods)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"# Visualization function for 3D drone\nfunction plot_drone_3d!(p, drone::Drone3D, state::State3D; color=:black)\n    x, y, z, _, _, _, ϕ, θ, ψ, _, _, _ = get_state(state)\n    _, _, radius, arm_length, _ = get_properties(drone)\n    \n    # Create rotation matrix\n    R = rotation_matrix(ψ, θ, ϕ)\n    \n    # Define arm endpoints in body frame (relative to center)\n    arm_endpoints = [\n        [arm_length, 0, 0],   # Right arm (X configuration)\n        [0, arm_length, 0],   # Front arm\n        [-arm_length, 0, 0],  # Left arm\n        [0, -arm_length, 0]   # Back arm\n    ]\n    \n    # Transform arm endpoints to world frame\n    world_endpoints = []\n    for endpoint in arm_endpoints\n        # Convert endpoint to column vector for matrix multiplication\n        endpoint_vec = reshape(endpoint, :, 1)\n        # Apply rotation and translation\n        world_point = R * endpoint_vec + [x, y, z]\n        push!(world_endpoints, vec(world_point))\n    end\n    \n    # Plot center\n    scatter!(p, [x], [y], [z], color=color, label=false, markersize=5)\n    \n    # Plot arms and motors\n    for endpoint in world_endpoints\n        # Draw arm\n        plot!(p, [x, endpoint[1]], [y, endpoint[2]], [z, endpoint[3]], \n              color=color, label=false, linewidth=2)\n        # Draw motor\n        scatter!(p, [endpoint[1]], [endpoint[2]], [endpoint[3]], \n                color=color, label=false, markersize=3)\n    end\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"plot_drone_3d! (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"function animate_drone_3d_multi(drone::Drone3D, states, targets; fps=30)\n    # Note: The rain animation is purely for visualization aesthetics\n    # and does not affect the drone's dynamics or trajectory planning\n    \n    # Create initial rain streaks\n    function generate_raindrops(n=50)\n        x = 4 * rand(n) .- 2  # range [-2, 2]\n        y = 4 * rand(n) .- 2\n        z1 = 2 .+ 4 * rand(n)  # start higher up to have some offscreen\n        z2 = z1 .- 0.3  # fixed length rain streaks\n        return (x, y, z1, z2)\n    end\n    \n    # Initialize raindrops\n    raindrops = generate_raindrops()\n    \n    animation = @animate for k in 1:size(states,2)\n        # Update raindrop positions\n        fall_speed = 0.1\n        z1 = raindrops[3] .- fall_speed\n        z2 = raindrops[4] .- fall_speed\n        \n        # Regenerate raindrops that have fallen below view\n        below_view = findall(z2 .<= -2)\n        if !isempty(below_view)\n            new_drops = generate_raindrops(length(below_view))\n            raindrops[1][below_view] = new_drops[1]\n            raindrops[2][below_view] = new_drops[2]\n            z1[below_view] = new_drops[3]\n            z2[below_view] = new_drops[4]\n        end\n        \n        # Update raindrops state\n        raindrops = (raindrops[1], raindrops[2], z1, z2)\n        \n        p = plot3d(\n            xlims=(-2, 2), ylims=(-2, 2), zlims=(-2, 2),\n            xlabel=\"X\", ylabel=\"Y\", zlabel=\"Z\",\n            camera=(45, 30),\n            title=\"Multi-Waypoint Drone Flight\",\n            background=:white\n        )\n        \n        # Draw rain streaks\n        for i in 1:length(raindrops[1])\n            if raindrops[4][i] > -2  # only draw if in view\n                plot!(p, [raindrops[1][i], raindrops[1][i]], \n                        [raindrops[2][i], raindrops[2][i]], \n                        [raindrops[3][i], raindrops[4][i]],\n                     color=:grey, \n                     linestyle=:dash,\n                     alpha=0.6,\n                     legend=false,\n                     linewidth=1)\n            end\n        end\n        \n        # Plot all targets\n        for (i, target) in enumerate(targets)\n            scatter!(p, [target[1]], [target[2]], [target[3]], \n                    label=i == 1 ? \"waypoints\" : false, \n                    color=:red,\n                    markersize=i == 1 ? 5 : 3)\n            \n            # Connect waypoints with lines\n            if i > 1\n                prev_target = targets[i-1]\n                plot!(p, [prev_target[1], target[1]], \n                         [prev_target[2], target[2]], \n                         [prev_target[3], target[3]],\n                     color=:red, linestyle=:dash, label=false, linewidth=1)\n            end\n        end\n        \n        # Plot drone\n        plot_drone_3d!(p, drone_3d, State3D(states[:, k]...))\n        \n        # Add trajectory trace (last 100 points)\n        trace_start = max(1, k-100)\n        if k > 1\n            plot!(p, states[1,trace_start:k], states[2,trace_start:k], states[3,trace_start:k],\n                  color=:blue, label=false, linewidth=1, linealpha=0.5)\n        end\n    end\n\n    gif(animation, \"drone_3d_multi.gif\", fps=fps, show_msg = false)\n\n    nothing\nend","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"animate_drone_3d_multi (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"# Create drone instance\ndrone_3d = Drone3D(\n    mass = 1.0,\n    inertia = diagm([0.1, 0.1, 0.15]),  # 3×3 diagonal inertia matrix\n    radius = 0.1,\n    arm_length = 0.2,\n    force_limit = 15.0\n)\n\nenv = Environment()\n\n# Initial state\nstart = State3D(\n    0.0, 0.0, 0.0,  # position (x, y, z)\n    0.0, 0.0, 0.0,  # velocity (vx, vy, vz)\n    0.0, 0.0, 0.0,  # orientation (ϕ, θ, ψ)\n    0.0, 0.0, 0.0   # angular velocity (ωx, ωy, ωz)\n)\n\n# Define a sequence of waypoints for a square pattern\nwaypoints = [\n    [1.0, 1.0, 1.0],    # Front-right corner\n    [-1.0, 1.0, 1.0],   # Front-left corner\n    [-1.0, -1.0, 1.0],  # Back-left corner\n    [1.0, -1.0, 1.0],   # Back-right corner\n    [0.0, 0.0, 0.0]     # Land at center\n]\n\n# Run simulation through all waypoints\ncombined_states, targets = move_through_waypoints(drone_3d, env, start, waypoints);","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Moving to waypoint 1: [1.0, 1.0, 1.0]\nMoving to waypoint 2: [-1.0, 1.0, 1.0]\nMoving to waypoint 3: [-1.0, -1.0, 1.0]\nMoving to waypoint 4: [1.0, -1.0, 1.0]\nMoving to waypoint 5: [0.0, 0.0, 0.0]","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"animate_drone_3d_multi(drone_3d, combined_states, targets)","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/drone_dynamics/","page":"Drone Dynamics","title":"Drone Dynamics","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/drone_dynamics/Project.toml`\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [37e2e46d] LinearAlgebra v1.11.0\n","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gamma_mixture/#Gamma-Mixture-Model","page":"Gamma Mixture","title":"Gamma Mixture Model","text":"","category":"section"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"This notebook implements one of the experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/.","category":"page"},{"location":"categories/problem_specific/gamma_mixture/#Load-packages","page":"Gamma Mixture","title":"Load packages","text":"","category":"section"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"using RxInfer, Random, StatsPlots","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# create custom structure for model parameters for simplicity\nstruct GammaMixtureModelParameters\n    nmixtures   # number of mixtures\n    priors_as   # tuple of priors for variable a\n    priors_bs   # tuple of priors for variable b\n    prior_s     # prior of variable s\nend","category":"page"},{"location":"categories/problem_specific/gamma_mixture/#Model-specification","page":"Gamma Mixture","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"@model function gamma_mixture_model(y, parameters)\n\n    # fetch information from struct\n    nmixtures = parameters.nmixtures\n    priors_as = parameters.priors_as\n    priors_bs = parameters.priors_bs\n    prior_s   = parameters.prior_s\n\n    # set prior on global selection variable\n    s ~ Dirichlet(probvec(prior_s))\n\n    # allocate variables for mixtures\n    local as\n    local bs\n\n    # set priors on variables of mixtures\n    for i in 1:nmixtures\n        as[i] ~ Gamma(shape = shape(priors_as[i]), rate = rate(priors_as[i]))\n        bs[i] ~ Gamma(shape = shape(priors_bs[i]), rate = rate(priors_bs[i]))\n    end\n\n    # allocate variables for local selection variable\n    local z\n    # specify local selection variable and data generating process\n    for i in 1:length(y)\n        z[i] ~ Categorical(s)\n        y[i] ~ GammaMixture(switch = z[i], a = as, b = bs)\n    end\n    \nend","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"constraints = @constraints begin \n\n    q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n\n    q(as) = q(as[begin])..q(as[end])\n    q(bs) = q(bs[begin])..q(bs[end])\n    \n    q(as)::PointMassFormConstraint(starting_point = (args...) -> [1.0])\nend","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"Constraints: \n  q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n  q(as) = q(as[(begin)..(end)])\n  q(bs) = q(bs[(begin)..(end)])\n  q(as) :: PointMassFormConstraint()","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# specify seed and number of data points\nrng = MersenneTwister(43)\nn_samples = 2500\n\n# specify parameters of mixture model that generates the data\n# Note that mixture components have exactly the same means\nmixtures  = [ Gamma(9.0, inv(27.0)), Gamma(90.0, inv(270.0)) ]\nnmixtures = length(mixtures)\nmixing    = rand(rng, nmixtures)\nmixing    = mixing ./ sum(mixing)\nmixture   = MixtureModel(mixtures, mixing)\n\n# generate data set\ndataset = rand(rng, mixture, n_samples);","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# specify priors of probabilistic model\n# NOTE: As the means of the mixtures \"collide\", we specify informative prior for selector variable\nnmixtures = 2\ngpriors = GammaMixtureModelParameters(\n    nmixtures,                                                    # number of mixtures\n    [ Gamma(1.0, 0.1), Gamma(1.0, 1.0) ],                         # priors on variables a\n    [ GammaShapeRate(10.0, 2.0), GammaShapeRate(1.0, 3.0) ],      # priors on variables b\n    Dirichlet(1e3*mixing)                                         # prior on variable s\n)\n\ngmodel         = gamma_mixture_model(parameters = gpriors)\ngdata          = (y = dataset, )\ninit           = @initialization begin \n    q(s) = gpriors.prior_s\n    q(z) = vague(Categorical, gpriors.nmixtures)\n    q(bs) = GammaShapeRate(1.0, 1.0)\nend\ngreturnvars    = (s = KeepLast(), z = KeepLast(), as = KeepEach(), bs = KeepEach())\n\ngoptions = (\n     \n    default_factorisation = MeanField() # Mixture models require Mean-Field assumption currently\n)\n\ngresult = infer(\n    model          = gmodel, \n    data           = gdata,\n    constraints    = constraints,\n    options        = (limit_stack_depth = 100,),\n    initialization = init,\n    returnvars     = greturnvars,\n    free_energy    = true,\n    iterations     = 250, \n    showprogress   = true\n);","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# extract inferred parameters\n_as, _bs = mean.(gresult.posteriors[:as][end]), mean.(gresult.posteriors[:bs][end])\n_dists   = map(g -> Gamma(g[1], inv(g[2])), zip(_as, _bs))\n_mixing = mean(gresult.posteriors[:s])\n\n# create model from inferred parameters\n_mixture   = MixtureModel(_dists, _mixing);","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# report on outcome of inference\nprintln(\"Generated means: $(mean(mixtures[1])) and $(mean(mixtures[2]))\")\nprintln(\"Inferred means: $(mean(_dists[1])) and $(mean(_dists[2]))\")\nprintln(\"========\")\nprintln(\"Generated mixing: $(mixing)\")\nprintln(\"Inferred mixing: $(_mixing)\")","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"Generated means: 0.3333333333333333 and 0.33333333333333337\nInferred means: 0.3350351536158398 and 0.33339098259987193\n========\nGenerated mixing: [0.4617110702349237, 0.5382889297650763]\nInferred mixing: [0.37273567102244054, 0.6272643289775595]","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# plot results\np1 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np1 = plot!(mixture, label=false, title=\"Generated mixtures\", linewidth=3.0)\n\np2 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np2 = plot!(_mixture, label=false, title=\"Inferred mixtures\", linewidth=3.0)\n\n# evaluate the convergence of the algorithm by monitoring the BFE\np3 = plot(gresult.free_energy, label=false, xlabel=\"iterations\", title=\"Bethe FE\")\n\nplot(plot(p1, p2, layout = @layout([ a; b ])), plot(p3), layout = @layout([ a b ]), size = (800, 400))","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/gamma_mixture/Project.toml`\n  [86711068] RxInfer v4.2.0\n  [f3b207a7] StatsPlots v0.15.7\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Multi-agent-Trajectory-Planning","page":"Multi-Agent Trajectory Planning","title":"Multi-agent Trajectory Planning","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"These examples demonstrate the use of RxInfer for trajectory planning in multi-agent situations. The animations show the inferred trajectories from probabilistic inference. The examples shown in this notebook are based on https://github.com/biaslab/MultiAgentTrajectoryPlanning/blob/main/door.jl, prepared by Michi-Tsubaki, extended by bvdmitri. The original code is a part of the paper Multi-Agent Trajectory Planning with NUV Priors by Bart van Erp.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Introduction","page":"Multi-Agent Trajectory Planning","title":"Introduction","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"This notebook demonstrates multi-agent trajectory planning using probabilistic inference with RxInfer.jl. In this example, we model multiple agents navigating through an environment with obstacles while trying to reach their respective goals. The planning problem is formulated as Bayesian inference, where:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Agent states evolve according to linear dynamics\nCollision avoidance between agents and obstacles is encoded as probabilistic constraints\nGoal-seeking behavior is represented as prior distributions","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"By performing inference on this probabilistic model, we can compute optimal trajectories that balance goal-reaching with collision avoidance. The visualization shows how agents coordinate their movements to navigate efficiently through the environment.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"using LinearAlgebra, RxInfer, Plots, LogExpFunctions, StableRNGs","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Environment-setup","page":"Multi-Agent Trajectory Planning","title":"Environment setup","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"To test our ideas, we need an environment to work with. We are going to create a simple environment consisting of a plane with boxes as obstacles. These boxes can be placed anywhere we want on the plane, allowing us to experiment with different configurations and scenarios. This flexible setup will help us evaluate how our multi-agent trajectory planning algorithms perform under various conditions and obstacle arrangements.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# A simple struct to represent a rectangle, which is defined by its center (x, y) and size (width, height)\nBase.@kwdef struct Rectangle\n    center::Tuple{Float64, Float64}\n    size::Tuple{Float64, Float64}\nend\n\nfunction plot_rectangle!(p, rect::Rectangle)\n    # Calculate the x-coordinates of the four corners\n    x_coords = rect.center[1] .+ rect.size[1]/2 * [-1, 1, 1, -1, -1]\n    # Calculate the y-coordinates of the four corners\n    y_coords = rect.center[2] .+ rect.size[2]/2 * [-1, -1, 1, 1, -1]\n    \n    # Plot the rectangle with a black fill\n    plot!(p, Shape(x_coords, y_coords), \n          label = \"\", \n          color = :black, \n          alpha = 0.5,\n          linewidth = 1.5,\n          fillalpha = 0.3)\nend\n\n# A simple struct to represent an environment, which is defined by a list of obctales,\n# and in this demo the obstacles are just rectangles\nBase.@kwdef struct Environment\n    obstacles::Vector{Rectangle}\nend\n\nfunction plot_environment!(p, env::Environment)\n    for obstacle in env.obstacles\n        plot_rectangle!(p, obstacle)\n    end\n    return p\nend\n\nfunction plot_environment(env::Environment)\n    p = plot(size = (800, 400), xlims = (-20, 20), ylims = (-20, 20), aspect_ratio = :equal)\n    plot_environment!(p, env)\n    return p\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"plot_environment (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In the code above, we've defined two key structures for our environment:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Rectangle: A simple structure representing rectangular obstacles, defined by:\ncenter: The (x,y) coordinates of the rectangle's center\nsize: The (width, height) of the rectangle\nEnvironment: A structure that contains a collection of obstacles (rectangles)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"We've also defined several plotting functions:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"plot_rectangle!: Adds a rectangle to an existing plot\nplot_environment!: Adds all obstacles in an environment to an existing plot\nplot_environment: Creates a new plot and displays the environment","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"These structures and functions provide the foundation for visualizing our 2D environment where multi-agent trajectory planning will take place.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Let's create a couple of different environments to demonstrate multi-agent trajectory planning. You can experiment with different obstacle configurations by modifying the rectangle positions, sizes, and quantities. This will allow you to test how the agents navigate around various obstacle arrangements and interact with each other in different scenarios.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Door-environment","page":"Multi-Agent Trajectory Planning","title":"Door environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In this environment, we'll create a scenario resembling a doorway that agents must navigate through. The environment will consist of two wall-like obstacles with a narrow passage between them, simulating a door or gateway. This setup will test the agents' ability to coordinate when passing through a constrained space, which is a common challenge in multi-agent path planning. The narrow passage will force agents to negotiate the right-of-way and potentially wait for each other to pass through, demonstrating emergent cooperative behaviors.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"door_environment = Environment(obstacles = [\n    Rectangle(center = (-40, 0), size = (70, 5)),\n    Rectangle(center = (40, 0), size = (70, 5))\n])\n\nplot_environment(door_environment)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Wall-environment","page":"Multi-Agent Trajectory Planning","title":"Wall environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In this environment, we'll create a scenario with a wall in the center that agents must navigate around. The environment will consist of a single elongated obstacle positioned in the middle of the space, forcing agents to choose whether to go above or below the wall. This setup will test the agents' ability to find efficient paths around obstacles and coordinate with each other to avoid congestion on either side of the wall. It represents a common scenario in multi-agent navigation where agents must make decisions about which route to take when faced with a barrier.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"wall_environment = Environment(obstacles = [\n    Rectangle(center = (0, 0), size = (10, 5))\n])\n\nplot_environment(wall_environment)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Combined-environment","page":"Multi-Agent Trajectory Planning","title":"Combined environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In this environment, we'll combine the door and wall scenarios to create a more complex navigation challenge. This environment will feature both a narrow doorway that agents must pass through and a wall obstacle they need to navigate around. This combined setup will test the agents' ability to handle multiple types of obstacles in sequence, requiring more sophisticated path planning and coordination. Agents will need to negotiate the doorway and then decide which path to take around the wall, or vice versa depending on their starting and goal positions. This represents a more realistic scenario where environments often contain various types of obstacles that require different navigation strategies.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"combined_environment = Environment(obstacles = [\n    Rectangle(center = (-50, 0), size = (70, 2)),\n    Rectangle(center = (50, -0), size = (70, 2)),\n    Rectangle(center = (5, -1), size = (3, 10))\n])\n\nplot_environment(combined_environment)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Agent-state","page":"Multi-Agent Trajectory Planning","title":"Agent state","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In this section, we define states and goals for our agents. Each agent has a initial position and target end position. These states will be used to drive agent movement through the environment. The trajectory planning algorithm will use this information to generate paths from start to destination while avoiding obstacles. We start by first defining the necessary structures and functions for the goals.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"\n# Agent plan, encodes start and goal states\nBase.@kwdef struct Agent\n    radius::Float64\n    initial_position::Tuple{Float64, Float64}\n    target_position::Tuple{Float64, Float64}\nend\n\nfunction plot_marker_at_position!(p, radius, position; color=\"red\", markersize=10.0, alpha=1.0, label=\"\")\n    # Draw the agent as a circle with the given radius\n    θ = range(0, 2π, 100)\n    \n    x_coords = position[1] .+ radius .* cos.(θ)\n    y_coords = position[2] .+ radius .* sin.(θ)\n    \n    plot!(p, Shape(x_coords, y_coords); color=color, label=label, alpha=alpha)\n    return p\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"plot_marker_at_position! (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Let see how does one of configurations for a single agent might look like in the first door environment. For this we will use two agents with different radius, as well as different initial and taget positions.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"function plot_agent_naive_plan!(p, agent; color = \"blue\")\n    plot_marker_at_position!(p, agent.radius, agent.initial_position, color = color)\n    plot_marker_at_position!(p, agent.radius, agent.target_position, color = color, alpha = 0.1)\n    quiver!(p, [ agent.initial_position[1] ], [ agent.initial_position[2] ], quiver = ([ agent.target_position[1] - agent.initial_position[1] ], [ agent.target_position[2] -  agent.initial_position[2] ]))\nend\n\nlet pe = plot_environment(door_environment)\n    agents = [ \n        Agent(radius = 2.5, initial_position = (-4, 10), target_position = (-10, -10)),\n        Agent(radius = 1.5, initial_position = (-10, 5), target_position = (10, -15)),\n        Agent(radius = 1.0, initial_position = (-15, -10), target_position = (10, 10)),\n        Agent(radius = 2.5, initial_position = (0, -10), target_position = (-10, 15))\n    ]\n    \n    colors = Plots.palette(:tab10)\n    \n    for (k, agent) in enumerate(agents)\n        plot_agent_naive_plan!(pe, agent, color = colors[k])\n    end\n    \n    pe\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The plot above illustrates that naive trajectory from initial to target position will obviously not work and the agents will hit either the wall or each other while trying to execute their plan. Thus we need to come up with a better plan and simultaneously take into account multiple agents in the same environment.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Next-Steps","page":"Multi-Agent Trajectory Planning","title":"Next Steps","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Now that we have set up our environment, defined our agents, and created utility functions, we are ready to build an RxInfer model to solve this multi-agent trajectory planning problem. In the following sections, we will:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Define a probabilistic model that captures the dynamics of our agents\nIncorporate collision avoidance constraints between agents and obstacles using NUV priors\nUse message passing to infer optimal trajectories\nVisualize the resulting paths","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"This will demonstrate how probabilistic programming with RxInfer can elegantly solve complex planning problems while handling uncertainty and constraints in a principled way.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Half-space-prior-implementation","page":"Multi-Agent Trajectory Planning","title":"Half space prior implementation","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"For our multi-agent trajectory planning model, we need to implement half-space priors to handle collision avoidance constraints. These priors allow us to model the requirement that agents must stay outside of obstacles and maintain safe distances from each other. The mathematical details and theoretical foundation of these half-space priors can be found in the paper referenced at the beginning of this notebook. The implementation below defines the necessary node and message-passing rules for incorporating these constraints into our probabilistic model.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# Define the probabilistic model for obstacles using halfspace constraints\nstruct Halfspace end\n\n@node Halfspace Stochastic [out, a, σ2, γ]\n\n# rule specification\n@rule Halfspace(:out, Marginalisation) (q_a::Any, q_σ2::Any, q_γ::Any) = begin\n    return NormalMeanVariance(mean(q_a) + mean(q_γ) * mean(q_σ2), mean(q_σ2))\nend\n\n@rule Halfspace(:σ2, Marginalisation) (q_out::Any, q_a::Any, q_γ::Any, ) = begin\n    # `BayesBase.TerminalProdArgument` is used to ensure that the result of the posterior computation is equal to this value\n    return BayesBase.TerminalProdArgument(PointMass( 1 / mean(q_γ) * sqrt(abs2(mean(q_out) - mean(q_a)) + var(q_out))))\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Distance-functions-for-collision-avoidance","page":"Multi-Agent Trajectory Planning","title":"Distance functions for collision avoidance","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In addition to the halfspace priors, we need to implement distance functions to properly handle collision avoidance between agents and obstacles. These functions will calculate the distance between agents and obstacles, which is essential for determining when collision avoidance constraints should be activated. The distance functions will be used to ensure that agents maintain safe distances from each other and from obstacles in the environment. In the next section, we'll define utility functions that include these distance calculations for different geometric shapes like rectangles and circles.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"softmin(x; l=10) = -logsumexp(-l .* x) / l\n\n# state here is a 4-dimensional vector [x, y, vx, vy]\nfunction distance(r::Rectangle, state)\n    if abs(state[1] - r.center[1]) > r.size[1] / 2 || abs(state[2] - r.center[2]) > r.size[2] / 2\n        # outside of rectangle\n        dx = max(abs(state[1] - r.center[1]) - r.size[1] / 2, 0)\n        dy = max(abs(state[2] - r.center[2]) - r.size[2] / 2, 0)\n        return sqrt(dx^2 + dy^2)\n    else\n        # inside rectangle\n        return max(abs(state[1] - r.center[1]) - r.size[1] / 2, abs(state[2] - r.center[2]) - r.size[2] / 2)\n    end\nend\n\nfunction distance(env::Environment, state)\n    return softmin([distance(obstacle, state) for obstacle in env.obstacles])\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"distance (generic function with 2 methods)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"We use the softmin function to create a smooth approximation of the minimum distance between an agent and multiple obstacles. Unlike the regular min function which returns the exact minimum value, softmin produces a differentiable approximation that considers all distances with a weighted average, heavily biased toward the smallest values.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The parameter l controls the \"sharpness\" of the approximation - with larger values making the function behave more like the true minimum. This smoothness is particularly valuable in optimization contexts as it:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Avoids discontinuities that could cause numerical issues during inference\nProvides gradient information from all obstacles, not just the closest one\nCreates a more stable optimization landscape for trajectory planning","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"When calculating the distance between an agent and the environment, softmin helps create a continuous repulsive field around all obstacles, allowing for more natural avoidance behaviors.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Model-specification","page":"Multi-Agent Trajectory Planning","title":"Model specification","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"We use RxInfer's @model macro to specify the model. In the current example we fix our model to exactly 4 agents to simplify the model creation and construction. We also define auxiliary functions g and h which computes distance with agent's radius offset and minimum distance between all agents pairwise.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# Helper function, distance with radius offset\nfunction g(environment, radius, state)\n    return distance(environment, state) - radius\nend\n\n# Helper function, finds minimum distances between agents pairwise\nfunction h(environment, radiuses, states...)\n    # Calculate pairwise distances between all agents\n    distances = Real[]\n    n = length(states)\n\n    for i in 1:n\n        for j in (i+1):n\n            push!(distances, norm(states[i] - states[j]) - radiuses[i] - radiuses[j])\n        end\n    end\n\n    return softmin(distances)\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"h (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# For more details about the model, please refer to the original paper\n@model function path_planning_model(environment, agents, goals, nr_steps)\n\n    # Model's parameters are fixed, refer to the original \n    # paper's implementation for more details about these parameters\n    local dt = 1\n    local A  = [1 dt 0 0; 0 1 0 0; 0 0 1 dt; 0 0 0 1]\n    local B  = [0 0; dt 0; 0 0; 0 dt]\n    local C  = [1 0 0 0; 0 0 1 0]\n    local γ  = 1\n\n    local control\n    local state\n    local path   \n    \n    # Extract radiuses of each agent in a separate collection\n    local rs = map((a) -> a.radius, agents)\n\n    # Model is fixed for 4 agents\n    for k in 1:4\n\n        # Prior on state, the state structure is 4 dimensional, where\n        # [ x_position, x_velocity, y_position, y_velocity ]\n        state[k, 1] ~ MvNormal(mean = zeros(4), covariance = 1e2I)\n\n        for t in 1:nr_steps\n\n            # Prior on controls\n            control[k, t] ~ MvNormal(mean = zeros(2), covariance = 1e-1I)\n\n            # State transition\n            state[k, t+1] ~ A * state[k, t] + B * control[k, t]\n\n            # Path model, the path structure is 2 dimensional, where \n            # [ x_position, y_position ]\n            path[k, t] ~ C * state[k, t+1]\n\n            # Environmental distance\n            zσ2[k, t] ~ GammaShapeRate(3 / 2, γ^2 / 2)\n            z[k, t]   ~ g(environment, rs[k], path[k, t])\n            \n            # Halfspase priors were defined previousle in this experiment\n            z[k, t] ~ Halfspace(0, zσ2[k, t], γ)\n\n        end\n\n        # goal priors (indexing reverse due to definition)\n        goals[1, k] ~ MvNormal(mean = state[k, 1], covariance = 1e-5I)\n        goals[2, k] ~ MvNormal(mean = state[k, nr_steps+1], covariance = 1e-5I)\n\n    end\n\n    for t = 1:nr_steps\n\n        # observation constraint\n        dσ2[t] ~ GammaShapeRate(3 / 2, γ^2 / 2)\n        d[t] ~ h(environment, rs, path[1, t], path[2, t], path[3, t], path[4, t])\n        d[t] ~ Halfspace(0, dσ2[t], γ)\n\n    end\n\nend\n\n@constraints function path_planning_constraints()\n    # Mean-field variational constraints on the parameters\n    q(d, dσ2) = q(d)q(dσ2)\n    q(z, zσ2) = q(z)q(zσ2)\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"path_planning_constraints (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Constraint-specification","page":"Multi-Agent Trajectory Planning","title":"Constraint specification","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"function path_planning(; environment, agents, nr_iterations = 350, nr_steps = 40, seed = 42)\n    # Fixed number of agents\n    nr_agents = 4\n\n    # Form goals compatible with the model\n    goals = hcat(\n        map(agents) do agent\n            return [\n                [ agent.initial_position[1], 0, agent.initial_position[2], 0 ],\n                [ agent.target_position[1], 0, agent.target_position[2], 0 ]\n            ]\n        end...\n    )\n    \n    rng = StableRNG(seed)\n    \n    # Initialize variables, more details about initialization \n    # can be found in the original paper\n    init = @initialization begin\n\n        q(dσ2) = repeat([PointMass(1)], nr_steps)\n        q(zσ2) = repeat([PointMass(1)], nr_agents, nr_steps)\n        q(control) = repeat([PointMass(0)], nr_steps)\n\n        μ(state) = MvNormalMeanCovariance(randn(rng, 4), 100I)\n        μ(path) = MvNormalMeanCovariance(randn(rng, 2), 100I)\n\n    end\n\n    # Define approximation methods for the non-linear functions used in the model\n    # `Linearization` is a simple and fast approximation method, but it is not\n    # the most accurate one. For more details about the approximation methods,\n    # please refer to the RxInfer documentation\n    door_meta = @meta begin \n        h() -> Linearization()\n        g() -> Linearization()\n    end\n\n    results = infer(\n        model \t\t\t= path_planning_model(environment = environment, agents = agents, nr_steps = nr_steps),\n        data  \t\t\t= (goals = goals, ),\n        initialization  = init,\n        constraints \t= path_planning_constraints(),\n        meta \t\t\t= door_meta,\n        iterations \t\t= nr_iterations,\n        returnvars \t\t= KeepLast(), \n        options         = (limit_stack_depth = 300, )\n    )\n\n    return results\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"path_planning (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"function execute_and_save_animation(environment, agents; gifname = \"result.gif\", kwargs...)\n    result = path_planning(environment = environment, agents = agents; kwargs...)\n    paths  = mean.(result.posteriors[:path])\n    \n    nr_agents, nr_steps = size(paths)\n    colors = Plots.palette(:tab10)\n\n    animation = @animate for t in 1:nr_steps\n        frame = plot_environment(environment)\n    \n        for k in 1:nr_agents\n            position = paths[k, t]          \n            path = paths[k, 1:t]\n            \n            plot_marker_at_position!(frame, agents[k].radius, position, color = colors[k])\n            plot_marker_at_position!(frame, agents[k].radius, agents[k].target_position, color = colors[k], alpha = 0.2)\n            plot!(frame, getindex.(path, 1), getindex.(path, 2); linestyle=:dash, label=\"\", color=colors[k])\n        end\n\n        frame\n    end\n\n    # assign the path to save the image\n    gif(animation, gifname, fps=15, show_msg = false)\n    \n    return nothing\nend","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation (generic function with 1 method)","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"# These are the same agents as in the beginning of the notebook, but copy-pasted here \n# for easier experimentation, closer to the actual experiments\nagents = [\n    Agent(radius = 2.5, initial_position = (-4, 10), target_position = (-10, -10)),\n    Agent(radius = 1.5, initial_position = (-10, 5), target_position = (10, -15)),\n    Agent(radius = 1.0, initial_position = (-15, -10), target_position = (10, 10)),\n    Agent(radius = 2.5, initial_position = (0, -10), target_position = (-10, 15))\n]","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"4-element Vector{Main.anonymous.Agent}:\n Main.anonymous.Agent(2.5, (-4.0, 10.0), (-10.0, -10.0))\n Main.anonymous.Agent(1.5, (-10.0, 5.0), (10.0, -15.0))\n Main.anonymous.Agent(1.0, (-15.0, -10.0), (10.0, 10.0))\n Main.anonymous.Agent(2.5, (0.0, -10.0), (-10.0, 15.0))","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Experiments-and-visualizations","page":"Multi-Agent Trajectory Planning","title":"Experiments and visualizations","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The experiments and animations below demonstrate the power of probabilistic inference for multi-agent trajectory planning in different environments. Let's analyze what we can observe in each scenario:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Door-environment-2","page":"Multi-Agent Trajectory Planning","title":"Door environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"In the door environment, we see four agents with different sizes navigating through a narrow passage. The agents demonstrate several interesting behaviors:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"When multiple agents approach the doorway simultaneously, they naturally form a queue, with some agents waiting for others to pass through first\nAgents slow down or speed up based on the presence of other agents near the doorway\nLarger agents (with bigger radius) effectively have precedence in tight spaces, as smaller agents can more easily find alternative paths","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The two different seeds (42 and 123) show how small changes in initialization can lead to different coordination patterns, highlighting the inherent variability in multi-agent systems.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(door_environment, agents; seed = 42, gifname = \"door_42.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(door_environment, agents; seed = 123, gifname = \"door_123.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Wall-environment-2","page":"Multi-Agent Trajectory Planning","title":"Wall environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The wall environment forces agents to choose whether to go above or below the obstacle and distribute themselves between the two possible paths to avoid congestion The choice of path (above or below) appears to be influenced by the agent's initial position.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(wall_environment, agents; seed = 42, gifname = \"wall_42.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(wall_environment, agents; seed = 123, gifname = \"wall_123.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Combined-environment-2","page":"Multi-Agent Trajectory Planning","title":"Combined environment","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The combined environment presents the most complex challenge, requiring agents to navigate both a doorway and a wall. This environment best showcases the power of the approach, as traditional reactive navigation methods would struggle with the compounding complexity of multiple obstacle types.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(combined_environment, agents; seed = 42, gifname = \"combined_42.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"execute_and_save_animation(combined_environment, agents; seed = 123, gifname = \"combined_123.gif\")","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"(Image: )","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Practical-Applications-and-Potential-Improvements","page":"Multi-Agent Trajectory Planning","title":"Practical Applications and Potential Improvements","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"The multi-agent trajectory planning approach demonstrated in this notebook has numerous real-world applications:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Warehouse robotics: Coordinating multiple robots in fulfillment centers to avoid collisions while efficiently picking and delivering items\nTraffic management: Planning trajectories for autonomous vehicles at intersections or in congested areas\nCrowd simulation: Modeling realistic human movement patterns in architectural design or emergency evacuation planning\nDrone swarms: Coordinating groups of UAVs for tasks like search and rescue, surveillance, or package delivery","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Future extensions to this work could include:","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Handling dynamic obstacles that move or change over time\nIncorporating uncertainty in agent dynamics and sensing\nScaling to much larger numbers of agents through more efficient inference algorithms\nAdding communication constraints between agents for more realistic modeling\nIncorporating heterogeneous agent types with different dynamics and capabilities","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/#Conclusion","page":"Multi-Agent Trajectory Planning","title":"Conclusion","text":"","category":"section"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"This notebook has demonstrated how probabilistic inference can be used to solve the complex problem of multi-agent trajectory planning. By formulating the planning problem as Bayesian inference, we've shown how agents can coordinate their movements to navigate through various challenging environments while avoiding collisions with obstacles and each other.","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/advanced_examples/multi-agent_trajectory_planning/","page":"Multi-Agent Trajectory Planning","title":"Multi-Agent Trajectory Planning","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/advanced_examples/multi-agent_trajectory_planning/Project.toml`\n  [2ab3a3ac] LogExpFunctions v0.3.29\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [37e2e46d] LinearAlgebra v1.11.0\n","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"","category":"page"},{"location":"categories/problem_specific/probit_model/#Probit-Model","page":"Probit Model","title":"Probit Model","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/#Estimation-of-pollutant","page":"Probit Model","title":"Estimation of pollutant","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Mortality y_t of fishs in a lake is observed over time. Mortality rate textBer(Phi(x_t)) is linked to the level of pollutant x_t in the lake according to the probit model (see below). The municipality wants to keep track of the pollution. To do so, the level of pollutant in the lake is tracked over time through observations of the fishs.","category":"page"},{"location":"categories/problem_specific/probit_model/#Objective","page":"Probit Model","title":"Objective","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Probit model aims to infer a random proces value from noisy binary observations of it. RxInfer comes with support for expectation propagation (EP). In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model. Here, the probit function links continuous variable x_t with the discrete variable y_t. The model is defined as:","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"beginaligned\n    u = 01 \n    x_0 sim mathcalN(0 100) \n    x_t sim mathcalN(x_t-1+ u 001) \n    y_t sim mathrmBer(Phi(x_t))\nendaligned","category":"page"},{"location":"categories/problem_specific/probit_model/#Import-packages","page":"Probit Model","title":"Import packages","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"using RxInfer, GraphPPL,StableRNGs, Random, Plots, Distributions\nusing StatsFuns: normcdf","category":"page"},{"location":"categories/problem_specific/probit_model/#Data-generation","page":"Probit Model","title":"Data generation","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n    \n    # hyper parameters\n    u = 0.1\n\n    # allocate space for data\n    data_x = zeros(nr_samples + 1)\n    data_y = zeros(nr_samples)\n    \n    # initialize data\n    data_x[1] = -2\n    \n    # generate data\n    for k in eachindex(data_y)\n        \n        # calculate new x\n        data_x[k+1] = data_x[k] + u + sqrt(0.01)*randn(rng)\n        \n        # calculate y\n        data_y[k] = normcdf(data_x[k+1]) > rand(rng)\n        \n    end\n    \n    # return data\n    return data_x, data_y\n    \nend;","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"n = 40","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"40","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"data_x, data_y = generate_data(n);","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"p = plot(xlabel = \"t\", ylabel = \"x, y\")\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\")","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/probit_model/#Model-specification","page":"Probit Model","title":"Model specification","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"@model function probit_model(y, prior_x)\n    \n    # specify uninformative prior\n    x_prev ~ prior_x\n    \n    # create model \n    for k in eachindex(y)\n        x[k] ~ Normal(mean = x_prev + 0.1, precision = 100)\n        y[k] ~ Probit(x[k]) where {\n            # Probit node by default uses RequireMessage pipeline with vague(NormalMeanPrecision) message as initial value for `in` edge\n            # To change initial value user may specify it manually, like. Changes to the initial message may improve stability in some situations\n            dependencies = RequireMessageFunctionalDependencies(in = NormalMeanPrecision(0.0, 0.01))\n        }\n        x_prev = x[k]\n    end\n    \nend;","category":"page"},{"location":"categories/problem_specific/probit_model/#Probit-Node","page":"Probit Model","title":"Probit Node","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Probit node needs an initialisation of the 'in' message because of this computation methodology. The input message is not directly calculated. First the marginal q(in) is computed and then the output message, this using the margianalisation formula. ","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"overrightarrowmu(x) overleftarrowmu(x) = q(x)","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Consequently an initial message overleftarrowmu(in) is needed to start iterate. It can be speficied as in the above example. Otherwise RxInfer will initiate it at a default value.","category":"page"},{"location":"categories/problem_specific/probit_model/#Inference","page":"Probit Model","title":"Inference","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"result = infer(\n    model = probit_model(prior_x=Normal(0.0, 100.0)), \n    data  = (y = data_y, ), \n    iterations = 5, \n    returnvars = (x = KeepLast(),),\n    free_energy  = true\n)","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Inference results:\n  Posteriors       | available for (x)\n  Free Energy:     | Real[25.6698, 18.0157, 17.9199, 17.9194, 17.9194]","category":"page"},{"location":"categories/problem_specific/probit_model/#Results","page":"Probit Model","title":"Results","text":"","category":"section"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"mx = result.posteriors[:x]\n\np = plot(xlabel = \"t\", ylabel = \"x, y\", legend = :bottomright)\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\", lw = 2)\np = plot!(mean.(mx)[2:end], ribbon = std.(mx)[2:end], fillalpha = 0.2, label=\"x (inferred mean)\")\n\nf = plot(xlabel = \"t\", ylabel = \"BFE\")\nf = plot!(result.free_energy, label = \"Bethe Free Energy\")\n\nplot(p, f, size = (800, 400))","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"(Image: )","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/problem_specific/probit_model/","page":"Probit Model","title":"Probit Model","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/problem_specific/probit_model/Project.toml`\n  [31c24e10] Distributions v0.25.117\n  [b3f8163a] GraphPPL v4.6.2\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [4c63d2b9] StatsFuns v1.3.2\n  [9a3f8284] Random v1.11.0\n","category":"page"},{"location":"how_to_contribute/#Contributing-to-RxInfer-Examples","page":"How to contribute","title":"Contributing to RxInfer Examples","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"We welcome contributions from the community! This guide will help you understand how to add new examples or improve existing ones in the RxInfer Examples collection. Here are the steps to follow to add a new example:","category":"page"},{"location":"how_to_contribute/#Location-and-Structure","page":"How to contribute","title":"Location and Structure","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Create a new Jupyter notebook in the appropriate category folder. Use examples/Basic Examples/ for fundamental concepts, examples/Advanced Examples/ for complex applications, or examples/Problem Specific/ for domain-specific use cases.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"note: Note\nYou can also introduce a new category by creating a new folder in the examples/ directory. In this case, you should also add a new entry in the docs/make.jl file.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Each example at the very least should have a clear, descriptive title, a meta.jl file in the same directory, a local Project.toml for dependencies, and any required data files.","category":"page"},{"location":"how_to_contribute/#Notebook-Guidelines","page":"How to contribute","title":"Notebook Guidelines","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"First Cell Requirements The first cell must be a markdown cell. It should contain ONLY the title as # <title>. The title should be descriptive and unique (avoid \"Overview\").\nEnvironment Setup The notebook will use the environment specified in the Project.toml file. Add any additional dependencies to the local project.\nContent Structure The notebook should have a clear introduction and problem description, model specification with explanations, inference procedure details, results analysis and visualization, and comprehensive comments for readability.\nSelf-Contained Code\nExamples must be fully self-contained without using include() statements\nAll code should be directly in the notebook cells\nDo not reference external Julia files\nUsers should be able to reproduce examples by simply copying and pasting from the documentation","category":"page"},{"location":"how_to_contribute/#Mathematical-Content","page":"How to contribute","title":"Mathematical Content","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"note: Note\nThe automatic rendering of equations is handled by the make.jl script and it does not understand the spaces after the $$ or $. Below are the rules for formatting equations.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Equation Formatting\nSome text\n\n$$\\begin{aligned}\n<latex equations here>\n\\end{aligned}$$\n\nSome other text\nDo not add spaces before or after the $$ or $\nEquation Rules\nNo space after opening $$ or $\nSeparate display equations with empty lines\nInline equations use single $...$, e.g. $$a + b$$ and not $$ a + b $$","category":"page"},{"location":"how_to_contribute/#Visualization-and-figures","page":"How to contribute","title":"Visualization and figures","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"All plots rendered with Plots.jl should display automatically\nAsset figures can be saved in the same directory as the notebook and referenced with ![](figure-name.png)\nSpecial figures (e.g., GIFs) should be saved to generated-in-the-notebook.gif in the same directory as the notebook\nReference saved figures as markdown with ![](generated-in-the-notebook.gif) right after the cell that generated it","category":"page"},{"location":"how_to_contribute/#Metadata-Requirements","page":"How to contribute","title":"Metadata Requirements","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Create a meta.jl file in your example's directory with:","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"return (\n    title = \"Your Example Title\",\n    description = \"\"\"\n    A clear description of what the example demonstrates.\n    \"\"\",\n    tags = [\"category\", \"relevant\", \"tags\", \"here\"]\n)","category":"page"},{"location":"how_to_contribute/#Testing-Your-Example","page":"How to contribute","title":"Testing Your Example","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"note: Note\nNote that building the examples locally requires Weave.jl package to be installed globally in your Julia environment. Use julia -e 'using Pkg; Pkg.add(\"Weave\")' to install it.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Local Testing\n# Test all examples\nmake examples\n\n# Test specific example\nmake example FILTER=YourNotebookName\n\n# Render the documentation\nmake docs\n\n# Preview the documentation\nmake preview\nBuild Caching\nThe build system caches the results of example compilation. If you make changes to an example and still see old errors after rebuilding:\n# Clear all build caches and artifacts\nmake clean\n\n# Then rebuild\nmake examples\nCommon Issues\nEnsure all dependencies are in Project.toml\nVerify plots display correctly\nTest with a clean environment\nIf errors persist after fixing, run make clean to clear cached builds","category":"page"},{"location":"how_to_contribute/#Important-Notes","page":"How to contribute","title":"Important Notes","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"note: Plotting Package Preference\nPlease use Plots.jl instead of PyPlot. PyPlot's installation significantly impacts CI build times.","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"warning: Documentation Generation\nEnsure your notebook renders correctly in the documentation by:Following equation formatting rules\nUsing proper cell types\nIncluding all necessary resources","category":"page"},{"location":"how_to_contribute/#Getting-Help","page":"How to contribute","title":"Getting Help","text":"","category":"section"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"If you're unsure about anything:","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Check existing examples for reference\nOpen an issue for guidance\nAsk in the discussions section","category":"page"},{"location":"how_to_contribute/","page":"How to contribute","title":"How to contribute","text":"Your contributions help make RxInfer.jl better for everyone!","category":"page"},{"location":"#RxInfer.jl-Examples","page":"Home","title":"RxInfer.jl Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to the examples gallery for RxInfer.jl, a Julia package for reactive message passing and probabilistic programming.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThis documentation is automatically generated from Jupyter notebooks in the repository. The examples are regularly tested to ensure they work with the latest version of RxInfer.jl.","category":"page"},{"location":"#About-RxInfer.jl","page":"Home","title":"About RxInfer.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RxInfer.jl is a Julia package that combines message passing-based inference with reactive programming paradigms. It provides:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A flexible framework for probabilistic programming\nReactive message passing for real-time inference\nEfficient and scalable inference algorithms\nSupport for both online and offline inference\nIntegration with the Julia ecosystem","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read more about RxInfer.jl in the RxInfer.jl Documentation.","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Browse our comprehensive collection of examples in the List of Examples section. Each example demonstrates different aspects of RxInfer.jl's capabilities and includes detailed explanations and code.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We welcome contributions from the community! Whether you want to fix bugs, improve existing examples, or add new ones, please check our contribution guide for detailed instructions and best practices.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To run these examples locally:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Clone the repository:","category":"page"},{"location":"","page":"Home","title":"Home","text":"git clone https://github.com/ReactiveBayes/RxInferExamples.jl.git","category":"page"},{"location":"","page":"Home","title":"Home","text":"Build the examples:","category":"page"},{"location":"","page":"Home","title":"Home","text":"make examples","category":"page"},{"location":"","page":"Home","title":"Home","text":"Build the documentation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"make docs","category":"page"},{"location":"","page":"Home","title":"Home","text":"Preview the documentation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"make preview","category":"page"},{"location":"#Resources","page":"Home","title":"Resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"How to Contribute\nRxInfer.jl Documentation\nRxInfer.jl GitHub Repository\nJulia Documentation","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: For Developers\nIf you're interested in how the examples and documentation are built, check out our Build System Documentation.","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RxInfer.jl and these examples are licensed under the MIT License. See the LICENSE file in the repository for more details.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/#Bayesian-Multinomial-Regression","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"This notebook is an introductory tutorial to Bayesian multinomial regression with RxInfer.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"using RxInfer, Plots, StableRNGs, Distributions, ExponentialFamily, StatsPlots\nimport ExponentialFamily: softmax","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/#Model-Description","page":"Bayesian Multinomial Regression","title":"Model Description","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"The key innovation in Linderman et al. (2015) is extending the Pólya-gamma augmentation scheme to the multinomial case. This allows us to transform the non-conjugate multinomial likelihood into a conditionally conjugate form by introducing auxiliary Pólya-gamma random variables.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"The multinomial regression model with Pólya-gamma augmentation can be written as: p(y  psi N) = textMultinomial(y N psi)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"where:","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"y\nis a K-dimensional vector of count data with N total counts\npsi\nis a K-1 -dimensional Gaussian random variable","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/#Implementation","page":"Bayesian Multinomial Regression","title":"Implementation","text":"","category":"section"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"In this notebook, we will implement the Pólya-gamma augmented Bayesian multinomial regression model with RxInfer by performing inference using message passing to estimate the posterior distribution of the regression coefficients","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"function generate_multinomial_data(rng=StableRNG(123);N = 3, k=3, nsamples = 1000)\n    Ψ = randn(rng, k)\n    p = softmax(Ψ)\n    X = rand(rng, Multinomial(N, p), nsamples)\n    X= [X[:,i] for i in 1:size(X,2)];\n    return X, Ψ,p\nend","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"generate_multinomial_data (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"nsamples = 5000\nN = 30\nk = 40\nX, Ψ, p = generate_multinomial_data(N=N,k=k,nsamples=nsamples);","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"The MultinomialPolya factor node is used to model the likelihood of the multinomial distribution. ","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Due to non-conjugacy of the likelihood and the prior distribution, we need to use a more complex inference algorithm. RxInfer provides an Expectation Propagation (EP) [2] algorithm to infer the posterior distribution. Due to EP's approximation, we need to specify an inbound message for the regression coefficients while using the MultinomialPolya factor node. This feature is implemented in the dependencies keyword argument during the creation of the MultinomialPolya factor node. ReactiveMP.jl provides a RequireMessageFunctionalDependencies type that is used to specify the inbound message for the regression coefficients ψ. Refer to the ReactiveMP.jl documentation for more information.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"@model function multinomial_model(obs, N, ξ_ψ, W_ψ)\n    ψ ~ MvNormalWeightedMeanPrecision(ξ_ψ, W_ψ)\n    obs .~ MultinomialPolya(N, ψ) where {dependencies = RequireMessageFunctionalDependencies(ψ = MvNormalWeightedMeanPrecision(ξ_ψ, W_ψ))}\nend","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"result = infer(\n    model = multinomial_model(ξ_ψ=zeros(k-1), W_ψ=rand(Wishart(3, diageye(k-1))), N=N),\n    data = (obs=X, ),\n    iterations = 50,\n    free_energy = true,\n    showprogress = true,\n    options = (\n        limit_stack_depth = 100,\n    )\n)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Inference results:\n  Posteriors       | available for (ψ)\n  Free Energy:     | Real[4.46569e5, 2.92801e5, 2.38339e5, 2.12674e5, 1.986\n37e5, 1.90206e5, 1.848e5, 1.81164e5, 1.78623e5, 1.76795e5  …  170370.0, 1.7\n0363e5, 1.70357e5, 1.70352e5, 1.70347e5, 1.70343e5, 1.70339e5, 1.70335e5, 1\n.70332e5, 1.7033e5]","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"plot(result.free_energy[1:end], \n     title=\"Free Energy Over Iterations\",\n     xlabel=\"Iteration\",\n     ylabel=\"Free Energy\",\n     linewidth=2,\n     legend=false,\n     grid=true,\n     )","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"predictive = @call_rule MultinomialPolya(:x, Marginalisation) (q_N = PointMass(N), q_ψ = result.posteriors[:ψ][end], meta = MultinomialPolyaMeta(21))\nprintln(\"Estimated data generation probabilities: $(predictive.p)\")\nprintln(\"True data generation probabilities: $(p)\")","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Estimated data generation probabilities: [0.012500686564987495, 0.027222779\n00084117, 0.004699975812457014, 0.012715794706234148, 0.01343997388970624, \n0.03792000859746627, 0.007973570601243232, 0.006729078538004615, 0.00593368\n6957322783, 0.003816497551554285, 0.0058806593671880385, 0.0039974531316937\n76, 0.004388139203563341, 0.035958777048946795, 0.1090325848597523, 0.07242\n206459021783, 0.026377221984298385, 0.023972029170301216, 0.009937834641641\n273, 0.008905008531579302, 0.03980039778709284, 0.005572704945257927, 0.008\n25741961798846, 0.026366017090935612, 0.006346868933117693, 0.0082611150956\n53505, 0.0096309915856949, 0.007347285740612368, 0.01752974964548303, 0.006\n740949114795702, 0.008695006289290104, 0.0038444249798769555, 0.01125914772\n1232581, 0.010519245544783038, 0.09451624739892181, 0.043478352423226146, 0\n.13221749436096303, 0.027359828232099287, 0.030887113219550014, 0.067545815\n5244256]\nTrue data generation probabilities: [0.012475572764691347, 0.02759115956301\n153, 0.004030932560100506, 0.013008651265311708, 0.012888510278451618, 0.03\n7656116813111006, 0.007242363105598982, 0.006930069564505769, 0.00538389836\n228327, 0.0036198124274772225, 0.005212387391120808, 0.003185556887255863, \n0.003820168769118259, 0.036849638787622915, 0.109428569898501, 0.0726075387\n5224316, 0.026079268674281158, 0.024477855252934583, 0.010207778995219957, \n0.008532295265944583, 0.040242532118754906, 0.005181587450423221, 0.0082073\n91370854009, 0.02741148713822125, 0.006623087410725917, 0.00836770271463416\n2, 0.009668643362989908, 0.007171783607096945, 0.016985615150215773, 0.0070\n80691453323701, 0.008297044496975403, 0.0037359000700039487, 0.011142755810\n390478, 0.010256554277897088, 0.09528238587772694, 0.04369806970660494, 0.1\n3308101804159636, 0.02665693577960761, 0.030479170124456504, 0.069201498658\n71575]","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"mse = mean((predictive.p - p).^2);\nprintln(\"MSE between estimated and true data generation probabilities: $mse\")","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"MSE between estimated and true data generation probabilities: 2.98495795366\n75624e-7","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"@model function multinomial_regression(obs, N, X, ϕ, ξβ, Wβ)\n    β ~ MvNormalWeightedMeanPrecision(ξβ, Wβ)\n    for i in eachindex(obs)\n        Ψ[i] := ϕ(X[i])*β\n        obs[i] ~ MultinomialPolya(N, Ψ[i]) where {dependencies = RequireMessageFunctionalDependencies(ψ = MvNormalWeightedMeanPrecision(zeros(length(obs[i])-1), diageye(length(obs[i])-1)))}\n    end\nend","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"function generate_regression_data(rng=StableRNG(123);ϕ = identity,N = 3, k=5, nsamples = 1000)\n    β = randn(rng, k)\n    X = randn(rng, nsamples, k, k)\n    X = [X[i,:,:] for i in 1:size(X,1)];\n    Ψ = ϕ.(X)\n    p = map(x -> logistic_stick_breaking(x*β), Ψ)\n    return map(x -> rand(rng, Multinomial(N, x)), p), X, β, p\nend","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"generate_regression_data (generic function with 2 methods)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"ϕ = x -> sin(x)\nobs_regression, X_regression, β_regression, p_regression = generate_regression_data(;nsamples = 5000, ϕ = ϕ);","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"reg_results = infer(  \n    model = multinomial_regression(N = 3, ϕ = ϕ, ξβ = zeros(5), Wβ = rand(Wishart(5, diageye(5)))),\n    data = (obs=obs_regression,X = X_regression ),\n    iterations = 20,\n    free_energy = true,\n    showprogress = true,\n    returnvars = KeepLast(),\n    options = (\n        limit_stack_depth = 100,\n    ) \n)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Inference results:\n  Posteriors       | available for (Ψ, β)\n  Free Energy:     | Real[11949.4, 11582.8, 11500.3, 11479.2, 11473.5, 1147\n1.9, 11471.5, 11471.4, 11471.3, 11471.3, 11471.3, 11471.3, 11471.3, 11471.3\n, 11471.3, 11471.3, 11471.3, 11471.3, 11471.3, 11471.3]","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"println(\"estimated β: with mean and covariance: $(mean_cov(reg_results.posteriors[:β]))\")\nprintln(\"true β: $(β_regression)\")","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"estimated β: with mean and covariance: ([-0.1144092413087327, 0.66349791558\n68727, -1.2548462408421799, -0.08411743180486715, -0.08041448211760642], [0\n.0001479134617400009 -2.2254992794110347e-6 3.5326895793662317e-6 -1.636271\n6267793924e-6 3.2368276753396337e-6; -2.2254992794110347e-6 0.0001517406001\n5081057 -1.934396255761039e-5 -1.8052859239320158e-7 1.302935356075217e-6; \n3.5326895793662317e-6 -1.934396255761039e-5 0.00017983156037404286 4.302103\n577558996e-6 5.863091423296406e-7; -1.6362716267793924e-6 -1.80528592393201\n58e-7 4.302103577558996e-6 0.0001401108971022439 3.2303464025294458e-6; 3.2\n368276753396337e-6 1.302935356075217e-6 5.863091423296406e-7 3.230346402529\n4458e-6 0.00013947656440207074])\ntrue β: [-0.12683768965424458, 0.6668851724871252, -1.2566124895590247, -0.\n08499562516549662, -0.094274004848194]","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"plot(reg_results.free_energy,\ntitle=\"Free Energy Over Iterations\",\nxlabel=\"Iteration\",\nylabel=\"Free Energy\",\nlinewidth=2,\nlegend=false,\ngrid=true,)","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"mse_β =  mean((mean(reg_results.posteriors[:β]) - β_regression).^2)\nprintln(\"MSE of β estimate: $mse_β\")","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"MSE of β estimate: 7.238341320268984e-5","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"We can visualize how the logistic stick-breaking transformation of the simplex coordinates of the regression coefficients affects the prior distribution of the regression coefficients and vice versa since the logistic stick-breaking transformation is invertible.","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"\n# Previous helper functions remain the same\nσ(x) = 1 / (1 + exp(-x))\nσ_inv(x) = log(x / (1 - x))\n\nfunction jacobian_det(π)\n    K = length(π)\n    det = 1.0\n    for k in 1:(K-1)\n        num = 1 - sum(π[1:(k-1)])\n        den = π[k] * (1 - sum(π[1:k]))\n        det *= num / den\n    end\n    return det\nend\n\nfunction ψ_to_π(ψ::Vector{Float64})\n    K = length(ψ) + 1\n    π = zeros(K)\n    for k in 1:(K-1)\n        π[k] = σ(ψ[k]) * (1 - sum(π[1:(k-1)]))\n    end\n    π[K] = 1 - sum(π[1:(K-1)])\n    return π\nend\n\nfunction π_to_ψ(π)\n    K = length(π)\n    ψ = zeros(K-1)\n    ψ[1] = σ_inv(π[1])\n    for k in 2:(K-1)\n        ψ[k] = σ_inv(π[k] / (1 - sum(π[1:(k-1)])))\n    end\n    return ψ\nend\n\n# Function to compute density in simplex coordinates\nfunction compute_simplex_density(x::Float64, y::Float64, Σ::Matrix{Float64})\n    # Check if point is inside triangle\n    if y < 0 || y > 1 || x < 0 || x > 1 || (x + y) > 1\n        return 0.0\n    end\n    \n    # Convert from simplex coordinates to π\n    π1 = x\n    π2 = y\n    π3 = 1 - x - y\n    \n    try\n        ψ = π_to_ψ([π1, π2, π3])\n        # Compute Gaussian density\n        dist = MvNormal(zeros(2), Σ)\n        return pdf(dist, ψ) * abs(jacobian_det([π1, π2, π3]))\n    catch\n        return 0.0\n    end\n   \nend\n\nfunction plot_transformed_densities()\n    # Create three different covariance matrices\n    ###For higher variances values needs scaling for proper visualization.\n    σ² = 1.0\n    Σ_corr = [σ² 0.9σ²; 0.9σ² σ²]\n    Σ_anticorr = [σ² -0.9σ²; -0.9σ² σ²]\n    Σ_uncorr = [σ² 0.0; 0.0 σ²]\n    \n    # Plot Gaussian densities\n    ψ1, ψ2 = range(-4sqrt(σ²), 4sqrt(σ²), length=500), range(-4sqrt(σ²), 4sqrt(σ²), length=100)\n    \n    p1 = contour(ψ1, ψ2, (x,y) -> pdf(MvNormal(zeros(2), Σ_corr), [x,y]),\n                 title=\"Correlated Prior\", xlabel=\"ψ₁\", ylabel=\"ψ₂\")\n    p2 = contour(ψ1, ψ2, (x,y) -> pdf(MvNormal(zeros(2), Σ_anticorr), [x,y]),\n                 title=\"Anti-correlated Prior\", xlabel=\"ψ₁\", ylabel=\"ψ₂\")\n    p3 = contour(ψ1, ψ2, (x,y) -> pdf(MvNormal(zeros(2), Σ_uncorr), [x,y]),\n                 title=\"Uncorrelated Prior\", xlabel=\"ψ₁\", ylabel=\"ψ₂\")\n    \n    # Plot simplex densities\n    n_points = 500\n    x = range(0, 1, length=n_points)\n    y = range(0, 1, length=n_points)\n    \n    # Plot simplices\n    p4 = contour(x, y, (x,y) -> compute_simplex_density(x, y, Σ_corr),\n                 title=\"Correlated Simplex\")\n    \n    # Add simplex boundaries and median lines\n    plot!(p4, [0,1,0,0], [0,0,1,0], color=:black, label=\"\")  # Triangle boundaries\n    \n    p5 = contour(x, y, (x,y) -> compute_simplex_density(x, y, Σ_anticorr),\n                 title=\"Anti-correlated Simplex\")\n    plot!(p5, [0,1,0,0], [0,0,1,0], color=:black, label=\"\")\n    \n    p6 = contour(x, y, (x,y) -> compute_simplex_density(x, y, Σ_uncorr),\n                 title=\"Uncorrelated Simplex\")\n    plot!(p6, [0,1,0,0], [0,0,1,0], color=:black, label=\"\")\n    \n    # Combine all plots\n    plot(p1, p2, p3, p4, p5, p6, layout=(2,3), size=(900,600))\nend\n\n# Generate the plots\nplot_transformed_densities()","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/bayesian_multinomial_regression/","page":"Bayesian Multinomial Regression","title":"Bayesian Multinomial Regression","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/bayesian_multinomial_regression/Project.toml`\n  [31c24e10] Distributions v0.25.117\n  [62312e5e] ExponentialFamily v2.0.1\n  [91a5bcdd] Plots v1.40.9\n  [86711068] RxInfer v4.2.0\n  [860ef19b] StableRNGs v1.0.2\n  [f3b207a7] StatsPlots v0.15.7\n","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"","category":"page"},{"location":"categories/basic_examples/pomdp_control/#POMDP-Control-with-Reactive-Inference","page":"Pomdp Control","title":"POMDP Control with Reactive Inference","text":"","category":"section"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"This notebook demonstrates how to perform control in Partially Observable Markov Decision Processes (POMDPs) using reactive message passing and variational inference in RxInfer.jl.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"We will cover:","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Setting up a simple POMDP model\nDefining the state transition and observation models\nImplementing the control policy\nPerforming inference and control using message passing\nVisualizing the results","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"using RxInfer\nusing Distributions\nusing Plots\nusing Random\nusing ProgressMeter","category":"page"},{"location":"categories/basic_examples/pomdp_control/#Environment-Setup","page":"Pomdp Control","title":"Environment Setup","text":"","category":"section"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"For this example, we will implement the Windy Gridworld environment using RxEnvironments.jl. The Windy Gridworld is a simple gridworld environment with deterministic transitions and observations. This code is adapted from the RxEnvironments.jl documentation, and a more elaborate explanation of can be found there. ","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"The environment consists of:","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"A grid with wind values for each column\nAn agent with a current position\nA goal position to reach","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"The agent can:","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Move in cardinal directions (one step at a time)\nObserve its current position\nBe affected by wind when moving","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"The wind effect is applied after each movement, potentially pushing the agent upward by 0-2 positions depending on the column.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"First we will define the environment and the agent.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"using RxEnvironments\nusing Plots\n\nstruct WindyGridWorld{N}\n    wind::NTuple{N,Int}\n    agents::Vector\n    goal::Tuple{Int,Int}\nend\n\nmutable struct WindyGridWorldAgent\n    position::Tuple{Int,Int}\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"\nRxEnvironments.update!(env::WindyGridWorld, dt) = nothing # The environment has no \"internal\" updating process over time\n\nfunction RxEnvironments.receive!(env::WindyGridWorld{N}, agent::WindyGridWorldAgent, action::Tuple{Int,Int}) where {N}\n    if action[1] != 0\n        @assert action[2] == 0 \"Only one of the two actions can be non-zero\"\n    elseif action[2] != 0\n        @assert action[1] == 0 \"Only one of the two actions can be non-zero\"\n    end\n    new_position = (agent.position[1] + action[1], agent.position[2] + action[2] + env.wind[agent.position[1]])\n    if all(elem -> 0 < elem < N, new_position)\n        agent.position = new_position\n    end\nend\n\nfunction RxEnvironments.what_to_send(env::WindyGridWorld, agent::WindyGridWorldAgent)\n    return agent.position\nend\n\nfunction RxEnvironments.what_to_send(agent::WindyGridWorldAgent, env::WindyGridWorld)\n    return agent.position\nend\n\nfunction RxEnvironments.add_to_state!(env::WindyGridWorld, agent::WindyGridWorldAgent)\n    push!(env.agents, agent)\nend\n\nfunction reset_env!(environment::RxEnvironments.RxEntity{<:WindyGridWorld,T,S,A}) where {T,S,A}\n    env = environment.decorated\n    for agent in env.agents\n        agent.position = (1, 1)\n    end\n    for subscriber in RxEnvironments.subscribers(environment)\n        send!(subscriber, environment, (1, 1))\n    end\nend\n\nfunction plot_environment(environment::RxEnvironments.RxEntity{<:WindyGridWorld,T,S,A}) where {T,S,A}\n    env = environment.decorated\n    p1 = scatter([env.goal[1]], [env.goal[2]], color=:blue, label=\"Goal\", xlims=(0, 6), ylims=(0, 6))\n    for agent in env.agents\n        p1 = scatter!([agent.position[1]], [agent.position[2]], color=:red, label=\"Agent\")\n    end\n    return p1\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"plot_environment (generic function with 1 method)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"env = RxEnvironment(WindyGridWorld((0, 1, 1, 1, 0), [], (4, 3)))\nagent = add!(env, WindyGridWorldAgent((1, 1)))\nplot_environment(env)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/pomdp_control/#Model-Setup","page":"Pomdp Control","title":"Model Setup","text":"","category":"section"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"First, we'll define our POMDP model structure. We will use the DiscreteTransition node in RxInfer to define the state transition model. The DiscreteTransition node is a special node that accepts any number of Categorical distributions as input, and outputs a Categorical distribution. This means that we can use it to define a state transition model that accepts the previous state and the control as Categorical random variables, but we can also use it to define our observation model! Furthermore, the DiscreteTransition node can be used both for parameter inference and for inference-as-planning, isn't that neat?","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"@model function pomdp_model(p_A, p_B, p_goal, p_control, previous_control, p_previous_state, current_y, future_y, T, m_A, m_B)\n    # Instantiate all model parameters with priors\n    A ~ p_A\n    B ~ p_B\n    previous_state ~ p_previous_state\n    \n    # Paremeter inference\n    current_state ~ DiscreteTransition(previous_state, B, previous_control)\n    current_y ~ DiscreteTransition(current_state, A)\n\n    prev_state = current_state\n    # Inference-as-planning\n    for t in 1:T\n        controls[t] ~ p_control\n        s[t] ~ DiscreteTransition(prev_state, m_B, controls[t])\n        future_y[t] ~ DiscreteTransition(s[t], m_A)\n        prev_state = s[t]\n    end\n    # Goal prior initialization\n    s[end] ~ p_goal\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Now, this model, because we use A and B for every timestep, contains loops, so we have to initialize the inference procedure properly. Furthermore, RxInfer does not support learning a joint probability distribution over the parameters and the states, so we have to supply the model with variational constraints that reflect this:","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"init = @initialization begin\n    q(A) = DirichletCollection(diageye(25) .+ 0.1)\n    q(B) = DirichletCollection(ones(25, 25, 4))\nend\n\nconstraints = @constraints begin\n    q(previous_state, previous_control, current_state, B) = q(previous_state, previous_control, current_state)q(B)\n    q(current_state, current_y, A) = q(current_state, current_y)q(A)\n    q(current_state, s, controls, B) = q(current_state, s, controls), q(B)\n    q(s, future_y, A) = q(s, future_y), q(A)\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Constraints: \n  q(previous_state, previous_control, current_state, B) = q(previous_state,\n previous_control, current_state)q(B)\n  q(current_state, current_y, A) = q(current_state, current_y)q(A)\n  q(current_state, s, controls, B) = q(current_state, s, controls)q(B)\n  q(s, future_y, A) = q(s, future_y)q(A)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Now, in order to use this model, we have to define the priors for the model parameters. The WindyGridworld environment has a 5-by-5 grid, so we need to instantiate a prior 25-by-25 transition matrices for every control! That's quite a lot of parameters, but as we will see, RxInfer will handle this just fine. We will give our agent a control space of 4 actions, so we need to instantiate 4 transition matrices. Furthermore, we have to transform the output from the environment to a 1-in-25 index, and the controls from a 1-in-4 index to a direction tuple.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"The prior on our observation model tells our model that the prior belief is to trust it's observations, but we might be able to deviate from this. However, in this example, the observation model is deterministic and has no noise, meaning that our agent won't have any reason to deviate from the prior.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"p_A = DirichletCollection(diageye(25) .+ 0.1)\np_B = DirichletCollection(ones(25, 25, 4))\n\nfunction grid_location_to_index(pos::Tuple{Int, Int})\n    return (pos[2] - 1) * 5 + pos[1]\nend\n\nfunction index_to_grid_location(index::Int)\n    return (index % 5, index ÷ 5 + 1,)\nend\n\nfunction index_to_one_hot(index::Int)\n    return [i == index ? 1.0 : 0.0 for i in 1:25]\nend\n\ngoal = Categorical(index_to_one_hot(grid_location_to_index((4, 3))))","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Distributions.Categorical{Float64, Vector{Float64}}(\nsupport: Base.OneTo(25)\np: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0\n, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"RxEnvironments.jl is a package that allows us to easily communicate between our agent and our environment. We can send actions to the environment, and the environment will automatically respond with the corresponding observations. In order to access these in our model, we can subscribe to the observations and then use the data function to access the last observation.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Now for our main control loop, we will use a receding horizon control strategy. We will first take an action, observe the environment, and then update our belief. We will then repeat this process for a horizon of 10 steps. In order to learn the parameters of our model, we will conduct this experiment 100 times. We can use the infer function from RxInfer to perform inference on our model.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"# Number of times to run the experiment\nn_experiments = 100\n# Number of steps in each experiment\nT = 4\nobservations = keep(Any)\n# Subscribe the agent to receive observations\nRxEnvironments.subscribe_to_observations!(agent, observations)\nsuccesses = []\n\n\n@showprogress for i in 1:n_experiments\n    # Reset environment to initial state and initialize state belief to starting position (1,1)\n    reset_env!(env)\n    p_s = Categorical(index_to_one_hot(grid_location_to_index((1, 1))))\n    # Initialize previous action as \"down\", as this is neutral from the starting position\n    policy = [Categorical([0.0, 0.0, 1.0, 0.0])]\n    prev_u = [0.0, 0.0, 1.0, 0.0]\n    # Run for T-1 steps in each experiment\n    for t in 1:T\n\n         # Convert policy to actual movement in environment\n         current_action = mode(first(policy))\n         if current_action == 1\n             send!(env, agent, (0, 1))  # Move up \n             prev_u = [1.0, 0.0, 0.0, 0.0]\n         elseif current_action == 2\n             send!(env, agent, (1, 0))  # Move right\n             prev_u = [0.0, 1.0, 0.0, 0.0]\n         elseif current_action == 3\n             send!(env, agent, (0, -1))  # Move down\n             prev_u = [0.0, 0.0, 1.0, 0.0]\n         elseif current_action == 4\n             send!(env, agent, (-1, 0))  # Move left\n             prev_u = [0.0, 0.0, 0.0, 1.0]\n         end\n\n        # Get last observation and convert to one-hot encoding\n        last_observation = index_to_one_hot(grid_location_to_index(RxEnvironments.data(last(observations))))\n        \n        # Perform inference using the POMDP model\n        inference_result = infer(\n            model = pomdp_model(\n                p_A = p_A,  # prior on observation model parameters\n                p_B = p_B,  # prior on transition model parameters\n                T = max(T - t, 1),  # remaining time steps\n                p_previous_state = p_s,  # posterior belief on previous state\n                p_goal = goal,  # prior on goal state\n                p_control = vague(Categorical, 4),  # prior over controls\n                m_A = mean(p_A),\n                m_B = mean(p_B)\n            ),\n            # Provide data for inference\n            data = (\n                previous_control = UnfactorizedData(prev_u),\n                current_y = UnfactorizedData(last_observation),\n                future_y = UnfactorizedData(fill(missing, max(T - t, 1)))\n            ),\n            constraints = constraints,\n            initialization = init,\n            iterations = 10\n        )\n        \n        # Update beliefs based on inference results\n        p_s = last(inference_result.posteriors[:current_state])  # Update state belief\n        policy = last(inference_result.posteriors[:controls])  # Get policy\n\n        # Update model parameters globally for the entire notebook\n        global p_A = last(inference_result.posteriors[:A])  # Update observation model\n        global p_B = last(inference_result.posteriors[:B])  # Update transition model\n\n        if RxEnvironments.data(last(observations)) == (4, 3)\n            break\n        end\n    end\n    if RxEnvironments.data(last(observations)) == (4, 3)\n        push!(successes, true)\n    else\n        push!(successes, false)\n    end\nend","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Now, in this example, we have used a trick: we supplied the mean of p_A and p_B to the model to do the predictions for the future in order to learn the controls. The real reason we did this is because we do not want messages from the future to influence the model parameters, instead only learning the model parameters from past data. This is a simple way to do this, but it is not the only way. We could have supplied the full distribution p_A and p_B to the model, and used A and B in the predictive step as well, but then we would need a separate way to make sure we do not use future messages to influence the model parameters.","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"mean(successes)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"0.85","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"We see that our agent is able to learn the optimal policy for this environment, and reaches the goal state in 85% of cases!","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"plot_environment(env)","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"(Image: )","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"note: Contributing\nThis example was automatically generated from a Jupyter notebook in the RxInferExamples.jl repository.We welcome and encourage contributions! You can help by:Improving this example\nCreating new examples \nReporting issues or bugs\nSuggesting enhancementsVisit our GitHub repository to get started. Together we can make RxInfer.jl even better! 💪","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"compat: Environment\nThis example was executed in a clean, isolated environment. Below are the exact package versions used:For reproducibility:Use the same package versions when running locally\nReport any issues with package compatibility","category":"page"},{"location":"categories/basic_examples/pomdp_control/","page":"Pomdp Control","title":"Pomdp Control","text":"Status `~/work/RxInferExamples.jl/RxInferExamples.jl/docs/src/categories/basic_examples/pomdp_control/Project.toml`\n  [31c24e10] Distributions v0.25.117\n  [91a5bcdd] Plots v1.40.9\n  [92933f4c] ProgressMeter v1.10.2\n  [5ea003d0] RxEnvironments v0.2.15\n  [86711068] RxInfer v4.2.0\n","category":"page"}]
}
